#!/usr/bin/env python3
"""
ç³»çµ±æ€§èƒ½åˆ†æå’Œæˆæœ¬æ§åˆ¶å·¥å…·
Phase 6: ç³»çµ±èª¿å„ªèˆ‡æˆæœ¬æ§åˆ¶åˆ†æ
"""

import json
import psutil
import time
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import argparse


class SystemPerformanceAnalyzer:
    """ç³»çµ±æ€§èƒ½åˆ†æå™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.metrics_history = []

    def collect_system_metrics(self) -> Dict:
        """æ”¶é›†ç³»çµ±æŒ‡æ¨™"""
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu": {
                "usage_percent": psutil.cpu_percent(interval=1),
                "count": psutil.cpu_count(),
                "count_logical": psutil.cpu_count(logical=True),
                "freq": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None
            },
            "memory": {
                "total": psutil.virtual_memory().total,
                "available": psutil.virtual_memory().available,
                "used": psutil.virtual_memory().used,
                "usage_percent": psutil.virtual_memory().percent,
                "swap_total": psutil.swap_memory().total,
                "swap_used": psutil.swap_memory().used,
                "swap_percent": psutil.swap_memory().percent
            },
            "disk": {
                path: {
                    "total": usage.total,
                    "used": usage.used,
                    "free": usage.free,
                    "usage_percent": usage.percent
                }
                for path in ["/", "/tmp"] if Path(path).exists()
                for usage in [psutil.disk_usage(path)]
            },
            "network": self._get_network_io(),
            "processes": self._get_process_info()
        }

    def _get_network_io(self) -> Dict:
        """ç²å–ç¶²è·¯ I/O çµ±è¨ˆ"""
        net_io = psutil.net_io_counters()
        return {
            "bytes_sent": net_io.bytes_sent,
            "bytes_recv": net_io.bytes_recv,
            "packets_sent": net_io.packets_sent,
            "packets_recv": net_io.packets_recv,
            "errin": net_io.errin,
            "errout": net_io.errout,
            "dropin": net_io.dropin,
            "dropout": net_io.dropout
        }

    def _get_process_info(self) -> Dict:
        """ç²å–é€²ç¨‹è³‡è¨Š"""
        processes = []
        docker_processes = []

        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent', 'cmdline']):
            try:
                pinfo = proc.info
                if pinfo['name'] and pinfo['cpu_percent'] is not None:
                    processes.append({
                        "pid": pinfo['pid'],
                        "name": pinfo['name'],
                        "cpu_percent": pinfo['cpu_percent'],
                        "memory_percent": pinfo['memory_percent'],
                        "cmdline": ' '.join(pinfo['cmdline'][:3]) if pinfo['cmdline'] else ""
                    })

                    # è­˜åˆ¥ Docker ç›¸é—œé€²ç¨‹
                    if any(keyword in pinfo['name'].lower()
                          for keyword in ['docker', 'containerd', 'dockerd']):
                        docker_processes.append(pinfo)

            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue

        # æŒ‰ CPU ä½¿ç”¨ç‡æ’åº
        processes.sort(key=lambda x: x['cpu_percent'], reverse=True)

        return {
            "total_processes": len(processes),
            "top_cpu_processes": processes[:10],
            "docker_processes": len(docker_processes)
        }

    def analyze_docker_containers(self) -> Dict:
        """åˆ†æ Docker å®¹å™¨æ€§èƒ½"""
        try:
            # ç²å–å®¹å™¨çµ±è¨ˆ
            result = subprocess.run([
                "docker", "stats", "--no-stream", "--format",
                "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}\t{{.NetIO}}\t{{.BlockIO}}"
            ], capture_output=True, text=True, check=True)

            containers = []
            lines = result.stdout.strip().split('\n')[1:]  # è·³éæ¨™é¡Œè¡Œ

            for line in lines:
                if line.strip():
                    parts = line.split('\t')
                    if len(parts) >= 6:
                        containers.append({
                            "name": parts[0],
                            "cpu_percent": parts[1],
                            "memory_usage": parts[2],
                            "memory_percent": parts[3],
                            "network_io": parts[4],
                            "block_io": parts[5]
                        })

            return {
                "containers": containers,
                "total_containers": len(containers)
            }

        except subprocess.CalledProcessError:
            return {"error": "Failed to get container stats"}

    def analyze_database_performance(self) -> Dict:
        """åˆ†æè³‡æ–™åº«æ€§èƒ½ (å¦‚æœå¯ç”¨)"""
        analysis = {
            "postgresql": {"available": False},
            "redis": {"available": False}
        }

        # æª¢æŸ¥ PostgreSQL
        try:
            result = subprocess.run([
                "psql", "-h", "localhost", "-U", "postgres", "-c",
                "SELECT version(), current_database(), current_user;"
            ], capture_output=True, text=True, timeout=5)

            if result.returncode == 0:
                analysis["postgresql"] = {
                    "available": True,
                    "version_info": result.stdout.strip()
                }
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired):
            pass

        # æª¢æŸ¥ Redis
        try:
            result = subprocess.run([
                "redis-cli", "ping"
            ], capture_output=True, text=True, timeout=5)

            if result.returncode == 0 and "PONG" in result.stdout:
                # ç²å– Redis è³‡è¨Š
                info_result = subprocess.run([
                    "redis-cli", "info", "memory"
                ], capture_output=True, text=True, timeout=5)

                analysis["redis"] = {
                    "available": True,
                    "ping_response": result.stdout.strip(),
                    "memory_info": info_result.stdout.strip() if info_result.returncode == 0 else None
                }
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired):
            pass

        return analysis

    def calculate_cost_metrics(self, metrics: Dict) -> Dict:
        """è¨ˆç®—æˆæœ¬æŒ‡æ¨™"""
        # åŸºæ–¼è³‡æºä½¿ç”¨ç‡çš„æˆæœ¬ä¼°ç®— (å‡è¨­å€¼)
        cpu_cost_per_hour = 0.05  # æ¯ CPU æ ¸å¿ƒæ¯å°æ™‚æˆæœ¬
        memory_cost_per_gb_hour = 0.01  # æ¯ GB è¨˜æ†¶é«”æ¯å°æ™‚æˆæœ¬
        storage_cost_per_gb_month = 0.10  # æ¯ GB å­˜å„²æ¯æœˆæˆæœ¬

        cpu_cores = metrics["cpu"]["count"]
        memory_gb = metrics["memory"]["total"] / (1024**3)

        # è¨ˆç®—ç£ç¢Ÿä½¿ç”¨é‡
        total_disk_gb = 0
        for disk_info in metrics["disk"].values():
            total_disk_gb += disk_info["used"] / (1024**3)

        hourly_cost = (
            cpu_cores * cpu_cost_per_hour +
            memory_gb * memory_cost_per_gb_hour
        )

        monthly_cost = (
            hourly_cost * 24 * 30 +
            total_disk_gb * storage_cost_per_gb_month
        )

        return {
            "hourly_cost_usd": round(hourly_cost, 4),
            "daily_cost_usd": round(hourly_cost * 24, 2),
            "monthly_cost_usd": round(monthly_cost, 2),
            "resource_breakdown": {
                "cpu_cores": cpu_cores,
                "memory_gb": round(memory_gb, 2),
                "storage_gb": round(total_disk_gb, 2)
            }
        }

    def generate_optimization_recommendations(self, metrics: Dict) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []

        # CPU å„ªåŒ–å»ºè­°
        cpu_usage = metrics["cpu"]["usage_percent"]
        if cpu_usage > 80:
            recommendations.append("ğŸ”¥ CPU ä½¿ç”¨ç‡éé«˜ (>80%)ï¼Œè€ƒæ…®å‚ç›´æ“´å±•æˆ–å„ªåŒ–æ‡‰ç”¨ç¨‹å¼")
        elif cpu_usage < 20:
            recommendations.append("ğŸ’¡ CPU ä½¿ç”¨ç‡è¼ƒä½ (<20%)ï¼Œå¯èƒ½å¯ä»¥æ¸›å°‘è³‡æºé…ç½®")

        # è¨˜æ†¶é«”å„ªåŒ–å»ºè­°
        memory_usage = metrics["memory"]["usage_percent"]
        if memory_usage > 85:
            recommendations.append("ğŸ”¥ è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜ (>85%)ï¼Œè€ƒæ…®å¢åŠ è¨˜æ†¶é«”æˆ–å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨")
        elif memory_usage < 30:
            recommendations.append("ğŸ’¡ è¨˜æ†¶é«”ä½¿ç”¨ç‡è¼ƒä½ (<30%)ï¼Œå¯ä»¥è€ƒæ…®æ¸›å°‘è¨˜æ†¶é«”é…ç½®")

        # ç£ç¢Ÿå„ªåŒ–å»ºè­°
        for path, disk_info in metrics["disk"].items():
            if disk_info["usage_percent"] > 90:
                recommendations.append(f"ğŸ”¥ ç£ç¢Ÿç©ºé–“ä¸è¶³ {path} (>{disk_info['usage_percent']:.1f}%)ï¼Œéœ€è¦æ¸…ç†æˆ–æ“´å®¹")
            elif disk_info["usage_percent"] > 80:
                recommendations.append(f"âš ï¸ ç£ç¢Ÿä½¿ç”¨ç‡è¼ƒé«˜ {path} ({disk_info['usage_percent']:.1f}%)ï¼Œå»ºè­°ç›£æ§")

        # é€²ç¨‹å„ªåŒ–å»ºè­°
        top_processes = metrics["processes"]["top_cpu_processes"][:5]
        high_cpu_processes = [p for p in top_processes if p["cpu_percent"] > 50]

        if high_cpu_processes:
            process_names = [p["name"] for p in high_cpu_processes]
            recommendations.append(f"ğŸ” ç™¼ç¾é«˜ CPU ä½¿ç”¨é€²ç¨‹: {', '.join(process_names)}ï¼Œå»ºè­°æª¢æŸ¥")

        # é€šç”¨å»ºè­°
        recommendations.extend([
            "ğŸ“Š å®šæœŸç›£æ§ç³»çµ±è³‡æºä½¿ç”¨ç‡",
            "ğŸ”„ å¯¦æ–½è‡ªå‹•åŒ–è³‡æºæ“´å±•ç­–ç•¥",
            "ğŸ“ˆ ä½¿ç”¨ Prometheus + Grafana é€²è¡ŒæŒçºŒç›£æ§",
            "ğŸ’° å®šæœŸæª¢æŸ¥æˆæœ¬æ•ˆç›Šä¸¦èª¿æ•´è³‡æºé…ç½®"
        ])

        return recommendations

    def run_performance_test(self, duration_seconds: int = 60) -> Dict:
        """é‹è¡Œæ€§èƒ½æ¸¬è©¦"""
        print(f"ğŸƒ é–‹å§‹ {duration_seconds} ç§’æ€§èƒ½æ¸¬è©¦...")

        start_time = time.time()
        samples = []

        while time.time() - start_time < duration_seconds:
            sample = {
                "timestamp": time.time(),
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent,
                "disk_io": psutil.disk_io_counters()._asdict(),
                "network_io": psutil.net_io_counters()._asdict()
            }
            samples.append(sample)
            time.sleep(5)  # æ¯ 5 ç§’æ”¶é›†ä¸€æ¬¡

        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        cpu_values = [s["cpu_percent"] for s in samples]
        memory_values = [s["memory_percent"] for s in samples]

        return {
            "duration_seconds": duration_seconds,
            "samples_count": len(samples),
            "cpu_stats": {
                "avg": sum(cpu_values) / len(cpu_values),
                "max": max(cpu_values),
                "min": min(cpu_values)
            },
            "memory_stats": {
                "avg": sum(memory_values) / len(memory_values),
                "max": max(memory_values),
                "min": min(memory_values)
            },
            "samples": samples
        }

    def generate_report(self, metrics: Dict, docker_stats: Dict,
                       db_analysis: Dict, cost_metrics: Dict,
                       recommendations: List[str]) -> str:
        """ç”Ÿæˆæ€§èƒ½åˆ†æå ±å‘Š"""
        report = [
            "# ç³»çµ±æ€§èƒ½åˆ†æå ±å‘Š",
            f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## ç³»çµ±è³‡æºæ¦‚æ³",
            f"- **CPU**: {metrics['cpu']['count']} æ ¸å¿ƒ, ä½¿ç”¨ç‡ {metrics['cpu']['usage_percent']:.1f}%",
            f"- **è¨˜æ†¶é«”**: {metrics['memory']['usage_percent']:.1f}% ä½¿ç”¨ "
            f"({self._format_bytes(metrics['memory']['used'])}/{self._format_bytes(metrics['memory']['total'])})",
            ""
        ]

        # ç£ç¢Ÿä½¿ç”¨æƒ…æ³
        report.append("### ç£ç¢Ÿä½¿ç”¨æƒ…æ³")
        for path, disk_info in metrics["disk"].items():
            report.append(
                f"- **{path}**: {disk_info['usage_percent']:.1f}% ä½¿ç”¨ "
                f"({self._format_bytes(disk_info['used'])}/{self._format_bytes(disk_info['total'])})"
            )
        report.append("")

        # å‰ 5 å€‹ CPU ä½¿ç”¨æœ€é«˜çš„é€²ç¨‹
        report.extend([
            "### Top 5 CPU ä½¿ç”¨é€²ç¨‹",
            "| é€²ç¨‹å | PID | CPU% | è¨˜æ†¶é«”% |",
            "|--------|-----|------|---------|"
        ])

        for proc in metrics["processes"]["top_cpu_processes"][:5]:
            report.append(
                f"| {proc['name']} | {proc['pid']} | {proc['cpu_percent']:.1f}% | {proc['memory_percent']:.1f}% |"
            )
        report.append("")

        # Docker å®¹å™¨çµ±è¨ˆ
        if docker_stats.get("containers"):
            report.extend([
                "## Docker å®¹å™¨æ€§èƒ½",
                f"é‹è¡Œä¸­çš„å®¹å™¨: {docker_stats['total_containers']}",
                "",
                "| å®¹å™¨å | CPU% | è¨˜æ†¶é«”ä½¿ç”¨ | è¨˜æ†¶é«”% |",
                "|--------|------|------------|---------|"
            ])

            for container in docker_stats["containers"][:10]:
                report.append(
                    f"| {container['name']} | {container['cpu_percent']} | "
                    f"{container['memory_usage']} | {container['memory_percent']} |"
                )
            report.append("")

        # è³‡æ–™åº«ç‹€æ…‹
        report.extend([
            "## è³‡æ–™åº«ç‹€æ…‹",
            f"- **PostgreSQL**: {'âœ… å¯ç”¨' if db_analysis['postgresql']['available'] else 'âŒ ä¸å¯ç”¨'}",
            f"- **Redis**: {'âœ… å¯ç”¨' if db_analysis['redis']['available'] else 'âŒ ä¸å¯ç”¨'}",
            ""
        ])

        # æˆæœ¬åˆ†æ
        report.extend([
            "## æˆæœ¬åˆ†æ (ä¼°ç®—)",
            f"- **æ¯å°æ™‚æˆæœ¬**: ${cost_metrics['hourly_cost_usd']}",
            f"- **æ¯æ—¥æˆæœ¬**: ${cost_metrics['daily_cost_usd']}",
            f"- **æ¯æœˆæˆæœ¬**: ${cost_metrics['monthly_cost_usd']}",
            "",
            "### è³‡æºé…ç½®",
            f"- **CPU æ ¸å¿ƒ**: {cost_metrics['resource_breakdown']['cpu_cores']}",
            f"- **è¨˜æ†¶é«”**: {cost_metrics['resource_breakdown']['memory_gb']} GB",
            f"- **å­˜å„²**: {cost_metrics['resource_breakdown']['storage_gb']} GB",
            ""
        ])

        # å„ªåŒ–å»ºè­°
        report.extend([
            "## å„ªåŒ–å»ºè­°",
            ""
        ])

        for i, recommendation in enumerate(recommendations, 1):
            report.append(f"{i}. {recommendation}")

        report.extend([
            "",
            "## ç›£æ§å»ºè­°",
            "- è¨­ç½®è³‡æºä½¿ç”¨ç‡å‘Šè­¦ (CPU > 80%, è¨˜æ†¶é«” > 85%, ç£ç¢Ÿ > 90%)",
            "- å¯¦æ–½è‡ªå‹•åŒ–æ€§èƒ½ç›£æ§",
            "- å®šæœŸåŸ·è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦",
            "- å»ºç«‹è³‡æºä½¿ç”¨è¶¨å‹¢åˆ†æ"
        ])

        return "\n".join(report)

    def _format_bytes(self, bytes_value: int) -> str:
        """æ ¼å¼åŒ–ä½å…ƒçµ„é¡¯ç¤º"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if bytes_value < 1024.0:
                return f"{bytes_value:.1f}{unit}"
            bytes_value /= 1024.0
        return f"{bytes_value:.1f}PB"


def main():
    parser = argparse.ArgumentParser(description="ç³»çµ±æ€§èƒ½åˆ†æå·¥å…·")
    parser.add_argument("--output", help="è¼¸å‡ºå ±å‘Šæ–‡ä»¶è·¯å¾‘", default="performance-report.md")
    parser.add_argument("--test", help="åŸ·è¡Œæ€§èƒ½æ¸¬è©¦ (ç§’)", type=int, default=0)
    parser.add_argument("--json", help="åŒæ™‚è¼¸å‡º JSON æ ¼å¼", action="store_true")

    args = parser.parse_args()

    project_root = Path(__file__).parent.parent
    analyzer = SystemPerformanceAnalyzer(project_root)

    print("ğŸ“Š ç³»çµ±æ€§èƒ½åˆ†æå·¥å…·")
    print("=" * 50)

    # æ”¶é›†åŸºæœ¬æŒ‡æ¨™
    print("ğŸ” æ”¶é›†ç³»çµ±æŒ‡æ¨™...")
    metrics = analyzer.collect_system_metrics()

    print("ğŸ³ åˆ†æ Docker å®¹å™¨...")
    docker_stats = analyzer.analyze_docker_containers()

    print("ğŸ—„ï¸ æª¢æŸ¥è³‡æ–™åº«ç‹€æ…‹...")
    db_analysis = analyzer.analyze_database_performance()

    print("ğŸ’° è¨ˆç®—æˆæœ¬æŒ‡æ¨™...")
    cost_metrics = analyzer.calculate_cost_metrics(metrics)

    print("ğŸ’¡ ç”Ÿæˆå„ªåŒ–å»ºè­°...")
    recommendations = analyzer.generate_optimization_recommendations(metrics)

    # åŸ·è¡Œæ€§èƒ½æ¸¬è©¦ (å¦‚æœæŒ‡å®š)
    performance_test = None
    if args.test > 0:
        performance_test = analyzer.run_performance_test(args.test)

    # ç”Ÿæˆå ±å‘Š
    print("ğŸ“ ç”Ÿæˆå ±å‘Š...")
    report = analyzer.generate_report(
        metrics, docker_stats, db_analysis, cost_metrics, recommendations
    )

    # è¼¸å‡ºå ±å‘Š
    output_path = Path(args.output)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"ğŸ“Š å ±å‘Šå·²ç”Ÿæˆ: {output_path}")

    # è¼¸å‡º JSON æ ¼å¼ (å¦‚æœè«‹æ±‚)
    if args.json:
        json_data = {
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics,
            "docker_stats": docker_stats,
            "database_analysis": db_analysis,
            "cost_metrics": cost_metrics,
            "recommendations": recommendations,
            "performance_test": performance_test
        }

        json_path = output_path.with_suffix('.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ JSON æ•¸æ“šå·²è¼¸å‡º: {json_path}")

    print("\nâœ… åˆ†æå®Œæˆ")
    return 0


if __name__ == "__main__":
    sys.exit(main())
