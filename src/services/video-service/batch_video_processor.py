#!/usr/bin/env python3
"""
æ‰¹é‡è¦–é »è™•ç†å¼•æ“
æ”¯æŒå¤§è¦æ¨¡è¦–é »è™•ç†ã€éšŠåˆ—ç®¡ç†ã€ä¸¦è¡Œè™•ç†ã€é€²åº¦è¿½è¹¤ç­‰åŠŸèƒ½
"""

import os
import logging
import asyncio
import aiofiles
import json
import hashlib
from typing import Dict, List, Optional, Callable, Any, Union
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from enum import Enum
import concurrent.futures
import threading
import time
import uuid
from queue import Queue, Empty
import pickle

try:
    from moviepy.editor import VideoClip
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False

# å°å…¥æˆ‘å€‘çš„é«˜ç´šè¦–é »å¼•æ“
try:
    from .advanced_video_engine import AdvancedVideoEngine, VideoConfig
    from .video_effects_system import VideoEffectsSystem, EffectConfig
    from .audio_video_sync import AudioVideoSyncEngine, SyncConfig
    CUSTOM_ENGINES_AVAILABLE = True
except ImportError:
    CUSTOM_ENGINES_AVAILABLE = False
    logging.warning("Custom video engines not available for batch processing")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class JobStatus(Enum):
    """ä½œæ¥­ç‹€æ…‹"""
    PENDING = "pending"
    QUEUED = "queued" 
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    PAUSED = "paused"

class Priority(Enum):
    """å„ªå…ˆç´š"""
    LOW = 0
    NORMAL = 1
    HIGH = 2
    URGENT = 3

@dataclass
class VideoJob:
    """è¦–é »è™•ç†ä½œæ¥­"""
    job_id: str
    job_type: str  # "professional", "effects", "sync", "custom"
    input_data: Dict[str, Any]
    output_config: Dict[str, Any]
    priority: Priority = Priority.NORMAL
    status: JobStatus = JobStatus.PENDING
    created_at: datetime = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    progress: float = 0.0
    error_message: Optional[str] = None
    result: Optional[Dict[str, Any]] = None
    retry_count: int = 0
    max_retries: int = 3
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

@dataclass
class BatchConfig:
    """æ‰¹é‡è™•ç†é…ç½®"""
    max_concurrent_jobs: int = 3
    max_queue_size: int = 100
    job_timeout: int = 3600  # 1å°æ™‚
    retry_delay: int = 60  # é‡è©¦å»¶é²
    save_intermediate: bool = True
    cleanup_completed: bool = False
    priority_queue: bool = True
    load_balancing: bool = True

@dataclass
class ProcessingStats:
    """è™•ç†çµ±è¨ˆ"""
    total_jobs: int = 0
    completed_jobs: int = 0
    failed_jobs: int = 0
    cancelled_jobs: int = 0
    processing_jobs: int = 0
    queued_jobs: int = 0
    average_processing_time: float = 0.0
    total_processing_time: float = 0.0
    start_time: datetime = None
    
    def __post_init__(self):
        if self.start_time is None:
            self.start_time = datetime.now()

class BatchVideoProcessor:
    """æ‰¹é‡è¦–é »è™•ç†å¼•æ“"""
    
    def __init__(
        self, 
        config: BatchConfig = None,
        storage_dir: str = "./batch_processing",
        cache_dir: str = "./cache/batch"
    ):
        self.config = config or BatchConfig()
        self.storage_dir = Path(storage_dir)
        self.cache_dir = Path(cache_dir)
        
        # å‰µå»ºç›®éŒ„
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½œæ¥­ç®¡ç†
        self.jobs: Dict[str, VideoJob] = {}
        self.job_queue = asyncio.Queue(maxsize=self.config.max_queue_size)
        self.processing_jobs: Dict[str, asyncio.Task] = {}
        self.stats = ProcessingStats()
        
        # è™•ç†å¼•æ“
        self.video_engine = None
        self.effects_system = None
        self.sync_engine = None
        
        # æ§åˆ¶ç‹€æ…‹
        self.is_running = False
        self.workers: List[asyncio.Task] = []
        self.shutdown_event = asyncio.Event()
        
        # é–å’ŒåŒæ­¥
        self.jobs_lock = asyncio.Lock()
        self.stats_lock = asyncio.Lock()
        
        # æŒä¹…åŒ–
        self.jobs_file = self.storage_dir / "jobs.json"
        self.stats_file = self.storage_dir / "stats.json"
        
        self._setup_logging()
        
    def _setup_logging(self):
        """è¨­ç½®æ—¥èªŒ"""
        log_file = self.storage_dir / "batch_processing.log"
        handler = logging.FileHandler(log_file)
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
    async def initialize_engines(self):
        """åˆå§‹åŒ–è™•ç†å¼•æ“"""
        if not CUSTOM_ENGINES_AVAILABLE:
            logger.warning("Custom video engines not available")
            return
            
        try:
            self.video_engine = AdvancedVideoEngine()
            self.effects_system = VideoEffectsSystem()
            self.sync_engine = AudioVideoSyncEngine()
            logger.info("Video processing engines initialized")
        except Exception as e:
            logger.error(f"Failed to initialize engines: {e}")
            
    async def start(self):
        """å•Ÿå‹•æ‰¹é‡è™•ç†å™¨"""
        if self.is_running:
            logger.warning("Batch processor is already running")
            return
            
        logger.info("Starting batch video processor")
        self.is_running = True
        self.shutdown_event.clear()
        
        # åˆå§‹åŒ–å¼•æ“
        await self.initialize_engines()
        
        # è¼‰å…¥æŒä¹…åŒ–æ•¸æ“š
        await self._load_persistent_data()
        
        # å•Ÿå‹•å·¥ä½œç·šç¨‹
        self.workers = []
        for i in range(self.config.max_concurrent_jobs):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)
            
        # å•Ÿå‹•ç›£æ§ä»»å‹™
        monitor_task = asyncio.create_task(self._monitor_jobs())
        self.workers.append(monitor_task)
        
        logger.info(f"Batch processor started with {len(self.workers)} workers")
        
    async def stop(self):
        """åœæ­¢æ‰¹é‡è™•ç†å™¨"""
        if not self.is_running:
            return
            
        logger.info("Stopping batch video processor")
        self.is_running = False
        self.shutdown_event.set()
        
        # å–æ¶ˆæ‰€æœ‰å·¥ä½œä»»å‹™
        for worker in self.workers:
            worker.cancel()
            
        # ç­‰å¾…å·¥ä½œä»»å‹™çµæŸ
        try:
            await asyncio.gather(*self.workers, return_exceptions=True)
        except Exception as e:
            logger.error(f"Error stopping workers: {e}")
            
        # ä¿å­˜æŒä¹…åŒ–æ•¸æ“š
        await self._save_persistent_data()
        
        logger.info("Batch processor stopped")
        
    async def submit_job(
        self, 
        job_type: str,
        input_data: Dict[str, Any],
        output_config: Dict[str, Any] = None,
        priority: Priority = Priority.NORMAL
    ) -> str:
        """æäº¤ä½œæ¥­"""
        
        if not self.is_running:
            raise RuntimeError("Batch processor is not running")
            
        # å‰µå»ºä½œæ¥­
        job_id = str(uuid.uuid4())
        job = VideoJob(
            job_id=job_id,
            job_type=job_type,
            input_data=input_data,
            output_config=output_config or {},
            priority=priority
        )
        
        async with self.jobs_lock:
            self.jobs[job_id] = job
            
        # æ·»åŠ åˆ°éšŠåˆ—
        try:
            await self.job_queue.put(job)
            job.status = JobStatus.QUEUED
            
            async with self.stats_lock:
                self.stats.total_jobs += 1
                self.stats.queued_jobs += 1
                
            logger.info(f"Job {job_id} submitted ({job_type})")
            
        except asyncio.QueueFull:
            job.status = JobStatus.FAILED
            job.error_message = "Queue is full"
            logger.error(f"Failed to queue job {job_id}: Queue is full")
            
        return job_id
        
    async def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """ç²å–ä½œæ¥­ç‹€æ…‹"""
        async with self.jobs_lock:
            job = self.jobs.get(job_id)
            if job:
                return {
                    "job_id": job.job_id,
                    "job_type": job.job_type,
                    "status": job.status.value,
                    "progress": job.progress,
                    "created_at": job.created_at.isoformat(),
                    "started_at": job.started_at.isoformat() if job.started_at else None,
                    "completed_at": job.completed_at.isoformat() if job.completed_at else None,
                    "error_message": job.error_message,
                    "result": job.result
                }
        return None
        
    async def cancel_job(self, job_id: str) -> bool:
        """å–æ¶ˆä½œæ¥­"""
        async with self.jobs_lock:
            job = self.jobs.get(job_id)
            if not job:
                return False
                
            if job.status in [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.CANCELLED]:
                return False
                
            job.status = JobStatus.CANCELLED
            
            # å¦‚æœæ­£åœ¨è™•ç†ï¼Œå–æ¶ˆè™•ç†ä»»å‹™
            if job_id in self.processing_jobs:
                task = self.processing_jobs[job_id]
                task.cancel()
                
        logger.info(f"Job {job_id} cancelled")
        return True
        
    async def get_batch_stats(self) -> Dict[str, Any]:
        """ç²å–æ‰¹é‡è™•ç†çµ±è¨ˆ"""
        async with self.stats_lock:
            stats_dict = asdict(self.stats)
            stats_dict['start_time'] = self.stats.start_time.isoformat()
            
            # è¨ˆç®—é‹è¡Œæ™‚é–“
            runtime = datetime.now() - self.stats.start_time
            stats_dict['runtime_seconds'] = runtime.total_seconds()
            
            # è¨ˆç®—è™•ç†é€Ÿç‡
            if self.stats.completed_jobs > 0 and runtime.total_seconds() > 0:
                stats_dict['jobs_per_minute'] = (
                    self.stats.completed_jobs / (runtime.total_seconds() / 60)
                )
            else:
                stats_dict['jobs_per_minute'] = 0.0
                
            return stats_dict
            
    async def _worker(self, worker_name: str):
        """å·¥ä½œç·šç¨‹"""
        logger.info(f"Worker {worker_name} started")
        
        while self.is_running and not self.shutdown_event.is_set():
            try:
                # ç²å–ä½œæ¥­ï¼ˆå¸¶è¶…æ™‚ï¼‰
                try:
                    job = await asyncio.wait_for(
                        self.job_queue.get(), 
                        timeout=1.0
                    )
                except asyncio.TimeoutError:
                    continue
                    
                # æª¢æŸ¥ä½œæ¥­æ˜¯å¦å·²è¢«å–æ¶ˆ
                if job.status == JobStatus.CANCELLED:
                    self.job_queue.task_done()
                    continue
                    
                # é–‹å§‹è™•ç†ä½œæ¥­
                await self._process_job(job, worker_name)
                self.job_queue.task_done()
                
            except asyncio.CancelledError:
                logger.info(f"Worker {worker_name} cancelled")
                break
            except Exception as e:
                logger.error(f"Worker {worker_name} error: {e}")
                
        logger.info(f"Worker {worker_name} stopped")
        
    async def _process_job(self, job: VideoJob, worker_name: str):
        """è™•ç†å–®å€‹ä½œæ¥­"""
        
        job.status = JobStatus.PROCESSING
        job.started_at = datetime.now()
        
        async with self.stats_lock:
            self.stats.processing_jobs += 1
            self.stats.queued_jobs -= 1
            
        logger.info(f"Worker {worker_name} processing job {job.job_id} ({job.job_type})")
        
        # å‰µå»ºè™•ç†ä»»å‹™
        processing_task = asyncio.create_task(
            self._execute_job(job)
        )
        
        # è¨˜éŒ„è™•ç†ä»»å‹™
        self.processing_jobs[job.job_id] = processing_task
        
        try:
            # ç­‰å¾…è™•ç†å®Œæˆï¼ˆå¸¶è¶…æ™‚ï¼‰
            result = await asyncio.wait_for(
                processing_task,
                timeout=self.config.job_timeout
            )
            
            # è™•ç†æˆåŠŸ
            job.status = JobStatus.COMPLETED
            job.completed_at = datetime.now()
            job.result = result
            job.progress = 1.0
            
            processing_time = (job.completed_at - job.started_at).total_seconds()
            
            async with self.stats_lock:
                self.stats.completed_jobs += 1
                self.stats.processing_jobs -= 1
                self.stats.total_processing_time += processing_time
                self.stats.average_processing_time = (
                    self.stats.total_processing_time / self.stats.completed_jobs
                )
                
            logger.info(f"Job {job.job_id} completed in {processing_time:.2f}s")
            
        except asyncio.TimeoutError:
            job.status = JobStatus.FAILED
            job.error_message = f"Job timed out after {self.config.job_timeout}s"
            logger.error(f"Job {job.job_id} timed out")
            
        except asyncio.CancelledError:
            job.status = JobStatus.CANCELLED
            logger.info(f"Job {job.job_id} was cancelled")
            
        except Exception as e:
            job.status = JobStatus.FAILED
            job.error_message = str(e)
            logger.error(f"Job {job.job_id} failed: {e}")
            
            # é‡è©¦é‚è¼¯
            if job.retry_count < job.max_retries:
                job.retry_count += 1
                job.status = JobStatus.PENDING
                logger.info(f"Retrying job {job.job_id} (attempt {job.retry_count})")
                
                # å»¶é²å¾Œé‡æ–°åŠ å…¥éšŠåˆ—
                await asyncio.sleep(self.config.retry_delay)
                await self.job_queue.put(job)
            else:
                async with self.stats_lock:
                    self.stats.failed_jobs += 1
                    self.stats.processing_jobs -= 1
                    
        finally:
            # æ¸…ç†è™•ç†ä»»å‹™è¨˜éŒ„
            if job.job_id in self.processing_jobs:
                del self.processing_jobs[job.job_id]
                
    async def _execute_job(self, job: VideoJob) -> Dict[str, Any]:
        """åŸ·è¡Œå…·é«”çš„ä½œæ¥­è™•ç†"""
        
        if job.job_type == "professional" and self.video_engine:
            return await self._process_professional_video(job)
        elif job.job_type == "effects" and self.effects_system:
            return await self._process_video_effects(job)
        elif job.job_type == "sync" and self.sync_engine:
            return await self._process_audio_sync(job)
        elif job.job_type == "custom":
            return await self._process_custom_job(job)
        else:
            raise ValueError(f"Unknown job type: {job.job_type}")
            
    async def _process_professional_video(self, job: VideoJob) -> Dict[str, Any]:
        """è™•ç†å°ˆæ¥­è¦–é »ç”Ÿæˆä½œæ¥­"""
        
        scenes = job.input_data.get('scenes', [])
        audio_file = job.input_data.get('audio_file')
        title = job.input_data.get('title', 'Batch Generated Video')
        style = job.input_data.get('style', 'cinematic')
        config = job.input_data.get('config')
        
        if config:
            video_config = VideoConfig(**config)
        else:
            video_config = None
            
        result = await self.video_engine.create_professional_video(
            scenes=scenes,
            audio_file=audio_file,
            title=title,
            style=style,
            config=video_config
        )
        
        return result
        
    async def _process_video_effects(self, job: VideoJob) -> Dict[str, Any]:
        """è™•ç†è¦–é »ç‰¹æ•ˆä½œæ¥­"""
        # é€™è£¡éœ€è¦å¯¦ç¾ç‰¹æ•ˆè™•ç†é‚è¼¯
        # ç”±æ–¼æ™‚é–“é—œä¿‚ï¼Œè¿”å›æ¨¡æ“¬çµæœ
        await asyncio.sleep(2)  # æ¨¡æ“¬è™•ç†æ™‚é–“
        return {"success": True, "message": "Effects applied"}
        
    async def _process_audio_sync(self, job: VideoJob) -> Dict[str, Any]:
        """è™•ç†éŸ³è¦–é »åŒæ­¥ä½œæ¥­"""
        # é€™è£¡éœ€è¦å¯¦ç¾åŒæ­¥è™•ç†é‚è¼¯
        await asyncio.sleep(3)  # æ¨¡æ“¬è™•ç†æ™‚é–“
        return {"success": True, "message": "Audio synced"}
        
    async def _process_custom_job(self, job: VideoJob) -> Dict[str, Any]:
        """è™•ç†è‡ªå®šç¾©ä½œæ¥­"""
        # åŸ·è¡Œè‡ªå®šç¾©è™•ç†å‡½æ•¸
        custom_func = job.input_data.get('custom_function')
        if custom_func and callable(custom_func):
            return await custom_func(job.input_data)
        else:
            raise ValueError("No valid custom function provided")
            
    async def _monitor_jobs(self):
        """ç›£æ§ä½œæ¥­ç‹€æ…‹"""
        logger.info("Job monitor started")
        
        while self.is_running and not self.shutdown_event.is_set():
            try:
                # å®šæœŸä¿å­˜ç‹€æ…‹
                await self._save_persistent_data()
                
                # æ¸…ç†å®Œæˆçš„ä½œæ¥­ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
                if self.config.cleanup_completed:
                    await self._cleanup_completed_jobs()
                    
                # ç­‰å¾…ä¸‹ä¸€æ¬¡æª¢æŸ¥
                await asyncio.sleep(30)  # 30ç§’æª¢æŸ¥ä¸€æ¬¡
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Monitor error: {e}")
                
        logger.info("Job monitor stopped")
        
    async def _cleanup_completed_jobs(self):
        """æ¸…ç†å·²å®Œæˆçš„ä½œæ¥­"""
        cutoff_time = datetime.now() - timedelta(hours=24)  # æ¸…ç†24å°æ™‚å‰çš„ä½œæ¥­
        
        async with self.jobs_lock:
            jobs_to_remove = []
            
            for job_id, job in self.jobs.items():
                if (job.status in [JobStatus.COMPLETED, JobStatus.FAILED] and
                    job.completed_at and job.completed_at < cutoff_time):
                    jobs_to_remove.append(job_id)
                    
            for job_id in jobs_to_remove:
                del self.jobs[job_id]
                
            if jobs_to_remove:
                logger.info(f"Cleaned up {len(jobs_to_remove)} old jobs")
                
    async def _save_persistent_data(self):
        """ä¿å­˜æŒä¹…åŒ–æ•¸æ“š"""
        try:
            # ä¿å­˜ä½œæ¥­æ•¸æ“š
            jobs_data = {}
            async with self.jobs_lock:
                for job_id, job in self.jobs.items():
                    jobs_data[job_id] = {
                        **asdict(job),
                        'created_at': job.created_at.isoformat(),
                        'started_at': job.started_at.isoformat() if job.started_at else None,
                        'completed_at': job.completed_at.isoformat() if job.completed_at else None,
                        'status': job.status.value,
                        'priority': job.priority.value
                    }
                    
            async with aiofiles.open(self.jobs_file, 'w') as f:
                await f.write(json.dumps(jobs_data, indent=2))
                
            # ä¿å­˜çµ±è¨ˆæ•¸æ“š
            stats_data = await self.get_batch_stats()
            async with aiofiles.open(self.stats_file, 'w') as f:
                await f.write(json.dumps(stats_data, indent=2))
                
        except Exception as e:
            logger.error(f"Failed to save persistent data: {e}")
            
    async def _load_persistent_data(self):
        """è¼‰å…¥æŒä¹…åŒ–æ•¸æ“š"""
        try:
            # è¼‰å…¥ä½œæ¥­æ•¸æ“š
            if self.jobs_file.exists():
                async with aiofiles.open(self.jobs_file, 'r') as f:
                    content = await f.read()
                    jobs_data = json.loads(content)
                    
                async with self.jobs_lock:
                    for job_id, job_dict in jobs_data.items():
                        # é‡å»ºä½œæ¥­å°è±¡
                        job = VideoJob(
                            job_id=job_dict['job_id'],
                            job_type=job_dict['job_type'],
                            input_data=job_dict['input_data'],
                            output_config=job_dict['output_config'],
                            priority=Priority(job_dict['priority']),
                            status=JobStatus(job_dict['status']),
                            created_at=datetime.fromisoformat(job_dict['created_at']),
                            started_at=datetime.fromisoformat(job_dict['started_at']) if job_dict['started_at'] else None,
                            completed_at=datetime.fromisoformat(job_dict['completed_at']) if job_dict['completed_at'] else None,
                            progress=job_dict.get('progress', 0.0),
                            error_message=job_dict.get('error_message'),
                            result=job_dict.get('result'),
                            retry_count=job_dict.get('retry_count', 0),
                            max_retries=job_dict.get('max_retries', 3)
                        )
                        self.jobs[job_id] = job
                        
                logger.info(f"Loaded {len(self.jobs)} jobs from persistent storage")
                
        except Exception as e:
            logger.error(f"Failed to load persistent data: {e}")


# ä¾¿æ·å‡½æ•¸
async def create_batch_processor(
    max_concurrent_jobs: int = 3,
    storage_dir: str = "./batch_processing"
) -> BatchVideoProcessor:
    """å‰µå»ºæ‰¹é‡è™•ç†å™¨"""
    
    config = BatchConfig(max_concurrent_jobs=max_concurrent_jobs)
    processor = BatchVideoProcessor(config=config, storage_dir=storage_dir)
    await processor.start()
    return processor

async def test_batch_processor():
    """æ¸¬è©¦æ‰¹é‡è™•ç†å™¨"""
    
    print("ğŸ­ Testing Batch Video Processor...")
    
    # å‰µå»ºè™•ç†å™¨
    processor = await create_batch_processor(max_concurrent_jobs=2)
    
    try:
        # æäº¤æ¸¬è©¦ä½œæ¥­
        job_ids = []
        
        for i in range(3):
            job_id = await processor.submit_job(
                job_type="custom",
                input_data={
                    "custom_function": lambda data: {"test": f"job_{i}_completed"},
                    "job_number": i
                },
                priority=Priority.NORMAL
            )
            job_ids.append(job_id)
            print(f"Submitted job {i}: {job_id}")
            
        # ç­‰å¾…ä¸€äº›è™•ç†æ™‚é–“
        await asyncio.sleep(5)
        
        # æª¢æŸ¥ä½œæ¥­ç‹€æ…‹
        for job_id in job_ids:
            status = await processor.get_job_status(job_id)
            if status:
                print(f"Job {job_id}: {status['status']} (progress: {status['progress']})")
                
        # ç²å–çµ±è¨ˆä¿¡æ¯
        stats = await processor.get_batch_stats()
        print(f"Batch stats: {stats}")
        
        print("âœ… Batch processor test completed!")
        
    finally:
        # åœæ­¢è™•ç†å™¨
        await processor.stop()
        
    return processor

if __name__ == "__main__":
    print("ğŸ¬ Batch Video Processor - Large Scale Video Processing")
    print("=" * 60)
    
    # æª¢æŸ¥ä¾è³´
    print(f"MoviePy available: {MOVIEPY_AVAILABLE}")
    print(f"Custom engines available: {CUSTOM_ENGINES_AVAILABLE}")
    
    # é‹è¡Œæ¸¬è©¦
    result = asyncio.run(test_batch_processor())
    print(f"Batch processor test successful: {result is not None}")