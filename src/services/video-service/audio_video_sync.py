#!/usr/bin/env python3
"""
æ™ºèƒ½éŸ³è¦–é »åŒæ­¥ç³»çµ±
æä¾›ç²¾ç¢ºçš„éŸ³è¦–é »åŒæ­¥ã€ç¯€æ‹æª¢æ¸¬ã€å‹•æ…‹å°é½Šç­‰åŠŸèƒ½
"""

import os
import logging
import numpy as np
import math
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import asyncio
from pathlib import Path

try:
    from moviepy.editor import (
        VideoClip, AudioClip, AudioFileClip, CompositeAudioClip,
        concatenate_audioclips
    )
    from moviepy.audio.fx import volumex, audio_fadein, audio_fadeout, afx
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False

try:
    import librosa
    import soundfile as sf
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False
    logging.warning("librosa not available. Advanced audio analysis disabled.")

try:
    from scipy import signal
    from scipy.ndimage import gaussian_filter1d
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    logging.warning("scipy not available. Some audio processing features disabled.")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AudioAnalysis:
    """éŸ³é »åˆ†æçµæœ"""
    duration: float
    tempo: Optional[float]
    beats: Optional[List[float]]
    onset_frames: Optional[List[int]]
    onset_times: Optional[List[float]]
    spectral_centroid: Optional[np.ndarray]
    rms_energy: Optional[np.ndarray]
    zero_crossing_rate: Optional[np.ndarray]
    chroma: Optional[np.ndarray]
    sample_rate: int

@dataclass
class SyncPoint:
    """åŒæ­¥é»"""
    video_time: float
    audio_time: float
    importance: float  # 0-1, é‡è¦ç¨‹åº¦
    sync_type: str  # 'beat', 'onset', 'manual', 'automatic'

@dataclass
class SyncConfig:
    """åŒæ­¥é…ç½®"""
    sync_method: str = "automatic"  # automatic, manual, beat_sync, onset_sync
    tolerance: float = 0.1  # åŒæ­¥å®¹å¿åº¦ï¼ˆç§’ï¼‰
    fade_duration: float = 0.5
    normalize_audio: bool = True
    remove_silence: bool = False
    tempo_adjustment: Optional[float] = None  # é€Ÿåº¦èª¿æ•´æ¯”ä¾‹

class AudioVideoSyncEngine:
    """æ™ºèƒ½éŸ³è¦–é »åŒæ­¥å¼•æ“"""
    
    def __init__(self, cache_dir: str = "./cache/audio"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.audio_cache = {}
        
    async def analyze_audio(self, audio_path: str) -> AudioAnalysis:
        """åˆ†æéŸ³é »ç‰¹å¾µ"""
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")
            
        cache_key = f"{audio_path}_{os.path.getmtime(audio_path)}"
        if cache_key in self.audio_cache:
            return self.audio_cache[cache_key]
            
        logger.info(f"Analyzing audio: {audio_path}")
        
        try:
            if LIBROSA_AVAILABLE:
                # ä½¿ç”¨librosaé€²è¡Œé«˜ç´šéŸ³é »åˆ†æ
                y, sr = librosa.load(audio_path)
                duration = len(y) / sr
                
                # ç¯€æ‹æª¢æ¸¬
                tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
                beat_times = librosa.times_like(beats)
                
                # éŸ³ç¬¦é–‹å§‹é»æª¢æ¸¬
                onset_frames = librosa.onset.onset_detect(
                    y=y, sr=sr, units='frames'
                )
                onset_times = librosa.frames_to_time(onset_frames, sr=sr)
                
                # é »è­œè³ªå¿ƒ
                spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
                
                # RMSèƒ½é‡
                rms_energy = librosa.feature.rms(y=y)[0]
                
                # éé›¶ç‡
                zero_crossing_rate = librosa.feature.zero_crossing_rate(y)[0]
                
                # è‰²åº¦ç‰¹å¾µ
                chroma = librosa.feature.chroma_stft(y=y, sr=sr)
                
                analysis = AudioAnalysis(
                    duration=duration,
                    tempo=float(tempo),
                    beats=beat_times.tolist(),
                    onset_frames=onset_frames.tolist(),
                    onset_times=onset_times.tolist(),
                    spectral_centroid=spectral_centroid,
                    rms_energy=rms_energy,
                    zero_crossing_rate=zero_crossing_rate,
                    chroma=chroma,
                    sample_rate=sr
                )
                
            else:
                # åŸºç¤åˆ†æï¼ˆä½¿ç”¨MoviePyï¼‰
                audio = AudioFileClip(audio_path)
                duration = audio.duration
                sample_rate = audio.fps
                
                analysis = AudioAnalysis(
                    duration=duration,
                    tempo=None,
                    beats=None,
                    onset_frames=None,
                    onset_times=None,
                    spectral_centroid=None,
                    rms_energy=None,
                    zero_crossing_rate=None,
                    chroma=None,
                    sample_rate=sample_rate
                )
                
                audio.close()
                
            self.audio_cache[cache_key] = analysis
            logger.info(f"Audio analysis complete: duration={analysis.duration:.2f}s, "
                       f"tempo={analysis.tempo}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"Audio analysis failed: {e}")
            raise
            
    async def detect_sync_points(
        self, 
        video_duration: float,
        audio_analysis: AudioAnalysis,
        sync_config: SyncConfig
    ) -> List[SyncPoint]:
        """æª¢æ¸¬åŒæ­¥é»"""
        
        sync_points = []
        
        if sync_config.sync_method == "beat_sync" and audio_analysis.beats:
            # åŸºæ–¼ç¯€æ‹çš„åŒæ­¥
            beats = audio_analysis.beats
            video_beats = np.linspace(0, video_duration, len(beats))
            
            for i, (video_time, audio_time) in enumerate(zip(video_beats, beats)):
                importance = 1.0 if i % 4 == 0 else 0.5  # å¼·æ‹æ›´é‡è¦
                sync_points.append(SyncPoint(
                    video_time=video_time,
                    audio_time=audio_time,
                    importance=importance,
                    sync_type="beat"
                ))
                
        elif sync_config.sync_method == "onset_sync" and audio_analysis.onset_times:
            # åŸºæ–¼éŸ³ç¬¦é–‹å§‹çš„åŒæ­¥
            onsets = audio_analysis.onset_times
            video_onsets = np.linspace(0, video_duration, len(onsets))
            
            for video_time, audio_time in zip(video_onsets, onsets):
                sync_points.append(SyncPoint(
                    video_time=video_time,
                    audio_time=audio_time,
                    importance=0.8,
                    sync_type="onset"
                ))
                
        elif sync_config.sync_method == "automatic":
            # è‡ªå‹•æª¢æ¸¬åŒæ­¥é»
            sync_points = await self._detect_automatic_sync_points(
                video_duration, audio_analysis
            )
            
        else:
            # åŸºç¤ç·šæ€§åŒæ­¥
            num_points = max(5, int(min(video_duration, audio_analysis.duration)))
            for i in range(num_points):
                progress = i / (num_points - 1) if num_points > 1 else 0
                sync_points.append(SyncPoint(
                    video_time=progress * video_duration,
                    audio_time=progress * audio_analysis.duration,
                    importance=1.0,
                    sync_type="automatic"
                ))
                
        logger.info(f"Detected {len(sync_points)} sync points using {sync_config.sync_method}")
        return sync_points
        
    async def _detect_automatic_sync_points(
        self, 
        video_duration: float,
        audio_analysis: AudioAnalysis
    ) -> List[SyncPoint]:
        """è‡ªå‹•æª¢æ¸¬åŒæ­¥é»"""
        
        sync_points = []
        
        # çµåˆå¤šç¨®ç‰¹å¾µæª¢æ¸¬é‡è¦æ™‚åˆ»
        if (audio_analysis.rms_energy is not None and 
            audio_analysis.spectral_centroid is not None):
            
            # èƒ½é‡å³°å€¼æª¢æ¸¬
            energy = audio_analysis.rms_energy
            if SCIPY_AVAILABLE:
                # å¹³æ»‘èƒ½é‡æ›²ç·š
                energy_smooth = gaussian_filter1d(energy, sigma=2)
                # æª¢æ¸¬å³°å€¼
                peaks, _ = signal.find_peaks(energy_smooth, height=np.mean(energy_smooth))
                
                # è½‰æ›ç‚ºæ™‚é–“
                time_per_frame = audio_analysis.duration / len(energy)
                peak_times = peaks * time_per_frame
                
                # æ˜ å°„åˆ°è¦–é »æ™‚é–“ç·š
                for peak_time in peak_times:
                    if peak_time <= audio_analysis.duration:
                        video_time = (peak_time / audio_analysis.duration) * video_duration
                        sync_points.append(SyncPoint(
                            video_time=video_time,
                            audio_time=peak_time,
                            importance=0.7,
                            sync_type="automatic"
                        ))
        
        # å¦‚æœæ²’æœ‰ç‰¹æ®ŠåŒæ­¥é»ï¼Œå‰µå»ºå‡å‹»åˆ†ä½ˆçš„åŒæ­¥é»
        if len(sync_points) < 3:
            num_points = 5
            for i in range(num_points):
                progress = i / (num_points - 1) if num_points > 1 else 0
                sync_points.append(SyncPoint(
                    video_time=progress * video_duration,
                    audio_time=progress * audio_analysis.duration,
                    importance=0.5,
                    sync_type="automatic"
                ))
                
        return sync_points
        
    async def sync_audio_to_video(
        self,
        video: VideoClip,
        audio_path: str,
        sync_config: SyncConfig
    ) -> VideoClip:
        """å°‡éŸ³é »åŒæ­¥åˆ°è¦–é »"""
        
        if not MOVIEPY_AVAILABLE:
            logger.error("MoviePy not available for audio-video sync")
            return video
            
        logger.info("Starting audio-video synchronization")
        
        try:
            # åˆ†æéŸ³é »
            audio_analysis = await self.analyze_audio(audio_path)
            
            # æª¢æ¸¬åŒæ­¥é»
            sync_points = await self.detect_sync_points(
                video.duration, audio_analysis, sync_config
            )
            
            # è™•ç†éŸ³é »
            audio = AudioFileClip(audio_path)
            
            # æ¨™æº–åŒ–éŸ³é »
            if sync_config.normalize_audio:
                audio = audio.fx(afx.normalize)
                
            # ç§»é™¤éœéŸ³ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if sync_config.remove_silence:
                audio = await self._remove_silence(audio)
                
            # èª¿æ•´éŸ³é »æ™‚é•·å’Œç¯€å¥
            synced_audio = await self._apply_sync_adjustments(
                audio, video.duration, sync_points, sync_config
            )
            
            # æ·»åŠ æ·¡å…¥æ·¡å‡º
            if sync_config.fade_duration > 0:
                synced_audio = synced_audio.fx(
                    audio_fadein, sync_config.fade_duration
                ).fx(
                    audio_fadeout, sync_config.fade_duration
                )
            
            # å°‡åŒæ­¥å¾Œçš„éŸ³é »é™„åŠ åˆ°è¦–é »
            result_video = video.set_audio(synced_audio)
            
            logger.info("Audio-video synchronization completed")
            return result_video
            
        except Exception as e:
            logger.error(f"Audio-video sync failed: {e}")
            # å›é€€ï¼šç°¡å–®éŸ³é »é™„åŠ 
            try:
                audio = AudioFileClip(audio_path)
                if audio.duration > video.duration:
                    audio = audio.subclip(0, video.duration)
                elif audio.duration < video.duration:
                    # å¾ªç’°æˆ–å»¶é•·éŸ³é »
                    repeats = int(video.duration / audio.duration) + 1
                    audio = CompositeAudioClip([audio] * repeats).subclip(0, video.duration)
                
                return video.set_audio(audio)
            except Exception as fallback_error:
                logger.error(f"Fallback audio sync failed: {fallback_error}")
                return video
                
    async def _apply_sync_adjustments(
        self,
        audio: AudioClip,
        target_duration: float,
        sync_points: List[SyncPoint],
        config: SyncConfig
    ) -> AudioClip:
        """æ‡‰ç”¨åŒæ­¥èª¿æ•´"""
        
        if config.tempo_adjustment and config.tempo_adjustment != 1.0:
            # èª¿æ•´éŸ³é »é€Ÿåº¦
            try:
                # ä½¿ç”¨time stretchingèª¿æ•´é€Ÿåº¦è€Œä¸æ”¹è®ŠéŸ³èª¿
                audio = audio.fx(afx.speedx, config.tempo_adjustment)
            except Exception as e:
                logger.warning(f"Tempo adjustment failed: {e}")
        
        # èª¿æ•´éŸ³é »é•·åº¦åŒ¹é…è¦–é »
        if abs(audio.duration - target_duration) > config.tolerance:
            if audio.duration > target_duration:
                # è£å‰ªéŸ³é »
                audio = audio.subclip(0, target_duration)
            else:
                # å»¶é•·éŸ³é »
                if target_duration / audio.duration <= 2:
                    # å¦‚æœä¸éœ€è¦å¤ªå¤šé‡è¤‡ï¼Œç›´æ¥å¾ªç’°
                    repeats = int(target_duration / audio.duration) + 1
                    extended_audio = CompositeAudioClip([audio] * repeats)
                    audio = extended_audio.subclip(0, target_duration)
                else:
                    # å¦‚æœå·®ç•°å¤ªå¤§ï¼Œæ·»åŠ éœéŸ³
                    silence_duration = target_duration - audio.duration
                    from moviepy.audio.AudioClip import AudioArrayClip
                    silence = AudioArrayClip(
                        np.zeros((int(silence_duration * audio.fps), 2)), 
                        fps=audio.fps
                    )
                    audio = concatenate_audioclips([audio, silence])
        
        return audio
        
    async def _remove_silence(self, audio: AudioClip) -> AudioClip:
        """ç§»é™¤éŸ³é »ä¸­çš„éœéŸ³ç‰‡æ®µ"""
        
        try:
            if LIBROSA_AVAILABLE:
                # ä½¿ç”¨librosaæª¢æ¸¬éœéŸ³
                audio_array = audio.to_soundarray()
                if len(audio_array.shape) > 1:
                    # ç«‹é«”è²è½‰å–®è²é“
                    audio_array = np.mean(audio_array, axis=1)
                
                # æª¢æ¸¬ééœéŸ³ç‰‡æ®µ
                non_silent_intervals = librosa.effects.split(
                    audio_array, top_db=20, frame_length=2048, hop_length=512
                )
                
                if len(non_silent_intervals) > 0:
                    # é‡æ–°çµ„åˆééœéŸ³ç‰‡æ®µ
                    clips = []
                    sample_rate = audio.fps
                    
                    for start_frame, end_frame in non_silent_intervals:
                        start_time = start_frame / sample_rate
                        end_time = end_frame / sample_rate
                        
                        if end_time - start_time > 0.1:  # è‡³å°‘0.1ç§’
                            clip_segment = audio.subclip(start_time, end_time)
                            clips.append(clip_segment)
                    
                    if clips:
                        return concatenate_audioclips(clips)
                        
        except Exception as e:
            logger.warning(f"Silence removal failed: {e}")
            
        return audio
        
    def create_rhythm_matched_video(
        self,
        video: VideoClip,
        audio_analysis: AudioAnalysis,
        beat_sync: bool = True
    ) -> VideoClip:
        """å‰µå»ºç¯€å¥åŒ¹é…çš„è¦–é »"""
        
        if not (beat_sync and audio_analysis.beats):
            return video
            
        try:
            beats = audio_analysis.beats
            video_duration = video.duration
            
            # æ ¹æ“šç¯€æ‹èª¿æ•´è¦–é »é€Ÿåº¦
            if len(beats) > 1:
                avg_beat_interval = np.mean(np.diff(beats))
                target_beat_interval = video_duration / len(beats)
                speed_ratio = avg_beat_interval / target_beat_interval
                
                if 0.5 <= speed_ratio <= 2.0:  # åˆç†çš„é€Ÿåº¦ç¯„åœ
                    video = video.fx(vfx.speedx, speed_ratio)
                    
            return video
            
        except Exception as e:
            logger.warning(f"Rhythm matching failed: {e}")
            return video
            
    async def create_audio_visualization(
        self,
        audio_analysis: AudioAnalysis,
        width: int = 1080,
        height: int = 200
    ) -> Optional[VideoClip]:
        """å‰µå»ºéŸ³é »å¯è¦–åŒ–"""
        
        if not (MOVIEPY_AVAILABLE and audio_analysis.rms_energy is not None):
            return None
            
        try:
            from moviepy.editor import ColorClip, CompositeVideoClip
            
            # å‰µå»ºåŸºç¤èƒŒæ™¯
            background = ColorClip(
                size=(width, height), 
                color=(0, 0, 0), 
                duration=audio_analysis.duration
            )
            
            # å‰µå»ºèƒ½é‡æ³¢å½¢å¯è¦–åŒ–
            energy = audio_analysis.rms_energy
            time_per_frame = audio_analysis.duration / len(energy)
            
            def make_frame(t):
                frame_index = int(t / time_per_frame)
                if frame_index >= len(energy):
                    frame_index = len(energy) - 1
                    
                current_energy = energy[frame_index]
                bar_height = int(current_energy * height * 10)  # æ”¾å¤§èƒ½é‡å€¼
                bar_height = min(bar_height, height)
                
                # å‰µå»ºç°¡å–®çš„èƒ½é‡æ¢
                frame = np.zeros((height, width, 3), dtype=np.uint8)
                
                # ç¹ªè£½èƒ½é‡æ¢ï¼ˆä¸­å¿ƒå°ç¨±ï¼‰
                if bar_height > 0:
                    center_y = height // 2
                    start_y = max(0, center_y - bar_height // 2)
                    end_y = min(height, center_y + bar_height // 2)
                    
                    # ç¶ è‰²èƒ½é‡æ¢
                    frame[start_y:end_y, :, 1] = 255
                    
                return frame
                
            visualization = VideoClip(make_frame, duration=audio_analysis.duration)
            
            return CompositeVideoClip([background, visualization])
            
        except Exception as e:
            logger.error(f"Audio visualization creation failed: {e}")
            return None


# ä¾¿æ·å‡½æ•¸
async def sync_audio_to_video(
    video: VideoClip,
    audio_path: str,
    sync_method: str = "automatic",
    normalize: bool = True,
    fade_duration: float = 0.5
) -> VideoClip:
    """åŒæ­¥éŸ³é »åˆ°è¦–é »çš„ä¾¿æ·å‡½æ•¸"""
    
    sync_engine = AudioVideoSyncEngine()
    config = SyncConfig(
        sync_method=sync_method,
        normalize_audio=normalize,
        fade_duration=fade_duration
    )
    
    return await sync_engine.sync_audio_to_video(video, audio_path, config)

async def test_audio_video_sync():
    """æ¸¬è©¦éŸ³è¦–é »åŒæ­¥ç³»çµ±"""
    
    print("ğŸµ Testing Audio-Video Sync Engine...")
    
    sync_engine = AudioVideoSyncEngine()
    
    # æ¸¬è©¦åŸºç¤åŠŸèƒ½
    print(f"Dependencies available:")
    print(f"  MoviePy: {MOVIEPY_AVAILABLE}")
    print(f"  librosa: {LIBROSA_AVAILABLE}")
    print(f"  scipy: {SCIPY_AVAILABLE}")
    
    # å¦‚æœæœ‰æ¸¬è©¦éŸ³é »æ–‡ä»¶ï¼Œå¯ä»¥é€²è¡Œå¯¦éš›æ¸¬è©¦
    test_audio = "test_audio.mp3"  # å‡è¨­çš„æ¸¬è©¦æ–‡ä»¶
    
    if os.path.exists(test_audio):
        try:
            analysis = await sync_engine.analyze_audio(test_audio)
            print(f"âœ… Audio analysis successful:")
            print(f"   Duration: {analysis.duration:.2f}s")
            print(f"   Tempo: {analysis.tempo}")
            print(f"   Beats: {len(analysis.beats) if analysis.beats else 0}")
        except Exception as e:
            print(f"âŒ Audio analysis failed: {e}")
    else:
        print("â„¹ï¸  No test audio file found, skipping actual analysis")
    
    print("âœ… Audio-Video Sync Engine initialized successfully!")
    return sync_engine

if __name__ == "__main__":
    print("ğŸ¬ Audio-Video Sync Engine - Intelligent Audio-Video Synchronization")
    print("=" * 75)
    
    # æª¢æŸ¥ä¾è³´
    print("Checking dependencies...")
    print(f"MoviePy available: {MOVIEPY_AVAILABLE}")
    print(f"librosa available: {LIBROSA_AVAILABLE}")
    print(f"scipy available: {SCIPY_AVAILABLE}")
    
    # é‹è¡Œæ¸¬è©¦
    result = asyncio.run(test_audio_video_sync())
    print(f"Sync engine ready: {result is not None}")