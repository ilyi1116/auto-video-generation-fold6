#!/usr/bin/env python3
"""
é«˜ç´šè¦–é »è™•ç†å¼•æ“ - ä¼æ¥­ç´šè¦–é »ç”Ÿæˆç³»çµ±
åŒ…å«æ™ºèƒ½åˆæˆã€å°ˆæ¥­è½‰å ´ã€å‹•æ…‹ç‰¹æ•ˆã€éŸ³è¦–é »åŒæ­¥ç­‰åŠŸèƒ½
"""

import os
import logging
import tempfile
import asyncio
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from dataclasses import dataclass, asdict
import hashlib
import concurrent.futures
import threading

try:
    from moviepy.editor import (
        VideoFileClip, AudioFileClip, ImageClip, CompositeVideoClip, 
        CompositeAudioClip, TextClip, concatenate_videoclips,
        vfx, afx, ColorClip, VideoClip
    )
    from moviepy.video.fx import resize, fadein, fadeout, crossfadein, crossfadeout
    from moviepy.audio.fx import volumex, audio_fadein, audio_fadeout
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False
    logging.warning("MoviePy not available. Advanced video processing disabled.")

try:
    from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
    import PIL
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False
    logging.warning("OpenCV not available. Some advanced features disabled.")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class VideoConfig:
    """è¦–é »é…ç½®åƒæ•¸"""
    width: int = 1080
    height: int = 1920
    fps: int = 30
    duration: int = 15
    bitrate: str = "8M"
    codec: str = "libx264"
    audio_codec: str = "aac"
    preset: str = "medium"  # ultrafast, fast, medium, slow, slower
    crf: int = 18  # è³ªé‡åƒæ•¸ (0-51, è¶Šä½è¶Šå¥½)
    
@dataclass 
class TransitionConfig:
    """è½‰å ´æ•ˆæœé…ç½®"""
    transition_type: str = "crossfade"  # crossfade, slide, zoom, fade, dissolve
    duration: float = 1.0
    direction: str = "left"  # left, right, up, down, center
    easing: str = "linear"  # linear, ease_in, ease_out, ease_in_out
    
@dataclass
class TextAnimation:
    """æ–‡å­—å‹•ç•«é…ç½®"""
    animation_type: str = "fade_in"  # fade_in, slide_in, scale_up, typewriter
    duration: float = 2.0
    delay: float = 0.0
    font_size: int = 48
    font_color: Tuple[int, int, int] = (255, 255, 255)
    outline_color: Optional[Tuple[int, int, int]] = (0, 0, 0)
    outline_width: int = 2
    shadow_offset: Tuple[int, int] = (2, 2)
    
@dataclass
class AudioProcessing:
    """éŸ³é »è™•ç†é…ç½®"""
    normalize: bool = True
    fade_in_duration: float = 0.5
    fade_out_duration: float = 0.5
    volume_adjustment: float = 1.0
    noise_reduction: bool = False
    compression: bool = False

class AdvancedVideoEngine:
    """é«˜ç´šè¦–é »è™•ç†å¼•æ“"""
    
    def __init__(self, output_dir: str = "./uploads/dev", cache_dir: str = "./cache"):
        self.output_dir = Path(output_dir)
        self.cache_dir = Path(cache_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # é»˜èªé…ç½®
        self.config = VideoConfig()
        self.transition_config = TransitionConfig()
        
        # æ€§èƒ½å„ªåŒ–
        self.max_workers = min(4, os.cpu_count() or 1)
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers)
        
        # ç·©å­˜ç³»çµ±
        self.cache = {}
        self._setup_logging()
        
    def _setup_logging(self):
        """è¨­ç½®è©³ç´°æ—¥èªŒ"""
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
        )
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        
    async def create_professional_video(
        self,
        scenes: List[Dict],
        audio_file: Optional[str] = None,
        title: str = "Professional Video",
        style: str = "cinematic",
        config: Optional[VideoConfig] = None
    ) -> Dict:
        """å‰µå»ºå°ˆæ¥­ç´šè¦–é »"""
        
        if not MOVIEPY_AVAILABLE:
            return self._create_fallback_response("MoviePy not available")
            
        start_time = datetime.now()
        logger.info(f"Starting professional video creation: {title}")
        
        try:
            if config:
                self.config = config
                
            # ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶å
            timestamp = int(datetime.now().timestamp())
            output_filename = f"professional_video_{timestamp}.mp4"
            output_path = self.output_dir / output_filename
            
            # è™•ç†å ´æ™¯
            video_clips = []
            for i, scene in enumerate(scenes):
                logger.info(f"Processing scene {i+1}/{len(scenes)}")
                clip = await self._process_scene(scene, i)
                if clip:
                    video_clips.append(clip)
                    
            if not video_clips:
                raise ValueError("No valid video clips generated")
                
            # æ‡‰ç”¨è½‰å ´æ•ˆæœ
            logger.info("Applying transitions between scenes")
            final_video = await self._apply_transitions(video_clips)
            
            # éŸ³é »è™•ç†
            if audio_file and os.path.exists(audio_file):
                logger.info("Processing and syncing audio")
                final_video = await self._process_and_sync_audio(final_video, audio_file)
                
            # æ·»åŠ å°ˆæ¥­ç´šæ¿¾é¡å’Œè‰²å½©æ ¡æ­£
            logger.info("Applying professional color grading and filters")
            final_video = await self._apply_professional_effects(final_video, style)
            
            # è¼¸å‡ºé«˜è³ªé‡è¦–é »
            logger.info("Rendering final video with optimized settings")
            await self._render_video_optimized(final_video, str(output_path))
            
            # æ¸…ç†è³‡æº
            final_video.close()
            for clip in video_clips:
                if hasattr(clip, 'close'):
                    clip.close()
                    
            processing_time = (datetime.now() - start_time).total_seconds()
            file_size = os.path.getsize(output_path) if os.path.exists(output_path) else 0
            
            return {
                "success": True,
                "video_url": f"/static/{output_filename}",
                "video_path": str(output_path),
                "title": title,
                "duration": self.config.duration,
                "resolution": f"{self.config.width}x{self.config.height}",
                "fps": self.config.fps,
                "format": "mp4",
                "file_size": file_size,
                "file_size_mb": round(file_size / (1024 * 1024), 2),
                "processing_time": round(processing_time, 2),
                "scenes_processed": len(scenes),
                "style": style,
                "quality": "professional",
                "codec": self.config.codec,
                "bitrate": self.config.bitrate,
                "generated_at": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Professional video creation failed: {e}", exc_info=True)
            return self._create_fallback_response(f"Error: {str(e)}")
            
    async def _process_scene(self, scene: Dict, scene_index: int) -> Optional[VideoClip]:
        """è™•ç†å–®å€‹å ´æ™¯"""
        
        scene_type = scene.get('type', 'image')
        duration = scene.get('duration', 3.0)
        
        if scene_type == 'image':
            return await self._create_image_scene(scene, duration)
        elif scene_type == 'text':
            return await self._create_text_scene(scene, duration)
        elif scene_type == 'video':
            return await self._create_video_scene(scene, duration)
        else:
            logger.warning(f"Unknown scene type: {scene_type}")
            return None
            
    async def _create_image_scene(self, scene: Dict, duration: float) -> Optional[VideoClip]:
        """å‰µå»ºåœ–åƒå ´æ™¯"""
        
        image_source = scene.get('source', '')
        effects = scene.get('effects', [])
        
        try:
            # ç²å–æˆ–ä¸‹è¼‰åœ–åƒ
            image_path = await self._prepare_image_advanced(image_source)
            if not image_path:
                return None
                
            # å‰µå»ºåŸºç¤åœ–åƒå‰ªè¼¯
            clip = ImageClip(image_path, duration=duration)
            clip = clip.resize((self.config.width, self.config.height))
            
            # æ‡‰ç”¨åœ–åƒå¢å¼·
            clip = await self._enhance_image_clip(clip, scene.get('enhancement', {}))
            
            # æ‡‰ç”¨å‹•ç•«æ•ˆæœ
            for effect in effects:
                clip = await self._apply_image_effect(clip, effect)
                
            return clip
            
        except Exception as e:
            logger.error(f"Error creating image scene: {e}")
            return None
            
    async def _create_text_scene(self, scene: Dict, duration: float) -> Optional[VideoClip]:
        """å‰µå»ºæ–‡å­—å ´æ™¯"""
        
        text_content = scene.get('content', '')
        animation = TextAnimation(**scene.get('animation', {}))
        
        try:
            # å‰µå»ºé«˜ç´šæ–‡å­—å‰ªè¼¯
            text_clip = await self._create_animated_text(
                text_content, 
                animation, 
                duration
            )
            
            # å‰µå»ºèƒŒæ™¯
            background = scene.get('background', {})
            if background.get('type') == 'color':
                bg_clip = ColorClip(
                    size=(self.config.width, self.config.height),
                    color=background.get('color', (0, 0, 0)),
                    duration=duration
                )
            elif background.get('type') == 'image':
                bg_image_path = await self._prepare_image_advanced(background.get('source', ''))
                if bg_image_path:
                    bg_clip = ImageClip(bg_image_path, duration=duration)
                    bg_clip = bg_clip.resize((self.config.width, self.config.height))
                    # æ·»åŠ æ¨¡ç³Šæ•ˆæœä½œç‚ºèƒŒæ™¯
                    bg_clip = bg_clip.fl_image(lambda img: self._apply_gaussian_blur(img, 15))
                else:
                    bg_clip = ColorClip(
                        size=(self.config.width, self.config.height),
                        color=(0, 0, 0),
                        duration=duration
                    )
            else:
                bg_clip = ColorClip(
                    size=(self.config.width, self.config.height),
                    color=(0, 0, 0),
                    duration=duration
                )
            
            # åˆæˆæ–‡å­—å’ŒèƒŒæ™¯
            final_clip = CompositeVideoClip([bg_clip, text_clip])
            return final_clip
            
        except Exception as e:
            logger.error(f"Error creating text scene: {e}")
            return None
            
    async def _create_animated_text(
        self, 
        text: str, 
        animation: TextAnimation, 
        duration: float
    ) -> TextClip:
        """å‰µå»ºå‹•ç•«æ–‡å­—"""
        
        # åŸºç¤æ–‡å­—è¨­ç½®
        font_path = self._get_best_font()
        
        # å‰µå»ºæ–‡å­—å‰ªè¼¯
        text_clip = TextClip(
            text,
            fontsize=animation.font_size,
            color='white' if not animation.font_color else 
                  f'rgb({animation.font_color[0]},{animation.font_color[1]},{animation.font_color[2]})',
            font=font_path,
            stroke_color='black' if animation.outline_color else None,
            stroke_width=animation.outline_width if animation.outline_color else 0
        ).set_duration(duration)
        
        # å±…ä¸­å®šä½
        text_clip = text_clip.set_position('center')
        
        # æ‡‰ç”¨å‹•ç•«
        if animation.animation_type == 'fade_in':
            text_clip = text_clip.set_start(animation.delay).fadein(animation.duration)
        elif animation.animation_type == 'slide_in':
            text_clip = text_clip.set_start(animation.delay)
            # å¾å³å´æ»‘å…¥
            text_clip = text_clip.set_position(lambda t: ('center' if t > animation.duration 
                                                        else (self.config.width + 100 - 
                                                              (self.config.width + 100) * 
                                                              (t / animation.duration), 'center')))
        elif animation.animation_type == 'scale_up':
            text_clip = text_clip.set_start(animation.delay)
            text_clip = text_clip.resize(lambda t: min(1, t / animation.duration))
        elif animation.animation_type == 'typewriter':
            # æ‰“å­—æ©Ÿæ•ˆæœéœ€è¦ç‰¹æ®Šè™•ç†
            text_clip = await self._create_typewriter_effect(text, animation, duration)
            
        return text_clip
        
    async def _create_typewriter_effect(
        self, 
        text: str, 
        animation: TextAnimation, 
        duration: float
    ) -> CompositeVideoClip:
        """å‰µå»ºæ‰“å­—æ©Ÿæ•ˆæœ"""
        
        clips = []
        chars_per_second = len(text) / animation.duration
        
        for i in range(len(text) + 1):
            partial_text = text[:i]
            if not partial_text:
                continue
                
            start_time = animation.delay + (i / chars_per_second)
            clip_duration = duration - start_time
            
            if clip_duration <= 0:
                continue
                
            partial_clip = TextClip(
                partial_text,
                fontsize=animation.font_size,
                color='white',
                font=self._get_best_font()
            ).set_start(start_time).set_duration(clip_duration).set_position('center')
            
            clips.append(partial_clip)
            
        return CompositeVideoClip(clips)
        
    async def _apply_transitions(self, clips: List[VideoClip]) -> VideoClip:
        """æ‡‰ç”¨å°ˆæ¥­è½‰å ´æ•ˆæœ"""
        
        if len(clips) <= 1:
            return clips[0] if clips else None
            
        transition_duration = self.transition_config.duration
        
        # æ ¹æ“šè½‰å ´é¡å‹è™•ç†
        if self.transition_config.transition_type == "crossfade":
            return await self._apply_crossfade_transition(clips, transition_duration)
        elif self.transition_config.transition_type == "slide":
            return await self._apply_slide_transition(clips, transition_duration)
        elif self.transition_config.transition_type == "zoom":
            return await self._apply_zoom_transition(clips, transition_duration)
        else:
            # é»˜èªä½¿ç”¨äº¤å‰æ·¡å…¥æ·¡å‡º
            return await self._apply_crossfade_transition(clips, transition_duration)
            
    async def _apply_crossfade_transition(
        self, 
        clips: List[VideoClip], 
        transition_duration: float
    ) -> VideoClip:
        """æ‡‰ç”¨äº¤å‰æ·¡å…¥æ·¡å‡ºè½‰å ´"""
        
        if len(clips) == 1:
            return clips[0]
            
        result_clips = [clips[0]]
        current_time = clips[0].duration - transition_duration
        
        for i in range(1, len(clips)):
            # ç•¶å‰ç‰‡æ®µæ·¡å‡º
            current_clip = clips[i-1].fadeout(transition_duration)
            
            # ä¸‹ä¸€å€‹ç‰‡æ®µæ·¡å…¥
            next_clip = clips[i].fadein(transition_duration).set_start(current_time)
            
            result_clips = [current_clip, next_clip]
            current_time += clips[i].duration - transition_duration
            
        return CompositeVideoClip(result_clips)
        
    async def _apply_professional_effects(self, video: VideoClip, style: str) -> VideoClip:
        """æ‡‰ç”¨å°ˆæ¥­ç´šè¦–è¦ºæ•ˆæœ"""
        
        if style == "cinematic":
            # é›»å½±ç´šèª¿è‰²
            video = video.fl_image(self._cinematic_color_grade)
            # æ·»åŠ è¼•å¾®çš„æš—è§’æ•ˆæœ
            video = video.fl_image(self._add_vignette)
            
        elif style == "modern":
            # ç¾ä»£é¢¨æ ¼ - é«˜å°æ¯”åº¦å’Œé£½å’Œåº¦
            video = video.fl_image(self._modern_color_grade)
            
        elif style == "vintage":
            # å¾©å¤é¢¨æ ¼
            video = video.fl_image(self._vintage_color_grade)
            # æ·»åŠ è† ç‰‡é¡†ç²’
            video = video.fl_image(self._add_film_grain)
            
        elif style == "minimal":
            # æ¥µç°¡é¢¨æ ¼ - é™ä½é£½å’Œåº¦
            video = video.fl_image(self._minimal_color_grade)
            
        return video
        
    def _cinematic_color_grade(self, img):
        """é›»å½±ç´šèª¿è‰²"""
        if not PIL_AVAILABLE:
            return img
            
        pil_img = Image.fromarray(img)
        
        # å¢åŠ å°æ¯”åº¦
        enhancer = ImageEnhance.Contrast(pil_img)
        pil_img = enhancer.enhance(1.2)
        
        # è¼•å¾®é™ä½é£½å’Œåº¦
        enhancer = ImageEnhance.Color(pil_img)
        pil_img = enhancer.enhance(0.9)
        
        # èª¿æ•´äº®åº¦
        enhancer = ImageEnhance.Brightness(pil_img)
        pil_img = enhancer.enhance(0.95)
        
        return np.array(pil_img)
        
    def _add_vignette(self, img):
        """æ·»åŠ æš—è§’æ•ˆæœ"""
        if not PIL_AVAILABLE:
            return img
            
        pil_img = Image.fromarray(img)
        width, height = pil_img.size
        
        # å‰µå»ºæš—è§’é®ç½©
        vignette = Image.new('L', (width, height), 0)
        vignette_draw = ImageDraw.Draw(vignette)
        
        # ç¹ªè£½å¾‘å‘æ¼¸è®Š
        center_x, center_y = width // 2, height // 2
        max_radius = min(width, height) // 2
        
        for radius in range(max_radius, 0, -1):
            alpha = int(255 * (radius / max_radius))
            vignette_draw.ellipse([
                center_x - radius, center_y - radius,
                center_x + radius, center_y + radius
            ], fill=alpha)
        
        # æ‡‰ç”¨æš—è§’
        vignette = vignette.convert('RGB')
        result = Image.blend(pil_img, vignette, 0.1)
        
        return np.array(result)
        
    async def _process_and_sync_audio(self, video: VideoClip, audio_file: str) -> VideoClip:
        """è™•ç†å’ŒåŒæ­¥éŸ³é »"""
        
        try:
            audio = AudioFileClip(audio_file)
            
            # éŸ³é »æ¨™æº–åŒ–
            audio = audio.fx(afx.normalize)
            
            # æ·¡å…¥æ·¡å‡º
            audio = audio.fx(audio_fadein, 0.5).fx(audio_fadeout, 0.5)
            
            # èª¿æ•´éŸ³é »é•·åº¦åŒ¹é…è¦–é »
            if audio.duration > video.duration:
                audio = audio.subclip(0, video.duration)
            elif audio.duration < video.duration:
                # å¾ªç’°éŸ³é »æˆ–æ·»åŠ éœéŸ³
                repeats = int(video.duration / audio.duration) + 1
                audio = CompositeAudioClip([audio] * repeats).subclip(0, video.duration)
            
            # å°‡éŸ³é »é™„åŠ åˆ°è¦–é »
            video = video.set_audio(audio)
            
            return video
            
        except Exception as e:
            logger.error(f"Audio processing failed: {e}")
            return video
            
    async def _render_video_optimized(self, video: VideoClip, output_path: str):
        """å„ªåŒ–æ¸²æŸ“è¦–é »"""
        
        # ä½¿ç”¨é«˜è³ªé‡è¨­ç½®æ¸²æŸ“
        video.write_videofile(
            output_path,
            fps=self.config.fps,
            codec=self.config.codec,
            bitrate=self.config.bitrate,
            audio_codec=self.config.audio_codec,
            preset=self.config.preset,
            ffmpeg_params=['-crf', str(self.config.crf)],
            threads=self.max_workers,
            verbose=False,
            logger=None
        )
        
    def _get_best_font(self) -> str:
        """ç²å–æœ€ä½³å¯ç”¨å­—é«”"""
        
        font_paths = [
            "/System/Library/Fonts/PingFang.ttc",  # macOS ä¸­æ–‡
            "/System/Library/Fonts/Helvetica.ttc",  # macOS è‹±æ–‡
            "/System/Library/Fonts/Arial.ttf",     # Windows
            "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",  # Linux
            "arial",  # ç³»çµ±å­—é«”åç¨±
            "helvetica"
        ]
        
        for font_path in font_paths:
            if os.path.exists(font_path):
                return font_path
                
        return "Arial"  # fallback
        
    async def _prepare_image_advanced(self, image_source: str) -> Optional[str]:
        """é«˜ç´šåœ–åƒé è™•ç†"""
        
        if not image_source:
            return None
            
        # ç”Ÿæˆç·©å­˜key
        cache_key = hashlib.md5(image_source.encode()).hexdigest()
        cache_path = self.cache_dir / f"img_{cache_key}.jpg"
        
        # æª¢æŸ¥ç·©å­˜
        if cache_path.exists():
            return str(cache_path)
            
        try:
            if image_source.startswith('http'):
                # ä¸‹è¼‰ç¶²è·¯åœ–ç‰‡
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    async with session.get(image_source) as response:
                        if response.status == 200:
                            content = await response.read()
                            
                            # ä¿å­˜åˆ°ç·©å­˜
                            with open(cache_path, 'wb') as f:
                                f.write(content)
                                
                            # å„ªåŒ–åœ–åƒ
                            await self._optimize_image(str(cache_path))
                            return str(cache_path)
            else:
                # æœ¬åœ°æ–‡ä»¶
                if os.path.exists(image_source):
                    # è¤‡è£½åˆ°ç·©å­˜ä¸¦å„ªåŒ–
                    import shutil
                    shutil.copy2(image_source, cache_path)
                    await self._optimize_image(str(cache_path))
                    return str(cache_path)
                    
        except Exception as e:
            logger.error(f"Failed to prepare image {image_source}: {e}")
            
        return None
        
    async def _optimize_image(self, image_path: str):
        """å„ªåŒ–åœ–åƒè³ªé‡"""
        
        if not PIL_AVAILABLE:
            return
            
        try:
            with Image.open(image_path) as img:
                # è½‰æ›ç‚ºRGB
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # èª¿æ•´å°ºå¯¸
                img = img.resize((self.config.width, self.config.height), Image.LANCZOS)
                
                # è¼•å¾®éŠ³åŒ–
                img = img.filter(ImageFilter.UnsharpMask(radius=1, percent=110, threshold=3))
                
                # ä¿å­˜å„ªåŒ–å¾Œçš„åœ–åƒ
                img.save(image_path, 'JPEG', quality=95, optimize=True)
                
        except Exception as e:
            logger.error(f"Image optimization failed: {e}")
            
    def _create_fallback_response(self, error_msg: str) -> Dict:
        """å‰µå»ºfallbackéŸ¿æ‡‰"""
        
        return {
            "success": False,
            "error": error_msg,
            "video_url": "#",
            "video_path": "",
            "title": "Failed Video Generation",
            "duration": 0,
            "resolution": f"{self.config.width}x{self.config.height}",
            "format": "mp4",
            "file_size": 0,
            "generated_at": datetime.utcnow().isoformat(),
            "note": "Advanced video processing requires MoviePy, PIL, and optionally OpenCV"
        }
        
    def _apply_gaussian_blur(self, img, radius: int):
        """æ‡‰ç”¨é«˜æ–¯æ¨¡ç³Š"""
        if not PIL_AVAILABLE:
            return img
            
        pil_img = Image.fromarray(img)
        blurred = pil_img.filter(ImageFilter.GaussianBlur(radius=radius))
        return np.array(blurred)
        
    def _modern_color_grade(self, img):
        """ç¾ä»£é¢¨æ ¼èª¿è‰²"""
        if not PIL_AVAILABLE:
            return img
            
        pil_img = Image.fromarray(img)
        
        # å¢åŠ å°æ¯”åº¦å’Œé£½å’Œåº¦
        enhancer = ImageEnhance.Contrast(pil_img)
        pil_img = enhancer.enhance(1.3)
        
        enhancer = ImageEnhance.Color(pil_img)
        pil_img = enhancer.enhance(1.2)
        
        return np.array(pil_img)
        
    def _vintage_color_grade(self, img):
        """å¾©å¤é¢¨æ ¼èª¿è‰²"""
        if not PIL_AVAILABLE:
            return img
            
        pil_img = Image.fromarray(img)
        
        # é™ä½é£½å’Œåº¦ï¼Œå¢åŠ æš–è‰²èª¿
        enhancer = ImageEnhance.Color(pil_img)
        pil_img = enhancer.enhance(0.8)
        
        # æ·»åŠ å¾©å¤æ¿¾é¡æ•ˆæœï¼ˆå¢åŠ é»ƒè‰²èª¿ï¼‰
        # é€™è£¡å¯ä»¥æ·»åŠ æ›´è¤‡é›œçš„è‰²å½©æ˜ å°„
        
        return np.array(pil_img)
        
    def _minimal_color_grade(self, img):
        """æ¥µç°¡é¢¨æ ¼èª¿è‰²"""
        if not PIL_AVAILABLE:
            return img
            
        pil_img = Image.fromarray(img)
        
        # å¤§å¹…é™ä½é£½å’Œåº¦
        enhancer = ImageEnhance.Color(pil_img)
        pil_img = enhancer.enhance(0.3)
        
        # è¼•å¾®å¢åŠ å°æ¯”åº¦
        enhancer = ImageEnhance.Contrast(pil_img)
        pil_img = enhancer.enhance(1.1)
        
        return np.array(pil_img)
        
    def _add_film_grain(self, img):
        """æ·»åŠ è† ç‰‡é¡†ç²’æ•ˆæœ"""
        if not PIL_AVAILABLE or not np:
            return img
            
        # ç”Ÿæˆéš¨æ©Ÿå™ªéŸ³
        noise = np.random.normal(0, 10, img.shape).astype(np.int16)
        
        # æ·»åŠ åˆ°åŸåœ–åƒ
        result = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)
        
        return result
        
    async def __aenter__(self):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)


# ä¾¿æ·å‡½æ•¸
async def create_professional_video(
    scenes: List[Dict],
    audio_file: Optional[str] = None,
    title: str = "Professional Video",
    style: str = "cinematic",
    config: Optional[VideoConfig] = None,
    output_dir: str = "./uploads/dev"
) -> Dict:
    """å‰µå»ºå°ˆæ¥­è¦–é »çš„ä¾¿æ·å‡½æ•¸"""
    
    async with AdvancedVideoEngine(output_dir) as engine:
        return await engine.create_professional_video(
            scenes=scenes,
            audio_file=audio_file,
            title=title,
            style=style,
            config=config
        )


# æ¸¬è©¦å’Œæ¼”ç¤º
async def test_advanced_video_engine():
    """æ¸¬è©¦é«˜ç´šè¦–é »å¼•æ“"""
    
    # å®šç¾©æ¸¬è©¦å ´æ™¯
    test_scenes = [
        {
            "type": "text",
            "content": "æ­¡è¿ä¾†åˆ°\né«˜ç´šè¦–é »ç”Ÿæˆæ¸¬è©¦",
            "duration": 3.0,
            "animation": {
                "animation_type": "fade_in",
                "duration": 1.0,
                "font_size": 64,
                "font_color": (255, 255, 255)
            },
            "background": {
                "type": "color",
                "color": (25, 25, 75)
            }
        },
        {
            "type": "image",
            "source": "https://picsum.photos/1080/1920?random=1",
            "duration": 4.0,
            "effects": ["zoom_in", "fade"],
            "enhancement": {
                "brightness": 1.1,
                "contrast": 1.2,
                "saturation": 1.1
            }
        },
        {
            "type": "text",
            "content": "å°ˆæ¥­ç´š\nè¦–é »è™•ç†æŠ€è¡“",
            "duration": 3.0,
            "animation": {
                "animation_type": "slide_in",
                "duration": 1.5,
                "font_size": 56
            },
            "background": {
                "type": "image",
                "source": "https://picsum.photos/1080/1920?random=2"
            }
        }
    ]
    
    # é«˜è³ªé‡é…ç½®
    hq_config = VideoConfig(
        width=1080,
        height=1920,
        fps=30,
        duration=10,
        bitrate="10M",
        crf=16,  # é«˜è³ªé‡
        preset="slow"  # æœ€ä½³å£“ç¸®
    )
    
    result = await create_professional_video(
        scenes=test_scenes,
        title="Advanced Video Engine Test",
        style="cinematic",
        config=hq_config
    )
    
    print("\nğŸ¬ Advanced Video Engine Test Results:")
    print("=" * 50)
    for key, value in result.items():
        print(f"{key}: {value}")
    
    return result


if __name__ == "__main__":
    print("ğŸš€ Advanced Video Engine - Professional Video Generation")
    print("=" * 60)
    
    # æª¢æŸ¥ä¾è³´
    print(f"MoviePy available: {MOVIEPY_AVAILABLE}")
    print(f"PIL available: {PIL_AVAILABLE}")
    print(f"OpenCV available: {OPENCV_AVAILABLE}")
    
    if MOVIEPY_AVAILABLE and PIL_AVAILABLE:
        # é‹è¡Œæ¸¬è©¦
        result = asyncio.run(test_advanced_video_engine())
        
        if result.get('success'):
            print("\nâœ… Advanced video generation test completed successfully!")
            print(f"ğŸ“Š Processing time: {result.get('processing_time', 0)}s")
            print(f"ğŸ“ File size: {result.get('file_size_mb', 0)}MB")
        else:
            print(f"\nâŒ Test failed: {result.get('error', 'Unknown error')}")
    else:
        print("\nâš ï¸  Missing required dependencies for advanced video processing")
        print("Install with: pip install moviepy pillow opencv-python")