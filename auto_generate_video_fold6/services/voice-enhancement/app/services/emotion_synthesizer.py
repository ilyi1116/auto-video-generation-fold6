"""
Emotion-aware Speech Synthesis
å¯¦ç¾å…·æœ‰æƒ…æ„Ÿè¡¨é”èƒ½åŠ›çš„èªéŸ³åˆæˆ
"""

import io
import numpy as np
import torch
from typing import Dict, List, Optional, Tuple
import structlog
from TTS.api import TTS
from speechemotionrecognition import SpeechEmotionRecognition
import librosa
import soundfile as sf
from transformers import Wav2Vec2Processor, Wav2Vec2Model

logger = structlog.get_logger()


class EmotionSynthesizer:
    """æƒ…æ„ŸèªéŸ³åˆæˆå™¨"""

    def __init__(self):
        self.tts_models = {}
        self.emotion_classifier = None
        self.emotion_embedder = None
        self.supported_emotions = [
            "neutral",
            "happy",
            "sad",
            "angry",
            "fear",
            "surprise",
            "disgust",
            "excited",
            "calm",
            "confident",
        ]
        self.supported_languages = [
            "zh-CN",
            "zh-TW",
            "en-US",
            "ja-JP",
            "ko-KR",
        ]

    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            logger.info("åˆå§‹åŒ–æƒ…æ„ŸèªéŸ³åˆæˆå™¨")

            # è¼‰å…¥ TTS æ¨¡å‹
            await self._load_tts_models()

            # è¼‰å…¥æƒ…æ„Ÿåˆ†é¡å™¨
            await self._load_emotion_classifier()

            # è¼‰å…¥æƒ…æ„ŸåµŒå…¥æ¨¡å‹
            await self._load_emotion_embedder()

            logger.info("æƒ…æ„ŸèªéŸ³åˆæˆå™¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error("æƒ…æ„ŸèªéŸ³åˆæˆå™¨åˆå§‹åŒ–å¤±æ•—", error=str(e))
            raise

    async def _load_tts_models(self):
        """è¼‰å…¥ TTS æ¨¡å‹"""
        models_config = {
            "zh-CN": "tts_models/zh-CN/baker/tacotron2-DDC-GST",
            "zh-TW": "tts_models/zh-CN/baker/tacotron2-DDC-GST",  # ä½¿ç”¨ä¸­æ–‡æ¨¡å‹
            "en-US": "tts_models/en/ljspeech/tacotron2-DDC",
            "ja-JP": "tts_models/ja/kokoro/tacotron2-DDC",
            "ko-KR": "tts_models/ko/kss/tacotron2-DDC",
        }

        for lang, model_name in models_config.items():
            try:
                self.tts_models[lang] = TTS(model_name)
                logger.info(f"å·²è¼‰å…¥ {lang} TTS æ¨¡å‹")
            except Exception as e:
                logger.warning(f"ç„¡æ³•è¼‰å…¥ {lang} TTS æ¨¡å‹", error=str(e))

    async def _load_emotion_classifier(self):
        """è¼‰å…¥æƒ…æ„Ÿåˆ†é¡å™¨"""
        try:
            self.emotion_classifier = SpeechEmotionRecognition()
            logger.info("å·²è¼‰å…¥æƒ…æ„Ÿåˆ†é¡å™¨")
        except Exception as e:
            logger.warning("ç„¡æ³•è¼‰å…¥æƒ…æ„Ÿåˆ†é¡å™¨", error=str(e))

    async def _load_emotion_embedder(self):
        """è¼‰å…¥æƒ…æ„ŸåµŒå…¥æ¨¡å‹"""
        try:
            self.emotion_processor = Wav2Vec2Processor.from_pretrained(
                "facebook/wav2vec2-base-960h"
            )
            self.emotion_model = Wav2Vec2Model.from_pretrained(
                "facebook/wav2vec2-base-960h"
            )
            logger.info("å·²è¼‰å…¥æƒ…æ„ŸåµŒå…¥æ¨¡å‹")
        except Exception as e:
            logger.warning("ç„¡æ³•è¼‰å…¥æƒ…æ„ŸåµŒå…¥æ¨¡å‹", error=str(e))

    async def synthesize_with_emotion(
        self,
        text: str,
        emotion: str = "neutral",
        intensity: float = 1.0,
        language: str = "zh-TW",
        voice_speed: float = 1.0,
        voice_pitch: float = 1.0,
    ) -> bytes:
        """
        åˆæˆå…·æœ‰æƒ…æ„Ÿçš„èªéŸ³

        Args:
            text: è¦åˆæˆçš„æ–‡å­—
            emotion: ç›®æ¨™æƒ…æ„Ÿ
            intensity: æƒ…æ„Ÿå¼·åº¦ (0.0-2.0)
            language: èªè¨€ä»£ç¢¼
            voice_speed: èªéŸ³é€Ÿåº¦
            voice_pitch: èªéŸ³éŸ³èª¿

        Returns:
            åˆæˆçš„éŸ³è¨Šæ•¸æ“š (bytes)
        """
        try:
            logger.info(
                "é–‹å§‹æƒ…æ„ŸèªéŸ³åˆæˆ",
                text=text[:50],
                emotion=emotion,
                language=language,
            )

            # é©—è­‰åƒæ•¸
            if emotion not in self.supported_emotions:
                emotion = "neutral"

            if language not in self.supported_languages:
                language = "zh-TW"

            # æ ¹æ“šæƒ…æ„Ÿèª¿æ•´æ–‡å­—
            enhanced_text = self._enhance_text_for_emotion(
                text, emotion, intensity
            )

            # é¸æ“‡åˆé©çš„ TTS æ¨¡å‹
            tts_model = self.tts_models.get(language)
            if not tts_model:
                # ä½¿ç”¨é»˜èªæ¨¡å‹
                tts_model = next(iter(self.tts_models.values()))

            # ç”ŸæˆåŸºç¤èªéŸ³
            with io.BytesIO() as audio_buffer:
                tts_model.tts_to_file(
                    text=enhanced_text,
                    file_path=audio_buffer,
                    emotion=(
                        emotion if hasattr(tts_model, "emotions") else None
                    ),
                )
                base_audio = audio_buffer.getvalue()

            # æ‡‰ç”¨æƒ…æ„Ÿå¾Œè™•ç†
            enhanced_audio = await self._apply_emotion_processing(
                base_audio, emotion, intensity, voice_speed, voice_pitch
            )

            logger.info("æƒ…æ„ŸèªéŸ³åˆæˆå®Œæˆ")
            return enhanced_audio

        except Exception as e:
            logger.error("æƒ…æ„ŸèªéŸ³åˆæˆå¤±æ•—", error=str(e))
            raise

    def _enhance_text_for_emotion(
        self, text: str, emotion: str, intensity: float
    ) -> str:
        """æ ¹æ“šæƒ…æ„Ÿå¢å¼·æ–‡å­—"""

        # æƒ…æ„Ÿæ¨™è¨˜æ˜ å°„
        emotion_markers = {
            "happy": ["ğŸ˜Š", "ï¼", "å“ˆå“ˆ"],
            "sad": ["ğŸ˜¢", "...", "å”‰"],
            "angry": ["ğŸ˜ ", "ï¼ï¼", "å“¼"],
            "excited": ["ğŸ˜†", "ï¼ï¼ï¼", "å“‡"],
            "calm": ["ğŸ˜Œ", "ã€‚", "å—¯"],
            "surprised": ["ğŸ˜²", "ï¼Ÿï¼", "å“‡"],
        }

        # æ ¹æ“šæƒ…æ„Ÿå¼·åº¦èª¿æ•´
        if intensity > 1.5:
            # é«˜å¼·åº¦ï¼šå¢åŠ æƒ…æ„Ÿæ¨™è¨˜
            markers = emotion_markers.get(emotion, [])
            if markers:
                text = f"{markers[0]} {text} {markers[1]}"
        elif intensity > 1.0:
            # ä¸­å¼·åº¦ï¼šè¼•å¾®èª¿æ•´
            if emotion == "happy":
                text = text.replace("ã€‚", "ï¼")
            elif emotion == "sad":
                text = text.replace("ï¼", "...")

        return text

    async def _apply_emotion_processing(
        self,
        audio_data: bytes,
        emotion: str,
        intensity: float,
        speed: float,
        pitch: float,
    ) -> bytes:
        """æ‡‰ç”¨æƒ…æ„Ÿå¾Œè™•ç†"""

        # å°‡ bytes è½‰æ›ç‚º numpy array
        audio_array, sample_rate = sf.read(io.BytesIO(audio_data))

        # æ ¹æ“šæƒ…æ„Ÿèª¿æ•´éŸ³è¨Šç‰¹æ€§
        emotion_params = self._get_emotion_parameters(emotion, intensity)

        # èª¿æ•´éŸ³èª¿
        pitch_factor = pitch * emotion_params.get("pitch_factor", 1.0)
        if pitch_factor != 1.0:
            audio_array = librosa.effects.pitch_shift(
                audio_array, sr=sample_rate, n_steps=pitch_factor * 12
            )

        # èª¿æ•´èªé€Ÿ
        speed_factor = speed * emotion_params.get("speed_factor", 1.0)
        if speed_factor != 1.0:
            audio_array = librosa.effects.time_stretch(
                audio_array, rate=speed_factor
            )

        # èª¿æ•´éŸ³é‡
        volume_factor = emotion_params.get("volume_factor", 1.0)
        audio_array = audio_array * volume_factor

        # æ·»åŠ æƒ…æ„Ÿç›¸é—œçš„éŸ³è¨Šæ•ˆæœ
        audio_array = self._apply_emotion_effects(
            audio_array, emotion, intensity
        )

        # è½‰æ›å› bytes
        with io.BytesIO() as output:
            sf.write(output, audio_array, sample_rate, format="WAV")
            return output.getvalue()

    def _get_emotion_parameters(self, emotion: str, intensity: float) -> Dict:
        """ç²å–æƒ…æ„Ÿåƒæ•¸"""

        base_params = {
            "happy": {
                "pitch_factor": 1.1,
                "speed_factor": 1.05,
                "volume_factor": 1.1,
            },
            "sad": {
                "pitch_factor": 0.9,
                "speed_factor": 0.95,
                "volume_factor": 0.8,
            },
            "angry": {
                "pitch_factor": 1.15,
                "speed_factor": 1.1,
                "volume_factor": 1.3,
            },
            "excited": {
                "pitch_factor": 1.2,
                "speed_factor": 1.15,
                "volume_factor": 1.2,
            },
            "calm": {
                "pitch_factor": 0.95,
                "speed_factor": 0.9,
                "volume_factor": 0.9,
            },
            "fear": {
                "pitch_factor": 1.25,
                "speed_factor": 1.2,
                "volume_factor": 0.7,
            },
        }

        params = base_params.get(
            emotion,
            {"pitch_factor": 1.0, "speed_factor": 1.0, "volume_factor": 1.0},
        )

        # æ ¹æ“šå¼·åº¦èª¿æ•´åƒæ•¸
        for key, value in params.items():
            if key != "volume_factor":
                # å¼·åº¦å½±éŸ¿éŸ³èª¿å’Œé€Ÿåº¦
                diff = abs(value - 1.0)
                params[key] = 1.0 + diff * intensity
            else:
                # éŸ³é‡æ ¹æ“šæƒ…æ„Ÿèª¿æ•´
                params[key] = value * intensity

        return params

    def _apply_emotion_effects(
        self, audio: np.ndarray, emotion: str, intensity: float
    ) -> np.ndarray:
        """æ‡‰ç”¨æƒ…æ„ŸéŸ³è¨Šæ•ˆæœ"""

        if emotion == "happy" and intensity > 1.2:
            # é–‹å¿ƒï¼šè¼•å¾®çš„å›è²æ•ˆæœ
            audio = self._add_echo(audio, delay=0.1, decay=0.3)

        elif emotion == "sad" and intensity > 1.0:
            # æ‚²å‚·ï¼šé™ä½é«˜é »
            audio = self._apply_lowpass_filter(audio)

        elif emotion == "angry" and intensity > 1.3:
            # æ†¤æ€’ï¼šè¼•å¾®å¤±çœŸ
            audio = self._add_distortion(audio, factor=0.1)

        elif emotion == "fear" and intensity > 1.1:
            # ææ‡¼ï¼šé¡«æŠ–æ•ˆæœ
            audio = self._add_tremolo(audio, rate=6.0, depth=0.3)

        return audio

    def _add_echo(
        self, audio: np.ndarray, delay: float, decay: float
    ) -> np.ndarray:
        """æ·»åŠ å›è²æ•ˆæœ"""
        delay_samples = int(delay * 22050)  # å‡è¨­ 22050 Hz
        echo = np.zeros_like(audio)
        echo[delay_samples:] = audio[:-delay_samples] * decay
        return audio + echo

    def _apply_lowpass_filter(self, audio: np.ndarray) -> np.ndarray:
        """æ‡‰ç”¨ä½é€šæ¿¾æ³¢å™¨"""
        from scipy import signal

        b, a = signal.butter(4, 0.3, "low")
        return signal.filtfilt(b, a, audio)

    def _add_distortion(self, audio: np.ndarray, factor: float) -> np.ndarray:
        """æ·»åŠ å¤±çœŸæ•ˆæœ"""
        return np.tanh(audio * (1 + factor))

    def _add_tremolo(
        self, audio: np.ndarray, rate: float, depth: float
    ) -> np.ndarray:
        """æ·»åŠ é¡«éŸ³æ•ˆæœ"""
        t = np.arange(len(audio)) / 22050
        tremolo = 1 + depth * np.sin(2 * np.pi * rate * t)
        return audio * tremolo

    async def analyze_emotion(self, audio_data: bytes) -> Dict:
        """åˆ†æéŸ³è¨Šä¸­çš„æƒ…æ„Ÿ"""
        try:
            if not self.emotion_classifier:
                return {"emotion": "neutral", "confidence": 0.0}

            # å°‡ bytes è½‰æ›ç‚ºè‡¨æ™‚æ–‡ä»¶
            with io.NamedTemporaryFile(suffix=".wav") as temp_file:
                temp_file.write(audio_data)
                temp_file.flush()

                # åˆ†ææƒ…æ„Ÿ
                emotion_result = self.emotion_classifier.predict_emotion(
                    temp_file.name
                )

                return {
                    "emotion": emotion_result.get("emotion", "neutral"),
                    "confidence": emotion_result.get("confidence", 0.0),
                    "all_emotions": emotion_result.get("all_emotions", {}),
                }

        except Exception as e:
            logger.error("æƒ…æ„Ÿåˆ†æå¤±æ•—", error=str(e))
            return {"emotion": "neutral", "confidence": 0.0}

    async def get_voice_characteristics(self, audio_data: bytes) -> Dict:
        """åˆ†æèªéŸ³ç‰¹å¾µ"""
        try:
            audio_array, sample_rate = sf.read(io.BytesIO(audio_data))

            # åŸºæœ¬ç‰¹å¾µ
            features = {
                "duration": len(audio_array) / sample_rate,
                "sample_rate": sample_rate,
                "amplitude": {
                    "mean": float(np.mean(np.abs(audio_array))),
                    "max": float(np.max(np.abs(audio_array))),
                    "std": float(np.std(audio_array)),
                },
            }

            # éŸ³èª¿ç‰¹å¾µ
            pitches, magnitudes = librosa.piptrack(
                y=audio_array, sr=sample_rate
            )
            pitch_values = pitches[magnitudes > np.median(magnitudes)]
            if len(pitch_values) > 0:
                features["pitch"] = {
                    "mean": float(np.mean(pitch_values)),
                    "std": float(np.std(pitch_values)),
                    "range": float(
                        np.max(pitch_values) - np.min(pitch_values)
                    ),
                }

            # é »è­œç‰¹å¾µ
            mfccs = librosa.feature.mfcc(
                y=audio_array, sr=sample_rate, n_mfcc=13
            )
            features["mfcc"] = {
                "mean": mfccs.mean(axis=1).tolist(),
                "std": mfccs.std(axis=1).tolist(),
            }

            # ç¯€å¥ç‰¹å¾µ
            tempo, beats = librosa.beat.beat_track(
                y=audio_array, sr=sample_rate
            )
            features["rhythm"] = {
                "tempo": float(tempo),
                "beat_count": len(beats),
            }

            return features

        except Exception as e:
            logger.error("èªéŸ³ç‰¹å¾µåˆ†æå¤±æ•—", error=str(e))
            return {}

    def get_supported_emotions(self) -> List[str]:
        """ç²å–æ”¯æ´çš„æƒ…æ„Ÿåˆ—è¡¨"""
        return self.supported_emotions.copy()

    def get_supported_languages(self) -> List[str]:
        """ç²å–æ”¯æ´çš„èªè¨€åˆ—è¡¨"""
        return self.supported_languages.copy()
