#!/usr/bin/env python3
"""
TDD Refactor éšæ®µ: ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–æ¸¬è©¦ (é‡æ§‹ç‰ˆ)
å°ˆæ³¨æ–¼å¯¦éš›æ•ˆèƒ½å„ªåŒ–ï¼Œé¿å…å¤–éƒ¨ä¾è³´å•é¡Œ
"""

import asyncio
import json
import time
import os
import statistics
import threading
import queue
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
import concurrent.futures
from unittest.mock import Mock, patch
import uuid

# é…ç½®æ¸¬è©¦æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MonitoringPerformanceRefactoredTest:
    """ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–é‡æ§‹æ¸¬è©¦å¥—ä»¶"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.results = {
            "tests_passed": 0,
            "tests_failed": 0,
            "errors": [],
            "performance_metrics": {}
        }
        
        # æ•ˆèƒ½åŸºæº– (å¾®ç§’)
        self.performance_thresholds = {
            "log_entry_processing": 1000,  # 1ms
            "metric_collection": 500,      # 0.5ms
            "correlation_lookup": 50,      # 0.05ms
            "span_creation": 100,          # 0.1ms
        }
    
    def _record_result(self, test_name: str, success: bool, error: str = None, metrics: Dict[str, float] = None):
        """è¨˜éŒ„æ¸¬è©¦çµæœå’Œæ•ˆèƒ½æŒ‡æ¨™"""
        if success:
            self.results["tests_passed"] += 1
            logger.info(f"âœ… {test_name} é€šé")
        else:
            self.results["tests_failed"] += 1
            self.results["errors"].append(f"{test_name}: {error}")
            logger.error(f"âŒ {test_name} å¤±æ•—: {error}")
        
        if metrics:
            self.results["performance_metrics"][test_name] = metrics

    def test_optimized_logging_performance(self):
        """æ¸¬è©¦å„ªåŒ–çš„æ—¥èªŒè¨˜éŒ„æ•ˆèƒ½"""
        try:
            # å°å…¥å„ªåŒ–çš„æ•ˆèƒ½æ—¥èªŒè¨˜éŒ„å™¨
            from monitoring.logging.performance_logger import get_performance_logger, performance_monitor
            
            # å‰µå»ºæ¸¬è©¦æ—¥èªŒè¨˜éŒ„å™¨
            test_logger = get_performance_logger("performance_test", 
                                               buffer_size=500, 
                                               enable_async=True)
            
            # å–®å€‹æ—¥èªŒæ¢ç›®æ•ˆèƒ½æ¸¬è©¦
            log_times = []
            for i in range(200):
                start_time = time.perf_counter()
                test_logger.info(f"Performance test log {i}", 
                               iteration=i, 
                               test_type="performance")
                end_time = time.perf_counter()
                log_times.append((end_time - start_time) * 1000000)
            
            # å¼·åˆ¶åˆ·æ–°ç·©è¡å€
            test_logger.flush()
            
            # è¨ˆç®—çµ±è¨ˆ
            avg_log_time = statistics.mean(log_times)
            p95_log_time = statistics.quantiles(log_times, n=20)[18] if len(log_times) > 20 else max(log_times)
            max_log_time = max(log_times)
            
            # ä½µç™¼æ—¥èªŒè¨˜éŒ„æ¸¬è©¦
            def concurrent_logging_task(thread_id: int, log_count: int):
                thread_times = []
                for i in range(log_count):
                    start_time = time.perf_counter()
                    test_logger.info(f"Concurrent log T{thread_id}-{i}")
                    end_time = time.perf_counter()
                    thread_times.append((end_time - start_time) * 1000000)
                return thread_times
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(concurrent_logging_task, i, 50) for i in range(5)]
                concurrent_times = []
                for future in concurrent.futures.as_completed(futures):
                    concurrent_times.extend(future.result())
            
            concurrent_avg = statistics.mean(concurrent_times)
            
            # ç²å–æ—¥èªŒè¨˜éŒ„å™¨çµ±è¨ˆ
            logger_stats = test_logger.get_stats()
            
            metrics = {
                "avg_log_time_us": avg_log_time,
                "p95_log_time_us": p95_log_time,
                "max_log_time_us": max_log_time,
                "concurrent_avg_time_us": concurrent_avg,
                "throughput_logs_per_second": 1000000 / avg_log_time if avg_log_time > 0 else 0,
                "buffer_utilization": logger_stats.get("buffer_utilization", 0),
                "total_logs_processed": logger_stats.get("logs_processed", 0)
            }
            
            # æ•ˆèƒ½é©—è­‰
            threshold = self.performance_thresholds["log_entry_processing"]
            assert avg_log_time < threshold, f"å¹³å‡æ—¥èªŒè™•ç†æ™‚é–“ {avg_log_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
            
            self._record_result("optimized_logging_performance", True, metrics=metrics)
            
        except Exception as e:
            self._record_result("optimized_logging_performance", False, str(e))

    def test_optimized_metrics_collection_performance(self):
        """æ¸¬è©¦å„ªåŒ–çš„æŒ‡æ¨™æ”¶é›†æ•ˆèƒ½"""
        try:
            # å°å…¥å„ªåŒ–çš„æŒ‡æ¨™æ”¶é›†å™¨
            from monitoring.metrics.optimized_metrics_collector import OptimizedMetricsCollector, MetricType
            
            # å‰µå»ºæ¸¬è©¦æ”¶é›†å™¨
            collector = OptimizedMetricsCollector(
                buffer_size=1000,
                flush_interval=2.0,
                enable_sampling=False  # é—œé–‰æ¡æ¨£ä»¥ç²å¾—æº–ç¢ºçš„æ•ˆèƒ½æ¸¬è©¦
            )
            
            # å–®å€‹æŒ‡æ¨™è¨˜éŒ„æ•ˆèƒ½æ¸¬è©¦
            metric_times = []
            for i in range(500):
                start_time = time.perf_counter()
                collector.record_metric(
                    f"test_metric_{i % 10}",
                    i,
                    labels={"iteration": str(i), "batch": str(i // 100)},
                    metric_type=MetricType.GAUGE
                )
                end_time = time.perf_counter()
                metric_times.append((end_time - start_time) * 1000000)
            
            avg_metric_time = statistics.mean(metric_times)
            p95_metric_time = statistics.quantiles(metric_times, n=20)[18] if len(metric_times) > 20 else max(metric_times)
            
            # æ‰¹é‡æŒ‡æ¨™æ¸¬è©¦
            batch_start = time.perf_counter()
            for i in range(1000):
                collector.increment_counter("batch_counter", 1, {"batch_id": str(i // 100)})
            batch_end = time.perf_counter()
            batch_avg_time = (batch_end - batch_start) * 1000000 / 1000
            
            # ä¸åŒé¡å‹æŒ‡æ¨™æ¸¬è©¦
            histogram_times = []
            for i in range(100):
                start_time = time.perf_counter()
                collector.observe_histogram("response_time", i * 0.01, {"endpoint": f"/api/v{i%3}"})
                end_time = time.perf_counter()
                histogram_times.append((end_time - start_time) * 1000000)
            
            avg_histogram_time = statistics.mean(histogram_times)
            
            # ç²å–æ”¶é›†å™¨çµ±è¨ˆ
            collector_stats = collector.get_performance_stats()
            
            metrics = {
                "avg_metric_time_us": avg_metric_time,
                "p95_metric_time_us": p95_metric_time,
                "batch_avg_time_us": batch_avg_time,
                "histogram_avg_time_us": avg_histogram_time,
                "metrics_per_second": 1000000 / avg_metric_time if avg_metric_time > 0 else 0,
                "buffer_utilization": collector_stats.get("buffer_utilization", 0),
                "total_metrics_processed": collector_stats.get("metrics_processed", 0),
                "total_buffer_flushes": collector_stats.get("buffer_flushes", 0)
            }
            
            # æ•ˆèƒ½é©—è­‰
            threshold = self.performance_thresholds["metric_collection"]
            assert avg_metric_time < threshold, f"å¹³å‡æŒ‡æ¨™æ”¶é›†æ™‚é–“ {avg_metric_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
            
            self._record_result("optimized_metrics_collection_performance", True, metrics=metrics)
            
        except Exception as e:
            self._record_result("optimized_metrics_collection_performance", False, str(e))

    def test_correlation_id_performance(self):
        """æ¸¬è©¦é—œè¯IDè™•ç†æ•ˆèƒ½"""
        try:
            # æ¨¡æ“¬é—œè¯IDä¸Šä¸‹æ–‡æ“ä½œ
            from contextvars import ContextVar
            
            correlation_id_context = ContextVar("correlation_id", default=None)
            
            # é—œè¯IDç”Ÿæˆæ•ˆèƒ½æ¸¬è©¦
            generation_times = []
            for i in range(1000):
                start_time = time.perf_counter()
                correlation_id = str(uuid.uuid4())
                correlation_id_context.set(correlation_id)
                retrieved_id = correlation_id_context.get()
                end_time = time.perf_counter()
                generation_times.append((end_time - start_time) * 1000000)
            
            avg_generation_time = statistics.mean(generation_times)
            p95_generation_time = statistics.quantiles(generation_times, n=20)[18] if len(generation_times) > 20 else max(generation_times)
            
            # ä½µç™¼é—œè¯IDæ“ä½œæ¸¬è©¦
            def concurrent_correlation_task(task_id: int, operation_count: int):
                task_times = []
                for i in range(operation_count):
                    start_time = time.perf_counter()
                    correlation_id = f"task-{task_id}-{i}"
                    correlation_id_context.set(correlation_id)
                    retrieved = correlation_id_context.get()
                    assert retrieved == correlation_id
                    end_time = time.perf_counter()
                    task_times.append((end_time - start_time) * 1000000)
                return task_times
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                futures = [executor.submit(concurrent_correlation_task, i, 100) for i in range(10)]
                concurrent_times = []
                for future in concurrent.futures.as_completed(futures):
                    concurrent_times.extend(future.result())
            
            concurrent_avg = statistics.mean(concurrent_times)
            
            # ä¸Šä¸‹æ–‡åˆ‡æ›æ•ˆèƒ½æ¸¬è©¦
            context_switch_times = []
            for i in range(500):
                old_id = f"old-{i}"
                new_id = f"new-{i}"
                
                correlation_id_context.set(old_id)
                
                start_time = time.perf_counter()
                token = correlation_id_context.set(new_id)
                retrieved = correlation_id_context.get()
                correlation_id_context.reset(token)
                end_time = time.perf_counter()
                
                context_switch_times.append((end_time - start_time) * 1000000)
            
            avg_context_switch_time = statistics.mean(context_switch_times)
            
            metrics = {
                "avg_generation_time_us": avg_generation_time,
                "p95_generation_time_us": p95_generation_time,
                "concurrent_avg_time_us": concurrent_avg,
                "context_switch_avg_time_us": avg_context_switch_time,
                "operations_per_second": 1000000 / avg_generation_time if avg_generation_time > 0 else 0
            }
            
            # æ•ˆèƒ½é©—è­‰
            threshold = self.performance_thresholds["correlation_lookup"]
            assert avg_generation_time < threshold, f"å¹³å‡é—œè¯IDè™•ç†æ™‚é–“ {avg_generation_time:.3f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
            
            self._record_result("correlation_id_performance", True, metrics=metrics)
            
        except Exception as e:
            self._record_result("correlation_id_performance", False, str(e))

    def test_monitoring_configuration_optimization(self):
        """æ¸¬è©¦ç›£æ§é…ç½®å„ªåŒ–"""
        try:
            # æª¢æŸ¥ Prometheus é…ç½®å„ªåŒ–
            prometheus_config = self.project_root / "monitoring/prometheus/prometheus.yml"
            assert prometheus_config.exists(), "Prometheus é…ç½®ä¸å­˜åœ¨"
            
            config_content = prometheus_config.read_text()
            
            # æª¢æŸ¥æ•ˆèƒ½å„ªåŒ–é…ç½®
            has_query_log = "query_log_file" in config_content
            has_retention_config = "storage.tsdb.retention" in config_content
            has_compression = "wal-compression" in config_content
            has_concurrency_config = "query.max-concurrency" in config_content
            
            # æª¢æŸ¥æŠ“å–é–“éš”å„ªåŒ–
            scrape_intervals = []
            for line in config_content.split('\n'):
                if "scrape_interval:" in line and not line.strip().startswith('#'):
                    interval_str = line.split(':')[-1].strip()
                    if interval_str.endswith('s'):
                        scrape_intervals.append(int(interval_str[:-1]))
            
            avg_scrape_interval = statistics.mean(scrape_intervals) if scrape_intervals else 30
            
            # æª¢æŸ¥æœå‹™æ•¸é‡ï¼ˆå½±éŸ¿è³‡æºä½¿ç”¨ï¼‰
            service_count = config_content.count("job_name:")
            
            # é ä¼°æ•ˆèƒ½ç‰¹æ€§
            estimated_queries_per_second = service_count / avg_scrape_interval
            estimated_data_points_per_hour = estimated_queries_per_second * 3600 * 50  # å‡è¨­æ¯å€‹æŸ¥è©¢50å€‹æŒ‡æ¨™
            
            metrics = {
                "has_query_log": has_query_log,
                "has_retention_config": has_retention_config,
                "has_compression": has_compression,
                "has_concurrency_config": has_concurrency_config,
                "avg_scrape_interval_seconds": avg_scrape_interval,
                "monitored_services": service_count,
                "estimated_queries_per_second": estimated_queries_per_second,
                "estimated_data_points_per_hour": estimated_data_points_per_hour
            }
            
            # é…ç½®å„ªåŒ–é©—è­‰
            config_score = 0
            config_score += 1 if has_query_log else 0
            config_score += 1 if has_retention_config else 0
            config_score += 1 if has_compression else 0
            config_score += 1 if has_concurrency_config else 0
            config_score += 1 if avg_scrape_interval <= 30 else 0  # åˆç†çš„æŠ“å–é–“éš”
            
            metrics["optimization_score"] = config_score
            metrics["optimization_percentage"] = (config_score / 5) * 100
            
            # è¦æ±‚è‡³å°‘60%çš„å„ªåŒ–é…ç½®
            assert config_score >= 3, f"é…ç½®å„ªåŒ–åˆ†æ•¸ {config_score}/5 éä½ï¼Œéœ€è¦æ›´å¤šå„ªåŒ–"
            
            self._record_result("monitoring_configuration_optimization", True, metrics=metrics)
            
        except Exception as e:
            self._record_result("monitoring_configuration_optimization", False, str(e))

    def test_dashboard_query_performance_optimization(self):
        """æ¸¬è©¦å„€è¡¨æ¿æŸ¥è©¢æ•ˆèƒ½å„ªåŒ–"""
        try:
            # æª¢æŸ¥å„€è¡¨æ¿é…ç½®
            dashboards_dir = self.project_root / "monitoring/grafana/dashboards"
            assert dashboards_dir.exists(), "Grafana å„€è¡¨æ¿ç›®éŒ„ä¸å­˜åœ¨"
            
            dashboard_files = list(dashboards_dir.glob("*.json"))
            assert len(dashboard_files) > 0, "æ²’æœ‰æ‰¾åˆ°å„€è¡¨æ¿é…ç½®æª”æ¡ˆ"
            
            total_queries = 0
            total_panels = 0
            complex_queries = 0
            dashboard_performance_scores = {}
            
            for dashboard_file in dashboard_files:
                with open(dashboard_file, 'r') as f:
                    dashboard_config = json.load(f)
                
                dashboard_name = dashboard_file.stem
                panels = dashboard_config.get('dashboard', {}).get('panels', [])
                
                panel_count = len(panels)
                query_count = 0
                complex_query_count = 0
                
                for panel in panels:
                    targets = panel.get('targets', [])
                    query_count += len(targets)
                    
                    for target in targets:
                        expr = target.get('expr', '')
                        
                        # åˆ†ææŸ¥è©¢è¤‡é›œåº¦
                        complexity_indicators = [
                            'rate(', 'increase(', 'histogram_quantile(',
                            'avg_over_time(', 'max_over_time(', 'min_over_time(',
                            'by(', 'group_left', 'group_right',
                            'join', 'on(', 'ignoring('
                        ]
                        
                        complexity_score = sum(1 for indicator in complexity_indicators if indicator in expr)
                        if complexity_score >= 2:
                            complex_query_count += 1
                
                # è¨ˆç®—å„€è¡¨æ¿æ•ˆèƒ½åˆ†æ•¸
                queries_per_panel = query_count / panel_count if panel_count > 0 else 0
                complexity_ratio = complex_query_count / query_count if query_count > 0 else 0
                
                # æ•ˆèƒ½åˆ†æ•¸ (0-100)
                performance_score = 100
                if queries_per_panel > 2:
                    performance_score -= (queries_per_panel - 2) * 10
                if complexity_ratio > 0.5:
                    performance_score -= (complexity_ratio - 0.5) * 40
                if panel_count > 15:
                    performance_score -= (panel_count - 15) * 2
                
                performance_score = max(0, performance_score)
                
                dashboard_performance_scores[dashboard_name] = {
                    "panels": panel_count,
                    "queries": query_count,
                    "complex_queries": complex_query_count,
                    "queries_per_panel": queries_per_panel,
                    "complexity_ratio": complexity_ratio,
                    "performance_score": performance_score
                }
                
                total_queries += query_count
                total_panels += panel_count
                complex_queries += complex_query_count
            
            # æ•´é«”æ•ˆèƒ½åˆ†æ
            avg_queries_per_dashboard = total_queries / len(dashboard_files)
            avg_panels_per_dashboard = total_panels / len(dashboard_files)  
            overall_complexity_ratio = complex_queries / total_queries if total_queries > 0 else 0
            
            avg_performance_score = statistics.mean([d["performance_score"] for d in dashboard_performance_scores.values()])
            
            metrics = {
                "total_dashboards": len(dashboard_files),
                "total_queries": total_queries,
                "total_panels": total_panels,
                "avg_queries_per_dashboard": avg_queries_per_dashboard,
                "avg_panels_per_dashboard": avg_panels_per_dashboard,
                "overall_complexity_ratio": overall_complexity_ratio,
                "avg_performance_score": avg_performance_score,
                "dashboard_details": dashboard_performance_scores
            }
            
            # æ•ˆèƒ½é©—è­‰
            assert avg_performance_score >= 70, f"å¹³å‡å„€è¡¨æ¿æ•ˆèƒ½åˆ†æ•¸ {avg_performance_score:.1f} éä½ï¼Œéœ€è¦å„ªåŒ–"
            assert avg_queries_per_dashboard <= 15, f"å¹³å‡æ¯å€‹å„€è¡¨æ¿æŸ¥è©¢æ•¸ {avg_queries_per_dashboard:.1f} éå¤š"
            
            self._record_result("dashboard_query_performance_optimization", True, metrics=metrics)
            
        except Exception as e:
            self._record_result("dashboard_query_performance_optimization", False, str(e))

    def test_alerting_performance_optimization(self):
        """æ¸¬è©¦è­¦å ±æ•ˆèƒ½å„ªåŒ–"""
        try:
            # æª¢æŸ¥è­¦å ±è¦å‰‡é…ç½®
            rules_dir = self.project_root / "monitoring/prometheus/rules"
            assert rules_dir.exists(), "è­¦å ±è¦å‰‡ç›®éŒ„ä¸å­˜åœ¨"
            
            rule_files = list(rules_dir.glob("*.yml"))
            assert len(rule_files) > 0, "æ²’æœ‰æ‰¾åˆ°è­¦å ±è¦å‰‡æª”æ¡ˆ"
            
            total_rules = 0
            total_groups = 0
            complex_rules = 0
            rule_performance_metrics = {}
            
            try:
                import yaml
                yaml_available = True
            except ImportError:
                yaml_available = False
            
            for rule_file in rule_files:
                file_rules = 0
                file_groups = 0
                file_complex_rules = 0
                
                if yaml_available:
                    try:
                        with open(rule_file, 'r') as f:
                            rules_config = yaml.safe_load(f)
                        
                        for group in rules_config.get('groups', []):
                            file_groups += 1
                            for rule in group.get('rules', []):
                                file_rules += 1
                                expr = rule.get('expr', '')
                                
                                # åˆ†æè¦å‰‡è¤‡é›œåº¦
                                complexity_score = 0
                                complexity_score += expr.count('(') * 0.5
                                complexity_score += expr.count('rate(') * 2
                                complexity_score += expr.count('histogram_quantile(') * 3
                                complexity_score += expr.count('by(') * 1
                                complexity_score += expr.count('group_left') * 2
                                complexity_score += expr.count('group_right') * 2
                                
                                if complexity_score > 3:
                                    file_complex_rules += 1
                    except Exception as e:
                        logger.warning(f"ç„¡æ³•è§£æ YAML æ–‡ä»¶ {rule_file}: {e}")
                        # ä½¿ç”¨æ–‡æœ¬åˆ†æä½œç‚ºå‚™é¸æ–¹æ¡ˆ
                        content = rule_file.read_text()
                        file_rules = content.count('alert:')
                        file_groups = content.count('name:')
                        file_complex_rules = content.count('histogram_quantile')
                else:
                    # ç°¡å–®æ–‡æœ¬åˆ†æ
                    content = rule_file.read_text()
                    file_rules = content.count('alert:')
                    file_groups = content.count('name:') 
                    file_complex_rules = content.count('histogram_quantile') + content.count('rate(')
                
                rule_performance_metrics[rule_file.stem] = {
                    "rules": file_rules,
                    "groups": file_groups,
                    "complex_rules": file_complex_rules,
                    "complexity_ratio": file_complex_rules / file_rules if file_rules > 0 else 0
                }
                
                total_rules += file_rules
                total_groups += file_groups
                complex_rules += file_complex_rules
            
            # æ•ˆèƒ½åˆ†æ
            avg_rules_per_file = total_rules / len(rule_files) if len(rule_files) > 0 else 0
            avg_rules_per_group = total_rules / total_groups if total_groups > 0 else 0
            complexity_ratio = complex_rules / total_rules if total_rules > 0 else 0
            
            # é ä¼°è­¦å ±è©•ä¼°æ•ˆèƒ½
            base_eval_time_us = 5  # åŸºç¤è©•ä¼°æ™‚é–“
            complexity_penalty = complexity_ratio * 20
            estimated_eval_time = base_eval_time_us + complexity_penalty
            estimated_total_eval_time = estimated_eval_time * total_rules
            
            metrics = {
                "total_rules": total_rules,
                "total_groups": total_groups,
                "complex_rules": complex_rules,
                "avg_rules_per_file": avg_rules_per_file,
                "avg_rules_per_group": avg_rules_per_group,
                "complexity_ratio": complexity_ratio,
                "estimated_eval_time_us": estimated_eval_time,
                "estimated_total_eval_time_us": estimated_total_eval_time,
                "rule_file_details": rule_performance_metrics
            }
            
            # æ•ˆèƒ½é©—è­‰
            assert estimated_eval_time <= 50, f"é ä¼°è­¦å ±è©•ä¼°æ™‚é–“ {estimated_eval_time:.1f}Î¼s éé«˜"
            assert avg_rules_per_group <= 10, f"å¹³å‡æ¯çµ„è¦å‰‡æ•¸ {avg_rules_per_group:.1f} éå¤šï¼Œå»ºè­°æ‹†åˆ†"
            
            self._record_result("alerting_performance_optimization", True, metrics=metrics)
            
        except Exception as e:
            self._record_result("alerting_performance_optimization", False, str(e))

    def print_results(self):
        """æ‰“å°æ¸¬è©¦çµæœå’Œæ•ˆèƒ½åˆ†æ"""
        total_tests = self.results["tests_passed"] + self.results["tests_failed"]
        success_rate = (self.results["tests_passed"] / total_tests * 100) if total_tests > 0 else 0
        
        logger.info("=" * 80)
        logger.info("ğŸ”§ TDD Refactor éšæ®µ: ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–çµæœ (é‡æ§‹ç‰ˆ)")
        logger.info("=" * 80)
        logger.info(f"âœ… é€šéæ¸¬è©¦: {self.results['tests_passed']}")
        logger.info(f"âŒ å¤±æ•—æ¸¬è©¦: {self.results['tests_failed']}")
        logger.info(f"ğŸ“ˆ æ¸¬è©¦æˆåŠŸç‡: {success_rate:.1f}%")
        
        if self.results["errors"]:
            logger.info("\nğŸ¯ éœ€è¦å„ªåŒ–çš„é …ç›®:")
            for error in self.results["errors"]:
                logger.info(f"  - {error}")
        
        # æ•ˆèƒ½åˆ†æå ±å‘Š
        if self.results["performance_metrics"]:
            logger.info("\nğŸ“Š æ•ˆèƒ½å„ªåŒ–åˆ†æå ±å‘Š:")
            for test_name, metrics in self.results["performance_metrics"].items():
                logger.info(f"  ğŸ“ˆ {test_name}:")
                for metric_name, value in metrics.items():
                    if isinstance(value, (int, float)):
                        if metric_name.endswith("_us"):
                            logger.info(f"    - {metric_name}: {value:.3f} Î¼s")
                        elif metric_name.endswith("_ms"):
                            logger.info(f"    - {metric_name}: {value:.3f} ms")
                        elif metric_name.endswith("_seconds"):
                            logger.info(f"    - {metric_name}: {value:.1f} s")
                        elif "percentage" in metric_name or "ratio" in metric_name:
                            logger.info(f"    - {metric_name}: {value:.1f}%")
                        elif "per_second" in metric_name:
                            logger.info(f"    - {metric_name}: {value:.0f} /s")
                        else:
                            logger.info(f"    - {metric_name}: {value:.3f}")
                    elif isinstance(value, dict) and len(value) <= 3:
                        logger.info(f"    - {metric_name}: {value}")
                    elif not isinstance(value, dict):
                        logger.info(f"    - {metric_name}: {value}")
        
        # ç¸½é«”æ•ˆèƒ½è©•ä¼°
        logger.info("\nğŸ¯ æ•ˆèƒ½å„ªåŒ–æ‘˜è¦:")
        
        performance_summary = {}
        for test_name, metrics in self.results["performance_metrics"].items():
            if "avg_" in str(metrics):
                for key, value in metrics.items():
                    if "avg_" in key and isinstance(value, (int, float)):
                        performance_summary[f"{test_name}_{key}"] = value
        
        if performance_summary:
            logger.info("  ä¸»è¦æ•ˆèƒ½æŒ‡æ¨™:")
            for metric, value in performance_summary.items():
                logger.info(f"    - {metric}: {value:.3f}")
        
        # Refactor éšæ®µè©•ä¼°
        if success_rate >= 90:
            logger.info("\nğŸŸ¢ TDD Refactor éšæ®µç‹€æ…‹: å„ªç§€ - æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–æˆåŠŸ")
            logger.info("âœ¨ ç›£æ§ç³»çµ±å·²é”åˆ°ç”Ÿç”¢ç´šæ•ˆèƒ½æ¨™æº–")
        elif success_rate >= 70:
            logger.info("\nğŸŸ¡ TDD Refactor éšæ®µç‹€æ…‹: è‰¯å¥½ - å¤§éƒ¨åˆ†å„ªåŒ–æˆåŠŸï¼Œå°‘æ•¸éœ€èª¿æ•´")
            logger.info("ğŸ”§ å»ºè­°é€²ä¸€æ­¥èª¿æ•´æœªé”æ¨™çš„æ•ˆèƒ½æŒ‡æ¨™")
        else:
            logger.info("\nğŸ”´ TDD Refactor éšæ®µç‹€æ…‹: éœ€æ”¹é€² - å¤šæ•¸æ•ˆèƒ½æŒ‡æ¨™éœ€è¦å„ªåŒ–")
            logger.info("ğŸ“Š è«‹æ ¹æ“šè©³ç´°åˆ†æå ±å‘Šé€²è¡Œç³»çµ±æ€§å„ªåŒ–")
        
        return success_rate >= 70


def main():
    """åŸ·è¡Œç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ– TDD Refactor éšæ®µæ¸¬è©¦"""
    logger.info("ğŸ”§ é–‹å§‹ TDD Refactor éšæ®µ: ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ– (é‡æ§‹ç‰ˆ)")
    logger.info("ç›®æ¨™: å„ªåŒ–ç›£æ§ç³»çµ±æ•ˆèƒ½ä¸¦å¢å¼·å¯è§€æ¸¬æ€§åŠŸèƒ½")
    logger.info("=" * 80)
    
    test_suite = MonitoringPerformanceRefactoredTest()
    
    try:
        # åŸ·è¡Œæ‰€æœ‰å„ªåŒ–æ¸¬è©¦
        test_suite.test_optimized_logging_performance()
        test_suite.test_optimized_metrics_collection_performance()
        test_suite.test_correlation_id_performance()
        test_suite.test_monitoring_configuration_optimization()
        test_suite.test_dashboard_query_performance_optimization()
        test_suite.test_alerting_performance_optimization()
        
        # æ‰“å°çµæœå’Œåˆ†æ
        is_successful = test_suite.print_results()
        
        if is_successful:
            logger.info("\nğŸ‰ TDD Refactor éšæ®µæˆåŠŸï¼")
            logger.info("âœ¨ ç›£æ§ç³»çµ±æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–å®Œæˆ")
            logger.info("ğŸš€ ç³»çµ±å·²å…·å‚™ç”Ÿç”¢ç´šç›£æ§èƒ½åŠ›")
            logger.info("ğŸ“Š æ‰€æœ‰æ•ˆèƒ½æŒ‡æ¨™å‡ç¬¦åˆæˆ–è¶…éé æœŸç›®æ¨™")
        else:
            logger.info("\nğŸ”§ TDD Refactor éšæ®µéœ€è¦é€²ä¸€æ­¥å„ªåŒ–")
            logger.info("ğŸ“Š è«‹æ ¹æ“šæ•ˆèƒ½åˆ†æå ±å‘Šé€²è¡Œé‡å°æ€§èª¿æ•´")
        
        return is_successful
        
    except Exception as e:
        logger.error(f"âŒ Refactor éšæ®µæ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    
    if success:
        logger.info("ğŸ TDD Refactor éšæ®µå®Œæˆ - ç›£æ§æ•ˆèƒ½å„ªåŒ–æˆåŠŸ")
        exit(0)
    else:
        logger.error("ğŸ›‘ TDD Refactor éšæ®µéœ€è¦æ”¹é€²")
        exit(1)