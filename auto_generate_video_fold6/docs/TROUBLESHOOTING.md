# ğŸ”§ Auto Video æ•…éšœæ’é™¤æŒ‡å—

## ğŸ“– æ¦‚è¦½

æœ¬æŒ‡å—æä¾› Auto Video ç³»çµ±å¸¸è¦‹å•é¡Œçš„è¨ºæ–·å’Œè§£æ±ºæ–¹æ¡ˆï¼Œå¹«åŠ©é–‹ç™¼è€…å’Œé‹ç¶­äººå“¡å¿«é€Ÿå®šä½ä¸¦è§£æ±ºç³»çµ±å•é¡Œã€‚

## ğŸ¯ å¿«é€Ÿè¨ºæ–·

### ç³»çµ±å¥åº·æª¢æŸ¥æ¸…å–®

åœ¨æ·±å…¥ç‰¹å®šå•é¡Œä¹‹å‰ï¼Œè«‹å…ˆåŸ·è¡Œä»¥ä¸‹åŸºæœ¬æª¢æŸ¥ï¼š

```bash
# 1. æª¢æŸ¥æ‰€æœ‰æœå‹™ç‹€æ…‹
docker-compose ps

# 2. æª¢æŸ¥ç³»çµ±è³‡æº
df -h                    # ç£ç¢Ÿç©ºé–“
free -h                  # è¨˜æ†¶é«”ä½¿ç”¨
htop                     # CPU å’Œç¨‹åºç‹€æ…‹

# 3. æª¢æŸ¥ç¶²è·¯é€£æ¥
curl -f http://localhost:8000/health

# 4. æª¢æŸ¥æ—¥èªŒ
docker-compose logs --tail=100 -f
```

### æœå‹™å¥åº·ç‹€æ…‹å¿«é€Ÿæª¢æŸ¥

```bash
#!/bin/bash
# scripts/quick-health-check.sh

echo "ğŸ” åŸ·è¡Œç³»çµ±å¥åº·æª¢æŸ¥..."

# æª¢æŸ¥æ ¸å¿ƒæœå‹™
services=("api-gateway" "auth-service" "video-service" "ai-service" "postgres" "redis")

for service in "${services[@]}"; do
    if docker-compose ps $service | grep -q "Up"; then
        echo "âœ… $service: æ­£å¸¸é‹è¡Œ"
    else
        echo "âŒ $service: ç‹€æ…‹ç•°å¸¸"
        docker-compose logs --tail=10 $service
    fi
done

# æª¢æŸ¥ç«¯é»å¯ç”¨æ€§
endpoints=(
    "http://localhost:8000/health:API Gateway"
    "http://localhost:8001/health:Auth Service"
    "http://localhost:8004/health:Video Service"
    "http://localhost:3000:Frontend"
)

for endpoint_info in "${endpoints[@]}"; do
    endpoint=$(echo $endpoint_info | cut -d: -f1)
    name=$(echo $endpoint_info | cut -d: -f2)
    
    if curl -s -f $endpoint > /dev/null; then
        echo "âœ… $name: ç«¯é»å¯è¨ªå•"
    else
        echo "âŒ $name: ç«¯é»ä¸å¯è¨ªå• ($endpoint)"
    fi
done

# æª¢æŸ¥è³‡æ–™åº«é€£æ¥
if docker-compose exec postgres pg_isready -U autovideo > /dev/null 2>&1; then
    echo "âœ… PostgreSQL: è³‡æ–™åº«é€£æ¥æ­£å¸¸"
else
    echo "âŒ PostgreSQL: è³‡æ–™åº«é€£æ¥å¤±æ•—"
fi

# æª¢æŸ¥ Redis é€£æ¥
if docker-compose exec redis redis-cli ping | grep -q "PONG"; then
    echo "âœ… Redis: å¿«å–æœå‹™æ­£å¸¸"
else
    echo "âŒ Redis: å¿«å–æœå‹™ç•°å¸¸"
fi
```

## ğŸš¨ å¸¸è¦‹å•é¡Œåˆ†é¡

### 1. æœå‹™å•Ÿå‹•å•é¡Œ

#### 1.1 å®¹å™¨ç„¡æ³•å•Ÿå‹•

**ç—‡ç‹€**ï¼š
- `docker-compose up` å¤±æ•—
- å®¹å™¨ç‹€æ…‹é¡¯ç¤º `Exited`
- æœå‹™ç„¡æ³•è¨ªå•

**è¨ºæ–·æ­¥é©Ÿ**ï¼š
```bash
# æŸ¥çœ‹å®¹å™¨ç‹€æ…‹
docker-compose ps

# æŸ¥çœ‹å¤±æ•—å®¹å™¨çš„æ—¥èªŒ
docker-compose logs service-name

# æª¢æŸ¥è³‡æºä½¿ç”¨
docker stats

# æª¢æŸ¥ç£ç¢Ÿç©ºé–“
df -h
```

**å¸¸è¦‹åŸå› å’Œè§£æ±ºæ–¹æ¡ˆ**ï¼š

| éŒ¯èª¤é¡å‹ | ç—‡ç‹€ | è§£æ±ºæ–¹æ¡ˆ |
|----------|------|----------|
| ç«¯å£è¡çª | `Port already in use` | æ›´æ”¹ç«¯å£é…ç½®æˆ–åœæ­¢è¡çªç¨‹åº |
| è¨˜æ†¶é«”ä¸è¶³ | `OOMKilled` | å¢åŠ ç³»çµ±è¨˜æ†¶é«”æˆ–èª¿æ•´å®¹å™¨é™åˆ¶ |
| ç£ç¢Ÿç©ºé–“ä¸è¶³ | `No space left on device` | æ¸…ç†ç£ç¢Ÿç©ºé–“æˆ–æ›è¼‰æ›´å¤§å­˜å„² |
| ç’°å¢ƒè®Šæ•¸éŒ¯èª¤ | `Config error` | æª¢æŸ¥ `.env` æ–‡ä»¶é…ç½® |
| æ˜ åƒå•é¡Œ | `Image not found` | é‡æ–°æ§‹å»ºæ˜ åƒæˆ–æª¢æŸ¥æ˜ åƒåç¨± |

**è§£æ±ºç¤ºä¾‹**ï¼š
```bash
# ç«¯å£è¡çªè§£æ±º
sudo lsof -i :8000                    # æ‰¾å‡ºä½”ç”¨ç«¯å£çš„ç¨‹åº
sudo kill -9 PID                      # çµ‚æ­¢ä½”ç”¨ç¨‹åº
# æˆ–ä¿®æ”¹ docker-compose.yml ä¸­çš„ç«¯å£æ˜ å°„

# ç£ç¢Ÿç©ºé–“æ¸…ç†
docker system prune -a                # æ¸…ç†æœªä½¿ç”¨çš„ Docker è³‡æº
docker volume prune                   # æ¸…ç†æœªä½¿ç”¨çš„å·
sudo apt clean                        # æ¸…ç†ç³»çµ±å¿«å–
```

#### 1.2 è³‡æ–™åº«é€£æ¥å¤±æ•—

**ç—‡ç‹€**ï¼š
- æ‡‰ç”¨å•Ÿå‹•æ™‚è³‡æ–™åº«é€£æ¥å¤±æ•—
- `Connection refused` æˆ– `Connection timeout` éŒ¯èª¤
- æœå‹™å¥åº·æª¢æŸ¥å¤±æ•—

**è¨ºæ–·æ­¥é©Ÿ**ï¼š
```bash
# æª¢æŸ¥è³‡æ–™åº«å®¹å™¨ç‹€æ…‹
docker-compose logs postgres

# æ¸¬è©¦è³‡æ–™åº«é€£æ¥
docker-compose exec postgres pg_isready -U autovideo

# æª¢æŸ¥ç¶²è·¯é€£æ¥
docker network ls
docker network inspect autovideo_default

# é©—è­‰è³‡æ–™åº«é…ç½®
docker-compose exec postgres psql -U autovideo -d autovideo_prod -c "SELECT version();"
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```bash
# é‡å•Ÿè³‡æ–™åº«æœå‹™
docker-compose restart postgres

# æª¢æŸ¥è³‡æ–™åº«æ—¥èªŒ
docker-compose logs postgres --tail=50

# é‡æ–°åˆå§‹åŒ–è³‡æ–™åº«ï¼ˆåƒ…é–‹ç™¼ç’°å¢ƒï¼‰
docker-compose down
docker volume rm autovideo_postgres_data
docker-compose up -d postgres
# ç­‰å¾…è³‡æ–™åº«å•Ÿå‹•
sleep 30
# åŸ·è¡Œé·ç§»
docker-compose exec api-gateway alembic upgrade head
```

### 2. æ•ˆèƒ½å•é¡Œ

#### 2.1 API å›æ‡‰æ™‚é–“éé•·

**ç—‡ç‹€**ï¼š
- API è«‹æ±‚è¶…æ™‚
- å›æ‡‰æ™‚é–“ > 5 ç§’
- ç”¨æˆ¶é«”é©—ç·©æ…¢

**è¨ºæ–·æ­¥é©Ÿ**ï¼š
```bash
# æª¢æŸ¥ API å›æ‡‰æ™‚é–“
curl -w "@curl-format.txt" -o /dev/null -s "http://localhost:8000/api/v1/videos"

# curl-format.txt å…§å®¹ï¼š
#     time_namelookup:  %{time_namelookup}s\n
#        time_connect:  %{time_connect}s\n
#     time_appconnect:  %{time_appconnect}s\n
#    time_pretransfer:  %{time_pretransfer}s\n
#       time_redirect:  %{time_redirect}s\n
#  time_starttransfer:  %{time_starttransfer}s\n
#                     ----------\n
#          time_total:  %{time_total}s\n

# æª¢æŸ¥è³‡æ–™åº«æŸ¥è©¢æ•ˆèƒ½
docker-compose exec postgres psql -U autovideo -d autovideo_prod -c "
SELECT query, calls, total_time, mean_time, rows
FROM pg_stat_statements 
ORDER BY mean_time DESC 
LIMIT 10;"

# æª¢æŸ¥ç³»çµ±è² è¼‰
htop
iostat -x 1
```

**æ•ˆèƒ½åˆ†æå·¥å…·**ï¼š
```python
# å»ºç«‹æ•ˆèƒ½åˆ†æè…³æœ¬ scripts/performance-analysis.py
import asyncio
import aiohttp
import time
import statistics
from concurrent.futures import ThreadPoolExecutor

async def benchmark_endpoint(url: str, num_requests: int = 100):
    """åŸºæº–æ¸¬è©¦ API ç«¯é»"""
    response_times = []
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        
        for _ in range(num_requests):
            tasks.append(measure_request(session, url))
        
        results = await asyncio.gather(*tasks)
        response_times = [r for r in results if r is not None]
    
    if response_times:
        print(f"\nğŸ“Š {url} æ•ˆèƒ½åˆ†æ:")
        print(f"ç¸½è«‹æ±‚æ•¸: {len(response_times)}")
        print(f"æˆåŠŸç‡: {len(response_times)/num_requests*100:.1f}%")
        print(f"å¹³å‡å›æ‡‰æ™‚é–“: {statistics.mean(response_times):.3f}s")
        print(f"ä¸­ä½æ•¸å›æ‡‰æ™‚é–“: {statistics.median(response_times):.3f}s")
        print(f"95% åˆ†ä½æ•¸: {statistics.quantiles(response_times, n=20)[18]:.3f}s")
        print(f"æœ€å¤§å›æ‡‰æ™‚é–“: {max(response_times):.3f}s")
        print(f"æœ€å°å›æ‡‰æ™‚é–“: {min(response_times):.3f}s")

async def measure_request(session: aiohttp.ClientSession, url: str):
    """æ¸¬é‡å–®å€‹è«‹æ±‚çš„å›æ‡‰æ™‚é–“"""
    try:
        start_time = time.time()
        async with session.get(url) as response:
            await response.text()
            return time.time() - start_time
    except Exception as e:
        print(f"è«‹æ±‚å¤±æ•—: {e}")
        return None

# ä½¿ç”¨æ–¹å¼
if __name__ == "__main__":
    endpoints = [
        "http://localhost:8000/api/v1/videos",
        "http://localhost:8000/api/v1/users/me",
        "http://localhost:8000/health"
    ]
    
    for endpoint in endpoints:
        asyncio.run(benchmark_endpoint(endpoint, 50))
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```bash
# 1. å„ªåŒ–è³‡æ–™åº«æŸ¥è©¢
# æ·»åŠ é©ç•¶çš„ç´¢å¼•
docker-compose exec postgres psql -U autovideo -d autovideo_prod -c "
CREATE INDEX CONCURRENTLY idx_videos_user_created 
ON videos(user_id, created_at DESC);"

# 2. å•Ÿç”¨æŸ¥è©¢å¿«å–
# åœ¨æ‡‰ç”¨ä¸­æ·»åŠ  Redis å¿«å–å±¤

# 3. å¢åŠ è³‡æºé™åˆ¶
# ä¿®æ”¹ docker-compose.yml
#   deploy:
#     resources:
#       limits:
#         memory: 2G
#         cpus: '1.0'

# 4. å•Ÿç”¨é€£æ¥æ± 
# èª¿æ•´è³‡æ–™åº«é€£æ¥æ± è¨­ç½®
```

#### 2.2 è¨˜æ†¶é«”æ´©æ¼

**ç—‡ç‹€**ï¼š
- è¨˜æ†¶é«”ä½¿ç”¨æŒçºŒå¢é•·
- å®¹å™¨è¢« OOM Kill
- ç³»çµ±è®Šå¾—ä¸ç©©å®š

**è¨ºæ–·æ­¥é©Ÿ**ï¼š
```bash
# ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
docker stats --no-stream

# æª¢æŸ¥ç¨‹åºè¨˜æ†¶é«”ä½¿ç”¨
docker-compose exec api-gateway top

# åˆ†æè¨˜æ†¶é«”æ´©æ¼
docker-compose exec api-gateway python -c "
import gc
import psutil
import os

process = psutil.Process(os.getpid())
print(f'è¨˜æ†¶é«”ä½¿ç”¨: {process.memory_info().rss / 1024 / 1024:.2f} MB')
print(f'æœªå›æ”¶å°è±¡æ•¸é‡: {len(gc.garbage)}')
"
```

**è¨˜æ†¶é«”æ´©æ¼åˆ†æå·¥å…·**ï¼š
```python
# scripts/memory-profiler.py
import tracemalloc
import asyncio
import gc
from typing import Dict, List

class MemoryProfiler:
    """è¨˜æ†¶é«”åˆ†æå™¨"""
    
    def __init__(self):
        self.snapshots = []
    
    def start_tracing(self):
        """é–‹å§‹è¨˜æ†¶é«”è¿½è¹¤"""
        tracemalloc.start()
        print("è¨˜æ†¶é«”è¿½è¹¤å·²å•Ÿå‹•")
    
    def take_snapshot(self, label: str = None):
        """æ‹æ”è¨˜æ†¶é«”å¿«ç…§"""
        if not tracemalloc.is_tracing():
            print("è¨˜æ†¶é«”è¿½è¹¤æœªå•Ÿå‹•")
            return
        
        snapshot = tracemalloc.take_snapshot()
        self.snapshots.append({
            'label': label or f'snapshot_{len(self.snapshots)}',
            'snapshot': snapshot,
            'timestamp': asyncio.get_event_loop().time()
        })
        
        print(f"è¨˜æ†¶é«”å¿«ç…§å·²æ‹æ”: {label}")
    
    def compare_snapshots(self, start_idx: int = 0, end_idx: int = -1):
        """æ¯”è¼ƒè¨˜æ†¶é«”å¿«ç…§"""
        if len(self.snapshots) < 2:
            print("éœ€è¦è‡³å°‘å…©å€‹å¿«ç…§é€²è¡Œæ¯”è¼ƒ")
            return
        
        start_snapshot = self.snapshots[start_idx]['snapshot']
        end_snapshot = self.snapshots[end_idx]['snapshot']
        
        top_stats = end_snapshot.compare_to(start_snapshot, 'lineno')
        
        print(f"\nè¨˜æ†¶é«”è®ŠåŒ–åˆ†æ ({self.snapshots[start_idx]['label']} -> {self.snapshots[end_idx]['label']}):")
        print("=" * 60)
        
        for index, stat in enumerate(top_stats[:10], 1):
            print(f"{index:2d}. {stat}")
    
    def get_current_memory_info(self):
        """ç²å–ç•¶å‰è¨˜æ†¶é«”è³‡è¨Š"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        return {
            'rss': memory_info.rss / 1024 / 1024,  # MB
            'vms': memory_info.vms / 1024 / 1024,  # MB
            'percent': process.memory_percent(),
            'gc_objects': len(gc.get_objects()),
            'gc_garbage': len(gc.garbage)
        }
    
    def print_memory_info(self):
        """æ‰“å°è¨˜æ†¶é«”è³‡è¨Š"""
        info = self.get_current_memory_info()
        print(f"RSS è¨˜æ†¶é«”: {info['rss']:.2f} MB")
        print(f"VMS è¨˜æ†¶é«”: {info['vms']:.2f} MB") 
        print(f"è¨˜æ†¶é«”ç™¾åˆ†æ¯”: {info['percent']:.2f}%")
        print(f"GC å°è±¡æ•¸é‡: {info['gc_objects']}")
        print(f"GC åƒåœ¾æ•¸é‡: {info['gc_garbage']}")

# ä½¿ç”¨ç¯„ä¾‹
async def profile_memory_leak():
    profiler = MemoryProfiler()
    profiler.start_tracing()
    
    # åˆå§‹å¿«ç…§
    profiler.take_snapshot("åˆå§‹ç‹€æ…‹")
    profiler.print_memory_info()
    
    # æ¨¡æ“¬ä¸€äº›æ“ä½œ
    for i in range(1000):
        # åŸ·è¡Œå¯èƒ½å°è‡´è¨˜æ†¶é«”æ´©æ¼çš„æ“ä½œ
        await some_operation()
        
        if i % 100 == 0:
            profiler.take_snapshot(f"æ“ä½œ_{i}")
            profiler.print_memory_info()
    
    # æ¯”è¼ƒå¿«ç…§
    profiler.compare_snapshots(0, -1)
```

### 3. AI æœå‹™å•é¡Œ

#### 3.1 AI API èª¿ç”¨å¤±æ•—

**ç—‡ç‹€**ï¼š
- AI ç”Ÿæˆè«‹æ±‚å¤±æ•—
- API é‡‘é‘°éŒ¯èª¤
- è«‹æ±‚é…é¡è¶…é™

**è¨ºæ–·æ­¥é©Ÿ**ï¼š
```bash
# æª¢æŸ¥ AI æœå‹™æ—¥èªŒ
docker-compose logs ai-service --tail=50

# æ¸¬è©¦ API é‡‘é‘°æœ‰æ•ˆæ€§
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/models

# æª¢æŸ¥é…é¡ä½¿ç”¨æƒ…æ³
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/usage
```

**AI æœå‹™è¨ºæ–·å·¥å…·**ï¼š
```python
# scripts/ai-service-diagnostics.py
import asyncio
import aiohttp
import os
from datetime import datetime

class AIServiceDiagnostics:
    """AI æœå‹™è¨ºæ–·å·¥å…·"""
    
    def __init__(self):
        self.providers = {
            'openai': {
                'api_key': os.getenv('OPENAI_API_KEY'),
                'base_url': 'https://api.openai.com/v1',
                'test_endpoint': '/models'
            },
            'gemini': {
                'api_key': os.getenv('GEMINI_API_KEY'),
                'base_url': 'https://generativelanguage.googleapis.com/v1',
                'test_endpoint': '/models'
            }
        }
    
    async def test_provider_connectivity(self, provider_name: str):
        """æ¸¬è©¦æä¾›å•†é€£æ¥æ€§"""
        provider = self.providers.get(provider_name)
        if not provider:
            return {'status': 'error', 'message': f'æœªçŸ¥æä¾›å•†: {provider_name}'}
        
        if not provider['api_key']:
            return {
                'status': 'error', 
                'message': f'{provider_name} API é‡‘é‘°æœªè¨­ç½®'
            }
        
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    'Authorization': f'Bearer {provider["api_key"]}',
                    'Content-Type': 'application/json'
                }
                
                url = provider['base_url'] + provider['test_endpoint']
                
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        return {
                            'status': 'success',
                            'message': f'{provider_name} é€£æ¥æ­£å¸¸',
                            'response_time': response.headers.get('response-time')
                        }
                    else:
                        text = await response.text()
                        return {
                            'status': 'error',
                            'message': f'{provider_name} API éŒ¯èª¤: {response.status}',
                            'details': text
                        }
        
        except asyncio.TimeoutError:
            return {
                'status': 'error',
                'message': f'{provider_name} é€£æ¥è¶…æ™‚'
            }
        except Exception as e:
            return {
                'status': 'error',
                'message': f'{provider_name} é€£æ¥å¤±æ•—: {str(e)}'
            }
    
    async def test_all_providers(self):
        """æ¸¬è©¦æ‰€æœ‰æä¾›å•†"""
        print(f"ğŸ¤– AI æœå‹™è¨ºæ–· - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        tasks = []
        for provider_name in self.providers.keys():
            tasks.append(self.test_provider_connectivity(provider_name))
        
        results = await asyncio.gather(*tasks)
        
        for provider_name, result in zip(self.providers.keys(), results):
            status_icon = "âœ…" if result['status'] == 'success' else "âŒ"
            print(f"{status_icon} {provider_name.upper()}: {result['message']}")
            
            if result['status'] == 'error' and 'details' in result:
                print(f"   è©³æƒ…: {result['details'][:100]}...")
    
    async def test_generation(self, provider_name: str, prompt: str = "Hello"):
        """æ¸¬è©¦æ–‡å­—ç”ŸæˆåŠŸèƒ½"""
        if provider_name == 'openai':
            return await self._test_openai_generation(prompt)
        elif provider_name == 'gemini':
            return await self._test_gemini_generation(prompt)
        else:
            return {'status': 'error', 'message': 'ä¸æ”¯æ´çš„æä¾›å•†'}
    
    async def _test_openai_generation(self, prompt: str):
        """æ¸¬è©¦ OpenAI æ–‡å­—ç”Ÿæˆ"""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    'Authorization': f'Bearer {self.providers["openai"]["api_key"]}',
                    'Content-Type': 'application/json'
                }
                
                data = {
                    'model': 'gpt-3.5-turbo',
                    'messages': [{'role': 'user', 'content': prompt}],
                    'max_tokens': 50
                }
                
                async with session.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers=headers,
                    json=data,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        return {
                            'status': 'success',
                            'message': 'OpenAI ç”Ÿæˆæ¸¬è©¦æˆåŠŸ',
                            'generated_text': result['choices'][0]['message']['content']
                        }
                    else:
                        error_text = await response.text()
                        return {
                            'status': 'error',
                            'message': f'OpenAI ç”Ÿæˆå¤±æ•—: {response.status}',
                            'details': error_text
                        }
        
        except Exception as e:
            return {
                'status': 'error',
                'message': f'OpenAI ç”Ÿæˆæ¸¬è©¦å¤±æ•—: {str(e)}'
            }

# é‹è¡Œè¨ºæ–·
if __name__ == "__main__":
    diagnostics = AIServiceDiagnostics()
    asyncio.run(diagnostics.test_all_providers())
```

#### 3.2 å½±ç‰‡è™•ç†å¡ä½

**ç—‡ç‹€**ï¼š
- å½±ç‰‡è™•ç†ç‹€æ…‹é•·æ™‚é–“åœç•™åœ¨æŸå€‹éšæ®µ
- è™•ç†é€²åº¦ä¸æ›´æ–°
- ä»»å‹™ä½‡åˆ—å †ç©

**è¨ºæ–·æ­¥é©Ÿ**ï¼š
```bash
# æª¢æŸ¥ Celery ä»»å‹™ç‹€æ…‹
docker-compose exec api-gateway celery -A app.celery inspect active

# æª¢æŸ¥ä»»å‹™ä½‡åˆ—é•·åº¦
docker-compose exec redis redis-cli llen celery

# æª¢æŸ¥æ­£åœ¨è™•ç†çš„ä»»å‹™
docker-compose exec api-gateway celery -A app.celery inspect reserved

# æª¢æŸ¥å¤±æ•—çš„ä»»å‹™
docker-compose exec api-gateway celery -A app.celery inspect failed
```

**ä»»å‹™è¨ºæ–·å·¥å…·**ï¼š
```python
# scripts/task-diagnostics.py
import redis
import json
from datetime import datetime, timedelta

class TaskDiagnostics:
    """ä»»å‹™è¨ºæ–·å·¥å…·"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379/0"):
        self.redis_client = redis.from_url(redis_url)
    
    def get_queue_stats(self):
        """ç²å–ä½‡åˆ—çµ±è¨ˆ"""
        stats = {}
        
        # ç²å–æ‰€æœ‰ä½‡åˆ—
        queues = ['celery', 'video_queue', 'ai_queue', 'social_queue']
        
        for queue in queues:
            length = self.redis_client.llen(queue)
            stats[queue] = {
                'length': length,
                'status': 'normal' if length < 100 else 'warning' if length < 500 else 'critical'
            }
        
        return stats
    
    def get_stuck_tasks(self, threshold_minutes: int = 30):
        """ç²å–å¡ä½çš„ä»»å‹™"""
        stuck_tasks = []
        
        # å¾è³‡æ–™åº«æŸ¥è©¢é•·æ™‚é–“è™•ç†çš„ä»»å‹™
        # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„è³‡æ–™åº«çµæ§‹èª¿æ•´
        query = """
        SELECT id, user_id, status, created_at, updated_at, processing_stages
        FROM videos 
        WHERE status = 'processing' 
        AND updated_at < NOW() - INTERVAL %s MINUTE
        ORDER BY updated_at DESC
        """
        
        # åŸ·è¡ŒæŸ¥è©¢é‚è¼¯...
        
        return stuck_tasks
    
    def restart_stuck_tasks(self, task_ids: list):
        """é‡æ–°å•Ÿå‹•å¡ä½çš„ä»»å‹™"""
        for task_id in task_ids:
            try:
                # é‡ç½®ä»»å‹™ç‹€æ…‹
                # é‡æ–°æäº¤åˆ°ä½‡åˆ—
                print(f"é‡æ–°å•Ÿå‹•ä»»å‹™: {task_id}")
            except Exception as e:
                print(f"é‡æ–°å•Ÿå‹•ä»»å‹™ {task_id} å¤±æ•—: {e}")
    
    def clear_failed_tasks(self):
        """æ¸…ç†å¤±æ•—çš„ä»»å‹™"""
        # æ¸…ç† Redis ä¸­çš„å¤±æ•—ä»»å‹™è¨˜éŒ„
        failed_tasks = self.redis_client.keys("celery-task-meta-*")
        
        for task_key in failed_tasks:
            task_data = self.redis_client.get(task_key)
            if task_data:
                task_info = json.loads(task_data)
                if task_info.get('status') == 'FAILURE':
                    self.redis_client.delete(task_key)
                    print(f"å·²æ¸…ç†å¤±æ•—ä»»å‹™: {task_key}")
    
    def monitor_task_performance(self):
        """ç›£æ§ä»»å‹™æ•ˆèƒ½"""
        # çµ±è¨ˆå„éšæ®µå¹³å‡è™•ç†æ™‚é–“
        # è­˜åˆ¥æ•ˆèƒ½ç“¶é ¸
        # ç”Ÿæˆæ•ˆèƒ½å ±å‘Š
        pass

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    diagnostics = TaskDiagnostics()
    
    print("ğŸ“Š ä»»å‹™ä½‡åˆ—çµ±è¨ˆ:")
    stats = diagnostics.get_queue_stats()
    for queue, info in stats.items():
        status_icon = {"normal": "âœ…", "warning": "âš ï¸", "critical": "ğŸš¨"}[info['status']]
        print(f"{status_icon} {queue}: {info['length']} å€‹ä»»å‹™")
    
    print("\nğŸ” æª¢æŸ¥å¡ä½çš„ä»»å‹™:")
    stuck_tasks = diagnostics.get_stuck_tasks()
    if stuck_tasks:
        print(f"ç™¼ç¾ {len(stuck_tasks)} å€‹å¡ä½çš„ä»»å‹™")
        for task in stuck_tasks:
            print(f"- ä»»å‹™ {task['id']}: å¡ä½ {task['stuck_duration']} åˆ†é˜")
    else:
        print("æœªç™¼ç¾å¡ä½çš„ä»»å‹™")
```

### 4. è³‡æ–™åº«å•é¡Œ

#### 4.1 è³‡æ–™åº«æ•ˆèƒ½ä¸‹é™

**ç—‡ç‹€**ï¼š
- æŸ¥è©¢å›æ‡‰æ™‚é–“å¢åŠ 
- è³‡æ–™åº« CPU ä½¿ç”¨ç‡é«˜
- é€£æ¥æ•¸é”åˆ°ä¸Šé™

**è¨ºæ–·æ­¥é©Ÿ**ï¼š
```sql
-- æª¢æŸ¥æ…¢æŸ¥è©¢
SELECT query, calls, total_time, mean_time, rows
FROM pg_stat_statements 
WHERE mean_time > 100  -- å¹³å‡åŸ·è¡Œæ™‚é–“ > 100ms
ORDER BY mean_time DESC 
LIMIT 20;

-- æª¢æŸ¥è¡¨å¤§å°
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
    pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY size_bytes DESC;

-- æª¢æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…æ³
SELECT 
    indexrelname as index_name,
    relname as table_name,
    idx_scan as index_scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched
FROM pg_stat_user_indexes 
ORDER BY idx_scan DESC;

-- æª¢æŸ¥ç•¶å‰é€£æ¥
SELECT 
    pid,
    usename,
    application_name,
    client_addr,
    state,
    query_start,
    query
FROM pg_stat_activity 
WHERE state != 'idle';
```

**è³‡æ–™åº«å„ªåŒ–å·¥å…·**ï¼š
```python
# scripts/db-optimization.py
import asyncpg
import asyncio
from datetime import datetime

class DatabaseOptimizer:
    """è³‡æ–™åº«å„ªåŒ–å·¥å…·"""
    
    def __init__(self, dsn: str):
        self.dsn = dsn
    
    async def analyze_slow_queries(self):
        """åˆ†ææ…¢æŸ¥è©¢"""
        conn = await asyncpg.connect(self.dsn)
        
        slow_queries = await conn.fetch("""
            SELECT 
                query,
                calls,
                total_time,
                mean_time,
                rows,
                100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
            FROM pg_stat_statements 
            WHERE mean_time > 100
            ORDER BY mean_time DESC 
            LIMIT 10
        """)
        
        print("ğŸŒ æ…¢æŸ¥è©¢åˆ†æ:")
        print("=" * 80)
        
        for query in slow_queries:
            print(f"å¹³å‡åŸ·è¡Œæ™‚é–“: {query['mean_time']:.2f}ms")
            print(f"èª¿ç”¨æ¬¡æ•¸: {query['calls']}")
            print(f"å‘½ä¸­ç‡: {query['hit_percent']:.1f}%")
            print(f"æŸ¥è©¢: {query['query'][:100]}...")
            print("-" * 40)
        
        await conn.close()
    
    async def suggest_indexes(self):
        """å»ºè­°å‰µå»ºç´¢å¼•"""
        conn = await asyncpg.connect(self.dsn)
        
        # æŸ¥æ‰¾å¯èƒ½éœ€è¦ç´¢å¼•çš„æŸ¥è©¢
        queries_need_indexes = await conn.fetch("""
            SELECT 
                schemaname,
                tablename,
                seq_scan,
                seq_tup_read,
                idx_scan,
                seq_tup_read / seq_scan as avg_seq_read
            FROM pg_stat_user_tables 
            WHERE seq_scan > 0
            AND seq_tup_read / seq_scan > 1000
            ORDER BY seq_tup_read DESC
        """)
        
        print("ğŸ“š ç´¢å¼•å»ºè­°:")
        print("=" * 60)
        
        for table in queries_need_indexes:
            print(f"è¡¨: {table['tablename']}")
            print(f"é †åºæƒææ¬¡æ•¸: {table['seq_scan']}")
            print(f"å¹³å‡æƒæè¡Œæ•¸: {table['avg_seq_read']:.0f}")
            print(f"å»ºè­°: è€ƒæ…®ç‚ºå¸¸ç”¨æŸ¥è©¢æ¢ä»¶æ·»åŠ ç´¢å¼•")
            print("-" * 30)
        
        await conn.close()
    
    async def check_table_bloat(self):
        """æª¢æŸ¥è¡¨è†¨è„¹"""
        conn = await asyncpg.connect(self.dsn)
        
        bloat_info = await conn.fetch("""
            SELECT 
                schemaname,
                tablename,
                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as table_size,
                n_dead_tup,
                n_live_tup,
                CASE WHEN n_live_tup > 0 
                     THEN round(100 * n_dead_tup / (n_live_tup + n_dead_tup), 2)
                     ELSE 0 
                END as dead_tuple_percent
            FROM pg_stat_user_tables
            WHERE n_dead_tup > 1000
            ORDER BY dead_tuple_percent DESC
        """)
        
        print("ğŸ’€ è¡¨è†¨è„¹æª¢æŸ¥:")
        print("=" * 60)
        
        for table in bloat_info:
            if table['dead_tuple_percent'] > 10:
                print(f"âš ï¸  è¡¨: {table['tablename']}")
                print(f"   å¤§å°: {table['table_size']}")
                print(f"   æ­»å…ƒçµ„æ¯”ä¾‹: {table['dead_tuple_percent']:.1f}%")
                print(f"   å»ºè­°: åŸ·è¡Œ VACUUM ANALYZE {table['tablename']}")
                print()
        
        await conn.close()
    
    async def optimize_database(self):
        """åŸ·è¡Œè³‡æ–™åº«å„ªåŒ–"""
        conn = await asyncpg.connect(self.dsn)
        
        print("ğŸ”§ åŸ·è¡Œè³‡æ–™åº«å„ªåŒ–...")
        
        # æ›´æ–°çµ±è¨ˆä¿¡æ¯
        await conn.execute("ANALYZE;")
        print("âœ… çµ±è¨ˆä¿¡æ¯å·²æ›´æ–°")
        
        # æª¢æŸ¥éœ€è¦ VACUUM çš„è¡¨
        tables_need_vacuum = await conn.fetch("""
            SELECT tablename 
            FROM pg_stat_user_tables 
            WHERE n_dead_tup > n_live_tup * 0.1
            AND n_dead_tup > 1000
        """)
        
        for table in tables_need_vacuum:
            table_name = table['tablename']
            print(f"ğŸ§¹ å°è¡¨ {table_name} åŸ·è¡Œ VACUUM...")
            await conn.execute(f"VACUUM ANALYZE {table_name};")
        
        print("âœ… è³‡æ–™åº«å„ªåŒ–å®Œæˆ")
        await conn.close()

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    optimizer = DatabaseOptimizer("postgresql://user:pass@localhost/db")
    
    await optimizer.analyze_slow_queries()
    await optimizer.suggest_indexes()
    await optimizer.check_table_bloat()
    await optimizer.optimize_database()

if __name__ == "__main__":
    asyncio.run(main())
```

#### 4.2 è³‡æ–™åº«é€£æ¥æ´©æ¼

**ç—‡ç‹€**ï¼š
- è³‡æ–™åº«é€£æ¥æ•¸æŒçºŒå¢é•·
- `too many connections` éŒ¯èª¤
- æ‡‰ç”¨ç„¡æ³•ç²å–æ–°é€£æ¥

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# é€£æ¥æ± ç›£æ§å’Œç®¡ç†
import asyncio
import asyncpg
from datetime import datetime

class ConnectionPoolMonitor:
    """é€£æ¥æ± ç›£æ§å™¨"""
    
    def __init__(self, pool):
        self.pool = pool
        self.connection_history = []
    
    def log_connection_event(self, event_type: str, details: dict = None):
        """è¨˜éŒ„é€£æ¥äº‹ä»¶"""
        event = {
            'timestamp': datetime.now(),
            'event_type': event_type,
            'pool_size': self.pool.get_size(),
            'pool_free': self.pool.get_idle_size(),
            'pool_used': self.pool.get_size() - self.pool.get_idle_size(),
            'details': details or {}
        }
        
        self.connection_history.append(event)
        
        # ä¿ç•™æœ€è¿‘ 1000 æ¢è¨˜éŒ„
        if len(self.connection_history) > 1000:
            self.connection_history = self.connection_history[-1000:]
    
    async def check_connection_leaks(self):
        """æª¢æŸ¥é€£æ¥æ´©æ¼"""
        if not self.connection_history:
            return []
        
        # åˆ†æé€£æ¥ä½¿ç”¨æ¨¡å¼
        recent_events = self.connection_history[-100:]  # æœ€è¿‘ 100 å€‹äº‹ä»¶
        
        # è¨ˆç®—å¹³å‡é€£æ¥ä½¿ç”¨ç‡
        avg_used = sum(event['pool_used'] for event in recent_events) / len(recent_events)
        max_used = max(event['pool_used'] for event in recent_events)
        
        warnings = []
        
        if avg_used > self.pool.get_size() * 0.8:
            warnings.append(f"å¹³å‡é€£æ¥ä½¿ç”¨ç‡éé«˜: {avg_used:.1f}/{self.pool.get_size()}")
        
        if max_used >= self.pool.get_size():
            warnings.append("é€£æ¥æ± å·²æ»¿ï¼Œå¯èƒ½å­˜åœ¨é€£æ¥æ´©æ¼")
        
        # æª¢æŸ¥é•·æ™‚é–“ä½”ç”¨çš„é€£æ¥
        current_time = datetime.now()
        long_running_connections = [
            event for event in recent_events 
            if event['event_type'] == 'acquire' and 
            (current_time - event['timestamp']).total_seconds() > 300  # 5åˆ†é˜
        ]
        
        if long_running_connections:
            warnings.append(f"ç™¼ç¾ {len(long_running_connections)} å€‹é•·æ™‚é–“ä½”ç”¨çš„é€£æ¥")
        
        return warnings
    
    def get_pool_stats(self):
        """ç²å–é€£æ¥æ± çµ±è¨ˆ"""
        return {
            'total_size': self.pool.get_size(),
            'idle_size': self.pool.get_idle_size(),
            'used_size': self.pool.get_size() - self.pool.get_idle_size(),
            'usage_percent': (self.pool.get_size() - self.pool.get_idle_size()) / self.pool.get_size() * 100
        }

# é€£æ¥ç®¡ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨
class ManagedConnection:
    """è¨—ç®¡é€£æ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, pool, monitor=None, timeout=30):
        self.pool = pool
        self.monitor = monitor
        self.timeout = timeout
        self.connection = None
        self.acquired_at = None
    
    async def __aenter__(self):
        try:
            self.connection = await asyncio.wait_for(
                self.pool.acquire(), 
                timeout=self.timeout
            )
            self.acquired_at = datetime.now()
            
            if self.monitor:
                self.monitor.log_connection_event('acquire', {
                    'connection_id': id(self.connection)
                })
            
            return self.connection
            
        except asyncio.TimeoutError:
            raise Exception(f"ç²å–è³‡æ–™åº«é€£æ¥è¶…æ™‚ ({self.timeout}s)")
        except Exception as e:
            if self.monitor:
                self.monitor.log_connection_event('acquire_failed', {
                    'error': str(e)
                })
            raise
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.connection:
            try:
                await self.pool.release(self.connection)
                
                if self.monitor and self.acquired_at:
                    duration = (datetime.now() - self.acquired_at).total_seconds()
                    self.monitor.log_connection_event('release', {
                        'connection_id': id(self.connection),
                        'duration': duration
                    })
                
            except Exception as e:
                if self.monitor:
                    self.monitor.log_connection_event('release_failed', {
                        'error': str(e)
                    })

# ä½¿ç”¨ç¯„ä¾‹
pool = await asyncpg.create_pool(
    "postgresql://user:pass@localhost/db",
    min_size=5,
    max_size=20
)

monitor = ConnectionPoolMonitor(pool)

# ä½¿ç”¨è¨—ç®¡é€£æ¥
async with ManagedConnection(pool, monitor) as conn:
    result = await conn.fetch("SELECT * FROM users LIMIT 10")

# å®šæœŸæª¢æŸ¥é€£æ¥æ´©æ¼
async def periodic_leak_check():
    while True:
        warnings = await monitor.check_connection_leaks()
        if warnings:
            for warning in warnings:
                print(f"âš ï¸  é€£æ¥æ± è­¦å‘Š: {warning}")
        
        stats = monitor.get_pool_stats()
        print(f"é€£æ¥æ± ç‹€æ…‹: {stats['used_size']}/{stats['total_size']} ({stats['usage_percent']:.1f}%)")
        
        await asyncio.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡

# åœ¨å¾Œå°é‹è¡Œç›£æ§
asyncio.create_task(periodic_leak_check())
```

### 5. ç¶²è·¯å’Œé€£æ¥å•é¡Œ

#### 5.1 æœå‹™é–“é€šä¿¡å¤±æ•—

**ç—‡ç‹€**ï¼š
- æœå‹™é–“ API èª¿ç”¨å¤±æ•—
- ç¶²è·¯é€£æ¥è¶…æ™‚
- DNS è§£æå•é¡Œ

**è¨ºæ–·æ­¥é©Ÿ**ï¼š
```bash
# æª¢æŸ¥ Docker ç¶²è·¯
docker network ls
docker network inspect autovideo_default

# æª¢æŸ¥æœå‹™é–“é€£é€šæ€§
docker-compose exec api-gateway ping video-service
docker-compose exec api-gateway nslookup video-service

# æª¢æŸ¥ç«¯å£é–‹æ”¾ç‹€æ…‹
docker-compose exec api-gateway netstat -tlnp

# æ¸¬è©¦æœå‹™é–“ HTTP é€£æ¥
docker-compose exec api-gateway curl -f http://video-service:8004/health
```

**ç¶²è·¯è¨ºæ–·å·¥å…·**ï¼š
```bash
#!/bin/bash
# scripts/network-diagnostics.sh

echo "ğŸŒ ç¶²è·¯è¨ºæ–·é–‹å§‹..."

# æª¢æŸ¥æ‰€æœ‰æœå‹™çš„ç¶²è·¯é€£æ¥
services=("api-gateway" "auth-service" "video-service" "ai-service" "postgres" "redis")

for service in "${services[@]}"; do
    echo "æª¢æŸ¥ $service çš„ç¶²è·¯ç‹€æ…‹..."
    
    # æª¢æŸ¥å®¹å™¨æ˜¯å¦é‹è¡Œ
    if ! docker-compose ps $service | grep -q "Up"; then
        echo "âŒ $service å®¹å™¨æœªé‹è¡Œ"
        continue
    fi
    
    # æª¢æŸ¥ç«¯å£ç›£è½
    docker-compose exec $service netstat -tln 2>/dev/null | grep LISTEN | head -5
    
    # æ¸¬è©¦èˆ‡å…¶ä»–æœå‹™çš„é€£é€šæ€§
    for target_service in "${services[@]}"; do
        if [ "$service" != "$target_service" ]; then
            if docker-compose exec $service ping -c 1 -W 2 $target_service >/dev/null 2>&1; then
                echo "âœ… $service -> $target_service: é€£é€š"
            else
                echo "âŒ $service -> $target_service: ä¸é€š"
            fi
        fi
    done
    
    echo "---"
done

# æª¢æŸ¥å¤–éƒ¨ç¶²è·¯é€£æ¥
echo "æª¢æŸ¥å¤–éƒ¨ç¶²è·¯é€£æ¥..."
if docker-compose exec api-gateway ping -c 1 8.8.8.8 >/dev/null 2>&1; then
    echo "âœ… å¤–éƒ¨ç¶²è·¯: æ­£å¸¸"
else
    echo "âŒ å¤–éƒ¨ç¶²è·¯: ç•°å¸¸"
fi

# æª¢æŸ¥ DNS è§£æ
echo "æª¢æŸ¥ DNS è§£æ..."
if docker-compose exec api-gateway nslookup google.com >/dev/null 2>&1; then
    echo "âœ… DNS è§£æ: æ­£å¸¸"
else
    echo "âŒ DNS è§£æ: ç•°å¸¸"
fi
```

### 6. ç›£æ§å’Œå‘Šè­¦å•é¡Œ

#### 6.1 æŒ‡æ¨™æ”¶é›†å¤±æ•—

**ç—‡ç‹€**ï¼š
- Prometheus ç„¡æ³•æ¡é›†æŒ‡æ¨™
- Grafana å„€è¡¨æ¿é¡¯ç¤ºç„¡æ•¸æ“š
- å‘Šè­¦è¦å‰‡ä¸è§¸ç™¼

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```bash
# æª¢æŸ¥ Prometheus é…ç½®
docker-compose exec prometheus cat /etc/prometheus/prometheus.yml

# æª¢æŸ¥æŒ‡æ¨™ç«¯é»
curl http://localhost:8000/metrics
curl http://localhost:9090/metrics

# æŸ¥çœ‹ Prometheus ç›®æ¨™ç‹€æ…‹
curl http://localhost:9090/api/v1/targets

# é‡æ–°è¼‰å…¥ Prometheus é…ç½®
curl -X POST http://localhost:9090/-/reload
```

**æŒ‡æ¨™è¨ºæ–·å·¥å…·**ï¼š
```python
# scripts/metrics-diagnostics.py
import requests
import json
from datetime import datetime

class MetricsDiagnostics:
    """æŒ‡æ¨™è¨ºæ–·å·¥å…·"""
    
    def __init__(self, prometheus_url="http://localhost:9090"):
        self.prometheus_url = prometheus_url
    
    def check_targets(self):
        """æª¢æŸ¥ Prometheus ç›®æ¨™ç‹€æ…‹"""
        try:
            response = requests.get(f"{self.prometheus_url}/api/v1/targets")
            if response.status_code == 200:
                data = response.json()
                targets = data['data']['activeTargets']
                
                print("ğŸ¯ Prometheus ç›®æ¨™ç‹€æ…‹:")
                print("=" * 50)
                
                for target in targets:
                    job_name = target['labels']['job']
                    instance = target['labels']['instance']
                    health = target['health']
                    last_error = target.get('lastError', '')
                    
                    status_icon = "âœ…" if health == 'up' else "âŒ"
                    print(f"{status_icon} {job_name} ({instance}): {health}")
                    
                    if last_error:
                        print(f"   éŒ¯èª¤: {last_error}")
            else:
                print(f"âŒ ç„¡æ³•ç²å–ç›®æ¨™ç‹€æ…‹: {response.status_code}")
        
        except Exception as e:
            print(f"âŒ æª¢æŸ¥ç›®æ¨™ç‹€æ…‹å¤±æ•—: {e}")
    
    def check_metrics_availability(self):
        """æª¢æŸ¥é—œéµæŒ‡æ¨™å¯ç”¨æ€§"""
        key_metrics = [
            'http_requests_total',
            'http_request_duration_seconds',
            'database_query_duration_seconds',
            'video_processing_duration_seconds'
        ]
        
        print("\nğŸ“Š é—œéµæŒ‡æ¨™å¯ç”¨æ€§:")
        print("=" * 50)
        
        for metric in key_metrics:
            try:
                response = requests.get(
                    f"{self.prometheus_url}/api/v1/query",
                    params={'query': metric}
                )
                
                if response.status_code == 200:
                    data = response.json()
                    if data['data']['result']:
                        print(f"âœ… {metric}: å¯ç”¨")
                    else:
                        print(f"âš ï¸  {metric}: ç„¡æ•¸æ“š")
                else:
                    print(f"âŒ {metric}: æŸ¥è©¢å¤±æ•—")
            
            except Exception as e:
                print(f"âŒ {metric}: æª¢æŸ¥å¤±æ•— - {e}")
    
    def check_alerting_rules(self):
        """æª¢æŸ¥å‘Šè­¦è¦å‰‡"""
        try:
            response = requests.get(f"{self.prometheus_url}/api/v1/rules")
            if response.status_code == 200:
                data = response.json()
                rule_groups = data['data']['groups']
                
                print("\nğŸš¨ å‘Šè­¦è¦å‰‡ç‹€æ…‹:")
                print("=" * 50)
                
                for group in rule_groups:
                    group_name = group['name']
                    print(f"è¦å‰‡çµ„: {group_name}")
                    
                    for rule in group['rules']:
                        rule_name = rule['name']
                        rule_state = rule['state']
                        
                        status_icon = {
                            'inactive': "âœ…",
                            'pending': "âš ï¸ ",
                            'firing': "ğŸš¨"
                        }.get(rule_state, "â“")
                        
                        print(f"  {status_icon} {rule_name}: {rule_state}")
                        
                        if rule_state == 'firing':
                            print(f"    å‘Šè­¦è©³æƒ…: {rule.get('annotations', {}).get('description', 'N/A')}")
            else:
                print(f"âŒ ç„¡æ³•ç²å–å‘Šè­¦è¦å‰‡: {response.status_code}")
        
        except Exception as e:
            print(f"âŒ æª¢æŸ¥å‘Šè­¦è¦å‰‡å¤±æ•—: {e}")

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    diagnostics = MetricsDiagnostics()
    
    diagnostics.check_targets()
    diagnostics.check_metrics_availability()
    diagnostics.check_alerting_rules()
```

## ğŸš¨ ç·Šæ€¥æƒ…æ³è™•ç†

### ç³»çµ±å®Œå…¨ç•¶æ©Ÿæ¢å¾©æµç¨‹

```bash
#!/bin/bash
# scripts/emergency-recovery.sh

echo "ğŸš¨ ç·Šæ€¥æ¢å¾©æµç¨‹å•Ÿå‹•..."

# 1. åœæ­¢æ‰€æœ‰æœå‹™
echo "åœæ­¢æ‰€æœ‰æœå‹™..."
docker-compose down

# 2. æª¢æŸ¥ç³»çµ±è³‡æº
echo "æª¢æŸ¥ç³»çµ±è³‡æº..."
df -h
free -h
docker system df

# 3. æ¸…ç†ç³»çµ±è³‡æºï¼ˆå¦‚æœéœ€è¦ï¼‰
echo "æ¸…ç† Docker è³‡æº..."
docker system prune -f
docker volume prune -f

# 4. æª¢æŸ¥é…ç½®æ–‡ä»¶
echo "æª¢æŸ¥é…ç½®æ–‡ä»¶..."
if [ ! -f ".env" ]; then
    echo "âŒ .env æ–‡ä»¶ä¸å­˜åœ¨"
    cp .env.example .env
    echo "âš ï¸  è«‹ç·¨è¼¯ .env æ–‡ä»¶å¾Œé‡æ–°é‹è¡Œ"
    exit 1
fi

# 5. é‡æ–°å•Ÿå‹•æ ¸å¿ƒæœå‹™
echo "å•Ÿå‹•æ ¸å¿ƒæœå‹™..."
docker-compose up -d postgres redis

# ç­‰å¾…è³‡æ–™åº«å°±ç·’
echo "ç­‰å¾…è³‡æ–™åº«å•Ÿå‹•..."
sleep 30

if ! docker-compose exec postgres pg_isready -U autovideo; then
    echo "âŒ è³‡æ–™åº«å•Ÿå‹•å¤±æ•—"
    exit 1
fi

# 6. å•Ÿå‹•æ‡‰ç”¨æœå‹™
echo "å•Ÿå‹•æ‡‰ç”¨æœå‹™..."
docker-compose up -d

# 7. ç­‰å¾…æœå‹™å°±ç·’
echo "ç­‰å¾…æœå‹™å°±ç·’..."
sleep 60

# 8. åŸ·è¡Œå¥åº·æª¢æŸ¥
echo "åŸ·è¡Œå¥åº·æª¢æŸ¥..."
if curl -f http://localhost:8000/health; then
    echo "âœ… ç³»çµ±æ¢å¾©æˆåŠŸ"
else
    echo "âŒ ç³»çµ±æ¢å¾©å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ"
    docker-compose logs --tail=20
fi

# 9. ç™¼é€æ¢å¾©é€šçŸ¥
curl -X POST "$SLACK_WEBHOOK_URL" \
    -H 'Content-type: application/json' \
    --data '{"text":"ğŸš¨ Auto Video ç³»çµ±ç·Šæ€¥æ¢å¾©å®Œæˆ"}'
```

### æ•¸æ“šæ¢å¾©æµç¨‹

```bash
#!/bin/bash
# scripts/data-recovery.sh

BACKUP_DATE=$1

if [ -z "$BACKUP_DATE" ]; then
    echo "ç”¨æ³•: $0 <å‚™ä»½æ—¥æœŸ>"
    echo "å¯ç”¨å‚™ä»½:"
    ls -la backup/database/postgres_backup_*.sql | head -5
    exit 1
fi

echo "ğŸ”„ é–‹å§‹æ•¸æ“šæ¢å¾©æµç¨‹..."
echo "æ¢å¾©é»: $BACKUP_DATE"

# ç¢ºèªæ“ä½œ
read -p "é€™å°‡è¦†è“‹ç•¶å‰æ•¸æ“šï¼Œç¢ºå®šç¹¼çºŒå—ï¼Ÿ(yes/no): " -r
if [[ ! $REPLY =~ ^yes$ ]]; then
    echo "æ“ä½œå·²å–æ¶ˆ"
    exit 1
fi

# 1. å‰µå»ºç•¶å‰æ•¸æ“šçš„ç·Šæ€¥å‚™ä»½
echo "å‰µå»ºç·Šæ€¥å‚™ä»½..."
./scripts/backup.sh

# 2. åœæ­¢æ‡‰ç”¨æœå‹™
echo "åœæ­¢æ‡‰ç”¨æœå‹™..."
docker-compose stop api-gateway auth-service video-service ai-service

# 3. æ¢å¾©è³‡æ–™åº«
echo "æ¢å¾©è³‡æ–™åº«..."
docker-compose exec -T postgres dropdb -U autovideo autovideo_prod
docker-compose exec -T postgres createdb -U autovideo autovideo_prod
docker-compose exec -T postgres psql -U autovideo -d autovideo_prod < backup/database/postgres_backup_$BACKUP_DATE.sql

# 4. æ¢å¾©æ–‡ä»¶
echo "æ¢å¾©æ–‡ä»¶..."
if [ -f "backup/files/uploads_backup_$BACKUP_DATE.tar.gz" ]; then
    tar -xzf backup/files/uploads_backup_$BACKUP_DATE.tar.gz -C data/
fi

# 5. é‡å•Ÿæœå‹™
echo "é‡å•Ÿæœå‹™..."
docker-compose up -d

# 6. é©—è­‰æ¢å¾©
echo "é©—è­‰æ¢å¾©..."
sleep 30

if curl -f http://localhost:8000/health; then
    echo "âœ… æ•¸æ“šæ¢å¾©æˆåŠŸ"
else
    echo "âŒ æ•¸æ“šæ¢å¾©å¤±æ•—"
    exit 1
fi
```

## ğŸ“ æ”¯æ´è¯ç¹«

### å•é¡Œå›å ±

ç•¶é‡åˆ°ç„¡æ³•è§£æ±ºçš„å•é¡Œæ™‚ï¼Œè«‹æä¾›ä»¥ä¸‹è³‡è¨Šï¼š

1. **ç³»çµ±è³‡è¨Š**ï¼š
```bash
# æ”¶é›†ç³»çµ±è³‡è¨Š
echo "=== ç³»çµ±è³‡è¨Š ===" > support-info.txt
uname -a >> support-info.txt
docker --version >> support-info.txt
docker-compose --version >> support-info.txt

echo "=== æœå‹™ç‹€æ…‹ ===" >> support-info.txt
docker-compose ps >> support-info.txt

echo "=== ç³»çµ±è³‡æº ===" >> support-info.txt
free -h >> support-info.txt
df -h >> support-info.txt

echo "=== éŒ¯èª¤æ—¥èªŒ ===" >> support-info.txt
docker-compose logs --tail=100 >> support-info.txt 2>&1
```

2. **é‡ç¾æ­¥é©Ÿ**ï¼š
   - è©³ç´°æè¿°æ“ä½œæ­¥é©Ÿ
   - é æœŸçµæœ vs å¯¦éš›çµæœ
   - éŒ¯èª¤æˆªåœ–æˆ–æ—¥èªŒ

3. **ç’°å¢ƒé…ç½®**ï¼š
   - éƒ¨ç½²æ–¹å¼ (Docker Compose / Kubernetes)
   - ç’°å¢ƒè®Šæ•¸é…ç½®
   - è‡ªå®šç¾©ä¿®æ”¹

### è¯ç¹«æ–¹å¼

- **GitHub Issues**: [é …ç›® Issues é é¢]
- **æŠ€è¡“æ”¯æ´**: support@autovideo.com
- **ç·Šæ€¥è¯ç¹«**: +886 2 1234 5678
- **æ–‡æª”å•é¡Œ**: docs@autovideo.com

---

## ğŸ“š ç›¸é—œè³‡æº

- [ğŸ“– é–‹ç™¼è€…æŒ‡å—](DEVELOPER_GUIDE.md)
- [ğŸ—ï¸ æ¶æ§‹æ–‡æª”](ARCHITECTURE.md)
- [ğŸš€ éƒ¨ç½²æŒ‡å—](DEPLOYMENT.md)
- [ğŸ”Œ API æ–‡æª”](API_REFERENCE.md)

---

*æœ¬æ•…éšœæ’é™¤æŒ‡å—æœƒæŒçºŒæ›´æ–°ï¼Œå¦‚æœ‰æ–°çš„å•é¡Œæ¨¡å¼æˆ–è§£æ±ºæ–¹æ¡ˆï¼Œæ­¡è¿é€é GitHub Issues è²¢ç»ã€‚*