# ğŸ—ï¸ Auto Video ç³»çµ±æ¶æ§‹è¨­è¨ˆæ–‡æª”

## ğŸ“‹ ç›®éŒ„

- [ç³»çµ±æ¦‚è¦½](#-ç³»çµ±æ¦‚è¦½)
- [æ¶æ§‹åŸå‰‡](#-æ¶æ§‹åŸå‰‡)
- [æŠ€è¡“æ£§é¸æ“‡](#-æŠ€è¡“æ£§é¸æ“‡)
- [å¾®æœå‹™æ¶æ§‹](#-å¾®æœå‹™æ¶æ§‹)
- [è³‡æ–™åº«è¨­è¨ˆ](#-è³‡æ–™åº«è¨­è¨ˆ)
- [API è¨­è¨ˆ](#-api-è¨­è¨ˆ)
- [å®‰å…¨æ¶æ§‹](#-å®‰å…¨æ¶æ§‹)
- [æ•ˆèƒ½èˆ‡æ“´å±•æ€§](#-æ•ˆèƒ½èˆ‡æ“´å±•æ€§)
- [ç›£æ§èˆ‡å¯è§€å¯Ÿæ€§](#-ç›£æ§èˆ‡å¯è§€å¯Ÿæ€§)
- [éƒ¨ç½²æ¶æ§‹](#-éƒ¨ç½²æ¶æ§‹)

## ğŸ¯ ç³»çµ±æ¦‚è¦½

Auto Video æ˜¯ä¸€å€‹ç¾ä»£åŒ–çš„ AI é©…å‹•å½±ç‰‡ç”Ÿæˆå¹³å°ï¼Œæ¡ç”¨é›²åŸç”Ÿå¾®æœå‹™æ¶æ§‹ï¼Œæä¾›å¾å…§å®¹å‰µä½œåˆ°ç¤¾ç¾¤åª’é«”ç™¼å¸ƒçš„å®Œæ•´è§£æ±ºæ–¹æ¡ˆã€‚

### ç³»çµ±ç›®æ¨™

- **é«˜å¯ç”¨æ€§**: 99.9% ç³»çµ±å¯ç”¨æ€§ä¿è­‰
- **é«˜æ“´å±•æ€§**: æ”¯æ´æ°´å¹³æ“´å±•è‡³æ•¸åè¬ç”¨æˆ¶
- **é«˜æ•ˆèƒ½**: API å›æ‡‰æ™‚é–“ < 200ms
- **å®‰å…¨æ€§**: ä¼æ¥­ç´šå®‰å…¨é˜²è­·
- **æ˜“ç¶­è­·æ€§**: æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œä¾¿æ–¼é–‹ç™¼å’Œç¶­è­·

### æ ¸å¿ƒåŠŸèƒ½åŸŸ

```mermaid
mindmap
  root((Auto Video))
    ç”¨æˆ¶ç®¡ç†
      èº«ä»½èªè­‰
      æ¬Šé™æ§åˆ¶
      å€‹äººæª”æ¡ˆ
    å…§å®¹å‰µä½œ
      è…³æœ¬ç”Ÿæˆ
      èªéŸ³åˆæˆ
      è¦–è¦ºç”Ÿæˆ
      å½±ç‰‡çµ„è£
    AI æœå‹™
      æ–‡å­—ç”Ÿæˆ
      åœ–åƒç”Ÿæˆ
      èªéŸ³å…‹éš†
      å…§å®¹å„ªåŒ–
    ç¤¾ç¾¤æ•´åˆ
      å¹³å°é€£æ¥
      å…§å®¹ç™¼å¸ƒ
      æ•¸æ“šåˆ†æ
    ç›£æ§é‹ç¶­
      ç³»çµ±ç›£æ§
      æ—¥èªŒåˆ†æ
      å‘Šè­¦ç®¡ç†
```

## ğŸ¨ æ¶æ§‹åŸå‰‡

### 1. å¾®æœå‹™åŸå‰‡

**å–®ä¸€è·è²¬åŸå‰‡**: æ¯å€‹å¾®æœå‹™å°ˆæ³¨æ–¼ç‰¹å®šçš„æ¥­å‹™åŠŸèƒ½
- èªè­‰æœå‹™ï¼šå°ˆé–€è™•ç†ç”¨æˆ¶èªè­‰å’Œæˆæ¬Š
- å½±ç‰‡æœå‹™ï¼šå°ˆé–€è™•ç†å½±ç‰‡ç”Ÿæˆå’Œç®¡ç†
- AI æœå‹™ï¼šå°ˆé–€è™•ç† AI æ¨¡å‹èª¿ç”¨å’Œç®¡ç†

**æœå‹™è‡ªæ²»**: æ¯å€‹æœå‹™ç¨ç«‹é–‹ç™¼ã€éƒ¨ç½²å’Œæ“´å±•
- ç¨ç«‹çš„è³‡æ–™åº«å’Œè³‡æ–™æ¨¡å‹
- ç¨ç«‹çš„æŠ€è¡“æ£§é¸æ“‡
- ç¨ç«‹çš„ç™¼å¸ƒé€±æœŸ

**å»ä¸­å¿ƒåŒ–æ²»ç†**: æœå‹™åœ˜éšŠæ“æœ‰å®Œæ•´çš„æŠ€è¡“æ±ºç­–æ¬Š
- æŠ€è¡“æ£§é¸æ“‡è‡ªç”±
- è³‡æ–™ç®¡ç†è‡ªä¸»
- éƒ¨ç½²ç­–ç•¥ç¨ç«‹

### 2. é›²åŸç”ŸåŸå‰‡

**å®¹å™¨åŒ–**: æ‰€æœ‰æœå‹™å‡æ¡ç”¨ Docker å®¹å™¨åŒ–
```dockerfile
# å¤šéšæ®µæ§‹å»ºç¯„ä¾‹
FROM python:3.11-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --user -r requirements.txt

FROM python:3.11-slim
WORKDIR /app
COPY --from=builder /root/.local /root/.local
COPY . .
CMD ["python", "main.py"]
```

**ç„¡ç‹€æ…‹è¨­è¨ˆ**: æœå‹™ä¸ä¿å­˜æœ¬åœ°ç‹€æ…‹
- æœƒè©±è³‡è¨Šå­˜å„²åœ¨ Redis
- æª”æ¡ˆå­˜å„²ä½¿ç”¨ S3
- è³‡æ–™åº«ç‹€æ…‹å¤–éƒ¨åŒ–

**å½ˆæ€§ä¼¸ç¸®**: åŸºæ–¼è² è¼‰è‡ªå‹•èª¿æ•´å¯¦ä¾‹æ•¸é‡
- Kubernetes HPA è‡ªå‹•æ“´å±•
- è³‡æºä½¿ç”¨ç‡ç›£æ§
- é æ¸¬æ€§æ“´å±•

### 3. API-First åŸå‰‡

**å¥‘ç´„å„ªå…ˆ**: API è¨­è¨ˆå…ˆæ–¼å¯¦ç¾
- OpenAPI è¦ç¯„å®šç¾©
- API æ–‡æª”è‡ªå‹•ç”Ÿæˆ
- å¥‘ç´„æ¸¬è©¦é©—è­‰

**ç‰ˆæœ¬åŒ–ç®¡ç†**: å‘å¾Œç›¸å®¹çš„ API æ¼”é€²
- èªç¾©åŒ–ç‰ˆæœ¬æ§åˆ¶
- å¤šç‰ˆæœ¬ä¸¦å­˜æ”¯æ´
- å¹³æ»‘å‡ç´šç­–ç•¥

**çµ±ä¸€ä»‹é¢**: ä¸€è‡´çš„ API è¨­è¨ˆæ¨¡å¼
- RESTful è¨­è¨ˆé¢¨æ ¼
- æ¨™æº– HTTP ç‹€æ…‹ç¢¼
- çµ±ä¸€éŒ¯èª¤è™•ç†æ ¼å¼

## ğŸ› ï¸ æŠ€è¡“æ£§é¸æ“‡

### å¾Œç«¯æŠ€è¡“æ£§

```mermaid
graph TB
    subgraph "æ‡‰ç”¨å±¤"
        FastAPI[FastAPI Framework]
        Pydantic[Pydantic Validation]
        SQLAlchemy[SQLAlchemy ORM]
    end
    
    subgraph "æœå‹™å±¤"
        Celery[Celery Task Queue]
        Redis[Redis Cache]
        PostgreSQL[PostgreSQL DB]
    end
    
    subgraph "åŸºç¤è¨­æ–½å±¤"
        Docker[Docker Container]
        Kubernetes[Kubernetes Orchestration]
        Nginx[Nginx Load Balancer]
    end
    
    FastAPI --> Celery
    FastAPI --> Redis
    FastAPI --> PostgreSQL
    Celery --> Redis
    SQLAlchemy --> PostgreSQL
```

**é¸æ“‡ç†ç”±ï¼š**

| æŠ€è¡“ | é¸æ“‡ç†ç”± | æ›¿ä»£æ–¹æ¡ˆ |
|------|----------|----------|
| FastAPI | é«˜æ•ˆèƒ½ã€è‡ªå‹•æ–‡æª”ç”Ÿæˆã€Type Hints æ”¯æ´ | Django REST, Flask |
| PostgreSQL | ACID æ”¯æ´ã€è¤‡é›œæŸ¥è©¢èƒ½åŠ›ã€JSON æ”¯æ´ | MySQL, MongoDB |
| Redis | é«˜æ•ˆèƒ½å¿«å–ã€è±å¯Œæ•¸æ“šçµæ§‹ã€æŒä¹…åŒ–é¸é … | Memcached, Hazelcast |
| Celery | æˆç†Ÿçš„åˆ†æ•£å¼ä»»å‹™ä½‡åˆ—ã€ç›£æ§å·¥å…·è±å¯Œ | RQ, Dramatiq |

### å‰ç«¯æŠ€è¡“æ£§

```mermaid
graph LR
    subgraph "æ¡†æ¶å±¤"
        SvelteKit[SvelteKit]
        TypeScript[TypeScript]
    end
    
    subgraph "æ¨£å¼å±¤"
        TailwindCSS[Tailwind CSS]
        PostCSS[PostCSS]
    end
    
    subgraph "å·¥å…·å±¤"
        Vite[Vite Build Tool]
        ESLint[ESLint Linter]
        Prettier[Prettier Formatter]
    end
    
    SvelteKit --> TypeScript
    SvelteKit --> TailwindCSS
    TypeScript --> Vite
    TailwindCSS --> PostCSS
```

**é¸æ“‡ç†ç”±ï¼š**
- **SvelteKit**: ç·¨è­¯æ™‚å„ªåŒ–ã€å°åŒ…é«”ç©ã€å„ªç§€çš„é–‹ç™¼é«”é©—
- **TypeScript**: é¡å‹å®‰å…¨ã€IDE æ”¯æ´ã€é‡æ§‹å‹å¥½
- **Tailwind CSS**: åŸå­åŒ– CSSã€è¨­è¨ˆç³»çµ±ä¸€è‡´æ€§ã€ç¶­è­·ä¾¿åˆ©

### AI æœå‹™æ•´åˆ

```mermaid
graph TB
    subgraph "æ–‡å­—ç”Ÿæˆ"
        OpenAI[OpenAI GPT]
        Gemini[Google Gemini]
        Claude[Anthropic Claude]
    end
    
    subgraph "åœ–åƒç”Ÿæˆ"
        DALL-E[DALL-E 3]
        Midjourney[Midjourney]
        StableDiffusion[Stable Diffusion]
    end
    
    subgraph "èªéŸ³åˆæˆ"
        ElevenLabs[ElevenLabs]
        Azure[Azure Speech]
        AWS[AWS Polly]
    end
    
    AIService[AI Service] --> OpenAI
    AIService --> DALL-E
    AIService --> ElevenLabs
```

## ğŸ”§ å¾®æœå‹™æ¶æ§‹

### æœå‹™åˆ†å±¤æ¶æ§‹

```mermaid
graph TB
    subgraph "ç”¨æˆ¶ä»‹é¢å±¤"
        WebApp[Web Application]
        MobileApp[Mobile App]
        API[Public API]
    end
    
    subgraph "API é–˜é“å™¨å±¤"
        Gateway[API Gateway]
        Auth[Authentication]
        RateLimit[Rate Limiting]
        LoadBalancer[Load Balancer]
    end
    
    subgraph "æ¥­å‹™æœå‹™å±¤"
        UserService[User Service]
        VideoService[Video Service]
        AIService[AI Service]
        SocialService[Social Service]
        AnalyticsService[Analytics Service]
    end
    
    subgraph "å¹³å°æœå‹™å±¤"
        NotificationService[Notification Service]
        FileService[File Service]
        ConfigService[Config Service]
        SchedulerService[Scheduler Service]
    end
    
    subgraph "è³‡æ–™å­˜å„²å±¤"
        PostgreSQL[PostgreSQL]
        Redis[Redis]
        ElasticSearch[ElasticSearch]
        S3[Object Storage]
    end
    
    WebApp --> Gateway
    MobileApp --> Gateway
    API --> Gateway
    
    Gateway --> UserService
    Gateway --> VideoService
    Gateway --> AIService
    Gateway --> SocialService
    Gateway --> AnalyticsService
    
    UserService --> PostgreSQL
    VideoService --> PostgreSQL
    VideoService --> S3
    AIService --> Redis
    AnalyticsService --> ElasticSearch
```

### æ ¸å¿ƒæœå‹™è©³è§£

#### 1. API Gateway (Port: 8000)

**è·è²¬ç¯„åœï¼š**
- è«‹æ±‚è·¯ç”±å’Œè½‰ç™¼
- èº«ä»½èªè­‰å’Œæˆæ¬Š
- è«‹æ±‚é™æµå’Œç†”æ–·
- æ—¥èªŒè¨˜éŒ„å’Œç›£æ§
- API ç‰ˆæœ¬ç®¡ç†

**æŠ€è¡“å¯¦ç¾ï¼š**
```python
# API Gateway æ ¸å¿ƒè·¯ç”±å¯¦ç¾
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import httpx
import time

app = FastAPI(title="Auto Video API Gateway")

# æœå‹™è¨»å†Šè¡¨
SERVICE_REGISTRY = {
    "auth": "http://auth-service:8001",
    "video": "http://video-service:8004", 
    "ai": "http://ai-service:8005",
    "social": "http://social-service:8006",
    "analytics": "http://analytics-service:8007"
}

@app.middleware("http")
async def gateway_middleware(request: Request, call_next):
    start_time = time.time()
    
    # è·¯ç”±è§£æ
    path_parts = request.url.path.strip("/").split("/")
    if len(path_parts) >= 3 and path_parts[0] == "api" and path_parts[1] == "v1":
        service_name = path_parts[2]
        
        if service_name in SERVICE_REGISTRY:
            # è½‰ç™¼åˆ°å°æ‡‰æœå‹™
            target_url = SERVICE_REGISTRY[service_name]
            new_path = "/" + "/".join(path_parts[3:])
            
            async with httpx.AsyncClient() as client:
                response = await client.request(
                    method=request.method,
                    url=f"{target_url}{new_path}",
                    headers=dict(request.headers),
                    content=await request.body()
                )
                
                # è¨˜éŒ„æŒ‡æ¨™
                process_time = time.time() - start_time
                # ç™¼é€åˆ° Prometheus...
                
                return Response(
                    content=response.content,
                    status_code=response.status_code,
                    headers=dict(response.headers)
                )
    
    # å¦‚æœä¸æ˜¯ API è«‹æ±‚ï¼Œç¹¼çºŒæ­£å¸¸è™•ç†
    response = await call_next(request)
    return response
```

#### 2. Video Service (Port: 8004)

**è·è²¬ç¯„åœï¼š**
- å½±ç‰‡å‰µå»ºå’Œç®¡ç†
- å½±ç‰‡è™•ç†ç‹€æ…‹è¿½è¹¤
- æª”æ¡ˆä¸Šå‚³å’Œå­˜å„²
- å½±ç‰‡å…ƒæ•¸æ“šç®¡ç†

**è³‡æ–™æ¨¡å‹ï¼š**
```python
# Video Service è³‡æ–™æ¨¡å‹
from sqlalchemy import Column, String, Integer, DateTime, Text, Enum, JSON
from sqlalchemy.ext.declarative import declarative_base
import enum

Base = declarative_base()

class VideoStatus(enum.Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class Video(Base):
    __tablename__ = "videos"
    
    id = Column(String, primary_key=True)
    user_id = Column(String, nullable=False, index=True)
    title = Column(String(200), nullable=False)
    description = Column(Text)
    script_content = Column(Text)
    status = Column(Enum(VideoStatus), default=VideoStatus.PENDING)
    duration = Column(Integer)  # ç§’
    file_url = Column(String)
    thumbnail_url = Column(String)
    platforms = Column(JSON)  # ç›®æ¨™å¹³å°åˆ—è¡¨
    tags = Column(JSON)  # æ¨™ç±¤åˆ—è¡¨
    processing_progress = Column(Integer, default=0)
    processing_stages = Column(JSON)  # è™•ç†éšæ®µè©³æƒ…
    created_at = Column(DateTime)
    updated_at = Column(DateTime)
    published_at = Column(DateTime)
    
    # æ€§èƒ½æŒ‡æ¨™
    statistics = Column(JSON)  # è§€çœ‹æ•¸ã€æŒ‰è®šæ•¸ç­‰
    
    # SEO å„ªåŒ–
    seo_title = Column(String(100))
    seo_description = Column(Text)
    seo_keywords = Column(JSON)
```

**è™•ç†æµç¨‹ï¼š**
```mermaid
sequenceDiagram
    participant User
    participant VideoService
    participant AIService
    participant FileStorage
    participant TaskQueue
    
    User->>VideoService: å‰µå»ºå½±ç‰‡è«‹æ±‚
    VideoService->>VideoService: å‰µå»ºå½±ç‰‡è¨˜éŒ„
    VideoService->>TaskQueue: æäº¤è™•ç†ä»»å‹™
    VideoService-->>User: è¿”å›å½±ç‰‡ ID
    
    TaskQueue->>AIService: ç”Ÿæˆè…³æœ¬
    AIService-->>TaskQueue: è…³æœ¬å…§å®¹
    
    TaskQueue->>AIService: ç”ŸæˆèªéŸ³
    AIService-->>TaskQueue: èªéŸ³æª”æ¡ˆ
    
    TaskQueue->>AIService: ç”Ÿæˆè¦–è¦ºå…§å®¹
    AIService-->>TaskQueue: åœ–åƒ/å½±ç‰‡ç´ æ
    
    TaskQueue->>VideoService: çµ„è£å½±ç‰‡
    VideoService->>FileStorage: ä¸Šå‚³æœ€çµ‚å½±ç‰‡
    VideoService->>VideoService: æ›´æ–°ç‹€æ…‹ç‚ºå®Œæˆ
```

#### 3. AI Service (Port: 8005)

**è·è²¬ç¯„åœï¼š**
- AI æ¨¡å‹èª¿ç”¨ç®¡ç†
- è«‹æ±‚éšŠåˆ—å’Œé™æµ
- çµæœå¿«å–å’Œå„ªåŒ–
- å¤šæä¾›å•†æ•…éšœè½‰ç§»

**æä¾›å•†æŠ½è±¡å±¤ï¼š**
```python
# AI Service æä¾›å•†æŠ½è±¡
from abc import ABC, abstractmethod
from typing import Dict, List, Any
import asyncio

class AIProvider(ABC):
    """AI æä¾›å•†æŠ½è±¡åŸºé¡"""
    
    @abstractmethod
    async def generate_text(self, prompt: str, **kwargs) -> str:
        pass
    
    @abstractmethod
    async def generate_image(self, prompt: str, **kwargs) -> str:
        pass
    
    @abstractmethod
    async def synthesize_speech(self, text: str, **kwargs) -> str:
        pass

class OpenAIProvider(AIProvider):
    """OpenAI æä¾›å•†å¯¦ç¾"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = OpenAI(api_key=api_key)
    
    async def generate_text(self, prompt: str, **kwargs) -> str:
        response = await self.client.chat.completions.create(
            model=kwargs.get("model", "gpt-4"),
            messages=[{"role": "user", "content": prompt}],
            max_tokens=kwargs.get("max_tokens", 2000)
        )
        return response.choices[0].message.content
    
    async def generate_image(self, prompt: str, **kwargs) -> str:
        response = await self.client.images.generate(
            model=kwargs.get("model", "dall-e-3"),
            prompt=prompt,
            size=kwargs.get("resolution", "1024x1024"),
            quality=kwargs.get("quality", "standard")
        )
        return response.data[0].url

class AIServiceManager:
    """AI æœå‹™ç®¡ç†å™¨"""
    
    def __init__(self):
        self.providers = {}
        self.fallback_chain = []
        self.cache = {}
    
    def register_provider(self, name: str, provider: AIProvider, priority: int = 0):
        """è¨»å†Š AI æä¾›å•†"""
        self.providers[name] = {
            "provider": provider,
            "priority": priority,
            "health": True
        }
        self._update_fallback_chain()
    
    def _update_fallback_chain(self):
        """æ›´æ–°æ•…éšœè½‰ç§»éˆ"""
        self.fallback_chain = sorted(
            self.providers.items(), 
            key=lambda x: x[1]["priority"], 
            reverse=True
        )
    
    async def generate_with_fallback(self, method: str, *args, **kwargs) -> Any:
        """å¸¶æ•…éšœè½‰ç§»çš„ç”Ÿæˆæ–¹æ³•"""
        last_error = None
        
        for provider_name, provider_info in self.fallback_chain:
            if not provider_info["health"]:
                continue
                
            try:
                provider = provider_info["provider"]
                method_func = getattr(provider, method)
                result = await method_func(*args, **kwargs)
                
                # è¨˜éŒ„æˆåŠŸ
                self._record_success(provider_name)
                return result
                
            except Exception as e:
                last_error = e
                self._record_failure(provider_name, e)
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±æ•—
        raise Exception(f"All AI providers failed. Last error: {last_error}")
    
    def _record_success(self, provider_name: str):
        """è¨˜éŒ„æˆåŠŸèª¿ç”¨"""
        self.providers[provider_name]["health"] = True
    
    def _record_failure(self, provider_name: str, error: Exception):
        """è¨˜éŒ„å¤±æ•—èª¿ç”¨"""
        # å¯¦ç¾ç†”æ–·é‚è¼¯
        # å¦‚æœå¤±æ•—ç‡è¶…éé–¾å€¼ï¼Œæš«æ™‚æ¨™è¨˜ç‚ºä¸å¥åº·
        pass
```

## ğŸ—„ï¸ è³‡æ–™åº«è¨­è¨ˆ

### è³‡æ–™åº«æ¶æ§‹é¸æ“‡

```mermaid
graph TB
    subgraph "é—œè¯å¼è³‡æ–™åº«"
        UserDB[(User Database)]
        VideoDB[(Video Database)]
        AnalyticsDB[(Analytics Database)]
    end
    
    subgraph "éé—œè¯å¼è³‡æ–™åº«"
        Redis[(Redis Cache)]
        S3[(Object Storage)]
        ES[(ElasticSearch)]
    end
    
    subgraph "åˆ†æè³‡æ–™åº«"
        ClickHouse[(ClickHouse)]
        TimeSeries[(Time Series DB)]
    end
    
    App[Application Layer] --> UserDB
    App --> VideoDB
    App --> Redis
    App --> S3
    VideoDB --> ES
    Analytics --> ClickHouse
```

### æ ¸å¿ƒè³‡æ–™æ¨¡å‹

#### ç”¨æˆ¶é ˜åŸŸæ¨¡å‹

```sql
-- ç”¨æˆ¶åŸºæœ¬è³‡è¨Šè¡¨
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    full_name VARCHAR(100),
    bio TEXT,
    avatar_url VARCHAR(500),
    website VARCHAR(500),
    location VARCHAR(100),
    email_verified BOOLEAN DEFAULT FALSE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    last_login_at TIMESTAMP WITH TIME ZONE
);

-- ç”¨æˆ¶è§’è‰²è¡¨
CREATE TABLE user_roles (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    role VARCHAR(50) NOT NULL,
    granted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    granted_by UUID REFERENCES users(id),
    expires_at TIMESTAMP WITH TIME ZONE,
    UNIQUE(user_id, role)
);

-- ç”¨æˆ¶è¨‚é–±è¡¨
CREATE TABLE user_subscriptions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    plan VARCHAR(50) NOT NULL,
    status VARCHAR(20) NOT NULL DEFAULT 'active',
    start_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    end_date TIMESTAMP WITH TIME ZONE,
    auto_renew BOOLEAN DEFAULT TRUE,
    payment_method JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- ç´¢å¼•å„ªåŒ–
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_created_at ON users(created_at);
CREATE INDEX idx_user_roles_user_id ON user_roles(user_id);
CREATE INDEX idx_user_subscriptions_user_id ON user_subscriptions(user_id);
```

#### å½±ç‰‡é ˜åŸŸæ¨¡å‹

```sql
-- å½±ç‰‡ä¸»è¡¨
CREATE TABLE videos (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    title VARCHAR(200) NOT NULL,
    description TEXT,
    script_content TEXT,
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    duration INTEGER, -- ç§’
    file_url VARCHAR(500),
    file_size BIGINT,
    thumbnail_url VARCHAR(500),
    resolution VARCHAR(20),
    format VARCHAR(10),
    platforms JSONB DEFAULT '[]',
    tags JSONB DEFAULT '[]',
    processing_progress INTEGER DEFAULT 0,
    processing_stages JSONB DEFAULT '{}',
    processing_error TEXT,
    seo_title VARCHAR(100),
    seo_description TEXT,
    seo_keywords JSONB DEFAULT '[]',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    published_at TIMESTAMP WITH TIME ZONE
);

-- å½±ç‰‡çµ±è¨ˆè¡¨
CREATE TABLE video_statistics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    video_id UUID NOT NULL REFERENCES videos(id) ON DELETE CASCADE,
    platform VARCHAR(50) NOT NULL,
    views INTEGER DEFAULT 0,
    likes INTEGER DEFAULT 0,
    dislikes INTEGER DEFAULT 0,
    shares INTEGER DEFAULT 0,
    comments INTEGER DEFAULT 0,
    watch_time_seconds BIGINT DEFAULT 0,
    engagement_rate DECIMAL(5,2),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(video_id, platform)
);

-- å½±ç‰‡è™•ç†æ­·å²è¡¨  
CREATE TABLE video_processing_history (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    video_id UUID NOT NULL REFERENCES videos(id) ON DELETE CASCADE,
    stage VARCHAR(50) NOT NULL,
    status VARCHAR(20) NOT NULL,
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    progress INTEGER DEFAULT 0,
    error_message TEXT,
    metadata JSONB DEFAULT '{}'
);

-- å½±ç‰‡ç‰ˆæœ¬è¡¨ï¼ˆæ”¯æ´ç‰ˆæœ¬ç®¡ç†ï¼‰
CREATE TABLE video_versions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    video_id UUID NOT NULL REFERENCES videos(id) ON DELETE CASCADE,
    version INTEGER NOT NULL,
    file_url VARCHAR(500) NOT NULL,
    file_size BIGINT,
    changes TEXT,
    created_by UUID NOT NULL REFERENCES users(id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(video_id, version)
);

-- ç´¢å¼•å„ªåŒ–
CREATE INDEX idx_videos_user_id ON videos(user_id);
CREATE INDEX idx_videos_status ON videos(status);
CREATE INDEX idx_videos_created_at ON videos(created_at);
CREATE INDEX idx_videos_published_at ON videos(published_at);
CREATE INDEX idx_videos_tags ON videos USING GIN(tags);
CREATE INDEX idx_video_statistics_video_id ON video_statistics(video_id);
CREATE INDEX idx_video_statistics_platform ON video_statistics(platform);
```

#### AI æœå‹™æ¨¡å‹

```sql
-- AI ç”Ÿæˆè¨˜éŒ„è¡¨
CREATE TABLE ai_generations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    type VARCHAR(50) NOT NULL, -- text, image, audio
    provider VARCHAR(50) NOT NULL,
    model VARCHAR(100),
    prompt TEXT NOT NULL,
    parameters JSONB DEFAULT '{}',
    result_url VARCHAR(500),
    result_metadata JSONB DEFAULT '{}',
    tokens_used INTEGER,
    cost_usd DECIMAL(10,4),
    processing_time_ms INTEGER,
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    error_message TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP WITH TIME ZONE
);

-- AI é…é¡ç®¡ç†è¡¨
CREATE TABLE ai_quotas (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    type VARCHAR(50) NOT NULL, -- text, image, audio
    period VARCHAR(20) NOT NULL, -- daily, monthly
    limit_amount INTEGER NOT NULL,
    used_amount INTEGER DEFAULT 0,
    reset_at TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(user_id, type, period)
);

-- ç´¢å¼•
CREATE INDEX idx_ai_generations_user_id ON ai_generations(user_id);
CREATE INDEX idx_ai_generations_type ON ai_generations(type);
CREATE INDEX idx_ai_generations_created_at ON ai_generations(created_at);
CREATE INDEX idx_ai_quotas_user_id ON ai_quotas(user_id);
```

### è³‡æ–™åº«æ•ˆèƒ½å„ªåŒ–

#### 1. æŸ¥è©¢å„ªåŒ–ç­–ç•¥

```sql
-- è¤‡åˆç´¢å¼•è¨­è¨ˆ
CREATE INDEX idx_videos_user_status_created ON videos(user_id, status, created_at DESC);

-- éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•æ´»èºæ•¸æ“šï¼‰
CREATE INDEX idx_videos_active ON videos(created_at) WHERE status != 'deleted';

-- è¡¨é”å¼ç´¢å¼•
CREATE INDEX idx_videos_title_search ON videos USING gin(to_tsvector('english', title));

-- è¦†è“‹ç´¢å¼•ï¼ˆé¿å…å›è¡¨æŸ¥è©¢ï¼‰
CREATE INDEX idx_videos_list_cover ON videos(user_id, created_at DESC) 
INCLUDE (title, status, duration, thumbnail_url);
```

#### 2. åˆ†å€ç­–ç•¥

```sql
-- æŒ‰æ™‚é–“åˆ†å€çš„å½±ç‰‡çµ±è¨ˆè¡¨
CREATE TABLE video_statistics_partitioned (
    id UUID NOT NULL,
    video_id UUID NOT NULL,
    platform VARCHAR(50) NOT NULL,
    date DATE NOT NULL,
    views INTEGER DEFAULT 0,
    likes INTEGER DEFAULT 0,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
) PARTITION BY RANGE (date);

-- å‰µå»ºæœˆåº¦åˆ†å€
CREATE TABLE video_statistics_2024_01 PARTITION OF video_statistics_partitioned
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE video_statistics_2024_02 PARTITION OF video_statistics_partitioned  
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');
```

#### 3. è®€å¯«åˆ†é›¢

```python
# è³‡æ–™åº«é€£æ¥ç®¡ç†
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

class DatabaseManager:
    def __init__(self):
        # ä¸»åº«ï¼ˆè®€å¯«ï¼‰
        self.master_engine = create_engine(
            "postgresql://user:pass@master-db:5432/autovideo",
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True
        )
        
        # å¾åº«ï¼ˆåªè®€ï¼‰
        self.slave_engines = [
            create_engine(
                "postgresql://user:pass@slave1-db:5432/autovideo",
                pool_size=10,
                max_overflow=20,
                pool_pre_ping=True
            ),
            create_engine(
                "postgresql://user:pass@slave2-db:5432/autovideo", 
                pool_size=10,
                max_overflow=20,
                pool_pre_ping=True
            )
        ]
        
        self.master_session = sessionmaker(bind=self.master_engine)
        self.slave_sessions = [
            sessionmaker(bind=engine) for engine in self.slave_engines
        ]
    
    def get_write_session(self):
        """ç²å–å¯«æœƒè©±"""
        return self.master_session()
    
    def get_read_session(self):
        """ç²å–è®€æœƒè©±ï¼ˆè² è¼‰å‡è¡¡ï¼‰"""
        import random
        session_factory = random.choice(self.slave_sessions)
        return session_factory()
```

## ğŸ”’ å®‰å…¨æ¶æ§‹

### å¤šå±¤å®‰å…¨é˜²è­·

```mermaid
graph TB
    subgraph "é‚Šç•Œå®‰å…¨"
        WAF[Web Application Firewall]
        DDoS[DDoS Protection]
        CDN[CDN & Rate Limiting]
    end
    
    subgraph "ç¶²è·¯å®‰å…¨"
        LB[Load Balancer]
        VPC[Virtual Private Cloud]
        Firewall[Network Firewall]
    end
    
    subgraph "æ‡‰ç”¨å®‰å…¨"
        Auth[Authentication]
        AuthZ[Authorization]
        Encryption[Data Encryption]
        Validation[Input Validation]
    end
    
    subgraph "è³‡æ–™å®‰å…¨"
        DBEncryption[Database Encryption]
        Backup[Encrypted Backup]
        Audit[Audit Logging]
    end
    
    Internet --> WAF
    WAF --> CDN
    CDN --> LB
    LB --> VPC
    VPC --> Auth
    Auth --> DBEncryption
```

### èº«ä»½èªè­‰èˆ‡æˆæ¬Š

#### JWT Token æ¶æ§‹

```python
# JWT ä»¤ç‰Œç®¡ç†
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
import jwt
from passlib.context import CryptContext

class JWTManager:
    def __init__(self, secret_key: str, algorithm: str = "HS256"):
        self.secret_key = secret_key
        self.algorithm = algorithm
        self.pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
    
    def create_tokens(self, user_data: Dict[str, Any]) -> Dict[str, str]:
        """å‰µå»ºè¨ªå•ä»¤ç‰Œå’Œåˆ·æ–°ä»¤ç‰Œ"""
        now = datetime.utcnow()
        
        # è¨ªå•ä»¤ç‰Œï¼ˆçŸ­æœŸï¼‰
        access_payload = {
            "user_id": user_data["id"],
            "username": user_data["username"],
            "email": user_data["email"],
            "roles": user_data.get("roles", ["user"]),
            "type": "access",
            "iat": now,
            "exp": now + timedelta(minutes=15),
            "jti": self._generate_jti()
        }
        
        # åˆ·æ–°ä»¤ç‰Œï¼ˆé•·æœŸï¼‰
        refresh_payload = {
            "user_id": user_data["id"],
            "type": "refresh", 
            "iat": now,
            "exp": now + timedelta(days=30),
            "jti": self._generate_jti()
        }
        
        return {
            "access_token": jwt.encode(access_payload, self.secret_key, self.algorithm),
            "refresh_token": jwt.encode(refresh_payload, self.secret_key, self.algorithm),
            "token_type": "bearer",
            "expires_in": 900  # 15 minutes
        }
    
    def verify_token(self, token: str, token_type: str = "access") -> Optional[Dict[str, Any]]:
        """é©—è­‰ä»¤ç‰Œ"""
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            
            if payload.get("type") != token_type:
                return None
            
            # æª¢æŸ¥ä»¤ç‰Œæ˜¯å¦åœ¨é»‘åå–®ä¸­
            if self._is_token_blacklisted(payload.get("jti")):
                return None
            
            return payload
            
        except jwt.ExpiredSignatureError:
            return None
        except jwt.JWTError:
            return None
    
    def refresh_access_token(self, refresh_token: str) -> Optional[Dict[str, str]]:
        """åˆ·æ–°è¨ªå•ä»¤ç‰Œ"""
        payload = self.verify_token(refresh_token, "refresh")
        if not payload:
            return None
        
        # å¾è³‡æ–™åº«ç²å–æœ€æ–°ç”¨æˆ¶è³‡è¨Š
        user_data = self._get_user_data(payload["user_id"])
        if not user_data or not user_data.get("is_active"):
            return None
        
        return self.create_tokens(user_data)
    
    def _generate_jti(self) -> str:
        """ç”Ÿæˆä»¤ç‰Œ ID"""
        import uuid
        return str(uuid.uuid4())
    
    def _is_token_blacklisted(self, jti: str) -> bool:
        """æª¢æŸ¥ä»¤ç‰Œæ˜¯å¦åœ¨é»‘åå–®ä¸­"""
        # å¾ Redis æª¢æŸ¥é»‘åå–®
        import redis
        r = redis.Redis()
        return r.exists(f"blacklist:{jti}")
    
    def blacklist_token(self, jti: str, expiry: int):
        """å°‡ä»¤ç‰ŒåŠ å…¥é»‘åå–®"""
        import redis
        r = redis.Redis()
        r.setex(f"blacklist:{jti}", expiry, "1")
```

#### åŸºæ–¼è§’è‰²çš„è¨ªå•æ§åˆ¶ (RBAC)

```python
# RBAC æ¬Šé™ç®¡ç†
from enum import Enum
from typing import List, Set
from functools import wraps

class Permission(Enum):
    # ç”¨æˆ¶æ¬Šé™
    USER_READ = "user:read"
    USER_UPDATE = "user:update"
    USER_DELETE = "user:delete"
    
    # å½±ç‰‡æ¬Šé™
    VIDEO_CREATE = "video:create"
    VIDEO_READ = "video:read"
    VIDEO_UPDATE = "video:update"
    VIDEO_DELETE = "video:delete"
    VIDEO_PUBLISH = "video:publish"
    
    # AI æ¬Šé™
    AI_GENERATE_TEXT = "ai:generate:text"
    AI_GENERATE_IMAGE = "ai:generate:image"
    AI_GENERATE_AUDIO = "ai:generate:audio"
    
    # ç®¡ç†å“¡æ¬Šé™
    ADMIN_USER_MANAGE = "admin:user:manage"
    ADMIN_SYSTEM_CONFIG = "admin:system:config"
    ADMIN_ANALYTICS_VIEW = "admin:analytics:view"

class Role(Enum):
    GUEST = "guest"
    USER = "user"
    CREATOR = "creator"
    PREMIUM = "premium"
    ADMIN = "admin"
    SUPER_ADMIN = "super_admin"

# è§’è‰²æ¬Šé™æ˜ å°„
ROLE_PERMISSIONS = {
    Role.GUEST: set(),
    Role.USER: {
        Permission.USER_READ,
        Permission.VIDEO_READ,
    },
    Role.CREATOR: {
        Permission.USER_READ,
        Permission.USER_UPDATE,
        Permission.VIDEO_CREATE,
        Permission.VIDEO_READ,
        Permission.VIDEO_UPDATE,
        Permission.VIDEO_DELETE,
        Permission.AI_GENERATE_TEXT,
    },
    Role.PREMIUM: {
        # åŒ…å« CREATOR çš„æ‰€æœ‰æ¬Šé™
        *ROLE_PERMISSIONS[Role.CREATOR],
        Permission.AI_GENERATE_IMAGE,
        Permission.AI_GENERATE_AUDIO,
        Permission.VIDEO_PUBLISH,
    },
    Role.ADMIN: {
        # åŒ…å« PREMIUM çš„æ‰€æœ‰æ¬Šé™
        *ROLE_PERMISSIONS[Role.PREMIUM],
        Permission.ADMIN_USER_MANAGE,
        Permission.ADMIN_ANALYTICS_VIEW,
    },
    Role.SUPER_ADMIN: {
        # æ‰€æœ‰æ¬Šé™
        *[p for p in Permission],
    }
}

class PermissionChecker:
    """æ¬Šé™æª¢æŸ¥å™¨"""
    
    @staticmethod
    def user_has_permission(user_roles: List[Role], required_permission: Permission) -> bool:
        """æª¢æŸ¥ç”¨æˆ¶æ˜¯å¦å…·æœ‰æŒ‡å®šæ¬Šé™"""
        user_permissions = set()
        for role in user_roles:
            user_permissions.update(ROLE_PERMISSIONS.get(role, set()))
        
        return required_permission in user_permissions
    
    @staticmethod
    def require_permission(permission: Permission):
        """æ¬Šé™è£é£¾å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # å¾è«‹æ±‚ä¸Šä¸‹æ–‡ç²å–ç•¶å‰ç”¨æˆ¶
                current_user = get_current_user()
                
                if not current_user:
                    raise HTTPException(status_code=401, detail="æœªèªè­‰")
                
                user_roles = [Role(role) for role in current_user.get("roles", [])]
                
                if not PermissionChecker.user_has_permission(user_roles, permission):
                    raise HTTPException(status_code=403, detail="æ¬Šé™ä¸è¶³")
                
                return await func(*args, **kwargs)
            return wrapper
        return decorator

# ä½¿ç”¨ç¯„ä¾‹
@require_permission(Permission.VIDEO_CREATE)
async def create_video(video_data: VideoCreateRequest):
    """å‰µå»ºå½±ç‰‡ï¼ˆéœ€è¦å‰µå»ºæ¬Šé™ï¼‰"""
    pass

@require_permission(Permission.ADMIN_USER_MANAGE)
async def manage_users():
    """ç®¡ç†ç”¨æˆ¶ï¼ˆéœ€è¦ç®¡ç†å“¡æ¬Šé™ï¼‰"""
    pass
```

### è³‡æ–™åŠ å¯†

#### æ•æ„Ÿè³‡æ–™åŠ å¯†

```python
# è³‡æ–™åŠ å¯†æœå‹™
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import base64
import os

class EncryptionService:
    """è³‡æ–™åŠ å¯†æœå‹™"""
    
    def __init__(self, password: str):
        # å¾å¯†ç¢¼ç”Ÿæˆé‡‘é‘°
        salt = os.urandom(16)
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )
        key = base64.urlsafe_b64encode(kdf.derive(password.encode()))
        self.fernet = Fernet(key)
        self.salt = salt
    
    def encrypt(self, data: str) -> str:
        """åŠ å¯†æ•¸æ“š"""
        encrypted_data = self.fernet.encrypt(data.encode())
        return base64.urlsafe_b64encode(encrypted_data).decode()
    
    def decrypt(self, encrypted_data: str) -> str:
        """è§£å¯†æ•¸æ“š"""
        encrypted_bytes = base64.urlsafe_b64decode(encrypted_data.encode())
        decrypted_data = self.fernet.decrypt(encrypted_bytes)
        return decrypted_data.decode()
    
    def encrypt_sensitive_fields(self, data: dict, sensitive_fields: list) -> dict:
        """åŠ å¯†æ•æ„Ÿæ¬„ä½"""
        encrypted_data = data.copy()
        for field in sensitive_fields:
            if field in encrypted_data and encrypted_data[field]:
                encrypted_data[field] = self.encrypt(str(encrypted_data[field]))
        return encrypted_data
    
    def decrypt_sensitive_fields(self, data: dict, sensitive_fields: list) -> dict:
        """è§£å¯†æ•æ„Ÿæ¬„ä½"""
        decrypted_data = data.copy()
        for field in sensitive_fields:
            if field in decrypted_data and decrypted_data[field]:
                try:
                    decrypted_data[field] = self.decrypt(decrypted_data[field])
                except Exception:
                    # å¦‚æœè§£å¯†å¤±æ•—ï¼Œä¿æŒåŸå€¼ï¼ˆå¯èƒ½æ˜¯æœªåŠ å¯†çš„èˆŠæ•¸æ“šï¼‰
                    pass
        return decrypted_data

# åœ¨æ¨¡å‹ä¸­ä½¿ç”¨åŠ å¯†
class EncryptedUser(Base):
    __tablename__ = "users"
    
    id = Column(String, primary_key=True)
    username = Column(String(50), unique=True, nullable=False)
    email = Column(String(255), unique=True, nullable=False)  # åŠ å¯†å­˜å„²
    phone = Column(String(20))  # åŠ å¯†å­˜å„²
    api_keys = Column(JSON)  # åŠ å¯†å­˜å„²
    
    @property
    def decrypted_email(self):
        """è§£å¯†å¾Œçš„éƒµç®±"""
        encryption_service = get_encryption_service()
        return encryption_service.decrypt(self.email)
    
    @decrypted_email.setter
    def decrypted_email(self, value):
        """è¨­ç½®åŠ å¯†éƒµç®±"""
        encryption_service = get_encryption_service()
        self.email = encryption_service.encrypt(value)
```

## âš¡ æ•ˆèƒ½èˆ‡æ“´å±•æ€§

### ç³»çµ±æ•ˆèƒ½ç›®æ¨™

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | ç›£æ§æ–¹å¼ |
|------|---------|----------|
| API å›æ‡‰æ™‚é–“ | P95 < 200ms | Prometheus + Grafana |
| è³‡æ–™åº«æŸ¥è©¢æ™‚é–“ | P95 < 50ms | pg_stat_statements |
| ç³»çµ±å¯ç”¨æ€§ | 99.9% | å¥åº·æª¢æŸ¥ + å‘Šè­¦ |
| ä½µç™¼ç”¨æˆ¶æ•¸ | 10,000+ | è² è¼‰æ¸¬è©¦ |
| å½±ç‰‡è™•ç†æ™‚é–“ | < 5 åˆ†é˜ | è™•ç†ä½‡åˆ—ç›£æ§ |

### å¿«å–ç­–ç•¥

#### å¤šå±¤å¿«å–æ¶æ§‹

```mermaid
graph TB
    subgraph "æ‡‰ç”¨å±¤å¿«å–"
        AppCache[Application Cache]
        QueryCache[Query Result Cache]
    end
    
    subgraph "åˆ†æ•£å¼å¿«å–"
        Redis[Redis Cluster]
        CDN[CDN Cache]
    end
    
    subgraph "è³‡æ–™åº«å¿«å–"
        DBCache[Database Buffer Pool]
        QueryPlan[Query Plan Cache]
    end
    
    Client --> CDN
    CDN --> AppCache
    AppCache --> Redis
    Redis --> DBCache
```

#### å¿«å–å¯¦ç¾ç­–ç•¥

```python
# æ™ºèƒ½å¿«å–ç®¡ç†
import asyncio
import json
from typing import Any, Optional, Callable
from datetime import datetime, timedelta
import redis.asyncio as redis

class CacheManager:
    """å¤šå±¤å¿«å–ç®¡ç†å™¨"""
    
    def __init__(self):
        self.redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)
        self.local_cache = {}  # æœ¬åœ°è¨˜æ†¶é«”å¿«å–
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "local_hits": 0,
            "redis_hits": 0
        }
    
    async def get(self, key: str) -> Optional[Any]:
        """å¤šå±¤å¿«å–ç²å–"""
        # L1: æœ¬åœ°è¨˜æ†¶é«”å¿«å–
        if key in self.local_cache:
            entry = self.local_cache[key]
            if not self._is_expired(entry):
                self.cache_stats["hits"] += 1
                self.cache_stats["local_hits"] += 1
                return entry["value"]
            else:
                del self.local_cache[key]
        
        # L2: Redis åˆ†æ•£å¼å¿«å–
        try:
            redis_value = await self.redis_client.get(key)
            if redis_value:
                value = json.loads(redis_value)
                # å›å¡«åˆ°æœ¬åœ°å¿«å–
                self._set_local_cache(key, value, ttl=300)
                self.cache_stats["hits"] += 1
                self.cache_stats["redis_hits"] += 1
                return value
        except Exception as e:
            print(f"Redis error: {e}")
        
        self.cache_stats["misses"] += 1
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600):
        """è¨­ç½®å¤šå±¤å¿«å–"""
        # è¨­ç½®æœ¬åœ°å¿«å–
        self._set_local_cache(key, value, ttl=min(ttl, 300))
        
        # è¨­ç½® Redis å¿«å–
        try:
            await self.redis_client.setex(key, ttl, json.dumps(value, default=str))
        except Exception as e:
            print(f"Redis set error: {e}")
    
    def _set_local_cache(self, key: str, value: Any, ttl: int):
        """è¨­ç½®æœ¬åœ°å¿«å–"""
        self.local_cache[key] = {
            "value": value,
            "expires_at": datetime.now() + timedelta(seconds=ttl)
        }
        
        # é™åˆ¶æœ¬åœ°å¿«å–å¤§å°
        if len(self.local_cache) > 1000:
            # ç§»é™¤æœ€èˆŠçš„æ¢ç›®
            oldest_key = min(self.local_cache.keys(), 
                           key=lambda k: self.local_cache[k]["expires_at"])
            del self.local_cache[oldest_key]
    
    def _is_expired(self, entry: dict) -> bool:
        """æª¢æŸ¥å¿«å–æ¢ç›®æ˜¯å¦éæœŸ"""
        return datetime.now() > entry["expires_at"]
    
    async def invalidate_pattern(self, pattern: str):
        """æŒ‰æ¨¡å¼å¤±æ•ˆå¿«å–"""
        # æ¸…ç†æœ¬åœ°å¿«å–
        keys_to_remove = [k for k in self.local_cache.keys() if pattern in k]
        for key in keys_to_remove:
            del self.local_cache[key]
        
        # æ¸…ç† Redis å¿«å–
        try:
            keys = await self.redis_client.keys(f"*{pattern}*")
            if keys:
                await self.redis_client.delete(*keys)
        except Exception as e:
            print(f"Redis invalidate error: {e}")
    
    def get_stats(self) -> dict:
        """ç²å–å¿«å–çµ±è¨ˆ"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = (self.cache_stats["hits"] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            **self.cache_stats,
            "hit_rate": round(hit_rate, 2),
            "local_cache_size": len(self.local_cache)
        }

# å¿«å–è£é£¾å™¨
def cached(ttl: int = 3600, key_prefix: str = ""):
    """å¿«å–è£é£¾å™¨"""
    def decorator(func: Callable):
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆå¿«å–éµ
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # å˜—è©¦å¾å¿«å–ç²å–
            cache_manager = get_cache_manager()
            cached_result = await cache_manager.get(cache_key)
            
            if cached_result is not None:
                return cached_result
            
            # åŸ·è¡ŒåŸå‡½æ•¸
            result = await func(*args, **kwargs)
            
            # å„²å­˜åˆ°å¿«å–
            await cache_manager.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¯„ä¾‹
@cached(ttl=1800, key_prefix="video")
async def get_video_details(video_id: str):
    """ç²å–å½±ç‰‡è©³æƒ…ï¼ˆå¸¶å¿«å–ï¼‰"""
    # å¾è³‡æ–™åº«æŸ¥è©¢...
    pass
```

### è³‡æ–™åº«æ•ˆèƒ½å„ªåŒ–

#### é€£æ¥æ± ç®¡ç†

```python
# è³‡æ–™åº«é€£æ¥æ± å„ªåŒ–
from sqlalchemy.pool import QueuePool
from sqlalchemy import create_engine
import asyncpg

class DatabasePoolManager:
    """è³‡æ–™åº«é€£æ¥æ± ç®¡ç†å™¨"""
    
    def __init__(self, config: dict):
        self.config = config
        self.pools = {}
        self._create_pools()
    
    def _create_pools(self):
        """å‰µå»ºé€£æ¥æ± """
        # ä¸»åº«é€£æ¥æ± ï¼ˆè®€å¯«ï¼‰
        self.pools["master"] = create_engine(
            self.config["master_url"],
            poolclass=QueuePool,
            pool_size=20,  # å¸¸é§é€£æ¥æ•¸
            max_overflow=30,  # æœ€å¤§æº¢å‡ºé€£æ¥æ•¸
            pool_timeout=30,  # ç²å–é€£æ¥è¶…æ™‚
            pool_recycle=3600,  # é€£æ¥å›æ”¶æ™‚é–“
            pool_pre_ping=True,  # é€£æ¥å‰æ¸¬è©¦
            echo=False
        )
        
        # å¾åº«é€£æ¥æ± ï¼ˆåªè®€ï¼‰
        for i, slave_url in enumerate(self.config["slave_urls"]):
            self.pools[f"slave_{i}"] = create_engine(
                slave_url,
                poolclass=QueuePool,
                pool_size=10,
                max_overflow=20,
                pool_timeout=30,
                pool_recycle=3600,
                pool_pre_ping=True,
                echo=False
            )
    
    def get_read_engine(self):
        """ç²å–è®€é€£æ¥ï¼ˆè² è¼‰å‡è¡¡ï¼‰"""
        import random
        slave_pools = [pool for name, pool in self.pools.items() if name.startswith("slave_")]
        return random.choice(slave_pools) if slave_pools else self.pools["master"]
    
    def get_write_engine(self):
        """ç²å–å¯«é€£æ¥"""
        return self.pools["master"]
    
    def get_pool_status(self):
        """ç²å–é€£æ¥æ± ç‹€æ…‹"""
        status = {}
        for name, pool in self.pools.items():
            status[name] = {
                "size": pool.pool.size(),
                "checked_in": pool.pool.checkedin(),
                "checked_out": pool.pool.checkedout(),
                "overflow": pool.pool.overflow(),
                "invalid": pool.pool.invalid()
            }
        return status

# éåŒæ­¥é€£æ¥æ± 
class AsyncDatabasePool:
    """éåŒæ­¥è³‡æ–™åº«é€£æ¥æ± """
    
    def __init__(self, dsn: str, min_size: int = 10, max_size: int = 20):
        self.dsn = dsn
        self.min_size = min_size
        self.max_size = max_size
        self.pool = None
    
    async def create_pool(self):
        """å‰µå»ºé€£æ¥æ± """
        self.pool = await asyncpg.create_pool(
            self.dsn,
            min_size=self.min_size,
            max_size=self.max_size,
            command_timeout=30,
            server_settings={
                'application_name': 'autovideo_backend',
                'timezone': 'UTC'
            }
        )
    
    async def execute_query(self, query: str, *args):
        """åŸ·è¡ŒæŸ¥è©¢"""
        async with self.pool.acquire() as connection:
            return await connection.fetch(query, *args)
    
    async def execute_transaction(self, queries: list):
        """åŸ·è¡Œäº‹å‹™"""
        async with self.pool.acquire() as connection:
            async with connection.transaction():
                results = []
                for query, args in queries:
                    result = await connection.fetch(query, *args)
                    results.append(result)
                return results
    
    async def close_pool(self):
        """é—œé–‰é€£æ¥æ± """
        if self.pool:
            await self.pool.close()
```

### éåŒæ­¥è™•ç†æ¶æ§‹

#### ä»»å‹™ä½‡åˆ—è¨­è¨ˆ

```python
# Celery ä»»å‹™ä½‡åˆ—é…ç½®
from celery import Celery
from celery.signals import task_prerun, task_postrun, task_failure
import logging

# Celery æ‡‰ç”¨é…ç½®
celery_app = Celery(
    'autovideo',
    broker='redis://redis:6379/0',
    backend='redis://redis:6379/0',
    include=[
        'tasks.video_processing',
        'tasks.ai_generation', 
        'tasks.social_publishing',
        'tasks.analytics'
    ]
)

# é…ç½®è¨­å®š
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    
    # ä»»å‹™è·¯ç”±
    task_routes={
        'tasks.video_processing.*': {'queue': 'video_queue'},
        'tasks.ai_generation.*': {'queue': 'ai_queue'},
        'tasks.social_publishing.*': {'queue': 'social_queue'},
        'tasks.analytics.*': {'queue': 'analytics_queue'},
    },
    
    # å·¥ä½œå™¨é…ç½®
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    
    # çµæœéæœŸæ™‚é–“
    result_expires=3600,
    
    # é‡è©¦é…ç½®
    task_default_retry_delay=60,
    task_max_retries=3,
    
    # ç›£æ§é…ç½®
    worker_send_task_events=True,
    task_send_sent_event=True,
)

# ä»»å‹™å„ªå…ˆç´šéšŠåˆ—
celery_app.conf.task_routes = {
    'tasks.urgent.*': {
        'queue': 'urgent',
        'routing_key': 'urgent',
        'priority': 10
    },
    'tasks.normal.*': {
        'queue': 'normal', 
        'routing_key': 'normal',
        'priority': 5
    },
    'tasks.low.*': {
        'queue': 'low',
        'routing_key': 'low', 
        'priority': 1
    }
}

# å½±ç‰‡è™•ç†ä»»å‹™
@celery_app.task(bind=True, max_retries=3)
def process_video_task(self, video_id: str, processing_config: dict):
    """å½±ç‰‡è™•ç†ä»»å‹™"""
    try:
        # æ›´æ–°è™•ç†ç‹€æ…‹
        update_video_status(video_id, "processing", 0)
        
        # éšæ®µ1: è…³æœ¬ç”Ÿæˆ
        update_video_status(video_id, "processing", 10)
        script_content = generate_script.delay(video_id, processing_config["script"]).get()
        
        # éšæ®µ2: èªéŸ³åˆæˆ
        update_video_status(video_id, "processing", 30)
        audio_url = synthesize_speech.delay(script_content, processing_config["voice"]).get()
        
        # éšæ®µ3: è¦–è¦ºç”Ÿæˆ
        update_video_status(video_id, "processing", 50)
        visual_assets = generate_visuals.delay(script_content, processing_config["visual"]).get()
        
        # éšæ®µ4: å½±ç‰‡çµ„è£
        update_video_status(video_id, "processing", 80)
        video_url = assemble_video.delay(audio_url, visual_assets, processing_config).get()
        
        # å®Œæˆè™•ç†
        update_video_status(video_id, "completed", 100, video_url=video_url)
        
        return {"status": "success", "video_url": video_url}
        
    except Exception as exc:
        # è¨˜éŒ„éŒ¯èª¤
        logging.error(f"Video processing failed for {video_id}: {exc}")
        
        # æ›´æ–°éŒ¯èª¤ç‹€æ…‹
        update_video_status(video_id, "failed", error_message=str(exc))
        
        # é‡è©¦é‚è¼¯
        if self.request.retries < self.max_retries:
            # æŒ‡æ•¸é€€é¿é‡è©¦
            countdown = 60 * (2 ** self.request.retries)
            raise self.retry(countdown=countdown, exc=exc)
        
        raise exc

# ä»»å‹™ç›£æ§
@task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):
    """ä»»å‹™é–‹å§‹å‰çš„è™•ç†"""
    logging.info(f"Task {task.name}[{task_id}] started")

@task_postrun.connect  
def task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kwds):
    """ä»»å‹™å®Œæˆå¾Œçš„è™•ç†"""
    logging.info(f"Task {task.name}[{task_id}] finished with state: {state}")

@task_failure.connect
def task_failure_handler(sender=None, task_id=None, exception=None, traceback=None, einfo=None, **kwds):
    """ä»»å‹™å¤±æ•—è™•ç†"""
    logging.error(f"Task {sender.name}[{task_id}] failed: {exception}")
    
    # ç™¼é€å‘Šè­¦é€šçŸ¥
    send_alert.delay(
        level="error",
        message=f"Task {sender.name} failed: {exception}",
        task_id=task_id
    )
```

## ğŸ“Š ç›£æ§èˆ‡å¯è§€å¯Ÿæ€§

### ç›£æ§æ¶æ§‹è¨­è¨ˆ

```mermaid
graph TB
    subgraph "è³‡æ–™æ”¶é›†å±¤"
        App[Application Metrics]
        Infra[Infrastructure Metrics]
        Logs[Application Logs]
        Traces[Distributed Traces]
    end
    
    subgraph "è³‡æ–™è™•ç†å±¤"
        Prometheus[Prometheus]
        Loki[Loki]
        Jaeger[Jaeger]
        ElasticSearch[ElasticSearch]
    end
    
    subgraph "å¯è¦–åŒ–å±¤"
        Grafana[Grafana Dashboards]
        Kibana[Kibana]
        AlertManager[Alert Manager]
    end
    
    subgraph "é€šçŸ¥å±¤"
        Slack[Slack]
        Email[Email]
        PagerDuty[PagerDuty]
        SMS[SMS]
    end
    
    App --> Prometheus
    Infra --> Prometheus
    Logs --> Loki
    Logs --> ElasticSearch
    Traces --> Jaeger
    
    Prometheus --> Grafana
    Loki --> Grafana
    ElasticSearch --> Kibana
    Jaeger --> Grafana
    
    Prometheus --> AlertManager
    AlertManager --> Slack
    AlertManager --> Email
    AlertManager --> PagerDuty
```

### æ‡‰ç”¨æŒ‡æ¨™ç›£æ§

```python
# Prometheus æŒ‡æ¨™æ”¶é›†
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from functools import wraps
import time

# å®šç¾©æŒ‡æ¨™
REQUEST_COUNT = Counter(
    'http_requests_total',
    'ç¸½ HTTP è«‹æ±‚æ•¸',
    ['method', 'endpoint', 'status_code', 'service']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP è«‹æ±‚æŒçºŒæ™‚é–“',
    ['method', 'endpoint', 'service'],
    buckets=[0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0, 7.5, 10.0]
)

ACTIVE_CONNECTIONS = Gauge(
    'active_connections',
    'ç•¶å‰æ´»èºé€£æ¥æ•¸',
    ['service']
)

VIDEO_PROCESSING_DURATION = Histogram(
    'video_processing_duration_seconds',
    'å½±ç‰‡è™•ç†æŒçºŒæ™‚é–“',
    ['stage', 'status'],
    buckets=[30, 60, 120, 300, 600, 1200, 1800]
)

AI_API_CALLS = Counter(
    'ai_api_calls_total',
    'AI API èª¿ç”¨ç¸½æ•¸',
    ['provider', 'model', 'type', 'status']
)

DATABASE_QUERY_DURATION = Histogram(
    'database_query_duration_seconds',
    'è³‡æ–™åº«æŸ¥è©¢æŒçºŒæ™‚é–“',
    ['query_type', 'table'],
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5]
)

# ç›£æ§è£é£¾å™¨
def monitor_requests(service_name: str):
    """HTTP è«‹æ±‚ç›£æ§è£é£¾å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(request, *args, **kwargs):
            start_time = time.time()
            status_code = 200
            
            try:
                response = await func(request, *args, **kwargs)
                if hasattr(response, 'status_code'):
                    status_code = response.status_code
                return response
            except Exception as e:
                status_code = 500
                raise e
            finally:
                # è¨˜éŒ„æŒ‡æ¨™
                REQUEST_COUNT.labels(
                    method=request.method,
                    endpoint=request.url.path,
                    status_code=status_code,
                    service=service_name
                ).inc()
                
                REQUEST_DURATION.labels(
                    method=request.method,
                    endpoint=request.url.path,
                    service=service_name
                ).observe(time.time() - start_time)
        
        return wrapper
    return decorator

def monitor_video_processing():
    """å½±ç‰‡è™•ç†ç›£æ§è£é£¾å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            stage = func.__name__
            start_time = time.time()
            status = "success"
            
            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                status = "failed"
                raise e
            finally:
                VIDEO_PROCESSING_DURATION.labels(
                    stage=stage,
                    status=status
                ).observe(time.time() - start_time)
        
        return wrapper
    return decorator

def monitor_ai_calls(provider: str, model: str, call_type: str):
    """AI èª¿ç”¨ç›£æ§è£é£¾å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            status = "success"
            
            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                status = "failed"
                raise e
            finally:
                AI_API_CALLS.labels(
                    provider=provider,
                    model=model,
                    type=call_type,
                    status=status
                ).inc()
        
        return wrapper
    return decorator

# è‡ªå®šç¾©æŒ‡æ¨™ç«¯é»
@app.get("/metrics")
async def metrics():
    """Prometheus æŒ‡æ¨™ç«¯é»"""
    return Response(generate_latest(), media_type="text/plain")

# å¥åº·æª¢æŸ¥æŒ‡æ¨™
class HealthChecker:
    """å¥åº·æª¢æŸ¥å™¨"""
    
    def __init__(self):
        self.health_metrics = {
            "database": Gauge('database_health', 'è³‡æ–™åº«å¥åº·ç‹€æ…‹'),
            "redis": Gauge('redis_health', 'Redis å¥åº·ç‹€æ…‹'),
            "ai_services": Gauge('ai_services_health', 'AI æœå‹™å¥åº·ç‹€æ…‹'),
            "storage": Gauge('storage_health', 'å­˜å„²æœå‹™å¥åº·ç‹€æ…‹')
        }
    
    async def check_database_health(self) -> bool:
        """æª¢æŸ¥è³‡æ–™åº«å¥åº·ç‹€æ…‹"""
        try:
            # åŸ·è¡Œç°¡å–®æŸ¥è©¢
            async with get_db_session() as session:
                result = await session.execute("SELECT 1")
                self.health_metrics["database"].set(1)
                return True
        except Exception as e:
            logging.error(f"Database health check failed: {e}")
            self.health_metrics["database"].set(0)
            return False
    
    async def check_redis_health(self) -> bool:
        """æª¢æŸ¥ Redis å¥åº·ç‹€æ…‹"""
        try:
            redis_client = get_redis_client()
            await redis_client.ping()
            self.health_metrics["redis"].set(1)
            return True
        except Exception as e:
            logging.error(f"Redis health check failed: {e}")
            self.health_metrics["redis"].set(0)
            return False
    
    async def run_health_checks(self):
        """åŸ·è¡Œæ‰€æœ‰å¥åº·æª¢æŸ¥"""
        tasks = [
            self.check_database_health(),
            self.check_redis_health(),
            # æ·»åŠ å…¶ä»–æª¢æŸ¥...
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        overall_health = all(isinstance(r, bool) and r for r in results)
        
        return {
            "status": "healthy" if overall_health else "unhealthy",
            "checks": {
                "database": results[0] if isinstance(results[0], bool) else False,
                "redis": results[1] if isinstance(results[1], bool) else False,
            },
            "timestamp": datetime.utcnow().isoformat()
        }

# å®šæœŸå¥åº·æª¢æŸ¥
async def periodic_health_check():
    """å®šæœŸå¥åº·æª¢æŸ¥ä»»å‹™"""
    health_checker = HealthChecker()
    
    while True:
        try:
            await health_checker.run_health_checks()
            await asyncio.sleep(30)  # æ¯30ç§’æª¢æŸ¥ä¸€æ¬¡
        except Exception as e:
            logging.error(f"Health check error: {e}")
            await asyncio.sleep(60)  # éŒ¯èª¤æ™‚å»¶é•·é–“éš”
```

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "\u9032\u884c\u5168\u9762\u67b6\u69cb\u5be9\u67e5\u8207\u4ee3\u78bc\u54c1\u8cea\u8a55\u4f30", "status": "completed", "priority": "high", "id": "architecture_review"}, {"content": "\u8b58\u5225\u9700\u8981\u91cd\u69cb\u7684\u95dc\u9375\u6a21\u7d44", "status": "completed", "priority": "high", "id": "identify_refactor_areas"}, {"content": "\u512a\u5316\u958b\u767c\u8005\u9ad4\u9a57\u8207\u7dad\u8b77\u6027", "status": "completed", "priority": "medium", "id": "improve_developer_experience"}, {"content": "\u5efa\u7acb\u5b8c\u6574\u7684\u958b\u767c\u6587\u6a94\u8207\u6307\u5357", "status": "completed", "priority": "medium", "id": "create_developer_docs"}, {"content": "\u5275\u5efa README.md \u4e3b\u6587\u6a94", "status": "completed", "priority": "high", "id": "create_main_readme"}, {"content": "\u5efa\u7acb\u958b\u767c\u8005\u5165\u9580\u6307\u5357", "status": "completed", "priority": "high", "id": "create_dev_guide"}, {"content": "\u64b0\u5beb API \u6587\u6a94", "status": "completed", "priority": "high", "id": "create_api_docs"}, {"content": "\u5efa\u7acb\u90e8\u7f72\u6307\u5357", "status": "completed", "priority": "medium", "id": "create_deployment_guide"}, {"content": "\u64b0\u5beb\u67b6\u69cb\u6587\u6a94", "status": "completed", "priority": "medium", "id": "create_architecture_docs"}, {"content": "\u5efa\u7acb\u6545\u969c\u6392\u9664\u6307\u5357", "status": "in_progress", "priority": "medium", "id": "create_troubleshooting"}]