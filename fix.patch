diff --git a/.claude/settings.local.json b/.claude/settings.local.json
index 5fe0665..d0422f6 100644
--- a/.claude/settings.local.json
+++ b/.claude/settings.local.json
@@ -8,7 +8,28 @@
       "Bash(mkdir:*)",
       "Bash(python scripts/validate_backup_config:*)",
       "Bash(chmod:*)",
-      "Bash(git add:*)"
+      "Bash(git add:*)",
+      "Bash(git commit:*)",
+      "Bash(git push:*)",
+      "Bash(ln:*)",
+      "Bash(diff:*)",
+      "Bash(ls:*)",
+      "Bash(rm:*)",
+      "Bash(tree:*)",
+      "Bash(find:*)",
+      "Bash(cp:*)",
+      "Bash(echo:*)",
+      "Bash(python -m pytest --version)",
+      "Bash(cd:*)",
+      "Bash(python:*)",
+      "Bash(black --version)",
+      "Bash(ruff:*)",
+      "Bash(black:*)",
+      "Bash(npm:*)",
+      "Bash(node:*)",
+      "Bash(pytest:*)",
+      "Bash(docker:*)",
+      "Bash(docker-compose:*)"
     ],
     "deny": []
   }
diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml
index 340aaff..e827b5d 100644
--- a/.github/workflows/ci.yml
+++ b/.github/workflows/ci.yml
@@ -60,7 +60,8 @@ jobs:
         env:
           SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
         with:
-          args: --severity-threshold=high --sarif-file-output=snyk.sarif
+          command: test
+          args: --all-projects --sarif-file-output=snyk.sarif
 
       - name: Upload Snyk results to GitHub Security
         uses: github/codeql-action/upload-sarif@v3
diff --git a/auto_generate_video_fold6/compliance/gdpr/gdpr-compliance.py b/auto_generate_video_fold6/compliance/gdpr/gdpr-compliance.py
index 29f79d3..e50f509 100644
--- a/auto_generate_video_fold6/compliance/gdpr/gdpr-compliance.py
+++ b/auto_generate_video_fold6/compliance/gdpr/gdpr-compliance.py
@@ -10,7 +10,16 @@ from datetime import datetime, timedelta
 from typing import Dict, List, Optional, Any
 from enum import Enum
 from dataclasses import dataclass, asdict
-from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, Boolean, JSON
+from sqlalchemy import (
+    create_engine,
+    Column,
+    Integer,
+    String,
+    DateTime,
+    Text,
+    Boolean,
+    JSON,
+)
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import sessionmaker
 import logging
@@ -82,8 +91,12 @@ class GDPRDataRequest(Base):
 
     id = Column(Integer, primary_key=True)
     user_id = Column(Integer, nullable=False)
-    request_type = Column(String(50), nullable=False)  # access, rectification, erasure, portability
-    status = Column(String(50), default="pending")  # pending, processing, completed, rejected
+    request_type = Column(
+        String(50), nullable=False
+    )  # access, rectification, erasure, portability
+    status = Column(
+        String(50), default="pending"
+    )  # pending, processing, completed, rejected
     requested_at = Column(DateTime, default=datetime.utcnow)
     completed_at = Column(DateTime)
     request_details = Column(JSON)
@@ -126,7 +139,9 @@ class GDPRComplianceManager:
                 ip_address=consent.ip_address,
                 user_agent=consent.user_agent,
                 details={
-                    "expires_at": consent.expires_at.isoformat() if consent.expires_at else None
+                    "expires_at": consent.expires_at.isoformat()
+                    if consent.expires_at
+                    else None
                 },
             )
 
@@ -143,7 +158,9 @@ class GDPRComplianceManager:
             await self.db.rollback()
             return False
 
-    async def withdraw_consent(self, user_id: int, purpose: DataProcessingPurpose) -> bool:
+    async def withdraw_consent(
+        self, user_id: int, purpose: DataProcessingPurpose
+    ) -> bool:
         """æ’¤å›åŒæ„"""
         try:
             log_entry = GDPRConsentLog(
@@ -157,7 +174,9 @@ class GDPRComplianceManager:
             self.db.add(log_entry)
             await self.db.commit()
 
-            logger.info(f"Consent withdrawn for user {user_id}, purpose: {purpose.value}")
+            logger.info(
+                f"Consent withdrawn for user {user_id}, purpose: {purpose.value}"
+            )
             return True
 
         except Exception as e:
@@ -165,13 +184,18 @@ class GDPRComplianceManager:
             await self.db.rollback()
             return False
 
-    async def check_consent(self, user_id: int, purpose: DataProcessingPurpose) -> bool:
+    async def check_consent(
+        self, user_id: int, purpose: DataProcessingPurpose
+    ) -> bool:
         """æª¢æŸ¥åŒæ„ç‹€æ…‹"""
         try:
             # æŸ¥æ‰¾æœ€æ–°çš„åŒæ„è¨˜éŒ„
             latest_consent = (
                 self.db.query(GDPRConsentLog)
-                .filter(GDPRConsentLog.user_id == user_id, GDPRConsentLog.purpose == purpose.value)
+                .filter(
+                    GDPRConsentLog.user_id == user_id,
+                    GDPRConsentLog.purpose == purpose.value,
+                )
                 .order_by(GDPRConsentLog.timestamp.desc())
                 .first()
             )
@@ -236,7 +260,9 @@ class DataRequestHandler:
         """å‰µå»ºè³‡æ–™è«‹æ±‚"""
         try:
             # ç”Ÿæˆé©—è­‰ä»¤ç‰Œ
-            verification_token = self._generate_verification_token(user_id, request_type)
+            verification_token = self._generate_verification_token(
+                user_id, request_type
+            )
 
             request = GDPRDataRequest(
                 user_id=user_id,
@@ -248,7 +274,9 @@ class DataRequestHandler:
             self.db.add(request)
             await self.db.commit()
 
-            logger.info(f"Data request created for user {user_id}, type: {request_type}")
+            logger.info(
+                f"Data request created for user {user_id}, type: {request_type}"
+            )
             return verification_token
 
         except Exception as e:
@@ -261,7 +289,10 @@ class DataRequestHandler:
         try:
             request = (
                 self.db.query(GDPRDataRequest)
-                .filter(GDPRDataRequest.id == request_id, GDPRDataRequest.request_type == "access")
+                .filter(
+                    GDPRDataRequest.id == request_id,
+                    GDPRDataRequest.request_type == "access",
+                )
                 .first()
             )
 
@@ -295,7 +326,10 @@ class DataRequestHandler:
         try:
             request = (
                 self.db.query(GDPRDataRequest)
-                .filter(GDPRDataRequest.id == request_id, GDPRDataRequest.request_type == "erasure")
+                .filter(
+                    GDPRDataRequest.id == request_id,
+                    GDPRDataRequest.request_type == "erasure",
+                )
                 .first()
             )
 
@@ -331,7 +365,8 @@ class DataRequestHandler:
             request = (
                 self.db.query(GDPRDataRequest)
                 .filter(
-                    GDPRDataRequest.id == request_id, GDPRDataRequest.request_type == "portability"
+                    GDPRDataRequest.id == request_id,
+                    GDPRDataRequest.request_type == "portability",
                 )
                 .first()
             )
@@ -369,11 +404,21 @@ class DataRequestHandler:
         """æ”¶é›†ç”¨æˆ¶æ‰€æœ‰è³‡æ–™"""
         user_data = {
             "user_profile": await self.user_service.get_user_profile(user_id),
-            "audio_files": await self.data_service.get_user_audio_files(user_id),
-            "training_jobs": await self.data_service.get_user_training_jobs(user_id),
-            "video_projects": await self.data_service.get_user_video_projects(user_id),
-            "social_accounts": await self.data_service.get_user_social_accounts(user_id),
-            "usage_analytics": await self.data_service.get_user_analytics(user_id),
+            "audio_files": await self.data_service.get_user_audio_files(
+                user_id
+            ),
+            "training_jobs": await self.data_service.get_user_training_jobs(
+                user_id
+            ),
+            "video_projects": await self.data_service.get_user_video_projects(
+                user_id
+            ),
+            "social_accounts": await self.data_service.get_user_social_accounts(
+                user_id
+            ),
+            "usage_analytics": await self.data_service.get_user_analytics(
+                user_id
+            ),
             "consent_history": await self._get_user_consent_history(user_id),
             "processing_logs": await self._get_user_processing_logs(user_id),
         }
@@ -386,29 +431,41 @@ class DataRequestHandler:
 
         try:
             # åˆªé™¤éŸ³é »æª”æ¡ˆ
-            audio_count = await self.data_service.delete_user_audio_files(user_id)
+            audio_count = await self.data_service.delete_user_audio_files(
+                user_id
+            )
             if audio_count > 0:
                 deleted_items.append(f"audio_files: {audio_count}")
 
             # åˆªé™¤è¨“ç·´ä»»å‹™
-            training_count = await self.data_service.delete_user_training_jobs(user_id)
+            training_count = await self.data_service.delete_user_training_jobs(
+                user_id
+            )
             if training_count > 0:
                 deleted_items.append(f"training_jobs: {training_count}")
 
             # åˆªé™¤å½±ç‰‡å°ˆæ¡ˆ
-            video_count = await self.data_service.delete_user_video_projects(user_id)
+            video_count = await self.data_service.delete_user_video_projects(
+                user_id
+            )
             if video_count > 0:
                 deleted_items.append(f"video_projects: {video_count}")
 
             # åˆªé™¤ç¤¾ç¾¤åª’é«”å¸³è™Ÿé€£çµ
-            social_count = await self.data_service.delete_user_social_accounts(user_id)
+            social_count = await self.data_service.delete_user_social_accounts(
+                user_id
+            )
             if social_count > 0:
                 deleted_items.append(f"social_accounts: {social_count}")
 
             # åŒ¿ååŒ–åˆ†æè³‡æ–™ï¼ˆä¿ç•™èšåˆçµ±è¨ˆï¼‰
-            analytics_count = await self.data_service.anonymize_user_analytics(user_id)
+            analytics_count = await self.data_service.anonymize_user_analytics(
+                user_id
+            )
             if analytics_count > 0:
-                deleted_items.append(f"analytics_anonymized: {analytics_count}")
+                deleted_items.append(
+                    f"analytics_anonymized: {analytics_count}"
+                )
 
             # æœ€å¾Œåˆªé™¤ç”¨æˆ¶å¸³è™Ÿ
             await self.user_service.delete_user_account(user_id)
@@ -466,7 +523,9 @@ class DataRequestHandler:
             for log in logs
         ]
 
-    def _generate_verification_token(self, user_id: int, request_type: str) -> str:
+    def _generate_verification_token(
+        self, user_id: int, request_type: str
+    ) -> str:
         """ç”Ÿæˆé©—è­‰ä»¤ç‰Œ"""
         content = f"{user_id}:{request_type}:{datetime.utcnow().isoformat()}"
         return hashlib.sha256(content.encode()).hexdigest()
@@ -521,27 +580,40 @@ class DataRetentionManager:
                 # åŸ·è¡Œæ¸…ç†å‹•ä½œ
                 await self._cleanup_expired_data(category, expired_logs)
                 compliance_report["actions_taken"].append(
-                    {"category": category.value, "cleaned_records": len(expired_logs)}
+                    {
+                        "category": category.value,
+                        "cleaned_records": len(expired_logs),
+                    }
                 )
 
         return compliance_report
 
-    async def _cleanup_expired_data(self, category: DataCategory, expired_logs: List):
+    async def _cleanup_expired_data(
+        self, category: DataCategory, expired_logs: List
+    ):
         """æ¸…ç†éæœŸè³‡æ–™"""
         try:
             for log in expired_logs:
                 if category == DataCategory.AUDIO_DATA:
-                    await self.data_service.delete_expired_audio_files(log.user_id, log.timestamp)
+                    await self.data_service.delete_expired_audio_files(
+                        log.user_id, log.timestamp
+                    )
                 elif category == DataCategory.VIDEO_DATA:
-                    await self.data_service.delete_expired_video_files(log.user_id, log.timestamp)
+                    await self.data_service.delete_expired_video_files(
+                        log.user_id, log.timestamp
+                    )
                 elif category == DataCategory.USAGE_DATA:
-                    await self.data_service.anonymize_expired_usage_data(log.user_id, log.timestamp)
+                    await self.data_service.anonymize_expired_usage_data(
+                        log.user_id, log.timestamp
+                    )
 
                 # åˆªé™¤è™•ç†æ—¥èªŒè¨˜éŒ„
                 self.db.delete(log)
 
             await self.db.commit()
-            logger.info(f"Cleaned up {len(expired_logs)} expired records for {category.value}")
+            logger.info(
+                f"Cleaned up {len(expired_logs)} expired records for {category.value}"
+            )
 
         except Exception as e:
             logger.error(f"Failed to cleanup expired data: {e}")
diff --git a/auto_generate_video_fold6/config/config_manager.py b/auto_generate_video_fold6/config/config_manager.py
index 76dad1d..6b3f2d4 100755
--- a/auto_generate_video_fold6/config/config_manager.py
+++ b/auto_generate_video_fold6/config/config_manager.py
@@ -4,13 +4,13 @@
 æ”¯æ´å¤šå±¤æ¬¡é…ç½®ç¹¼æ‰¿èˆ‡å‹•æ…‹è¼‰å…¥
 """
 
+import copy
 import json
-import os
 import logging
-from typing import Dict, Any, Optional, List
-from pathlib import Path
+import os
 from datetime import datetime
-import copy
+from pathlib import Path
+from typing import Any
 
 logger = logging.getLogger(__name__)
 
@@ -19,29 +19,36 @@ class ConfigManager:
     """çµ±ä¸€é…ç½®ç®¡ç†å™¨"""
 
     def __init__(self, config_dir: str = None):
-        self.config_dir = Path(config_dir or os.path.join(os.path.dirname(__file__)))
-        self.current_config: Dict[str, Any] = {}
-        self.base_config: Dict[str, Any] = {}
-        self.mode_config: Dict[str, Any] = {}
+        self.config_dir = Path(
+            config_dir or os.path.join(os.path.dirname(__file__))
+        )
+        self.current_config: dict[str, Any] = {}
+        self.base_config: dict[str, Any] = {}
+        self.mode_config: dict[str, Any] = {}
         self.current_mode: str = "base"
 
         # è¼‰å…¥åŸºç¤é…ç½®
         self._load_base_config()
 
+        # åˆå§‹åŒ–ç•¶å‰é…ç½®
+        self._merge_configs()
+
     def _load_base_config(self) -> None:
         """è¼‰å…¥åŸºç¤é…ç½®"""
         base_config_path = self.config_dir / "base-config.json"
 
         if not base_config_path.exists():
             logger.error(f"åŸºç¤é…ç½®æª”æ¡ˆä¸å­˜åœ¨: {base_config_path}")
-            raise FileNotFoundError(f"Base config not found: {base_config_path}")
+            raise FileNotFoundError(
+                f"Base config not found: {base_config_path}"
+            )
 
-        with open(base_config_path, "r", encoding="utf-8") as f:
+        with open(base_config_path, encoding="utf-8") as f:
             self.base_config = json.load(f)
 
         logger.info("å·²è¼‰å…¥åŸºç¤é…ç½®")
 
-    def load_mode_config(self, mode: str) -> Dict[str, Any]:
+    def load_mode_config(self, mode: str) -> dict[str, Any]:
         """è¼‰å…¥ç‰¹å®šæ¨¡å¼é…ç½®"""
         mode_config_path = self.config_dir / f"{mode}-config.json"
 
@@ -49,7 +56,7 @@ class ConfigManager:
             logger.warning(f"æ¨¡å¼é…ç½®æª”æ¡ˆä¸å­˜åœ¨: {mode_config_path}")
             return {}
 
-        with open(mode_config_path, "r", encoding="utf-8") as f:
+        with open(mode_config_path, encoding="utf-8") as f:
             mode_config = json.load(f)
 
         logger.info(f"å·²è¼‰å…¥ {mode} æ¨¡å¼é…ç½®")
@@ -83,10 +90,16 @@ class ConfigManager:
             "config_dir": str(self.config_dir),
         }
 
-    def _deep_merge(self, base: Dict[str, Any], override: Dict[str, Any]) -> None:
+    def _deep_merge(
+        self, base: dict[str, Any], override: dict[str, Any]
+    ) -> None:
         """æ·±åº¦åˆä½µå­—å…¸"""
         for key, value in override.items():
-            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
+            if (
+                key in base
+                and isinstance(base[key], dict)
+                and isinstance(value, dict)
+            ):
                 self._deep_merge(base[key], value)
             else:
                 base[key] = value
@@ -117,19 +130,19 @@ class ConfigManager:
         # è¨­ç½®å€¼
         current[keys[-1]] = value
 
-    def get_service_config(self, service_name: str) -> Dict[str, Any]:
+    def get_service_config(self, service_name: str) -> dict[str, Any]:
         """ç²å–ç‰¹å®šæœå‹™é…ç½®"""
         return self.get(f"services.{service_name}", {})
 
-    def get_generation_config(self) -> Dict[str, Any]:
+    def get_generation_config(self) -> dict[str, Any]:
         """ç²å–ç”Ÿæˆé…ç½®"""
         return self.get("generation", {})
 
-    def get_cost_config(self) -> Dict[str, Any]:
+    def get_cost_config(self) -> dict[str, Any]:
         """ç²å–æˆæœ¬æ§åˆ¶é…ç½®"""
         return self.get("cost_control", {})
 
-    def get_resource_config(self) -> Dict[str, Any]:
+    def get_resource_config(self) -> dict[str, Any]:
         """ç²å–è³‡æºé…ç½®"""
         return self.get("resources", {})
 
@@ -165,7 +178,9 @@ class ConfigManager:
 
     def get_api_rate_limit(self, provider: str) -> int:
         """ç²å– API é€Ÿç‡é™åˆ¶"""
-        return self.get(f"cost_control.api_rate_limits.{provider}_requests_per_hour", 100)
+        return self.get(
+            f"cost_control.api_rate_limits.{provider}_requests_per_hour", 100
+        )
 
     def save_current_config(self, filename: str = None) -> str:
         """ä¿å­˜ç•¶å‰é…ç½®åˆ°æª”æ¡ˆ"""
@@ -180,7 +195,7 @@ class ConfigManager:
         logger.info(f"å·²ä¿å­˜ç•¶å‰é…ç½®åˆ°: {output_path}")
         return str(output_path)
 
-    def validate_config(self) -> List[str]:
+    def validate_config(self) -> list[str]:
         """é©—è­‰é…ç½®å®Œæ•´æ€§"""
         errors = []
 
@@ -188,7 +203,7 @@ class ConfigManager:
         required_fields = [
             "generation.daily_video_limit",
             "generation.platforms",
-            "ai_services.text_generation.provider",
+            "ai_services.text_generation.primary_provider",
             "ai_services.image_generation.provider",
         ]
 
@@ -207,15 +222,19 @@ class ConfigManager:
 
         return errors
 
-    def get_platform_config(self, platform: str) -> Dict[str, Any]:
+    def get_platform_config(self, platform: str) -> dict[str, Any]:
         """ç²å–ç‰¹å®šå¹³å°é…ç½®"""
         return self.get(f"video_styles.{platform}", {})
 
-    def get_content_template(self, category: str, template_type: str = "intro") -> List[str]:
+    def get_content_template(
+        self, category: str, template_type: str = "intro"
+    ) -> list[str]:
         """ç²å–å…§å®¹æ¨¡æ¿"""
-        return self.get(f"content_templates.{category}.{template_type}_templates", [])
+        return self.get(
+            f"content_templates.{category}.{template_type}_templates", []
+        )
 
-    def get_enabled_platforms(self) -> List[str]:
+    def get_enabled_platforms(self) -> list[str]:
         """ç²å–å•Ÿç”¨çš„å¹³å°åˆ—è¡¨"""
         return self.get("generation.platforms", [])
 
@@ -241,7 +260,7 @@ class ConfigManager:
         logger.info(f"å·²åŒ¯å‡ºé…ç½®åˆ°: {export_path}")
         return str(export_path)
 
-    def get_summary(self) -> Dict[str, Any]:
+    def get_summary(self) -> dict[str, Any]:
         """ç²å–é…ç½®æ‘˜è¦"""
         return {
             "current_mode": self.current_mode,
diff --git a/auto_generate_video_fold6/conftest.py b/auto_generate_video_fold6/conftest.py
index 8f43b78..58a0fc0 100644
--- a/auto_generate_video_fold6/conftest.py
+++ b/auto_generate_video_fold6/conftest.py
@@ -52,7 +52,9 @@ async def test_engine():
 async def db_session(test_engine) -> AsyncGenerator[AsyncSession, None]:
     """å‰µå»ºæ¸¬è©¦ç”¨çš„è³‡æ–™åº«æœƒè©±"""
     async with test_engine.begin() as conn:
-        async_session = sessionmaker(conn, class_=AsyncSession, expire_on_commit=False)
+        async_session = sessionmaker(
+            conn, class_=AsyncSession, expire_on_commit=False
+        )
         async with async_session() as session:
             yield session
 
@@ -89,7 +91,9 @@ def mock_s3():
 @pytest.fixture
 async def async_client() -> AsyncGenerator[AsyncClient, None]:
     """å‰µå»ºç•°æ­¥ HTTP å®¢æˆ¶ç«¯ä¾›æ¸¬è©¦ä½¿ç”¨"""
-    async with AsyncClient(base_url="http://testserver", timeout=30.0) as client:
+    async with AsyncClient(
+        base_url="http://testserver", timeout=30.0
+    ) as client:
         yield client
 
 
@@ -117,7 +121,11 @@ def sample_project_data():
         "description": "A test project for automated testing",
         "status": "draft",
         "user_id": 1,
-        "settings": {"platform": "youtube", "duration": "short", "tone": "professional"},
+        "settings": {
+            "platform": "youtube",
+            "duration": "short",
+            "tone": "professional",
+        },
         "created_at": "2024-01-01T00:00:00Z",
         "updated_at": "2024-01-01T00:00:00Z",
     }
@@ -131,16 +139,30 @@ def sample_script_data():
         "title": "Test Script",
         "content": "This is a test script content for automated testing.",
         "scenes": [
-            {"id": 1, "text": "Welcome to our test video", "duration": 3, "type": "intro"},
+            {
+                "id": 1,
+                "text": "Welcome to our test video",
+                "duration": 3,
+                "type": "intro",
+            },
             {
                 "id": 2,
                 "text": "This is the main content of our test",
                 "duration": 8,
                 "type": "content",
             },
-            {"id": 3, "text": "Thank you for watching", "duration": 2, "type": "outro"},
+            {
+                "id": 3,
+                "text": "Thank you for watching",
+                "duration": 2,
+                "type": "outro",
+            },
         ],
-        "metadata": {"duration": 13, "word_count": 85, "reading_time": "13 seconds"},
+        "metadata": {
+            "duration": 13,
+            "word_count": 85,
+            "reading_time": "13 seconds",
+        },
         "project_id": 1,
         "created_at": "2024-01-01T00:00:00Z",
         "updated_at": "2024-01-01T00:00:00Z",
@@ -158,11 +180,30 @@ def mock_ai_service():
             "title": "Generated Test Script",
             "content": "This is AI generated content for testing.",
             "scenes": [
-                {"id": 1, "text": "Generated intro", "duration": 3, "type": "intro"},
-                {"id": 2, "text": "Generated content", "duration": 8, "type": "content"},
-                {"id": 3, "text": "Generated outro", "duration": 2, "type": "outro"},
+                {
+                    "id": 1,
+                    "text": "Generated intro",
+                    "duration": 3,
+                    "type": "intro",
+                },
+                {
+                    "id": 2,
+                    "text": "Generated content",
+                    "duration": 8,
+                    "type": "content",
+                },
+                {
+                    "id": 3,
+                    "text": "Generated outro",
+                    "duration": 2,
+                    "type": "outro",
+                },
             ],
-            "metadata": {"duration": 13, "word_count": 67, "reading_time": "13 seconds"},
+            "metadata": {
+                "duration": 13,
+                "word_count": 67,
+                "reading_time": "13 seconds",
+            },
         }
     )
 
@@ -214,7 +255,10 @@ def auth_headers(sample_user_data):
     """æä¾›èªè­‰æ¨™é ­"""
     # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒç”ŸæˆçœŸå¯¦çš„ JWT token
     fake_token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.test.token"
-    return {"Authorization": f"Bearer {fake_token}", "Content-Type": "application/json"}
+    return {
+        "Authorization": f"Bearer {fake_token}",
+        "Content-Type": "application/json",
+    }
 
 
 # æ¸¬è©¦æ¨™è¨˜è¨­å®š
@@ -225,11 +269,19 @@ pytest_plugins = []
 def pytest_configure(config):
     """é…ç½® pytest"""
     config.addinivalue_line("markers", "unit: marks tests as unit tests")
-    config.addinivalue_line("markers", "integration: marks tests as integration tests")
+    config.addinivalue_line(
+        "markers", "integration: marks tests as integration tests"
+    )
     config.addinivalue_line("markers", "slow: marks tests as slow running")
-    config.addinivalue_line("markers", "requires_db: marks tests that require database")
-    config.addinivalue_line("markers", "requires_redis: marks tests that require Redis")
-    config.addinivalue_line("markers", "requires_s3: marks tests that require S3")
+    config.addinivalue_line(
+        "markers", "requires_db: marks tests that require database"
+    )
+    config.addinivalue_line(
+        "markers", "requires_redis: marks tests that require Redis"
+    )
+    config.addinivalue_line(
+        "markers", "requires_s3: marks tests that require S3"
+    )
 
 
 # è‡ªå‹•ä½¿ç”¨ fixtures
diff --git a/auto_generate_video_fold6/examples/ai_integration_demo.py b/auto_generate_video_fold6/examples/ai_integration_demo.py
index 891bab0..4ea2935 100644
--- a/auto_generate_video_fold6/examples/ai_integration_demo.py
+++ b/auto_generate_video_fold6/examples/ai_integration_demo.py
@@ -23,7 +23,11 @@ async def demo_gemini_integration():
         import sys
         import os
 
-        sys.path.append(os.path.join(os.path.dirname(__file__), "..", "services", "ai-service"))
+        sys.path.append(
+            os.path.join(
+                os.path.dirname(__file__), "..", "services", "ai-service"
+            )
+        )
         from gemini_client import (
             generate_video_script,
             analyze_trends,
@@ -59,7 +63,9 @@ async def demo_gemini_integration():
             print("âœ… è¶¨å‹¢åˆ†ææˆåŠŸ:")
             print(f"ç—…æ¯’æ½›åŠ›: {analysis.get('viral_potential', 'N/A')}/10")
             print(f"ç›®æ¨™å—çœ¾: {analysis.get('target_audience', 'N/A')}")
-            print(f"æ¨è–¦å¹³å°: {', '.join(analysis.get('recommended_platforms', []))}")
+            print(
+                f"æ¨è–¦å¹³å°: {', '.join(analysis.get('recommended_platforms', []))}"
+            )
         else:
             print(f"âŒ è¶¨å‹¢åˆ†æå¤±æ•—: {analysis.get('error')}")
 
@@ -68,7 +74,9 @@ async def demo_gemini_integration():
         async with GeminiClient(api_key=api_key) as client:
             result = await client.generate_content(
                 prompt="ç‚ºç§‘æŠ€çŸ­å½±ç‰‡å‰µä½œå¸å¼•äººçš„é–‹å ´ç™½ï¼Œè¦æ±‚ç”Ÿå‹•æœ‰è¶£",
-                generation_config=GeminiGenerationConfig(temperature=0.9, max_output_tokens=150),
+                generation_config=GeminiGenerationConfig(
+                    temperature=0.9, max_output_tokens=150
+                ),
             )
 
             if result.success:
@@ -91,8 +99,16 @@ async def demo_suno_integration():
         import sys
         import os
 
-        sys.path.append(os.path.join(os.path.dirname(__file__), "..", "services", "music-service"))
-        from suno_client import generate_music_for_video, SunoClient, MusicGenerationRequest
+        sys.path.append(
+            os.path.join(
+                os.path.dirname(__file__), "..", "services", "music-service"
+            )
+        )
+        from suno_client import (
+            generate_music_for_video,
+            SunoClient,
+            MusicGenerationRequest,
+        )
 
         api_key = os.getenv("SUNO_API_KEY")
         if not api_key:
@@ -119,14 +135,18 @@ async def demo_suno_integration():
             if music_result.audio_url:
                 async with SunoClient(api_key=api_key) as client:
                     output_path = Path("examples/demo_music.mp3")
-                    success = await client.download_audio(music_result.audio_url, output_path)
+                    success = await client.download_audio(
+                        music_result.audio_url, output_path
+                    )
 
                     if success:
                         print(f"âœ… éŸ³æ¨‚æ–‡ä»¶å·²ä¸‹è¼‰åˆ°: {output_path}")
                     else:
                         print("âŒ éŸ³æ¨‚æ–‡ä»¶ä¸‹è¼‰å¤±æ•—")
         else:
-            print(f"âŒ éŸ³æ¨‚ç”Ÿæˆå¤±æ•—: {music_result.error_message if music_result else 'æœªçŸ¥éŒ¯èª¤'}")
+            print(
+                f"âŒ éŸ³æ¨‚ç”Ÿæˆå¤±æ•—: {music_result.error_message if music_result else 'æœªçŸ¥éŒ¯èª¤'}"
+            )
 
         # 2. ç”Ÿæˆä¸åŒé¢¨æ ¼çš„éŸ³æ¨‚
         print("\nğŸª ç”Ÿæˆå¨›æ¨‚é¢¨æ ¼éŸ³æ¨‚...")
@@ -139,13 +159,17 @@ async def demo_suno_integration():
                 title="å¨›æ¨‚ç¯€ç›®èƒŒæ™¯éŸ³æ¨‚",
             )
 
-            entertainment_result = await client.generate_music(entertainment_request)
+            entertainment_result = await client.generate_music(
+                entertainment_request
+            )
 
             if entertainment_result.status == "completed":
                 print("âœ… å¨›æ¨‚é¢¨æ ¼éŸ³æ¨‚ç”ŸæˆæˆåŠŸ!")
                 print(f"æ¨™é¡Œ: {entertainment_result.title}")
             else:
-                print(f"âŒ å¨›æ¨‚é¢¨æ ¼éŸ³æ¨‚ç”Ÿæˆå¤±æ•—: {entertainment_result.error_message}")
+                print(
+                    f"âŒ å¨›æ¨‚é¢¨æ ¼éŸ³æ¨‚ç”Ÿæˆå¤±æ•—: {entertainment_result.error_message}"
+                )
 
     except ImportError as e:
         print(f"âŒ å°å…¥éŒ¯èª¤: {e}")
@@ -193,7 +217,9 @@ async def demo_ai_orchestrator():
             fallback_enabled=True,
         )
 
-        analysis_response = await orchestrator.process_request(analysis_request)
+        analysis_response = await orchestrator.process_request(
+            analysis_request
+        )
 
         if analysis_response.success:
             print("âœ… å…§å®¹åˆ†ææˆåŠŸ:")
@@ -209,7 +235,9 @@ async def demo_ai_orchestrator():
         # 3. éŸ³æ¨‚ç”Ÿæˆ
         print("\nğŸµ æ™ºèƒ½éŸ³æ¨‚ç”Ÿæˆ...")
         music_content = await generate_music_for_video(
-            prompt="ç§‘æŠ€æ„Ÿåè¶³çš„èƒŒæ™¯éŸ³æ¨‚ï¼Œé©åˆç”¢å“ä»‹ç´¹", duration=20, style="futuristic, tech"
+            prompt="ç§‘æŠ€æ„Ÿåè¶³çš„èƒŒæ™¯éŸ³æ¨‚ï¼Œé©åˆç”¢å“ä»‹ç´¹",
+            duration=20,
+            style="futuristic, tech",
         )
 
         if music_content:
@@ -253,7 +281,9 @@ async def demo_complete_workflow():
                 style="professional",
                 api_key=gemini_key,
             )
-            print(f"âœ… è…³æœ¬: {script[:100]}..." if script else "âŒ è…³æœ¬ç”Ÿæˆå¤±æ•—")
+            print(
+                f"âœ… è…³æœ¬: {script[:100]}..." if script else "âŒ è…³æœ¬ç”Ÿæˆå¤±æ•—"
+            )
         else:
             script = "AI æŠ€è¡“æ­£åœ¨å¿«é€Ÿç™¼å±•ï¼Œæ”¹è®Šè‘—æˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼..."
             print("âš ï¸ ä½¿ç”¨é è¨­è…³æœ¬ï¼ˆæœªè¨­ç½® Gemini API Keyï¼‰")
@@ -278,12 +308,18 @@ async def demo_complete_workflow():
 
         # 3. åˆ†æå’Œå„ªåŒ–
         print("\n3ï¸âƒ£ å…§å®¹åˆ†æå’Œå„ªåŒ–å»ºè­°...")
-        from services.ai_service.ai_orchestrator import AIOrchestrator, AIRequest, AITaskType
+        from services.ai_service.ai_orchestrator import (
+            AIOrchestrator,
+            AIRequest,
+            AITaskType,
+        )
 
         orchestrator = AIOrchestrator()
 
         trend_request = AIRequest(
-            task_type=AITaskType.TREND_ANALYSIS, prompt=script, fallback_enabled=True
+            task_type=AITaskType.TREND_ANALYSIS,
+            prompt=script,
+            fallback_enabled=True,
         )
 
         trend_response = await orchestrator.process_request(trend_request)
diff --git a/auto_generate_video_fold6/monitoring/analytics/log_analyzer.py b/auto_generate_video_fold6/monitoring/analytics/log_analyzer.py
index 50a5f4d..f97117a 100644
--- a/auto_generate_video_fold6/monitoring/analytics/log_analyzer.py
+++ b/auto_generate_video_fold6/monitoring/analytics/log_analyzer.py
@@ -111,7 +111,9 @@ class LogAnalyzer:
         }
 
         try:
-            result = await self.es.search(index="auto-video-logs-*", body=query, size=0)
+            result = await self.es.search(
+                index="auto-video-logs-*", body=query, size=0
+            )
 
             total_errors = result["hits"]["total"]["value"]
 
@@ -130,7 +132,9 @@ class LogAnalyzer:
                 await self.send_alert(alert)
 
             # åˆ†æéŒ¯èª¤åˆ†ä½ˆ
-            services_with_errors = result["aggregations"]["error_by_service"]["buckets"]
+            services_with_errors = result["aggregations"]["error_by_service"][
+                "buckets"
+            ]
             for service_bucket in services_with_errors:
                 service = service_bucket["key"]
                 error_count = service_bucket["doc_count"]
@@ -168,16 +172,26 @@ class LogAnalyzer:
             },
             "aggs": {
                 "avg_response_time": {"avg": {"field": "duration_ms"}},
-                "p95_response_time": {"percentiles": {"field": "duration_ms", "percents": [95]}},
-                "slow_requests": {"filter": {"range": {"duration_ms": {"gte": 5000}}}},
+                "p95_response_time": {
+                    "percentiles": {"field": "duration_ms", "percents": [95]}
+                },
+                "slow_requests": {
+                    "filter": {"range": {"duration_ms": {"gte": 5000}}}
+                },
             },
         }
 
         try:
-            result = await self.es.search(index="auto-video-logs-*", body=query, size=0)
+            result = await self.es.search(
+                index="auto-video-logs-*", body=query, size=0
+            )
 
-            p95_time = result["aggregations"]["p95_response_time"]["values"]["95.0"]
-            slow_requests = result["aggregations"]["slow_requests"]["doc_count"]
+            p95_time = result["aggregations"]["p95_response_time"]["values"][
+                "95.0"
+            ]
+            slow_requests = result["aggregations"]["slow_requests"][
+                "doc_count"
+            ]
 
             # æª¢æŸ¥å›æ‡‰æ™‚é–“
             if p95_time > self.thresholds["response_time_p95"]:
@@ -224,7 +238,11 @@ class LogAnalyzer:
                     "bool": {
                         "must": [
                             {"match": {"message": "failed login"}},
-                            {"range": {"@timestamp": {"gte": since.isoformat()}}},
+                            {
+                                "range": {
+                                    "@timestamp": {"gte": since.isoformat()}
+                                }
+                            },
                         ]
                     }
                 },
@@ -236,7 +254,11 @@ class LogAnalyzer:
                     "bool": {
                         "must": [
                             {"terms": {"status_code": [401, 403]}},
-                            {"range": {"@timestamp": {"gte": since.isoformat()}}},
+                            {
+                                "range": {
+                                    "@timestamp": {"gte": since.isoformat()}
+                                }
+                            },
                         ]
                     }
                 },
@@ -247,8 +269,16 @@ class LogAnalyzer:
                 "query": {
                     "bool": {
                         "must": [
-                            {"regexp": {"message": ".*(attack|breach|injection|xss).*"}},
-                            {"range": {"@timestamp": {"gte": since.isoformat()}}},
+                            {
+                                "regexp": {
+                                    "message": ".*(attack|breach|injection|xss).*"
+                                }
+                            },
+                            {
+                                "range": {
+                                    "@timestamp": {"gte": since.isoformat()}
+                                }
+                            },
                         ]
                     }
                 },
@@ -258,7 +288,9 @@ class LogAnalyzer:
         for sec_query in security_queries:
             try:
                 result = await self.es.search(
-                    index="auto-video-logs-*", body={"query": sec_query["query"]}, size=100
+                    index="auto-video-logs-*",
+                    body={"query": sec_query["query"]},
+                    size=100,
                 )
 
                 event_count = result["hits"]["total"]["value"]
@@ -269,13 +301,17 @@ class LogAnalyzer:
                     ip_counter = Counter()
                     for event in events:
                         source = event["_source"]
-                        ip = source.get("ip_address") or source.get("source_ip")
+                        ip = source.get("ip_address") or source.get(
+                            "source_ip"
+                        )
                         if ip:
                             ip_counter[ip] += 1
 
                     # æª¢æŸ¥æ˜¯å¦æœ‰ä¾†è‡ªåŒä¸€ IP çš„å¤§é‡äº‹ä»¶
                     for ip, count in ip_counter.items():
-                        if count >= self.thresholds.get("failed_login_attempts", 10):
+                        if count >= self.thresholds.get(
+                            "failed_login_attempts", 10
+                        ):
                             alert = Alert(
                                 id=f"security_{sec_query['name']}_{ip}_{int(time.time())}",
                                 type="security",
@@ -293,7 +329,9 @@ class LogAnalyzer:
                             await self.send_alert(alert)
 
             except Exception as e:
-                logger.error(f"Security analysis failed for {sec_query['name']}: {e}")
+                logger.error(
+                    f"Security analysis failed for {sec_query['name']}: {e}"
+                )
 
     async def analyze_business_metrics(self):
         """åˆ†ææ¥­å‹™æŒ‡æ¨™"""
@@ -324,26 +362,38 @@ class LogAnalyzer:
                     ]
                 }
             },
-            "aggs": {"status_breakdown": {"terms": {"field": "status.keyword"}}},
+            "aggs": {
+                "status_breakdown": {"terms": {"field": "status.keyword"}}
+            },
         }
 
         try:
             # ç”¨æˆ¶è¨»å†Šåˆ†æ
-            user_result = await self.es.search(index="auto-video-logs-*", body=user_query, size=0)
+            user_result = await self.es.search(
+                index="auto-video-logs-*", body=user_query, size=0
+            )
             user_registrations = user_result["hits"]["total"]["value"]
 
             # å½±ç‰‡ç”Ÿæˆåˆ†æ
-            video_result = await self.es.search(index="auto-video-logs-*", body=video_query, size=0)
+            video_result = await self.es.search(
+                index="auto-video-logs-*", body=video_query, size=0
+            )
             total_generations = video_result["hits"]["total"]["value"]
 
             # è¨ˆç®—æˆåŠŸç‡
-            status_buckets = video_result["aggregations"]["status_breakdown"]["buckets"]
+            status_buckets = video_result["aggregations"]["status_breakdown"][
+                "buckets"
+            ]
             success_count = 0
             for bucket in status_buckets:
                 if bucket["key"] == "success":
                     success_count = bucket["doc_count"]
 
-            success_rate = success_count / total_generations if total_generations > 0 else 1.0
+            success_rate = (
+                success_count / total_generations
+                if total_generations > 0
+                else 1.0
+            )
 
             # å„²å­˜æ¥­å‹™æŒ‡æ¨™åˆ° Redis
             metrics = {
@@ -353,7 +403,9 @@ class LogAnalyzer:
                 "timestamp": now.isoformat(),
             }
 
-            self.redis.setex("business_metrics", 3600, json.dumps(metrics))  # 1å°æ™‚éæœŸ
+            self.redis.setex(
+                "business_metrics", 3600, json.dumps(metrics)
+            )  # 1å°æ™‚éæœŸ
 
             # æª¢æŸ¥æ¥­å‹™ç•°å¸¸
             if success_rate < 0.8 and total_generations > 10:
@@ -365,7 +417,10 @@ class LogAnalyzer:
                     message=f"Video generation success rate is {success_rate:.2%}",
                     service="video-service",
                     timestamp=now,
-                    data={"success_rate": success_rate, "total_generations": total_generations},
+                    data={
+                        "success_rate": success_rate,
+                        "total_generations": total_generations,
+                    },
                 )
                 await self.send_alert(alert)
 
@@ -401,7 +456,9 @@ class LogAnalyzer:
         }
 
         try:
-            result = await self.es.search(index="auto-video-logs-*", body=system_query, size=0)
+            result = await self.es.search(
+                index="auto-video-logs-*", body=system_query, size=0
+            )
 
             if "aggregations" in result:
                 avg_memory = result["aggregations"]["avg_memory"]["value"]
@@ -452,8 +509,12 @@ class LogAnalyzer:
     async def generate_daily_insights(self):
         """ç”Ÿæˆæ¯æ—¥æ´å¯Ÿå ±å‘Š"""
         yesterday = datetime.utcnow() - timedelta(days=1)
-        start_of_yesterday = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
-        end_of_yesterday = yesterday.replace(hour=23, minute=59, second=59, microsecond=999999)
+        start_of_yesterday = yesterday.replace(
+            hour=0, minute=0, second=0, microsecond=0
+        )
+        end_of_yesterday = yesterday.replace(
+            hour=23, minute=59, second=59, microsecond=999999
+        )
 
         insights = {
             "date": yesterday.date().isoformat(),
@@ -503,7 +564,8 @@ class LogAnalyzer:
         # å„²å­˜åˆ° Elasticsearch
         try:
             await self.es.index(
-                index=f"alerts-{datetime.utcnow().strftime('%Y.%m.%d')}", body=alert_data
+                index=f"alerts-{datetime.utcnow().strftime('%Y.%m.%d')}",
+                body=alert_data,
             )
         except Exception as e:
             logger.error(f"Failed to index alert: {e}")
diff --git a/auto_generate_video_fold6/monitoring/analytics_dashboard.py b/auto_generate_video_fold6/monitoring/analytics_dashboard.py
index ab57784..d3f9b5b 100755
--- a/auto_generate_video_fold6/monitoring/analytics_dashboard.py
+++ b/auto_generate_video_fold6/monitoring/analytics_dashboard.py
@@ -50,12 +50,23 @@ class AnalyticsDashboard:
                 # æ¨™é¡Œ
                 html.Div(
                     [
-                        html.H1("ğŸ¬ Auto Video Generation åˆ†æå„€è¡¨æ¿", className="dashboard-title"),
-                        html.P("å¯¦æ™‚ç›£æ§å’Œæ•¸æ“šåˆ†æ", className="dashboard-subtitle"),
+                        html.H1(
+                            "ğŸ¬ Auto Video Generation åˆ†æå„€è¡¨æ¿",
+                            className="dashboard-title",
+                        ),
+                        html.P(
+                            "å¯¦æ™‚ç›£æ§å’Œæ•¸æ“šåˆ†æ",
+                            className="dashboard-subtitle",
+                        ),
                         html.Div(
                             [
-                                html.Span("æœ€å¾Œæ›´æ–°: ", className="update-label"),
-                                html.Span(id="last-update-time", className="update-time"),
+                                html.Span(
+                                    "æœ€å¾Œæ›´æ–°: ", className="update-label"
+                                ),
+                                html.Span(
+                                    id="last-update-time",
+                                    className="update-time",
+                                ),
                             ],
                             className="update-info",
                         ),
@@ -67,16 +78,30 @@ class AnalyticsDashboard:
                     [
                         html.Div(
                             [
-                                html.Label("æ™‚é–“ç¯„åœ:", className="control-label"),
+                                html.Label(
+                                    "æ™‚é–“ç¯„åœ:", className="control-label"
+                                ),
                                 dcc.Dropdown(
                                     id="time-range-dropdown",
                                     options=[
                                         {"label": "ä»Šå¤©", "value": "today"},
-                                        {"label": "æ˜¨å¤©", "value": "yesterday"},
+                                        {
+                                            "label": "æ˜¨å¤©",
+                                            "value": "yesterday",
+                                        },
                                         {"label": "éå»7å¤©", "value": "7days"},
-                                        {"label": "éå»30å¤©", "value": "30days"},
-                                        {"label": "æœ¬æœˆ", "value": "this_month"},
-                                        {"label": "ä¸Šæœˆ", "value": "last_month"},
+                                        {
+                                            "label": "éå»30å¤©",
+                                            "value": "30days",
+                                        },
+                                        {
+                                            "label": "æœ¬æœˆ",
+                                            "value": "this_month",
+                                        },
+                                        {
+                                            "label": "ä¸Šæœˆ",
+                                            "value": "last_month",
+                                        },
                                     ],
                                     value="7days",
                                     className="control-dropdown",
@@ -86,14 +111,25 @@ class AnalyticsDashboard:
                         ),
                         html.Div(
                             [
-                                html.Label("æ•¸æ“šé¡å‹:", className="control-label"),
+                                html.Label(
+                                    "æ•¸æ“šé¡å‹:", className="control-label"
+                                ),
                                 dcc.Dropdown(
                                     id="data-type-dropdown",
                                     options=[
                                         {"label": "æˆæœ¬åˆ†æ", "value": "cost"},
-                                        {"label": "ç”Ÿæˆçµ±è¨ˆ", "value": "generation"},
-                                        {"label": "æ•ˆèƒ½æŒ‡æ¨™", "value": "performance"},
-                                        {"label": "éŒ¯èª¤åˆ†æ", "value": "errors"},
+                                        {
+                                            "label": "ç”Ÿæˆçµ±è¨ˆ",
+                                            "value": "generation",
+                                        },
+                                        {
+                                            "label": "æ•ˆèƒ½æŒ‡æ¨™",
+                                            "value": "performance",
+                                        },
+                                        {
+                                            "label": "éŒ¯èª¤åˆ†æ",
+                                            "value": "errors",
+                                        },
                                     ],
                                     value="cost",
                                     className="control-dropdown",
@@ -104,10 +140,14 @@ class AnalyticsDashboard:
                         html.Div(
                             [
                                 html.Button(
-                                    "åˆ·æ–°æ•¸æ“š", id="refresh-button", className="refresh-button"
+                                    "åˆ·æ–°æ•¸æ“š",
+                                    id="refresh-button",
+                                    className="refresh-button",
                                 ),
                                 html.Button(
-                                    "åŒ¯å‡ºå ±å‘Š", id="export-button", className="export-button"
+                                    "åŒ¯å‡ºå ±å‘Š",
+                                    id="export-button",
+                                    className="export-button",
                                 ),
                             ],
                             className="button-group",
@@ -123,14 +163,23 @@ class AnalyticsDashboard:
                 # ä¸»è¦åœ–è¡¨å€åŸŸ
                 html.Div(
                     [
-                        html.Div([dcc.Graph(id="main-chart")], className="chart-container"),
-                        html.Div([dcc.Graph(id="secondary-chart")], className="chart-container"),
+                        html.Div(
+                            [dcc.Graph(id="main-chart")],
+                            className="chart-container",
+                        ),
+                        html.Div(
+                            [dcc.Graph(id="secondary-chart")],
+                            className="chart-container",
+                        ),
                     ],
                     className="charts-section",
                 ),
                 # è©³ç´°æ•¸æ“šè¡¨æ ¼
                 html.Div(
-                    [html.H3("è©³ç´°æ•¸æ“š", className="section-title"), html.Div(id="data-table")],
+                    [
+                        html.H3("è©³ç´°æ•¸æ“š", className="section-title"),
+                        html.Div(id="data-table"),
+                    ],
                     className="table-section",
                 ),
                 # éš±è—çš„ div ç”¨æ–¼å­˜å„²æ•¸æ“š
@@ -221,7 +270,10 @@ class AnalyticsDashboard:
         """è¨­ç½®å›èª¿å‡½æ•¸"""
 
         @self.app.callback(
-            [Output("data-store", "children"), Output("last-update-time", "children")],
+            [
+                Output("data-store", "children"),
+                Output("last-update-time", "children"),
+            ],
             [
                 Input("time-range-dropdown", "value"),
                 Input("data-type-dropdown", "value"),
@@ -237,7 +289,10 @@ class AnalyticsDashboard:
                 logger.error(f"ç²å–æ•¸æ“šå¤±æ•—: {e}")
                 return [json.dumps({}), "ç²å–å¤±æ•—"]
 
-        @self.app.callback(Output("metrics-cards", "children"), [Input("data-store", "children")])
+        @self.app.callback(
+            Output("metrics-cards", "children"),
+            [Input("data-store", "children")],
+        )
         def update_metrics_cards(data_json):
             """æ›´æ–°æŒ‡æ¨™å¡ç‰‡"""
             try:
@@ -249,7 +304,10 @@ class AnalyticsDashboard:
 
         @self.app.callback(
             Output("main-chart", "figure"),
-            [Input("data-store", "children"), Input("data-type-dropdown", "value")],
+            [
+                Input("data-store", "children"),
+                Input("data-type-dropdown", "value"),
+            ],
         )
         def update_main_chart(data_json, data_type):
             """æ›´æ–°ä¸»åœ–è¡¨"""
@@ -262,7 +320,10 @@ class AnalyticsDashboard:
 
         @self.app.callback(
             Output("secondary-chart", "figure"),
-            [Input("data-store", "children"), Input("data-type-dropdown", "value")],
+            [
+                Input("data-store", "children"),
+                Input("data-type-dropdown", "value"),
+            ],
         )
         def update_secondary_chart(data_json, data_type):
             """æ›´æ–°æ¬¡åœ–è¡¨"""
@@ -273,7 +334,9 @@ class AnalyticsDashboard:
                 logger.error(f"æ›´æ–°æ¬¡åœ–è¡¨å¤±æ•—: {e}")
                 return go.Figure()
 
-        @self.app.callback(Output("data-table", "children"), [Input("data-store", "children")])
+        @self.app.callback(
+            Output("data-table", "children"), [Input("data-store", "children")]
+        )
         def update_data_table(data_json):
             """æ›´æ–°æ•¸æ“šè¡¨æ ¼"""
             try:
@@ -283,7 +346,9 @@ class AnalyticsDashboard:
                 logger.error(f"æ›´æ–°æ•¸æ“šè¡¨æ ¼å¤±æ•—: {e}")
                 return html.P("æ•¸æ“šè¼‰å…¥å¤±æ•—")
 
-    async def _fetch_data(self, time_range: str, data_type: str) -> Dict[str, Any]:
+    async def _fetch_data(
+        self, time_range: str, data_type: str
+    ) -> Dict[str, Any]:
         """ç²å–æ•¸æ“š"""
         end_date = date.today()
 
@@ -305,7 +370,9 @@ class AnalyticsDashboard:
                 end_date = date(end_date.year, 1, 1) - timedelta(days=1)
             else:
                 start_date = date(end_date.year, end_date.month - 1, 1)
-                end_date = date(end_date.year, end_date.month, 1) - timedelta(days=1)
+                end_date = date(end_date.year, end_date.month, 1) - timedelta(
+                    days=1
+                )
         else:
             start_date = end_date - timedelta(days=7)
 
@@ -321,7 +388,9 @@ class AnalyticsDashboard:
         else:
             return {}
 
-    async def _fetch_cost_data(self, start_date: date, end_date: date) -> Dict[str, Any]:
+    async def _fetch_cost_data(
+        self, start_date: date, end_date: date
+    ) -> Dict[str, Any]:
         """ç²å–æˆæœ¬æ•¸æ“š"""
         if not self.cost_tracker:
             return self._get_mock_cost_data(start_date, end_date)
@@ -332,7 +401,10 @@ class AnalyticsDashboard:
                 summary = await self.cost_tracker.get_daily_summary(start_date)
                 return {
                     "type": "cost",
-                    "period": {"start": start_date.isoformat(), "end": end_date.isoformat()},
+                    "period": {
+                        "start": start_date.isoformat(),
+                        "end": end_date.isoformat(),
+                    },
                     "total_cost": summary.total_cost,
                     "api_calls": summary.api_calls_count,
                     "providers": summary.providers_breakdown,
@@ -350,20 +422,31 @@ class AnalyticsDashboard:
                 weekly_report = await self.cost_tracker.get_weekly_report()
                 return {
                     "type": "cost",
-                    "period": {"start": start_date.isoformat(), "end": end_date.isoformat()},
+                    "period": {
+                        "start": start_date.isoformat(),
+                        "end": end_date.isoformat(),
+                    },
                     "total_cost": weekly_report["total_cost"],
                     "api_calls": weekly_report["total_calls"],
                     "average_daily": weekly_report["average_daily_cost"],
                     "daily_data": [
-                        {"date": date_str, "cost": stats["cost"], "calls": stats["calls"]}
-                        for date_str, stats in weekly_report["daily_stats"].items()
+                        {
+                            "date": date_str,
+                            "cost": stats["cost"],
+                            "calls": stats["calls"],
+                        }
+                        for date_str, stats in weekly_report[
+                            "daily_stats"
+                        ].items()
                     ],
                 }
         except Exception as e:
             logger.error(f"ç²å–æˆæœ¬æ•¸æ“šå¤±æ•—: {e}")
             return self._get_mock_cost_data(start_date, end_date)
 
-    def _get_mock_cost_data(self, start_date: date, end_date: date) -> Dict[str, Any]:
+    def _get_mock_cost_data(
+        self, start_date: date, end_date: date
+    ) -> Dict[str, Any]:
         """ç²å–æ¨¡æ“¬æˆæœ¬æ•¸æ“š"""
         import random
 
@@ -377,7 +460,11 @@ class AnalyticsDashboard:
             daily_calls = random.randint(30, 80)
 
             daily_data.append(
-                {"date": current_date.isoformat(), "cost": daily_cost, "calls": daily_calls}
+                {
+                    "date": current_date.isoformat(),
+                    "cost": daily_cost,
+                    "calls": daily_calls,
+                }
             )
 
             total_cost += daily_cost
@@ -386,7 +473,10 @@ class AnalyticsDashboard:
 
         return {
             "type": "cost",
-            "period": {"start": start_date.isoformat(), "end": end_date.isoformat()},
+            "period": {
+                "start": start_date.isoformat(),
+                "end": end_date.isoformat(),
+            },
             "total_cost": total_cost,
             "api_calls": total_calls,
             "providers": {
@@ -402,7 +492,9 @@ class AnalyticsDashboard:
             "daily_data": daily_data,
         }
 
-    async def _fetch_generation_data(self, start_date: date, end_date: date) -> Dict[str, Any]:
+    async def _fetch_generation_data(
+        self, start_date: date, end_date: date
+    ) -> Dict[str, Any]:
         """ç²å–ç”Ÿæˆæ•¸æ“šï¼ˆæ¨¡æ“¬ï¼‰"""
         import random
 
@@ -420,7 +512,9 @@ class AnalyticsDashboard:
                     "date": current_date.isoformat(),
                     "videos": videos,
                     "success": success,
-                    "success_rate": (success / videos) * 100 if videos > 0 else 0,
+                    "success_rate": (success / videos) * 100
+                    if videos > 0
+                    else 0,
                 }
             )
 
@@ -430,10 +524,15 @@ class AnalyticsDashboard:
 
         return {
             "type": "generation",
-            "period": {"start": start_date.isoformat(), "end": end_date.isoformat()},
+            "period": {
+                "start": start_date.isoformat(),
+                "end": end_date.isoformat(),
+            },
             "total_videos": total_videos,
             "total_success": total_success,
-            "success_rate": (total_success / total_videos) * 100 if total_videos > 0 else 0,
+            "success_rate": (total_success / total_videos) * 100
+            if total_videos > 0
+            else 0,
             "platforms": {
                 "TikTok": int(total_videos * 0.45),
                 "Instagram": int(total_videos * 0.35),
@@ -442,7 +541,9 @@ class AnalyticsDashboard:
             "daily_data": daily_data,
         }
 
-    async def _fetch_performance_data(self, start_date: date, end_date: date) -> Dict[str, Any]:
+    async def _fetch_performance_data(
+        self, start_date: date, end_date: date
+    ) -> Dict[str, Any]:
         """ç²å–æ•ˆèƒ½æ•¸æ“šï¼ˆæ¨¡æ“¬ï¼‰"""
         import random
 
@@ -463,20 +564,30 @@ class AnalyticsDashboard:
 
         return {
             "type": "performance",
-            "period": {"start": start_date.isoformat(), "end": end_date.isoformat()},
+            "period": {
+                "start": start_date.isoformat(),
+                "end": end_date.isoformat(),
+            },
             "daily_data": daily_data,
         }
 
-    async def _fetch_error_data(self, start_date: date, end_date: date) -> Dict[str, Any]:
+    async def _fetch_error_data(
+        self, start_date: date, end_date: date
+    ) -> Dict[str, Any]:
         """ç²å–éŒ¯èª¤æ•¸æ“šï¼ˆæ¨¡æ“¬ï¼‰"""
         import random
 
         error_types = ["APIéŒ¯èª¤", "ç¶²è·¯è¶…æ™‚", "è³‡æºä¸è¶³", "é…ç½®éŒ¯èª¤", "å…¶ä»–"]
-        error_data = {error_type: random.randint(0, 10) for error_type in error_types}
+        error_data = {
+            error_type: random.randint(0, 10) for error_type in error_types
+        }
 
         return {
             "type": "errors",
-            "period": {"start": start_date.isoformat(), "end": end_date.isoformat()},
+            "period": {
+                "start": start_date.isoformat(),
+                "end": end_date.isoformat(),
+            },
             "total_errors": sum(error_data.values()),
             "error_types": error_data,
         }
@@ -493,14 +604,20 @@ class AnalyticsDashboard:
             cards = [
                 html.Div(
                     [
-                        html.Div(f"${data.get('total_cost', 0):.2f}", className="metric-value"),
+                        html.Div(
+                            f"${data.get('total_cost', 0):.2f}",
+                            className="metric-value",
+                        ),
                         html.Div("ç¸½æˆæœ¬", className="metric-label"),
                     ],
                     className="metric-card",
                 ),
                 html.Div(
                     [
-                        html.Div(str(data.get("api_calls", 0)), className="metric-value"),
+                        html.Div(
+                            str(data.get("api_calls", 0)),
+                            className="metric-value",
+                        ),
                         html.Div("API å‘¼å«æ¬¡æ•¸", className="metric-label"),
                     ],
                     className="metric-card",
@@ -521,21 +638,30 @@ class AnalyticsDashboard:
             cards = [
                 html.Div(
                     [
-                        html.Div(str(data.get("total_videos", 0)), className="metric-value"),
+                        html.Div(
+                            str(data.get("total_videos", 0)),
+                            className="metric-value",
+                        ),
                         html.Div("ç¸½å½±ç‰‡æ•¸", className="metric-label"),
                     ],
                     className="metric-card",
                 ),
                 html.Div(
                     [
-                        html.Div(f"{data.get('success_rate', 0):.1f}%", className="metric-value"),
+                        html.Div(
+                            f"{data.get('success_rate', 0):.1f}%",
+                            className="metric-value",
+                        ),
                         html.Div("æˆåŠŸç‡", className="metric-label"),
                     ],
                     className="metric-card",
                 ),
                 html.Div(
                     [
-                        html.Div(str(data.get("total_success", 0)), className="metric-value"),
+                        html.Div(
+                            str(data.get("total_success", 0)),
+                            className="metric-value",
+                        ),
                         html.Div("æˆåŠŸç”Ÿæˆ", className="metric-label"),
                     ],
                     className="metric-card",
@@ -544,10 +670,14 @@ class AnalyticsDashboard:
 
         return cards
 
-    def _create_main_chart(self, data: Dict[str, Any], data_type: str) -> go.Figure:
+    def _create_main_chart(
+        self, data: Dict[str, Any], data_type: str
+    ) -> go.Figure:
         """å‰µå»ºä¸»åœ–è¡¨"""
         if not data:
-            return go.Figure().add_annotation(text="ç„¡æ•¸æ“š", x=0.5, y=0.5, showarrow=False)
+            return go.Figure().add_annotation(
+                text="ç„¡æ•¸æ“š", x=0.5, y=0.5, showarrow=False
+            )
 
         if data_type == "cost":
             # æˆæœ¬è¶¨å‹¢åœ–
@@ -594,10 +724,14 @@ class AnalyticsDashboard:
 
         return go.Figure()
 
-    def _create_secondary_chart(self, data: Dict[str, Any], data_type: str) -> go.Figure:
+    def _create_secondary_chart(
+        self, data: Dict[str, Any], data_type: str
+    ) -> go.Figure:
         """å‰µå»ºæ¬¡åœ–è¡¨"""
         if not data:
-            return go.Figure().add_annotation(text="ç„¡æ•¸æ“š", x=0.5, y=0.5, showarrow=False)
+            return go.Figure().add_annotation(
+                text="ç„¡æ•¸æ“š", x=0.5, y=0.5, showarrow=False
+            )
 
         if data_type == "cost":
             # ä¾›æ‡‰å•†åˆ†å¸ƒåœ–
@@ -626,7 +760,9 @@ class AnalyticsDashboard:
             error_types = data.get("error_types", {})
             if error_types:
                 fig = px.bar(
-                    x=list(error_types.keys()), y=list(error_types.values()), title="éŒ¯èª¤é¡å‹åˆ†å¸ƒ"
+                    x=list(error_types.keys()),
+                    y=list(error_types.values()),
+                    title="éŒ¯èª¤é¡å‹åˆ†å¸ƒ",
                 )
                 return fig
 
@@ -647,12 +783,19 @@ class AnalyticsDashboard:
         # å‰µå»ºè¡¨æ ¼
         table_header = [html.Thead([html.Tr([html.Th(h) for h in headers])])]
         table_body = [
-            html.Tbody([html.Tr([html.Td(row.get(h, "")) for h in headers]) for row in daily_data])
+            html.Tbody(
+                [
+                    html.Tr([html.Td(row.get(h, "")) for h in headers])
+                    for row in daily_data
+                ]
+            )
         ]
 
         return html.Table(table_header + table_body, className="table")
 
-    def run(self, host: str = "127.0.0.1", port: int = 8050, debug: bool = False):
+    def run(
+        self, host: str = "127.0.0.1", port: int = 8050, debug: bool = False
+    ):
         """é‹è¡Œå„€è¡¨æ¿"""
         logger.info(f"å•Ÿå‹•åˆ†æå„€è¡¨æ¿: http://{host}:{port}")
         self.app.run_server(host=host, port=port, debug=debug)
diff --git a/auto_generate_video_fold6/monitoring/application_monitoring.py b/auto_generate_video_fold6/monitoring/application_monitoring.py
index e20d00f..c4b7518 100644
--- a/auto_generate_video_fold6/monitoring/application_monitoring.py
+++ b/auto_generate_video_fold6/monitoring/application_monitoring.py
@@ -57,7 +57,9 @@ class ApplicationMonitoring:
         # æ¥­å‹™æŒ‡æ¨™
         self.business_metrics = {
             "user_registrations": Counter(
-                "user_registrations_total", "Total user registrations", ["service"]
+                "user_registrations_total",
+                "Total user registrations",
+                ["service"],
             ),
             "video_generations": Counter(
                 "video_generations_total",
@@ -70,7 +72,9 @@ class ApplicationMonitoring:
                 ["request_type", "status", "service"],
             ),
             "user_sessions": Gauge(
-                "active_user_sessions", "Number of active user sessions", ["service"]
+                "active_user_sessions",
+                "Number of active user sessions",
+                ["service"],
             ),
             "processing_queue_size": Gauge(
                 "processing_queue_size",
@@ -112,7 +116,9 @@ class ApplicationMonitoring:
         # è¨­å®šæŒ‡æ¨™ä¸­ä»‹è»Ÿé«”
         if self.metrics_middleware:
             app.middleware("http")(self.metrics_middleware)
-            from .middleware.prometheus_middleware import setup_metrics_endpoint
+            from .middleware.prometheus_middleware import (
+                setup_metrics_endpoint,
+            )
 
             setup_metrics_endpoint(app, self.metrics_middleware)
 
@@ -168,7 +174,11 @@ class ApplicationMonitoring:
                     if not result:
                         overall_status = "not_ready"
                 except Exception as e:
-                    checks[name] = {"status": "error", "error": str(e), "timestamp": time.time()}
+                    checks[name] = {
+                        "status": "error",
+                        "error": str(e),
+                        "timestamp": time.time(),
+                    }
                     overall_status = "not_ready"
 
             response = {
@@ -274,10 +284,14 @@ class ApplicationMonitoring:
                 metric.labels(**labels).observe(value)
 
     @asynccontextmanager
-    async def monitor_operation(self, operation_name: str, labels: Dict[str, str] = None):
+    async def monitor_operation(
+        self, operation_name: str, labels: Dict[str, str] = None
+    ):
         """ç›£æ§æ“ä½œçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
         labels = labels or {}
-        labels.update({"operation": operation_name, "service": self.service_name})
+        labels.update(
+            {"operation": operation_name, "service": self.service_name}
+        )
 
         start_time = time.time()
 
@@ -292,9 +306,15 @@ class ApplicationMonitoring:
 
                 # è¨˜éŒ„æˆåŠŸæŒ‡æ¨™
                 duration = time.time() - start_time
-                self.metrics_middleware.record_ai_request(operation_name, "success", duration)
+                self.metrics_middleware.record_ai_request(
+                    operation_name, "success", duration
+                )
 
-                self.logger.info("Operation completed successfully", duration=duration, **labels)
+                self.logger.info(
+                    "Operation completed successfully",
+                    duration=duration,
+                    **labels,
+                )
 
             except Exception as e:
                 # è¨˜éŒ„éŒ¯èª¤
@@ -361,7 +381,9 @@ def create_monitored_app(
 
     # å‰µå»ºç›£æ§å¯¦ä¾‹
     monitoring = ApplicationMonitoring(
-        service_name=service_name, service_version=service_version, environment=environment
+        service_name=service_name,
+        service_version=service_version,
+        environment=environment,
     )
 
     # è¨­å®šæ‡‰ç”¨ç¨‹å¼ç›£æ§
@@ -375,13 +397,17 @@ def create_monitored_app(
 
     # æ·»åŠ  Redis å¥åº·æª¢æŸ¥
     if redis_url:
-        monitoring.add_health_check("redis", monitoring.create_redis_health_check(redis_url))
+        monitoring.add_health_check(
+            "redis", monitoring.create_redis_health_check(redis_url)
+        )
 
     return app, monitoring
 
 
 # è£é£¾å™¨
-def monitor_endpoint(operation_name: str = None, labels: Dict[str, str] = None):
+def monitor_endpoint(
+    operation_name: str = None, labels: Dict[str, str] = None
+):
     """ç«¯é»ç›£æ§è£é£¾å™¨"""
 
     def decorator(func):
diff --git a/auto_generate_video_fold6/monitoring/backup/backup_monitor.py b/auto_generate_video_fold6/monitoring/backup/backup_monitor.py
index faf6bd3..261f849 100644
--- a/auto_generate_video_fold6/monitoring/backup/backup_monitor.py
+++ b/auto_generate_video_fold6/monitoring/backup/backup_monitor.py
@@ -15,24 +15,31 @@ logger = logging.getLogger(__name__)
 
 class BackupMonitor:
     """å‚™ä»½ç›£æ§å™¨"""
-    
+
     def __init__(self, log_dir: str = "/var/log/backups"):
         self.log_dir = Path(log_dir)
         self.log_dir.mkdir(exist_ok=True)
-    
-    def log_backup_start(self, backup_id: str, backup_type: str, source: str) -> None:
+
+    def log_backup_start(
+        self, backup_id: str, backup_type: str, source: str
+    ) -> None:
         """è¨˜éŒ„å‚™ä»½é–‹å§‹"""
         log_entry = {
             "timestamp": datetime.now().isoformat(),
             "event": "backup_started",
             "backup_id": backup_id,
             "backup_type": backup_type,
-            "source": source
+            "source": source,
         }
         self._write_log(log_entry)
-    
-    def log_backup_complete(self, backup_id: str, success: bool, 
-                          size_bytes: int = 0, message: str = "") -> None:
+
+    def log_backup_complete(
+        self,
+        backup_id: str,
+        success: bool,
+        size_bytes: int = 0,
+        message: str = "",
+    ) -> None:
         """è¨˜éŒ„å‚™ä»½å®Œæˆ"""
         log_entry = {
             "timestamp": datetime.now().isoformat(),
@@ -40,49 +47,53 @@ class BackupMonitor:
             "backup_id": backup_id,
             "success": success,
             "size_bytes": size_bytes,
-            "message": message
+            "message": message,
         }
         self._write_log(log_entry)
-    
-    def log_restore_start(self, restore_id: str, backup_id: str, target: str) -> None:
+
+    def log_restore_start(
+        self, restore_id: str, backup_id: str, target: str
+    ) -> None:
         """è¨˜éŒ„æ¢å¾©é–‹å§‹"""
         log_entry = {
             "timestamp": datetime.now().isoformat(),
             "event": "restore_started",
             "restore_id": restore_id,
             "backup_id": backup_id,
-            "target": target
+            "target": target,
         }
         self._write_log(log_entry)
-    
-    def log_restore_complete(self, restore_id: str, success: bool, message: str = "") -> None:
+
+    def log_restore_complete(
+        self, restore_id: str, success: bool, message: str = ""
+    ) -> None:
         """è¨˜éŒ„æ¢å¾©å®Œæˆ"""
         log_entry = {
             "timestamp": datetime.now().isoformat(),
             "event": "restore_completed",
             "restore_id": restore_id,
             "success": success,
-            "message": message
+            "message": message,
         }
         self._write_log(log_entry)
-    
+
     def _write_log(self, log_entry: Dict) -> None:
         """å¯«å…¥æ—¥èªŒ"""
         today = datetime.now().strftime("%Y-%m-%d")
         log_file = self.log_dir / f"backup_{today}.log"
-        
+
         with open(log_file, "a", encoding="utf-8") as f:
             f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
-    
+
     def get_recent_backups(self, days: int = 7) -> List[Dict]:
         """ç²å–æœ€è¿‘çš„å‚™ä»½è¨˜éŒ„"""
         records = []
         start_date = datetime.now() - timedelta(days=days)
-        
+
         for i in range(days + 1):
             date = start_date + timedelta(days=i)
             log_file = self.log_dir / f"backup_{date.strftime('%Y-%m-%d')}.log"
-            
+
             if log_file.exists():
                 with open(log_file, "r", encoding="utf-8") as f:
                     for line in f:
@@ -91,27 +102,45 @@ class BackupMonitor:
                             records.append(record)
                         except json.JSONDecodeError:
                             continue
-        
+
         return sorted(records, key=lambda x: x.get("timestamp", ""))
-    
+
     def generate_backup_report(self, days: int = 7) -> Dict:
         """ç”Ÿæˆå‚™ä»½å ±å‘Š"""
         records = self.get_recent_backups(days)
-        
+
         # çµ±è¨ˆè³‡è¨Š
-        backup_starts = [r for r in records if r.get("event") == "backup_started"]
-        backup_completes = [r for r in records if r.get("event") == "backup_completed"]
-        successful_backups = [r for r in backup_completes if r.get("success", False)]
-        failed_backups = [r for r in backup_completes if not r.get("success", True)]
-        
-        restore_starts = [r for r in records if r.get("event") == "restore_started"]
-        restore_completes = [r for r in records if r.get("event") == "restore_completed"]
-        successful_restores = [r for r in restore_completes if r.get("success", False)]
-        failed_restores = [r for r in restore_completes if not r.get("success", True)]
-        
+        backup_starts = [
+            r for r in records if r.get("event") == "backup_started"
+        ]
+        backup_completes = [
+            r for r in records if r.get("event") == "backup_completed"
+        ]
+        successful_backups = [
+            r for r in backup_completes if r.get("success", False)
+        ]
+        failed_backups = [
+            r for r in backup_completes if not r.get("success", True)
+        ]
+
+        restore_starts = [
+            r for r in records if r.get("event") == "restore_started"
+        ]
+        restore_completes = [
+            r for r in records if r.get("event") == "restore_completed"
+        ]
+        successful_restores = [
+            r for r in restore_completes if r.get("success", False)
+        ]
+        failed_restores = [
+            r for r in restore_completes if not r.get("success", True)
+        ]
+
         # è¨ˆç®—ç¸½å‚™ä»½å¤§å°
-        total_backup_size = sum(r.get("size_bytes", 0) for r in successful_backups)
-        
+        total_backup_size = sum(
+            r.get("size_bytes", 0) for r in successful_backups
+        )
+
         report = {
             "period": f"æœ€è¿‘ {days} å¤©",
             "generated_at": datetime.now().isoformat(),
@@ -120,20 +149,28 @@ class BackupMonitor:
                 "total_completed": len(backup_completes),
                 "successful": len(successful_backups),
                 "failed": len(failed_backups),
-                "success_rate": (len(successful_backups) / len(backup_completes) * 100) if backup_completes else 0,
+                "success_rate": (
+                    len(successful_backups) / len(backup_completes) * 100
+                )
+                if backup_completes
+                else 0,
                 "total_size_bytes": total_backup_size,
-                "total_size_mb": round(total_backup_size / (1024 * 1024), 2)
+                "total_size_mb": round(total_backup_size / (1024 * 1024), 2),
             },
             "restore_statistics": {
                 "total_started": len(restore_starts),
                 "total_completed": len(restore_completes),
                 "successful": len(successful_restores),
                 "failed": len(failed_restores),
-                "success_rate": (len(successful_restores) / len(restore_completes) * 100) if restore_completes else 0
+                "success_rate": (
+                    len(successful_restores) / len(restore_completes) * 100
+                )
+                if restore_completes
+                else 0,
             },
-            "recent_failures": failed_backups + failed_restores
+            "recent_failures": failed_backups + failed_restores,
         }
-        
+
         return report
 
 
@@ -141,4 +178,4 @@ if __name__ == "__main__":
     # ç¤ºä¾‹ç”¨æ³•
     monitor = BackupMonitor()
     report = monitor.generate_backup_report()
-    print(json.dumps(report, indent=2, ensure_ascii=False))
\ No newline at end of file
+    print(json.dumps(report, indent=2, ensure_ascii=False))
diff --git a/auto_generate_video_fold6/monitoring/budget_controller.py b/auto_generate_video_fold6/monitoring/budget_controller.py
index 7e9a7e8..61a6e26 100644
--- a/auto_generate_video_fold6/monitoring/budget_controller.py
+++ b/auto_generate_video_fold6/monitoring/budget_controller.py
@@ -70,9 +70,15 @@ class BudgetController:
 
         # é è¨­é ç®—è¦å‰‡
         self.default_rules = [
-            BudgetRule(0.8, ActionType.CONTINUE, "é ç®—ä½¿ç”¨é”åˆ° 80%ï¼Œè«‹æ³¨æ„æˆæœ¬æ§åˆ¶"),
-            BudgetRule(0.9, ActionType.THROTTLE, "é ç®—ä½¿ç”¨é”åˆ° 90%ï¼Œå•Ÿå‹•é™æµæ¨¡å¼"),
-            BudgetRule(0.95, ActionType.PAUSE, "é ç®—ä½¿ç”¨é”åˆ° 95%ï¼Œæš«åœéå¿…è¦æ“ä½œ"),
+            BudgetRule(
+                0.8, ActionType.CONTINUE, "é ç®—ä½¿ç”¨é”åˆ° 80%ï¼Œè«‹æ³¨æ„æˆæœ¬æ§åˆ¶"
+            ),
+            BudgetRule(
+                0.9, ActionType.THROTTLE, "é ç®—ä½¿ç”¨é”åˆ° 90%ï¼Œå•Ÿå‹•é™æµæ¨¡å¼"
+            ),
+            BudgetRule(
+                0.95, ActionType.PAUSE, "é ç®—ä½¿ç”¨é”åˆ° 95%ï¼Œæš«åœéå¿…è¦æ“ä½œ"
+            ),
             BudgetRule(1.0, ActionType.STOP, "é ç®—å·²ç”¨å®Œï¼Œåœæ­¢æ‰€æœ‰ä»˜è²»æ“ä½œ"),
         ]
 
@@ -118,7 +124,9 @@ class BudgetController:
         rules.sort(key=lambda x: x.threshold_percentage)
         return rules if rules else self.default_rules
 
-    async def check_budget_and_decide(self, estimated_cost: float = 0) -> BudgetDecision:
+    async def check_budget_and_decide(
+        self, estimated_cost: float = 0
+    ) -> BudgetDecision:
         """æª¢æŸ¥é ç®—ä¸¦åšå‡ºæ±ºç­–"""
         try:
             # ç²å–ç•¶å‰é ç®—ç‹€æ…‹
@@ -128,10 +136,14 @@ class BudgetController:
             current_cost = budget_status["current_cost"]
             budget_limit = budget_status["budget_limit"]
             estimated_total = current_cost + estimated_cost
-            usage_rate = estimated_total / budget_limit if budget_limit > 0 else 0
+            usage_rate = (
+                estimated_total / budget_limit if budget_limit > 0 else 0
+            )
 
             # æ ¹æ“šè¦å‰‡æ±ºå®šè¡Œå‹•
-            decision = self._make_decision(usage_rate, budget_status, estimated_cost)
+            decision = self._make_decision(
+                usage_rate, budget_status, estimated_cost
+            )
 
             # æ›´æ–°çµ±è¨ˆ
             self._update_stats(decision)
@@ -159,7 +171,10 @@ class BudgetController:
             )
 
     def _make_decision(
-        self, usage_rate: float, budget_status: Dict[str, Any], estimated_cost: float
+        self,
+        usage_rate: float,
+        budget_status: Dict[str, Any],
+        estimated_cost: float,
     ) -> BudgetDecision:
         """æ ¹æ“šä½¿ç”¨ç‡å’Œè¦å‰‡åšå‡ºæ±ºç­–"""
 
@@ -190,7 +205,9 @@ class BudgetController:
         # æ ¹æ“šè¦å‰‡ç”Ÿæˆæ±ºç­–
         status = self._determine_status(usage_rate)
         can_continue = self._can_continue(applicable_rule.action, usage_rate)
-        suggested_actions = self._generate_suggestions(status, usage_rate, estimated_cost)
+        suggested_actions = self._generate_suggestions(
+            status, usage_rate, estimated_cost
+        )
 
         return BudgetDecision(
             status=status,
@@ -230,7 +247,9 @@ class BudgetController:
 
         # æª¢æŸ¥é…ç½®ä¸­çš„åœæ­¢è¨­å®š
         if self.config_manager:
-            stop_on_exceeded = self.config_manager.get("cost_control.stop_on_budget_exceeded", True)
+            stop_on_exceeded = self.config_manager.get(
+                "cost_control.stop_on_budget_exceeded", True
+            )
             if stop_on_exceeded and usage_rate >= 1.0:
                 return False
 
@@ -289,12 +308,16 @@ class BudgetController:
         # ç‰¹æ®Šæª¢æŸ¥
         if decision.action == ActionType.THROTTLE:
             # é™æµæ¨¡å¼ä¸‹çš„é¡å¤–æª¢æŸ¥
-            if estimated_cost > decision.remaining_budget * 0.1:  # è¶…éå‰©é¤˜é ç®—çš„10%
+            if (
+                estimated_cost > decision.remaining_budget * 0.1
+            ):  # è¶…éå‰©é¤˜é ç®—çš„10%
                 return False, "é™æµæ¨¡å¼ä¸‹ï¼Œå–®æ¬¡æ“ä½œæˆæœ¬éé«˜"
 
         return True, decision.message
 
-    async def post_operation_update(self, actual_cost: float, operation_result: bool = True):
+    async def post_operation_update(
+        self, actual_cost: float, operation_result: bool = True
+    ):
         """æ“ä½œå¾Œæ›´æ–°"""
         if actual_cost > 0:
             # è¨˜éŒ„å¯¦éš›æˆæœ¬ï¼ˆé€™æ‡‰è©²ç”± CostTracker è™•ç†ï¼‰
@@ -314,7 +337,9 @@ class BudgetController:
         elif decision.action == ActionType.STOP:
             self.daily_stats["stop_events"] += 1
 
-    async def _log_decision(self, decision: BudgetDecision, budget_status: Dict[str, Any]):
+    async def _log_decision(
+        self, decision: BudgetDecision, budget_status: Dict[str, Any]
+    ):
         """è¨˜éŒ„é ç®—æ±ºç­–"""
         log_entry = {
             "timestamp": datetime.now().isoformat(),
@@ -322,7 +347,9 @@ class BudgetController:
             "action": decision.action.value,
             "can_continue": decision.can_continue,
             "message": decision.message,
-            "usage_rate": decision.metadata.get("usage_rate", 0) if decision.metadata else 0,
+            "usage_rate": decision.metadata.get("usage_rate", 0)
+            if decision.metadata
+            else 0,
             "current_cost": decision.current_usage,
             "remaining_budget": decision.remaining_budget,
             "budget_limit": budget_status.get("budget_limit", 0),
@@ -365,17 +392,23 @@ class BudgetController:
                 }
                 for rule in self.rules
             ],
-            "last_check": self.last_check_time.isoformat() if self.last_check_time else None,
+            "last_check": self.last_check_time.isoformat()
+            if self.last_check_time
+            else None,
             "generated_at": datetime.now().isoformat(),
         }
 
-    async def adjust_budget_dynamically(self, new_budget: float, reason: str = ""):
+    async def adjust_budget_dynamically(
+        self, new_budget: float, reason: str = ""
+    ):
         """å‹•æ…‹èª¿æ•´é ç®—"""
         if not self.config_manager:
             logger.warning("ç„¡é…ç½®ç®¡ç†å™¨ï¼Œç„¡æ³•å‹•æ…‹èª¿æ•´é ç®—")
             return False
 
-        old_budget = self.config_manager.get("cost_control.daily_budget_usd", 100.0)
+        old_budget = self.config_manager.get(
+            "cost_control.daily_budget_usd", 100.0
+        )
 
         # æ›´æ–°é…ç½®
         self.config_manager.set("cost_control.daily_budget_usd", new_budget)
@@ -414,7 +447,9 @@ class BudgetController:
         """ç²å–ç•¶å‰ç‹€æ…‹"""
         return {
             "status": self.current_status.value,
-            "last_check": self.last_check_time.isoformat() if self.last_check_time else None,
+            "last_check": self.last_check_time.isoformat()
+            if self.last_check_time
+            else None,
             "daily_stats": self.daily_stats,
             "rules_count": len(self.rules),
         }
@@ -439,7 +474,9 @@ async def main():
     print("=== é ç®—æ§åˆ¶å™¨æ¸¬è©¦ ===")
 
     # æ¨¡æ“¬æ“ä½œå‰æª¢æŸ¥
-    can_proceed, message = await controller.pre_operation_check("video_generation", 2.5)
+    can_proceed, message = await controller.pre_operation_check(
+        "video_generation", 2.5
+    )
     print(f"æ“ä½œå‰æª¢æŸ¥: {can_proceed}, è¨Šæ¯: {message}")
 
     # æ¨¡æ“¬é ç®—æ±ºç­–
diff --git a/auto_generate_video_fold6/monitoring/business_metrics/business_metrics_collector.py b/auto_generate_video_fold6/monitoring/business_metrics/business_metrics_collector.py
index 1f52ae2..f895bf2 100644
--- a/auto_generate_video_fold6/monitoring/business_metrics/business_metrics_collector.py
+++ b/auto_generate_video_fold6/monitoring/business_metrics/business_metrics_collector.py
@@ -17,6 +17,7 @@ from enum import Enum
 # Prometheus metrics (optional dependency)
 try:
     from prometheus_client import Counter, Histogram, Gauge, Summary, Info
+
     PROMETHEUS_AVAILABLE = True
 except ImportError:
     PROMETHEUS_AVAILABLE = False
@@ -25,16 +26,20 @@ from ..logging.structured_logger import get_logger
 
 logger = get_logger(__name__)
 
+
 class MetricType(Enum):
     """æŒ‡æ¨™é¡å‹æšèˆ‰"""
+
     COUNTER = "counter"
     GAUGE = "gauge"
     HISTOGRAM = "histogram"
     SUMMARY = "summary"
 
+
 @dataclass
 class BusinessMetricDefinition:
     """æ¥­å‹™æŒ‡æ¨™å®šç¾©"""
+
     name: str
     type: MetricType
     description: str
@@ -42,62 +47,71 @@ class BusinessMetricDefinition:
     unit: str = ""
     business_impact: str = "medium"  # low, medium, high, critical
     sla_target: str = ""
-    
+
     def __post_init__(self):
         # ç¢ºä¿æ¨™ç±¤æ˜¯æ¸…å–®
         if isinstance(self.labels, str):
             self.labels = [self.labels]
 
+
 @dataclass
 class MetricRecord:
     """æŒ‡æ¨™è¨˜éŒ„"""
+
     name: str
     value: Union[int, float]
     labels: Dict[str, str] = field(default_factory=dict)
     timestamp: datetime = field(default_factory=datetime.utcnow)
     unit: str = ""
 
+
 class BusinessMetricsCollector:
     """æ¥­å‹™æŒ‡æ¨™æ”¶é›†å™¨"""
-    
+
     def __init__(self, metrics_definitions_path: Optional[str] = None):
         self.metrics_definitions: Dict[str, BusinessMetricDefinition] = {}
         self.prometheus_metrics: Dict[str, Any] = {}
-        self.fallback_metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
+        self.fallback_metrics: Dict[str, deque] = defaultdict(
+            lambda: deque(maxlen=10000)
+        )
         self.metrics_lock = threading.Lock()
-        
+
         # è¼‰å…¥æŒ‡æ¨™å®šç¾©
         if metrics_definitions_path:
             self.load_metrics_definitions(metrics_definitions_path)
         else:
             self._create_default_metrics()
-        
+
         # åˆå§‹åŒ– Prometheus æŒ‡æ¨™
         self._initialize_prometheus_metrics()
-    
+
     def load_metrics_definitions(self, file_path: str):
         """å¾ JSON æ–‡ä»¶è¼‰å…¥æŒ‡æ¨™å®šç¾©"""
         try:
-            with open(file_path, 'r', encoding='utf-8') as f:
+            with open(file_path, "r", encoding="utf-8") as f:
                 definitions_data = json.load(f)
-            
+
             for name, definition in definitions_data.items():
                 self.metrics_definitions[name] = BusinessMetricDefinition(
                     name=name,
-                    type=MetricType(definition['type']),
-                    description=definition['description'],
-                    labels=definition.get('labels', []),
-                    unit=definition.get('unit', ''),
-                    business_impact=definition.get('business_impact', 'medium'),
-                    sla_target=definition.get('sla_target', '')
+                    type=MetricType(definition["type"]),
+                    description=definition["description"],
+                    labels=definition.get("labels", []),
+                    unit=definition.get("unit", ""),
+                    business_impact=definition.get(
+                        "business_impact", "medium"
+                    ),
+                    sla_target=definition.get("sla_target", ""),
                 )
-            
-            logger.info(f"Loaded {len(self.metrics_definitions)} business metrics definitions")
-            
+
+            logger.info(
+                f"Loaded {len(self.metrics_definitions)} business metrics definitions"
+            )
+
         except Exception as e:
             logger.error(f"Failed to load metrics definitions: {e}")
             self._create_default_metrics()
-    
+
     def _create_default_metrics(self):
         """å»ºç«‹é è¨­æ¥­å‹™æŒ‡æ¨™å®šç¾©"""
         default_metrics = {
@@ -108,7 +122,7 @@ class BusinessMetricsCollector:
                 labels=["status", "video_type", "platform", "user_tier"],
                 unit="count",
                 business_impact="high",
-                sla_target="> 1000 per day"
+                sla_target="> 1000 per day",
             ),
             "user_engagement_rate": BusinessMetricDefinition(
                 name="user_engagement_rate",
@@ -117,7 +131,7 @@ class BusinessMetricsCollector:
                 labels=["platform", "content_type", "user_segment"],
                 unit="percentage",
                 business_impact="high",
-                sla_target="> 5%"
+                sla_target="> 5%",
             ),
             "content_generation_time": BusinessMetricDefinition(
                 name="content_generation_time",
@@ -126,7 +140,7 @@ class BusinessMetricsCollector:
                 labels=["content_type", "quality_level"],
                 unit="seconds",
                 business_impact="medium",
-                sla_target="< 300 seconds (95th percentile)"
+                sla_target="< 300 seconds (95th percentile)",
             ),
             "revenue_per_user": BusinessMetricDefinition(
                 name="revenue_per_user",
@@ -135,88 +149,99 @@ class BusinessMetricsCollector:
                 labels=["user_tier", "billing_period"],
                 unit="currency",
                 business_impact="critical",
-                sla_target="> $10 per month"
-            )
+                sla_target="> $10 per month",
+            ),
         }
-        
+
         self.metrics_definitions.update(default_metrics)
-    
+
     def _initialize_prometheus_metrics(self):
         """åˆå§‹åŒ– Prometheus æŒ‡æ¨™"""
         if not PROMETHEUS_AVAILABLE:
-            logger.warning("Prometheus client not available, using fallback metrics storage")
+            logger.warning(
+                "Prometheus client not available, using fallback metrics storage"
+            )
             return
-        
+
         for name, definition in self.metrics_definitions.items():
             try:
                 if definition.type == MetricType.COUNTER:
                     self.prometheus_metrics[name] = Counter(
                         name=name,
                         documentation=definition.description,
-                        labelnames=definition.labels
+                        labelnames=definition.labels,
                     )
                 elif definition.type == MetricType.GAUGE:
                     self.prometheus_metrics[name] = Gauge(
                         name=name,
                         documentation=definition.description,
-                        labelnames=definition.labels
+                        labelnames=definition.labels,
                     )
                 elif definition.type == MetricType.HISTOGRAM:
                     self.prometheus_metrics[name] = Histogram(
                         name=name,
                         documentation=definition.description,
-                        labelnames=definition.labels
+                        labelnames=definition.labels,
                     )
                 elif definition.type == MetricType.SUMMARY:
                     self.prometheus_metrics[name] = Summary(
                         name=name,
                         documentation=definition.description,
-                        labelnames=definition.labels
+                        labelnames=definition.labels,
                     )
-                
+
                 logger.debug(f"Initialized Prometheus metric: {name}")
-                
+
             except Exception as e:
-                logger.error(f"Failed to initialize Prometheus metric {name}: {e}")
-    
-    def record_metric(self, name: str, value: Union[int, float], 
-                     labels: Optional[Dict[str, str]] = None):
+                logger.error(
+                    f"Failed to initialize Prometheus metric {name}: {e}"
+                )
+
+    def record_metric(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """è¨˜éŒ„æ¥­å‹™æŒ‡æ¨™"""
         if name not in self.metrics_definitions:
             logger.warning(f"Unknown metric: {name}")
             return
-        
+
         if labels is None:
             labels = {}
-        
+
         definition = self.metrics_definitions[name]
-        
+
         # è¨˜éŒ„åˆ° Prometheus
         if PROMETHEUS_AVAILABLE and name in self.prometheus_metrics:
             try:
                 prometheus_metric = self.prometheus_metrics[name]
-                
+
                 if definition.type == MetricType.COUNTER:
                     if labels:
                         prometheus_metric.labels(**labels).inc(value)
                     else:
                         prometheus_metric.inc(value)
-                        
+
                 elif definition.type == MetricType.GAUGE:
                     if labels:
                         prometheus_metric.labels(**labels).set(value)
                     else:
                         prometheus_metric.set(value)
-                        
-                elif definition.type in [MetricType.HISTOGRAM, MetricType.SUMMARY]:
+
+                elif definition.type in [
+                    MetricType.HISTOGRAM,
+                    MetricType.SUMMARY,
+                ]:
                     if labels:
                         prometheus_metric.labels(**labels).observe(value)
                     else:
                         prometheus_metric.observe(value)
-                        
+
             except Exception as e:
                 logger.error(f"Failed to record Prometheus metric {name}: {e}")
-        
+
         # å¾Œå‚™å„²å­˜
         with self.metrics_lock:
             record = MetricRecord(
@@ -224,11 +249,11 @@ class BusinessMetricsCollector:
                 value=value,
                 labels=labels,
                 timestamp=datetime.utcnow(),
-                unit=definition.unit
+                unit=definition.unit,
             )
-            
+
             self.fallback_metrics[name].append(record)
-        
+
         # è¨˜éŒ„æ—¥èªŒ
         logger.info(
             f"Business metric recorded: {name} = {value}",
@@ -236,44 +261,58 @@ class BusinessMetricsCollector:
             metric_value=value,
             metric_labels=labels,
             business_impact=definition.business_impact,
-            sla_target=definition.sla_target
+            sla_target=definition.sla_target,
         )
-    
-    def increment_counter(self, name: str, amount: Union[int, float] = 1, 
-                         labels: Optional[Dict[str, str]] = None):
+
+    def increment_counter(
+        self,
+        name: str,
+        amount: Union[int, float] = 1,
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """å¢åŠ è¨ˆæ•¸å™¨æŒ‡æ¨™"""
         self.record_metric(name, amount, labels)
-    
-    def set_gauge(self, name: str, value: Union[int, float], 
-                  labels: Optional[Dict[str, str]] = None):
+
+    def set_gauge(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """è¨­å®šå„€è¡¨æŒ‡æ¨™"""
         self.record_metric(name, value, labels)
-    
-    def observe_histogram(self, name: str, value: Union[int, float], 
-                         labels: Optional[Dict[str, str]] = None):
+
+    def observe_histogram(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """è§€å¯Ÿç›´æ–¹åœ–æŒ‡æ¨™"""
         self.record_metric(name, value, labels)
-    
-    def get_metric_summary(self, name: str, 
-                          time_window: timedelta = timedelta(hours=1)) -> Dict[str, Any]:
+
+    def get_metric_summary(
+        self, name: str, time_window: timedelta = timedelta(hours=1)
+    ) -> Dict[str, Any]:
         """ç²å–æŒ‡æ¨™æ‘˜è¦"""
         if name not in self.metrics_definitions:
             return {}
-        
+
         cutoff_time = datetime.utcnow() - time_window
-        
+
         with self.metrics_lock:
             records = [
-                record for record in self.fallback_metrics[name]
+                record
+                for record in self.fallback_metrics[name]
                 if record.timestamp > cutoff_time
             ]
-        
+
         if not records:
             return {"name": name, "records_count": 0}
-        
+
         values = [record.value for record in records]
         definition = self.metrics_definitions[name]
-        
+
         summary = {
             "name": name,
             "type": definition.type.value,
@@ -283,23 +322,25 @@ class BusinessMetricsCollector:
             "records_count": len(records),
             "time_window_hours": time_window.total_seconds() / 3600,
             "latest_value": values[-1] if values else None,
-            "unit": definition.unit
+            "unit": definition.unit,
         }
-        
+
         if definition.type == MetricType.COUNTER:
             summary["total"] = sum(values)
-            summary["rate_per_hour"] = sum(values) / (time_window.total_seconds() / 3600)
-        
+            summary["rate_per_hour"] = sum(values) / (
+                time_window.total_seconds() / 3600
+            )
+
         elif definition.type == MetricType.GAUGE:
             summary["current_value"] = values[-1] if values else None
             summary["min_value"] = min(values)
             summary["max_value"] = max(values)
             summary["avg_value"] = sum(values) / len(values)
-        
+
         elif definition.type in [MetricType.HISTOGRAM, MetricType.SUMMARY]:
             sorted_values = sorted(values)
             count = len(sorted_values)
-            
+
             summary["min"] = sorted_values[0]
             summary["max"] = sorted_values[-1]
             summary["avg"] = sum(values) / count
@@ -307,53 +348,55 @@ class BusinessMetricsCollector:
             summary["p90"] = sorted_values[int(count * 0.9)]
             summary["p95"] = sorted_values[int(count * 0.95)]
             summary["p99"] = sorted_values[int(count * 0.99)]
-        
+
         return summary
-    
-    def get_all_metrics_summary(self, 
-                               time_window: timedelta = timedelta(hours=1)) -> Dict[str, Any]:
+
+    def get_all_metrics_summary(
+        self, time_window: timedelta = timedelta(hours=1)
+    ) -> Dict[str, Any]:
         """ç²å–æ‰€æœ‰æŒ‡æ¨™æ‘˜è¦"""
         summaries = {}
-        
+
         for name in self.metrics_definitions.keys():
             summaries[name] = self.get_metric_summary(name, time_window)
-        
+
         return {
             "timestamp": datetime.utcnow().isoformat(),
             "time_window_hours": time_window.total_seconds() / 3600,
             "metrics": summaries,
-            "total_metrics": len(summaries)
+            "total_metrics": len(summaries),
         }
-    
+
     def collect_metrics(self) -> Dict[str, Any]:
         """æ”¶é›†æ‰€æœ‰æ¥­å‹™æŒ‡æ¨™"""
         return self.get_all_metrics_summary()
-    
+
     def get_critical_metrics_status(self) -> Dict[str, Any]:
         """ç²å–é—œéµæŒ‡æ¨™ç‹€æ…‹"""
         critical_metrics = {
-            name: definition for name, definition in self.metrics_definitions.items()
+            name: definition
+            for name, definition in self.metrics_definitions.items()
             if definition.business_impact in ["high", "critical"]
         }
-        
+
         status = {
             "timestamp": datetime.utcnow().isoformat(),
             "critical_metrics_count": len(critical_metrics),
-            "metrics_status": {}
+            "metrics_status": {},
         }
-        
+
         for name, definition in critical_metrics.items():
             summary = self.get_metric_summary(name, timedelta(hours=1))
-            
+
             metric_status = {
                 "name": name,
                 "business_impact": definition.business_impact,
                 "sla_target": definition.sla_target,
                 "records_count": summary.get("records_count", 0),
                 "latest_value": summary.get("latest_value"),
-                "status": "unknown"
+                "status": "unknown",
             }
-            
+
             # ç°¡å–®çš„ SLA ç‹€æ…‹åˆ¤æ–·ï¼ˆå¯ä»¥æ ¹æ“šå¯¦éš›éœ€æ±‚æ“´å±•ï¼‰
             if summary.get("records_count", 0) > 0:
                 if definition.business_impact == "critical":
@@ -362,18 +405,22 @@ class BusinessMetricsCollector:
                     metric_status["status"] = "active"
             else:
                 metric_status["status"] = "no_data"
-            
+
             status["metrics_status"][name] = metric_status
-        
+
         return status
 
+
 # å…¨åŸŸæ¥­å‹™æŒ‡æ¨™æ”¶é›†å™¨å¯¦ä¾‹
 business_metrics = BusinessMetricsCollector(
     "/data/data/com.termux/files/home/myProject/auto_generate_video_fold6/monitoring/business_metrics/metrics_definition.json"
 )
 
+
 # ä¾¿æ·å‡½æ•¸
-def record_video_generation(status: str, video_type: str, platform: str, user_tier: str = "free"):
+def record_video_generation(
+    status: str, video_type: str, platform: str, user_tier: str = "free"
+):
     """è¨˜éŒ„å½±ç‰‡ç”ŸæˆæŒ‡æ¨™"""
     business_metrics.increment_counter(
         "video_generation_count",
@@ -381,12 +428,17 @@ def record_video_generation(status: str, video_type: str, platform: str, user_ti
             "status": status,
             "video_type": video_type,
             "platform": platform,
-            "user_tier": user_tier
-        }
+            "user_tier": user_tier,
+        },
     )
 
-def record_user_engagement(platform: str, engagement_rate: float, 
-                          content_type: str = "video", user_segment: str = "general"):
+
+def record_user_engagement(
+    platform: str,
+    engagement_rate: float,
+    content_type: str = "video",
+    user_segment: str = "general",
+):
     """è¨˜éŒ„ç”¨æˆ¶åƒèˆ‡åº¦"""
     business_metrics.set_gauge(
         "user_engagement_rate",
@@ -394,85 +446,92 @@ def record_user_engagement(platform: str, engagement_rate: float,
         labels={
             "platform": platform,
             "content_type": content_type,
-            "user_segment": user_segment
-        }
+            "user_segment": user_segment,
+        },
     )
 
-def record_content_generation_time(duration_seconds: float, content_type: str, 
-                                  quality_level: str = "standard"):
+
+def record_content_generation_time(
+    duration_seconds: float, content_type: str, quality_level: str = "standard"
+):
     """è¨˜éŒ„å…§å®¹ç”Ÿæˆæ™‚é–“"""
     business_metrics.observe_histogram(
         "content_generation_time",
         duration_seconds,
-        labels={
-            "content_type": content_type,
-            "quality_level": quality_level
-        }
+        labels={"content_type": content_type, "quality_level": quality_level},
     )
 
-def record_revenue_per_user(revenue: float, user_tier: str, billing_period: str = "monthly"):
+
+def record_revenue_per_user(
+    revenue: float, user_tier: str, billing_period: str = "monthly"
+):
     """è¨˜éŒ„æ¯ç”¨æˆ¶æ”¶å…¥"""
     business_metrics.set_gauge(
         "revenue_per_user",
         revenue,
-        labels={
-            "user_tier": user_tier,
-            "billing_period": billing_period
-        }
+        labels={"user_tier": user_tier, "billing_period": billing_period},
     )
 
+
 async def collect_system_metrics():
     """æ”¶é›†ç³»çµ±å±¤ç´šçš„æ¥­å‹™æŒ‡æ¨™"""
     try:
         # é€™å€‹å‡½æ•¸å¯ä»¥å®šæœŸåŸ·è¡Œä»¥æ”¶é›†ç³»çµ±æŒ‡æ¨™
         # ä¾‹å¦‚ï¼šç”¨æˆ¶æ•¸é‡ã€å­˜å„²ä½¿ç”¨é‡ã€è™•ç†ä»»å‹™æ•¸ç­‰
-        
+
         logger.info("Collecting system-level business metrics")
-        
+
         # ç¤ºä¾‹ï¼šè¨˜éŒ„ç³»çµ±å¯ç”¨æ€§ï¼ˆé€™è£¡ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šï¼‰
         import random
+
         system_availability = random.uniform(99.0, 100.0)
         business_metrics.set_gauge(
             "system_availability",
             system_availability,
-            labels={"service": "overall", "region": "default"}
+            labels={"service": "overall", "region": "default"},
         )
-        
+
     except Exception as e:
         logger.error(f"Failed to collect system metrics: {e}")
 
+
 class MetricsReporter:
     """æŒ‡æ¨™å ±å‘Šå™¨"""
-    
+
     def __init__(self, collector: BusinessMetricsCollector):
         self.collector = collector
         self.logger = get_logger("metrics_reporter")
-    
+
     async def generate_daily_report(self) -> Dict[str, Any]:
         """ç”Ÿæˆæ¯æ—¥æŒ‡æ¨™å ±å‘Š"""
         report = {
             "report_type": "daily",
             "timestamp": datetime.utcnow().isoformat(),
             "date": datetime.utcnow().date().isoformat(),
-            "summary": self.collector.get_all_metrics_summary(timedelta(days=1)),
-            "critical_status": self.collector.get_critical_metrics_status()
+            "summary": self.collector.get_all_metrics_summary(
+                timedelta(days=1)
+            ),
+            "critical_status": self.collector.get_critical_metrics_status(),
         }
-        
+
         self.logger.info("Generated daily metrics report")
         return report
-    
+
     async def generate_hourly_report(self) -> Dict[str, Any]:
         """ç”Ÿæˆæ¯å°æ™‚æŒ‡æ¨™å ±å‘Š"""
         report = {
             "report_type": "hourly",
             "timestamp": datetime.utcnow().isoformat(),
             "hour": datetime.utcnow().strftime("%Y-%m-%d %H:00"),
-            "summary": self.collector.get_all_metrics_summary(timedelta(hours=1)),
-            "critical_status": self.collector.get_critical_metrics_status()
+            "summary": self.collector.get_all_metrics_summary(
+                timedelta(hours=1)
+            ),
+            "critical_status": self.collector.get_critical_metrics_status(),
         }
-        
+
         self.logger.info("Generated hourly metrics report")
         return report
 
+
 # å…¨åŸŸæŒ‡æ¨™å ±å‘Šå™¨
-metrics_reporter = MetricsReporter(business_metrics)
\ No newline at end of file
+metrics_reporter = MetricsReporter(business_metrics)
diff --git a/auto_generate_video_fold6/monitoring/business_metrics/metrics_collector.py b/auto_generate_video_fold6/monitoring/business_metrics/metrics_collector.py
index c68966b..8195259 100644
--- a/auto_generate_video_fold6/monitoring/business_metrics/metrics_collector.py
+++ b/auto_generate_video_fold6/monitoring/business_metrics/metrics_collector.py
@@ -17,6 +17,7 @@ from enum import Enum
 # Prometheus metrics (optional dependency)
 try:
     from prometheus_client import Counter, Histogram, Gauge, Summary, Info
+
     PROMETHEUS_AVAILABLE = True
 except ImportError:
     PROMETHEUS_AVAILABLE = False
@@ -25,16 +26,20 @@ from ..logging.structured_logger import get_logger
 
 logger = get_logger(__name__)
 
+
 class MetricType(Enum):
     """æŒ‡æ¨™é¡å‹æšèˆ‰"""
+
     COUNTER = "counter"
     GAUGE = "gauge"
     HISTOGRAM = "histogram"
     SUMMARY = "summary"
 
+
 @dataclass
 class BusinessMetricDefinition:
     """æ¥­å‹™æŒ‡æ¨™å®šç¾©"""
+
     name: str
     type: MetricType
     description: str
@@ -42,62 +47,71 @@ class BusinessMetricDefinition:
     unit: str = ""
     business_impact: str = "medium"  # low, medium, high, critical
     sla_target: str = ""
-    
+
     def __post_init__(self):
         # ç¢ºä¿æ¨™ç±¤æ˜¯æ¸…å–®
         if isinstance(self.labels, str):
             self.labels = [self.labels]
 
+
 @dataclass
 class MetricRecord:
     """æŒ‡æ¨™è¨˜éŒ„"""
+
     name: str
     value: Union[int, float]
     labels: Dict[str, str] = field(default_factory=dict)
     timestamp: datetime = field(default_factory=datetime.utcnow)
     unit: str = ""
 
+
 class BusinessMetricsCollector:
     """æ¥­å‹™æŒ‡æ¨™æ”¶é›†å™¨"""
-    
+
     def __init__(self, metrics_definitions_path: Optional[str] = None):
         self.metrics_definitions: Dict[str, BusinessMetricDefinition] = {}
         self.prometheus_metrics: Dict[str, Any] = {}
-        self.fallback_metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
+        self.fallback_metrics: Dict[str, deque] = defaultdict(
+            lambda: deque(maxlen=10000)
+        )
         self.metrics_lock = threading.Lock()
-        
+
         # è¼‰å…¥æŒ‡æ¨™å®šç¾©
         if metrics_definitions_path:
             self.load_metrics_definitions(metrics_definitions_path)
         else:
             self._create_default_metrics()
-        
+
         # åˆå§‹åŒ– Prometheus æŒ‡æ¨™
         self._initialize_prometheus_metrics()
-    
+
     def load_metrics_definitions(self, file_path: str):
         """å¾ JSON æ–‡ä»¶è¼‰å…¥æŒ‡æ¨™å®šç¾©"""
         try:
-            with open(file_path, 'r', encoding='utf-8') as f:
+            with open(file_path, "r", encoding="utf-8") as f:
                 definitions_data = json.load(f)
-            
+
             for name, definition in definitions_data.items():
                 self.metrics_definitions[name] = BusinessMetricDefinition(
                     name=name,
-                    type=MetricType(definition['type']),
-                    description=definition['description'],
-                    labels=definition.get('labels', []),
-                    unit=definition.get('unit', ''),
-                    business_impact=definition.get('business_impact', 'medium'),
-                    sla_target=definition.get('sla_target', '')
+                    type=MetricType(definition["type"]),
+                    description=definition["description"],
+                    labels=definition.get("labels", []),
+                    unit=definition.get("unit", ""),
+                    business_impact=definition.get(
+                        "business_impact", "medium"
+                    ),
+                    sla_target=definition.get("sla_target", ""),
                 )
-            
-            logger.info(f"Loaded {len(self.metrics_definitions)} business metrics definitions")
-            
+
+            logger.info(
+                f"Loaded {len(self.metrics_definitions)} business metrics definitions"
+            )
+
         except Exception as e:
             logger.error(f"Failed to load metrics definitions: {e}")
             self._create_default_metrics()
-    
+
     def _create_default_metrics(self):
         """å»ºç«‹é è¨­æ¥­å‹™æŒ‡æ¨™å®šç¾©"""
         default_metrics = {
@@ -108,7 +122,7 @@ class BusinessMetricsCollector:
                 labels=["status", "video_type", "platform", "user_tier"],
                 unit="count",
                 business_impact="high",
-                sla_target="> 1000 per day"
+                sla_target="> 1000 per day",
             ),
             "user_engagement_rate": BusinessMetricDefinition(
                 name="user_engagement_rate",
@@ -117,7 +131,7 @@ class BusinessMetricsCollector:
                 labels=["platform", "content_type", "user_segment"],
                 unit="percentage",
                 business_impact="high",
-                sla_target="> 5%"
+                sla_target="> 5%",
             ),
             "content_generation_time": BusinessMetricDefinition(
                 name="content_generation_time",
@@ -126,7 +140,7 @@ class BusinessMetricsCollector:
                 labels=["content_type", "quality_level"],
                 unit="seconds",
                 business_impact="medium",
-                sla_target="< 300 seconds (95th percentile)"
+                sla_target="< 300 seconds (95th percentile)",
             ),
             "revenue_per_user": BusinessMetricDefinition(
                 name="revenue_per_user",
@@ -135,88 +149,99 @@ class BusinessMetricsCollector:
                 labels=["user_tier", "billing_period"],
                 unit="currency",
                 business_impact="critical",
-                sla_target="> $10 per month"
-            )
+                sla_target="> $10 per month",
+            ),
         }
-        
+
         self.metrics_definitions.update(default_metrics)
-    
+
     def _initialize_prometheus_metrics(self):
         """åˆå§‹åŒ– Prometheus æŒ‡æ¨™"""
         if not PROMETHEUS_AVAILABLE:
-            logger.warning("Prometheus client not available, using fallback metrics storage")
+            logger.warning(
+                "Prometheus client not available, using fallback metrics storage"
+            )
             return
-        
+
         for name, definition in self.metrics_definitions.items():
             try:
                 if definition.type == MetricType.COUNTER:
                     self.prometheus_metrics[name] = Counter(
                         name=name,
                         documentation=definition.description,
-                        labelnames=definition.labels
+                        labelnames=definition.labels,
                     )
                 elif definition.type == MetricType.GAUGE:
                     self.prometheus_metrics[name] = Gauge(
                         name=name,
                         documentation=definition.description,
-                        labelnames=definition.labels
+                        labelnames=definition.labels,
                     )
                 elif definition.type == MetricType.HISTOGRAM:
                     self.prometheus_metrics[name] = Histogram(
                         name=name,
                         documentation=definition.description,
-                        labelnames=definition.labels
+                        labelnames=definition.labels,
                     )
                 elif definition.type == MetricType.SUMMARY:
                     self.prometheus_metrics[name] = Summary(
                         name=name,
                         documentation=definition.description,
-                        labelnames=definition.labels
+                        labelnames=definition.labels,
                     )
-                
+
                 logger.debug(f"Initialized Prometheus metric: {name}")
-                
+
             except Exception as e:
-                logger.error(f"Failed to initialize Prometheus metric {name}: {e}")
-    
-    def record_metric(self, name: str, value: Union[int, float], 
-                     labels: Optional[Dict[str, str]] = None):
+                logger.error(
+                    f"Failed to initialize Prometheus metric {name}: {e}"
+                )
+
+    def record_metric(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """è¨˜éŒ„æ¥­å‹™æŒ‡æ¨™"""
         if name not in self.metrics_definitions:
             logger.warning(f"Unknown metric: {name}")
             return
-        
+
         if labels is None:
             labels = {}
-        
+
         definition = self.metrics_definitions[name]
-        
+
         # è¨˜éŒ„åˆ° Prometheus
         if PROMETHEUS_AVAILABLE and name in self.prometheus_metrics:
             try:
                 prometheus_metric = self.prometheus_metrics[name]
-                
+
                 if definition.type == MetricType.COUNTER:
                     if labels:
                         prometheus_metric.labels(**labels).inc(value)
                     else:
                         prometheus_metric.inc(value)
-                        
+
                 elif definition.type == MetricType.GAUGE:
                     if labels:
                         prometheus_metric.labels(**labels).set(value)
                     else:
                         prometheus_metric.set(value)
-                        
-                elif definition.type in [MetricType.HISTOGRAM, MetricType.SUMMARY]:
+
+                elif definition.type in [
+                    MetricType.HISTOGRAM,
+                    MetricType.SUMMARY,
+                ]:
                     if labels:
                         prometheus_metric.labels(**labels).observe(value)
                     else:
                         prometheus_metric.observe(value)
-                        
+
             except Exception as e:
                 logger.error(f"Failed to record Prometheus metric {name}: {e}")
-        
+
         # å¾Œå‚™å„²å­˜
         with self.metrics_lock:
             record = MetricRecord(
@@ -224,11 +249,11 @@ class BusinessMetricsCollector:
                 value=value,
                 labels=labels,
                 timestamp=datetime.utcnow(),
-                unit=definition.unit
+                unit=definition.unit,
             )
-            
+
             self.fallback_metrics[name].append(record)
-        
+
         # è¨˜éŒ„æ—¥èªŒ
         logger.info(
             f"Business metric recorded: {name} = {value}",
@@ -236,44 +261,58 @@ class BusinessMetricsCollector:
             metric_value=value,
             metric_labels=labels,
             business_impact=definition.business_impact,
-            sla_target=definition.sla_target
+            sla_target=definition.sla_target,
         )
-    
-    def increment_counter(self, name: str, amount: Union[int, float] = 1, 
-                         labels: Optional[Dict[str, str]] = None):
+
+    def increment_counter(
+        self,
+        name: str,
+        amount: Union[int, float] = 1,
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """å¢åŠ è¨ˆæ•¸å™¨æŒ‡æ¨™"""
         self.record_metric(name, amount, labels)
-    
-    def set_gauge(self, name: str, value: Union[int, float], 
-                  labels: Optional[Dict[str, str]] = None):
+
+    def set_gauge(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """è¨­å®šå„€è¡¨æŒ‡æ¨™"""
         self.record_metric(name, value, labels)
-    
-    def observe_histogram(self, name: str, value: Union[int, float], 
-                         labels: Optional[Dict[str, str]] = None):
+
+    def observe_histogram(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """è§€å¯Ÿç›´æ–¹åœ–æŒ‡æ¨™"""
         self.record_metric(name, value, labels)
-    
-    def get_metric_summary(self, name: str, 
-                          time_window: timedelta = timedelta(hours=1)) -> Dict[str, Any]:
+
+    def get_metric_summary(
+        self, name: str, time_window: timedelta = timedelta(hours=1)
+    ) -> Dict[str, Any]:
         """ç²å–æŒ‡æ¨™æ‘˜è¦"""
         if name not in self.metrics_definitions:
             return {}
-        
+
         cutoff_time = datetime.utcnow() - time_window
-        
+
         with self.metrics_lock:
             records = [
-                record for record in self.fallback_metrics[name]
+                record
+                for record in self.fallback_metrics[name]
                 if record.timestamp > cutoff_time
             ]
-        
+
         if not records:
             return {"name": name, "records_count": 0}
-        
+
         values = [record.value for record in records]
         definition = self.metrics_definitions[name]
-        
+
         summary = {
             "name": name,
             "type": definition.type.value,
@@ -283,23 +322,25 @@ class BusinessMetricsCollector:
             "records_count": len(records),
             "time_window_hours": time_window.total_seconds() / 3600,
             "latest_value": values[-1] if values else None,
-            "unit": definition.unit
+            "unit": definition.unit,
         }
-        
+
         if definition.type == MetricType.COUNTER:
             summary["total"] = sum(values)
-            summary["rate_per_hour"] = sum(values) / (time_window.total_seconds() / 3600)
-        
+            summary["rate_per_hour"] = sum(values) / (
+                time_window.total_seconds() / 3600
+            )
+
         elif definition.type == MetricType.GAUGE:
             summary["current_value"] = values[-1] if values else None
             summary["min_value"] = min(values)
             summary["max_value"] = max(values)
             summary["avg_value"] = sum(values) / len(values)
-        
+
         elif definition.type in [MetricType.HISTOGRAM, MetricType.SUMMARY]:
             sorted_values = sorted(values)
             count = len(sorted_values)
-            
+
             summary["min"] = sorted_values[0]
             summary["max"] = sorted_values[-1]
             summary["avg"] = sum(values) / count
@@ -307,49 +348,51 @@ class BusinessMetricsCollector:
             summary["p90"] = sorted_values[int(count * 0.9)]
             summary["p95"] = sorted_values[int(count * 0.95)]
             summary["p99"] = sorted_values[int(count * 0.99)]
-        
+
         return summary
-    
-    def get_all_metrics_summary(self, 
-                               time_window: timedelta = timedelta(hours=1)) -> Dict[str, Any]:
+
+    def get_all_metrics_summary(
+        self, time_window: timedelta = timedelta(hours=1)
+    ) -> Dict[str, Any]:
         """ç²å–æ‰€æœ‰æŒ‡æ¨™æ‘˜è¦"""
         summaries = {}
-        
+
         for name in self.metrics_definitions.keys():
             summaries[name] = self.get_metric_summary(name, time_window)
-        
+
         return {
             "timestamp": datetime.utcnow().isoformat(),
             "time_window_hours": time_window.total_seconds() / 3600,
             "metrics": summaries,
-            "total_metrics": len(summaries)
+            "total_metrics": len(summaries),
         }
-    
+
     def get_critical_metrics_status(self) -> Dict[str, Any]:
         """ç²å–é—œéµæŒ‡æ¨™ç‹€æ…‹"""
         critical_metrics = {
-            name: definition for name, definition in self.metrics_definitions.items()
+            name: definition
+            for name, definition in self.metrics_definitions.items()
             if definition.business_impact in ["high", "critical"]
         }
-        
+
         status = {
             "timestamp": datetime.utcnow().isoformat(),
             "critical_metrics_count": len(critical_metrics),
-            "metrics_status": {}
+            "metrics_status": {},
         }
-        
+
         for name, definition in critical_metrics.items():
             summary = self.get_metric_summary(name, timedelta(hours=1))
-            
+
             metric_status = {
                 "name": name,
                 "business_impact": definition.business_impact,
                 "sla_target": definition.sla_target,
                 "records_count": summary.get("records_count", 0),
                 "latest_value": summary.get("latest_value"),
-                "status": "unknown"
+                "status": "unknown",
             }
-            
+
             # ç°¡å–®çš„ SLA ç‹€æ…‹åˆ¤æ–·ï¼ˆå¯ä»¥æ ¹æ“šå¯¦éš›éœ€æ±‚æ“´å±•ï¼‰
             if summary.get("records_count", 0) > 0:
                 if definition.business_impact == "critical":
@@ -358,18 +401,22 @@ class BusinessMetricsCollector:
                     metric_status["status"] = "active"
             else:
                 metric_status["status"] = "no_data"
-            
+
             status["metrics_status"][name] = metric_status
-        
+
         return status
 
+
 # å…¨åŸŸæ¥­å‹™æŒ‡æ¨™æ”¶é›†å™¨å¯¦ä¾‹
 business_metrics = BusinessMetricsCollector(
     "/data/data/com.termux/files/home/myProject/auto_generate_video_fold6/monitoring/business_metrics/metrics_definition.json"
 )
 
+
 # ä¾¿æ·å‡½æ•¸
-def record_video_generation(status: str, video_type: str, platform: str, user_tier: str = "free"):
+def record_video_generation(
+    status: str, video_type: str, platform: str, user_tier: str = "free"
+):
     """è¨˜éŒ„å½±ç‰‡ç”ŸæˆæŒ‡æ¨™"""
     business_metrics.increment_counter(
         "video_generation_count",
@@ -377,12 +424,17 @@ def record_video_generation(status: str, video_type: str, platform: str, user_ti
             "status": status,
             "video_type": video_type,
             "platform": platform,
-            "user_tier": user_tier
-        }
+            "user_tier": user_tier,
+        },
     )
 
-def record_user_engagement(platform: str, engagement_rate: float, 
-                          content_type: str = "video", user_segment: str = "general"):
+
+def record_user_engagement(
+    platform: str,
+    engagement_rate: float,
+    content_type: str = "video",
+    user_segment: str = "general",
+):
     """è¨˜éŒ„ç”¨æˆ¶åƒèˆ‡åº¦"""
     business_metrics.set_gauge(
         "user_engagement_rate",
@@ -390,85 +442,92 @@ def record_user_engagement(platform: str, engagement_rate: float,
         labels={
             "platform": platform,
             "content_type": content_type,
-            "user_segment": user_segment
-        }
+            "user_segment": user_segment,
+        },
     )
 
-def record_content_generation_time(duration_seconds: float, content_type: str, 
-                                  quality_level: str = "standard"):
+
+def record_content_generation_time(
+    duration_seconds: float, content_type: str, quality_level: str = "standard"
+):
     """è¨˜éŒ„å…§å®¹ç”Ÿæˆæ™‚é–“"""
     business_metrics.observe_histogram(
         "content_generation_time",
         duration_seconds,
-        labels={
-            "content_type": content_type,
-            "quality_level": quality_level
-        }
+        labels={"content_type": content_type, "quality_level": quality_level},
     )
 
-def record_revenue_per_user(revenue: float, user_tier: str, billing_period: str = "monthly"):
+
+def record_revenue_per_user(
+    revenue: float, user_tier: str, billing_period: str = "monthly"
+):
     """è¨˜éŒ„æ¯ç”¨æˆ¶æ”¶å…¥"""
     business_metrics.set_gauge(
         "revenue_per_user",
         revenue,
-        labels={
-            "user_tier": user_tier,
-            "billing_period": billing_period
-        }
+        labels={"user_tier": user_tier, "billing_period": billing_period},
     )
 
+
 async def collect_system_metrics():
     """æ”¶é›†ç³»çµ±å±¤ç´šçš„æ¥­å‹™æŒ‡æ¨™"""
     try:
         # é€™å€‹å‡½æ•¸å¯ä»¥å®šæœŸåŸ·è¡Œä»¥æ”¶é›†ç³»çµ±æŒ‡æ¨™
         # ä¾‹å¦‚ï¼šç”¨æˆ¶æ•¸é‡ã€å­˜å„²ä½¿ç”¨é‡ã€è™•ç†ä»»å‹™æ•¸ç­‰
-        
+
         logger.info("Collecting system-level business metrics")
-        
+
         # ç¤ºä¾‹ï¼šè¨˜éŒ„ç³»çµ±å¯ç”¨æ€§ï¼ˆé€™è£¡ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šï¼‰
         import random
+
         system_availability = random.uniform(99.0, 100.0)
         business_metrics.set_gauge(
             "system_availability",
             system_availability,
-            labels={"service": "overall", "region": "default"}
+            labels={"service": "overall", "region": "default"},
         )
-        
+
     except Exception as e:
         logger.error(f"Failed to collect system metrics: {e}")
 
+
 class MetricsReporter:
     """æŒ‡æ¨™å ±å‘Šå™¨"""
-    
+
     def __init__(self, collector: BusinessMetricsCollector):
         self.collector = collector
         self.logger = get_logger("metrics_reporter")
-    
+
     async def generate_daily_report(self) -> Dict[str, Any]:
         """ç”Ÿæˆæ¯æ—¥æŒ‡æ¨™å ±å‘Š"""
         report = {
             "report_type": "daily",
             "timestamp": datetime.utcnow().isoformat(),
             "date": datetime.utcnow().date().isoformat(),
-            "summary": self.collector.get_all_metrics_summary(timedelta(days=1)),
-            "critical_status": self.collector.get_critical_metrics_status()
+            "summary": self.collector.get_all_metrics_summary(
+                timedelta(days=1)
+            ),
+            "critical_status": self.collector.get_critical_metrics_status(),
         }
-        
+
         self.logger.info("Generated daily metrics report")
         return report
-    
+
     async def generate_hourly_report(self) -> Dict[str, Any]:
         """ç”Ÿæˆæ¯å°æ™‚æŒ‡æ¨™å ±å‘Š"""
         report = {
             "report_type": "hourly",
             "timestamp": datetime.utcnow().isoformat(),
             "hour": datetime.utcnow().strftime("%Y-%m-%d %H:00"),
-            "summary": self.collector.get_all_metrics_summary(timedelta(hours=1)),
-            "critical_status": self.collector.get_critical_metrics_status()
+            "summary": self.collector.get_all_metrics_summary(
+                timedelta(hours=1)
+            ),
+            "critical_status": self.collector.get_critical_metrics_status(),
         }
-        
+
         self.logger.info("Generated hourly metrics report")
         return report
 
+
 # å…¨åŸŸæŒ‡æ¨™å ±å‘Šå™¨
-metrics_reporter = MetricsReporter(business_metrics)
\ No newline at end of file
+metrics_reporter = MetricsReporter(business_metrics)
diff --git a/auto_generate_video_fold6/monitoring/cost_tracker.py b/auto_generate_video_fold6/monitoring/cost_tracker.py
index cc4f8c7..f125eed 100644
--- a/auto_generate_video_fold6/monitoring/cost_tracker.py
+++ b/auto_generate_video_fold6/monitoring/cost_tracker.py
@@ -80,22 +80,42 @@ class CostTracker:
         self.cost_rates = {
             ProviderType.OPENAI.value: {
                 "gpt-4": {"input_per_1k": 0.03, "output_per_1k": 0.06},
-                "gpt-3.5-turbo": {"input_per_1k": 0.0015, "output_per_1k": 0.002},
+                "gpt-3.5-turbo": {
+                    "input_per_1k": 0.0015,
+                    "output_per_1k": 0.002,
+                },
                 "gpt-4-turbo": {"input_per_1k": 0.01, "output_per_1k": 0.03},
             },
             ProviderType.STABILITY_AI.value: {
                 "stable-diffusion-xl": {"per_image": 0.04},
                 "stable-diffusion-3": {"per_image": 0.065},
             },
-            ProviderType.ELEVENLABS.value: {"voice_synthesis": {"per_character": 0.00003}},
+            ProviderType.ELEVENLABS.value: {
+                "voice_synthesis": {"per_character": 0.00003}
+            },
             ProviderType.ANTHROPIC.value: {
-                "claude-3-opus": {"input_per_1k": 0.015, "output_per_1k": 0.075},
-                "claude-3-sonnet": {"input_per_1k": 0.003, "output_per_1k": 0.015},
+                "claude-3-opus": {
+                    "input_per_1k": 0.015,
+                    "output_per_1k": 0.075,
+                },
+                "claude-3-sonnet": {
+                    "input_per_1k": 0.003,
+                    "output_per_1k": 0.015,
+                },
             },
             ProviderType.GEMINI.value: {
-                "gemini-pro": {"input_per_1k": 0.0005, "output_per_1k": 0.0015},
-                "gemini-1.5-pro": {"input_per_1k": 0.0035, "output_per_1k": 0.0105},
-                "gemini-1.5-flash": {"input_per_1k": 0.000075, "output_per_1k": 0.0003},
+                "gemini-pro": {
+                    "input_per_1k": 0.0005,
+                    "output_per_1k": 0.0015,
+                },
+                "gemini-1.5-pro": {
+                    "input_per_1k": 0.0035,
+                    "output_per_1k": 0.0105,
+                },
+                "gemini-1.5-flash": {
+                    "input_per_1k": 0.000075,
+                    "output_per_1k": 0.0003,
+                },
             },
             ProviderType.SUNO.value: {
                 "chirp-v3": {"per_minute": 0.5},  # ä¼°ç®—åƒ¹æ ¼
@@ -179,7 +199,12 @@ class CostTracker:
 
         # è¨ˆç®—æˆæœ¬
         cost = self._calculate_cost(
-            provider, model, operation_type, tokens_used, characters_used, images_generated
+            provider,
+            model,
+            operation_type,
+            tokens_used,
+            characters_used,
+            images_generated,
         )
 
         # å»ºç«‹è¨˜éŒ„
@@ -190,7 +215,8 @@ class CostTracker:
             operation_type=operation_type,
             tokens_used=tokens_used,
             cost_usd=cost,
-            request_id=request_id or f"{provider}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
+            request_id=request_id
+            or f"{provider}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
             success=success,
             metadata=metadata,
         )
@@ -233,8 +259,12 @@ class CostTracker:
         # æ ¹æ“šæ“ä½œé¡å‹è¨ˆç®—æˆæœ¬
         if operation_type == "text_generation":
             # å‡è¨­è¼¸å…¥è¼¸å‡ºæ¯”ä¾‹ 1:1
-            input_cost = (tokens_used / 2 / 1000) * model_rates.get("input_per_1k", 0)
-            output_cost = (tokens_used / 2 / 1000) * model_rates.get("output_per_1k", 0)
+            input_cost = (tokens_used / 2 / 1000) * model_rates.get(
+                "input_per_1k", 0
+            )
+            output_cost = (tokens_used / 2 / 1000) * model_rates.get(
+                "output_per_1k", 0
+            )
             return input_cost + output_cost
 
         elif operation_type == "image_generation":
@@ -245,7 +275,9 @@ class CostTracker:
 
         elif operation_type == "music_generation":
             # Suno éŸ³æ¨‚ç”Ÿæˆæˆæœ¬æŒ‰åˆ†é˜è¨ˆç®—
-            duration_minutes = tokens_used / 60.0 if tokens_used > 0 else 0.5  # é è¨­30ç§’
+            duration_minutes = (
+                tokens_used / 60.0 if tokens_used > 0 else 0.5
+            )  # é è¨­30ç§’
             return duration_minutes * model_rates.get("per_minute", 0.5)
 
         else:
@@ -305,7 +337,12 @@ class CostTracker:
             results = cursor.fetchall()
 
         # é‡å»ºå¿«å–
-        self._daily_cache = {"total_cost": 0.0, "call_count": 0, "providers": {}, "operations": {}}
+        self._daily_cache = {
+            "total_cost": 0.0,
+            "call_count": 0,
+            "providers": {},
+            "operations": {},
+        }
 
         for row in results:
             total_cost, call_count, provider, operation_type = row
@@ -327,7 +364,9 @@ class CostTracker:
         if not self.config_manager:
             return
 
-        daily_budget = self.config_manager.get("cost_control.daily_budget_usd", 100.0)
+        daily_budget = self.config_manager.get(
+            "cost_control.daily_budget_usd", 100.0
+        )
         current_cost = self._daily_cache.get("total_cost", 0) + new_cost
 
         # é ç®—ä½¿ç”¨ç‡è­¦å‘Š
@@ -337,12 +376,16 @@ class CostTracker:
             logger.warning(
                 f"âš ï¸  é ç®—è­¦å‘Š: å·²ä½¿ç”¨ {usage_rate:.1%} ({current_cost:.2f}/${daily_budget})"
             )
-            await self._send_budget_alert("critical", usage_rate, current_cost, daily_budget)
+            await self._send_budget_alert(
+                "critical", usage_rate, current_cost, daily_budget
+            )
         elif usage_rate >= 0.8:
             logger.warning(
                 f"ğŸ“Š é ç®—æé†’: å·²ä½¿ç”¨ {usage_rate:.1%} ({current_cost:.2f}/${daily_budget})"
             )
-            await self._send_budget_alert("warning", usage_rate, current_cost, daily_budget)
+            await self._send_budget_alert(
+                "warning", usage_rate, current_cost, daily_budget
+            )
 
     async def _send_budget_alert(
         self, level: str, usage_rate: float, current_cost: float, budget: float
@@ -375,7 +418,9 @@ class CostTracker:
         async with aiofiles.open(alert_file, "w", encoding="utf-8") as f:
             await f.write(json.dumps(alerts, indent=2, ensure_ascii=False))
 
-    async def get_daily_summary(self, target_date: date = None) -> DailyCostSummary:
+    async def get_daily_summary(
+        self, target_date: date = None
+    ) -> DailyCostSummary:
         """ç²å–æ¯æ—¥æˆæœ¬æ‘˜è¦"""
         if target_date is None:
             target_date = date.today()
@@ -386,7 +431,9 @@ class CostTracker:
 
             daily_budget = 100.0
             if self.config_manager:
-                daily_budget = self.config_manager.get("cost_control.daily_budget_usd", 100.0)
+                daily_budget = self.config_manager.get(
+                    "cost_control.daily_budget_usd", 100.0
+                )
 
             total_cost = self._daily_cache.get("total_cost", 0)
 
@@ -461,7 +508,12 @@ class CostTracker:
         for row in results:
             call_date, daily_cost, daily_calls, provider, operation_type = row
             if call_date not in daily_stats:
-                daily_stats[call_date] = {"cost": 0, "calls": 0, "providers": {}, "operations": {}}
+                daily_stats[call_date] = {
+                    "cost": 0,
+                    "calls": 0,
+                    "providers": {},
+                    "operations": {},
+                }
 
             daily_stats[call_date]["cost"] += daily_cost or 0
             daily_stats[call_date]["calls"] += daily_calls or 0
@@ -472,13 +524,18 @@ class CostTracker:
                 daily_stats[call_date]["operations"][operation_type] = 0
 
             daily_stats[call_date]["providers"][provider] += daily_cost or 0
-            daily_stats[call_date]["operations"][operation_type] += daily_cost or 0
+            daily_stats[call_date]["operations"][operation_type] += (
+                daily_cost or 0
+            )
 
             total_cost += daily_cost or 0
             total_calls += daily_calls or 0
 
         return {
-            "period": {"start": start_date.isoformat(), "end": end_date.isoformat()},
+            "period": {
+                "start": start_date.isoformat(),
+                "end": end_date.isoformat(),
+            },
             "total_cost": total_cost,
             "total_calls": total_calls,
             "average_daily_cost": total_cost / 7,
@@ -494,7 +551,8 @@ class CostTracker:
             "current_cost": summary.total_cost,
             "budget_limit": summary.budget_limit,
             "budget_remaining": summary.budget_remaining,
-            "usage_percentage": (summary.total_cost / summary.budget_limit) * 100,
+            "usage_percentage": (summary.total_cost / summary.budget_limit)
+            * 100,
             "is_over_budget": summary.is_over_budget,
             "can_continue": not summary.is_over_budget
             or not self._should_stop_on_budget_exceeded(),
@@ -506,7 +564,9 @@ class CostTracker:
         """æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœ¨é ç®—è¶…æ”¯æ™‚åœæ­¢"""
         if not self.config_manager:
             return True
-        return self.config_manager.get("cost_control.stop_on_budget_exceeded", True)
+        return self.config_manager.get(
+            "cost_control.stop_on_budget_exceeded", True
+        )
 
     async def export_cost_data(self, days: int = 30) -> str:
         """åŒ¯å‡ºæˆæœ¬è³‡æ–™"""
@@ -542,13 +602,18 @@ class CostTracker:
 
         export_data = {
             "export_date": datetime.now().isoformat(),
-            "period": {"start": start_date.isoformat(), "end": end_date.isoformat()},
+            "period": {
+                "start": start_date.isoformat(),
+                "end": end_date.isoformat(),
+            },
             "total_records": len(data),
             "records": data,
         }
 
         async with aiofiles.open(export_file, "w", encoding="utf-8") as f:
-            await f.write(json.dumps(export_data, indent=2, ensure_ascii=False))
+            await f.write(
+                json.dumps(export_data, indent=2, ensure_ascii=False)
+            )
 
         logger.info(f"æˆæœ¬è³‡æ–™å·²åŒ¯å‡ºè‡³: {export_file}")
         return str(export_file)
@@ -573,9 +638,14 @@ async def main():
     print("=== æˆæœ¬è¿½è¹¤å™¨æ¸¬è©¦ ===")
 
     # æ¨¡æ“¬ä¸€äº› API å‘¼å«
-    await tracker.track_api_call("openai", "gpt-4", "text_generation", tokens_used=1000)
     await tracker.track_api_call(
-        "stability_ai", "stable-diffusion-xl", "image_generation", images_generated=2
+        "openai", "gpt-4", "text_generation", tokens_used=1000
+    )
+    await tracker.track_api_call(
+        "stability_ai",
+        "stable-diffusion-xl",
+        "image_generation",
+        images_generated=2,
     )
     await tracker.track_api_call(
         "elevenlabs", "voice_synthesis", "voice_synthesis", characters_used=500
diff --git a/auto_generate_video_fold6/monitoring/docker-performance-monitor.py b/auto_generate_video_fold6/monitoring/docker-performance-monitor.py
index a47b26d..804ee35 100644
--- a/auto_generate_video_fold6/monitoring/docker-performance-monitor.py
+++ b/auto_generate_video_fold6/monitoring/docker-performance-monitor.py
@@ -20,17 +20,19 @@ import sys
 # Configure logging
 logging.basicConfig(
     level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
     handlers=[
-        logging.FileHandler('docker-performance.log'),
-        logging.StreamHandler()
-    ]
+        logging.FileHandler("docker-performance.log"),
+        logging.StreamHandler(),
+    ],
 )
 logger = logging.getLogger(__name__)
 
+
 @dataclass
 class ContainerMetrics:
     """Container performance metrics"""
+
     name: str
     id: str
     status: str
@@ -45,9 +47,11 @@ class ContainerMetrics:
     pids: int
     timestamp: str
 
+
 @dataclass
 class SystemMetrics:
     """System-wide performance metrics"""
+
     cpu_percent: float
     memory_total: int
     memory_available: int
@@ -56,9 +60,11 @@ class SystemMetrics:
     load_average: List[float]
     timestamp: str
 
+
 @dataclass
 class PerformanceAlert:
     """Performance alert definition"""
+
     type: str
     severity: str
     message: str
@@ -67,9 +73,10 @@ class PerformanceAlert:
     threshold: float
     timestamp: str
 
+
 class DockerPerformanceMonitor:
     """Advanced Docker performance monitoring and optimization"""
-    
+
     def __init__(self, config_file: str = "monitor-config.json"):
         self.client = docker.from_env()
         self.config = self._load_config(config_file)
@@ -77,17 +84,19 @@ class DockerPerformanceMonitor:
         self.alerts: List[PerformanceAlert] = []
         self.running = False
         self.monitor_thread = None
-        
+
         # Performance thresholds
-        self.cpu_threshold = self.config.get('cpu_threshold', 80.0)
-        self.memory_threshold = self.config.get('memory_threshold', 85.0)
-        self.disk_threshold = self.config.get('disk_threshold', 90.0)
-        self.network_threshold = self.config.get('network_threshold', 100 * 1024 * 1024)  # 100MB/s
-        
+        self.cpu_threshold = self.config.get("cpu_threshold", 80.0)
+        self.memory_threshold = self.config.get("memory_threshold", 85.0)
+        self.disk_threshold = self.config.get("disk_threshold", 90.0)
+        self.network_threshold = self.config.get(
+            "network_threshold", 100 * 1024 * 1024
+        )  # 100MB/s
+
         # Monitoring intervals
-        self.monitor_interval = self.config.get('monitor_interval', 30)
-        self.history_retention = self.config.get('history_retention_hours', 24)
-        
+        self.monitor_interval = self.config.get("monitor_interval", 30)
+        self.history_retention = self.config.get("history_retention_hours", 24)
+
         # Setup signal handlers
         signal.signal(signal.SIGINT, self._signal_handler)
         signal.signal(signal.SIGTERM, self._signal_handler)
@@ -96,9 +105,15 @@ class DockerPerformanceMonitor:
         """Load monitoring configuration"""
         default_config = {
             "services_to_monitor": [
-                "frontend", "api-gateway", "trend-service", 
-                "video-service", "social-service", "scheduler-service",
-                "postgres", "redis", "minio"
+                "frontend",
+                "api-gateway",
+                "trend-service",
+                "video-service",
+                "social-service",
+                "scheduler-service",
+                "postgres",
+                "redis",
+                "minio",
             ],
             "cpu_threshold": 80.0,
             "memory_threshold": 85.0,
@@ -108,17 +123,17 @@ class DockerPerformanceMonitor:
             "history_retention_hours": 24,
             "enable_auto_restart": True,
             "enable_scaling_recommendations": True,
-            "alert_webhook_url": None
+            "alert_webhook_url": None,
         }
-        
+
         try:
             if Path(config_file).exists():
-                with open(config_file, 'r') as f:
+                with open(config_file, "r") as f:
                     user_config = json.load(f)
                     default_config.update(user_config)
         except Exception as e:
             logger.warning(f"Could not load config file: {e}")
-        
+
         return default_config
 
     def _signal_handler(self, signum, frame):
@@ -132,42 +147,54 @@ class DockerPerformanceMonitor:
         try:
             # Get container stats
             stats = container.stats(stream=False)
-            
+
             # Calculate CPU percentage
-            cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - \
-                       stats['precpu_stats']['cpu_usage']['total_usage']
-            system_delta = stats['cpu_stats']['system_cpu_usage'] - \
-                          stats['precpu_stats']['system_cpu_usage']
-            
+            cpu_delta = (
+                stats["cpu_stats"]["cpu_usage"]["total_usage"]
+                - stats["precpu_stats"]["cpu_usage"]["total_usage"]
+            )
+            system_delta = (
+                stats["cpu_stats"]["system_cpu_usage"]
+                - stats["precpu_stats"]["system_cpu_usage"]
+            )
+
             cpu_percent = 0.0
             if system_delta > 0:
-                cpu_percent = (cpu_delta / system_delta) * \
-                             len(stats['cpu_stats']['cpu_usage']['percpu_usage']) * 100.0
-            
+                cpu_percent = (
+                    (cpu_delta / system_delta)
+                    * len(stats["cpu_stats"]["cpu_usage"]["percpu_usage"])
+                    * 100.0
+                )
+
             # Memory metrics
-            memory_usage = stats['memory_stats']['usage']
-            memory_limit = stats['memory_stats']['limit']
+            memory_usage = stats["memory_stats"]["usage"]
+            memory_limit = stats["memory_stats"]["limit"]
             memory_percent = (memory_usage / memory_limit) * 100.0
-            
+
             # Network metrics
             network_rx = network_tx = 0
-            if 'networks' in stats:
-                for interface in stats['networks'].values():
-                    network_rx += interface.get('rx_bytes', 0)
-                    network_tx += interface.get('tx_bytes', 0)
-            
+            if "networks" in stats:
+                for interface in stats["networks"].values():
+                    network_rx += interface.get("rx_bytes", 0)
+                    network_tx += interface.get("tx_bytes", 0)
+
             # Block I/O metrics
             block_read = block_write = 0
-            if 'blkio_stats' in stats and 'io_service_bytes_recursive' in stats['blkio_stats']:
-                for entry in stats['blkio_stats']['io_service_bytes_recursive']:
-                    if entry['op'] == 'Read':
-                        block_read += entry['value']
-                    elif entry['op'] == 'Write':
-                        block_write += entry['value']
-            
+            if (
+                "blkio_stats" in stats
+                and "io_service_bytes_recursive" in stats["blkio_stats"]
+            ):
+                for entry in stats["blkio_stats"][
+                    "io_service_bytes_recursive"
+                ]:
+                    if entry["op"] == "Read":
+                        block_read += entry["value"]
+                    elif entry["op"] == "Write":
+                        block_write += entry["value"]
+
             # PIDs count
-            pids = stats.get('pids_stats', {}).get('current', 0)
-            
+            pids = stats.get("pids_stats", {}).get("current", 0)
+
             return ContainerMetrics(
                 name=container.name,
                 id=container.short_id,
@@ -181,9 +208,9 @@ class DockerPerformanceMonitor:
                 block_read=block_read,
                 block_write=block_write,
                 pids=pids,
-                timestamp=datetime.utcnow().isoformat()
+                timestamp=datetime.utcnow().isoformat(),
             )
-            
+
         except Exception as e:
             logger.error(f"Error getting metrics for {container.name}: {e}")
             return None
@@ -194,13 +221,13 @@ class DockerPerformanceMonitor:
             # CPU and memory
             cpu_percent = psutil.cpu_percent(interval=1)
             memory = psutil.virtual_memory()
-            
+
             # Disk usage for root filesystem
-            disk = psutil.disk_usage('/')
-            
+            disk = psutil.disk_usage("/")
+
             # Load average
             load_avg = list(psutil.getloadavg())
-            
+
             return SystemMetrics(
                 cpu_percent=cpu_percent,
                 memory_total=memory.total,
@@ -208,159 +235,207 @@ class DockerPerformanceMonitor:
                 memory_percent=memory.percent,
                 disk_usage_percent=disk.percent,
                 load_average=load_avg,
-                timestamp=datetime.utcnow().isoformat()
+                timestamp=datetime.utcnow().isoformat(),
             )
-            
+
         except Exception as e:
             logger.error(f"Error getting system metrics: {e}")
             return None
 
-    def check_performance_alerts(self, container_metrics: List[ContainerMetrics], 
-                               system_metrics: SystemMetrics):
+    def check_performance_alerts(
+        self,
+        container_metrics: List[ContainerMetrics],
+        system_metrics: SystemMetrics,
+    ):
         """Check for performance issues and generate alerts"""
         alerts = []
-        
+
         # System-level alerts
         if system_metrics.cpu_percent > self.cpu_threshold:
-            alerts.append(PerformanceAlert(
-                type="system_cpu",
-                severity="warning" if system_metrics.cpu_percent < 95 else "critical",
-                message=f"High system CPU usage: {system_metrics.cpu_percent:.1f}%",
-                container=None,
-                value=system_metrics.cpu_percent,
-                threshold=self.cpu_threshold,
-                timestamp=datetime.utcnow().isoformat()
-            ))
-        
+            alerts.append(
+                PerformanceAlert(
+                    type="system_cpu",
+                    severity="warning"
+                    if system_metrics.cpu_percent < 95
+                    else "critical",
+                    message=f"High system CPU usage: {system_metrics.cpu_percent:.1f}%",
+                    container=None,
+                    value=system_metrics.cpu_percent,
+                    threshold=self.cpu_threshold,
+                    timestamp=datetime.utcnow().isoformat(),
+                )
+            )
+
         if system_metrics.memory_percent > self.memory_threshold:
-            alerts.append(PerformanceAlert(
-                type="system_memory",
-                severity="warning" if system_metrics.memory_percent < 95 else "critical",
-                message=f"High system memory usage: {system_metrics.memory_percent:.1f}%",
-                container=None,
-                value=system_metrics.memory_percent,
-                threshold=self.memory_threshold,
-                timestamp=datetime.utcnow().isoformat()
-            ))
-        
+            alerts.append(
+                PerformanceAlert(
+                    type="system_memory",
+                    severity="warning"
+                    if system_metrics.memory_percent < 95
+                    else "critical",
+                    message=f"High system memory usage: {system_metrics.memory_percent:.1f}%",
+                    container=None,
+                    value=system_metrics.memory_percent,
+                    threshold=self.memory_threshold,
+                    timestamp=datetime.utcnow().isoformat(),
+                )
+            )
+
         if system_metrics.disk_usage_percent > self.disk_threshold:
-            alerts.append(PerformanceAlert(
-                type="system_disk",
-                severity="warning" if system_metrics.disk_usage_percent < 95 else "critical",
-                message=f"High disk usage: {system_metrics.disk_usage_percent:.1f}%",
-                container=None,
-                value=system_metrics.disk_usage_percent,
-                threshold=self.disk_threshold,
-                timestamp=datetime.utcnow().isoformat()
-            ))
-        
+            alerts.append(
+                PerformanceAlert(
+                    type="system_disk",
+                    severity="warning"
+                    if system_metrics.disk_usage_percent < 95
+                    else "critical",
+                    message=f"High disk usage: {system_metrics.disk_usage_percent:.1f}%",
+                    container=None,
+                    value=system_metrics.disk_usage_percent,
+                    threshold=self.disk_threshold,
+                    timestamp=datetime.utcnow().isoformat(),
+                )
+            )
+
         # Container-level alerts
         for metrics in container_metrics:
             if metrics.cpu_percent > self.cpu_threshold:
-                alerts.append(PerformanceAlert(
-                    type="container_cpu",
-                    severity="warning" if metrics.cpu_percent < 95 else "critical",
-                    message=f"High CPU usage in {metrics.name}: {metrics.cpu_percent:.1f}%",
-                    container=metrics.name,
-                    value=metrics.cpu_percent,
-                    threshold=self.cpu_threshold,
-                    timestamp=datetime.utcnow().isoformat()
-                ))
-            
+                alerts.append(
+                    PerformanceAlert(
+                        type="container_cpu",
+                        severity="warning"
+                        if metrics.cpu_percent < 95
+                        else "critical",
+                        message=f"High CPU usage in {metrics.name}: {metrics.cpu_percent:.1f}%",
+                        container=metrics.name,
+                        value=metrics.cpu_percent,
+                        threshold=self.cpu_threshold,
+                        timestamp=datetime.utcnow().isoformat(),
+                    )
+                )
+
             if metrics.memory_percent > self.memory_threshold:
-                alerts.append(PerformanceAlert(
-                    type="container_memory",
-                    severity="warning" if metrics.memory_percent < 95 else "critical",
-                    message=f"High memory usage in {metrics.name}: {metrics.memory_percent:.1f}%",
-                    container=metrics.name,
-                    value=metrics.memory_percent,
-                    threshold=self.memory_threshold,
-                    timestamp=datetime.utcnow().isoformat()
-                ))
-        
+                alerts.append(
+                    PerformanceAlert(
+                        type="container_memory",
+                        severity="warning"
+                        if metrics.memory_percent < 95
+                        else "critical",
+                        message=f"High memory usage in {metrics.name}: {metrics.memory_percent:.1f}%",
+                        container=metrics.name,
+                        value=metrics.memory_percent,
+                        threshold=self.memory_threshold,
+                        timestamp=datetime.utcnow().isoformat(),
+                    )
+                )
+
         # Add new alerts to the list
         self.alerts.extend(alerts)
-        
+
         # Log alerts
         for alert in alerts:
-            level = logging.WARNING if alert.severity == "warning" else logging.CRITICAL
+            level = (
+                logging.WARNING
+                if alert.severity == "warning"
+                else logging.CRITICAL
+            )
             logger.log(level, f"ALERT: {alert.message}")
-        
+
         return alerts
 
-    def generate_optimization_recommendations(self, 
-                                           container_metrics: List[ContainerMetrics], 
-                                           system_metrics: SystemMetrics) -> List[str]:
+    def generate_optimization_recommendations(
+        self,
+        container_metrics: List[ContainerMetrics],
+        system_metrics: SystemMetrics,
+    ) -> List[str]:
         """Generate performance optimization recommendations"""
         recommendations = []
-        
+
         # Analyze container resource usage
-        high_cpu_containers = [m for m in container_metrics if m.cpu_percent > 70]
-        high_memory_containers = [m for m in container_metrics if m.memory_percent > 70]
-        
+        high_cpu_containers = [
+            m for m in container_metrics if m.cpu_percent > 70
+        ]
+        high_memory_containers = [
+            m for m in container_metrics if m.memory_percent > 70
+        ]
+
         if high_cpu_containers:
             for container in high_cpu_containers:
                 recommendations.append(
                     f"Consider increasing CPU limits for {container.name} "
                     f"(current usage: {container.cpu_percent:.1f}%)"
                 )
-        
+
         if high_memory_containers:
             for container in high_memory_containers:
                 recommendations.append(
                     f"Consider increasing memory limits for {container.name} "
                     f"(current usage: {container.memory_percent:.1f}%)"
                 )
-        
+
         # System-level recommendations
         if system_metrics.memory_percent > 80:
             recommendations.append(
                 "System memory usage is high. Consider adding more RAM or optimizing applications."
             )
-        
+
         if system_metrics.load_average[0] > psutil.cpu_count():
             recommendations.append(
                 f"System load average ({system_metrics.load_average[0]:.2f}) exceeds CPU count. "
                 "Consider reducing container concurrency or scaling horizontally."
             )
-        
+
         return recommendations
 
-    def save_metrics_to_file(self, container_metrics: List[ContainerMetrics], 
-                           system_metrics: SystemMetrics):
+    def save_metrics_to_file(
+        self,
+        container_metrics: List[ContainerMetrics],
+        system_metrics: SystemMetrics,
+    ):
         """Save metrics to JSON file for analysis"""
         try:
             metrics_data = {
                 "timestamp": datetime.utcnow().isoformat(),
                 "system": asdict(system_metrics),
-                "containers": [asdict(m) for m in container_metrics if m]
+                "containers": [asdict(m) for m in container_metrics if m],
             }
-            
+
             # Add to history
             self.metrics_history.append(metrics_data)
-            
+
             # Cleanup old data
-            cutoff_time = datetime.utcnow() - timedelta(hours=self.history_retention)
+            cutoff_time = datetime.utcnow() - timedelta(
+                hours=self.history_retention
+            )
             self.metrics_history = [
-                m for m in self.metrics_history 
-                if datetime.fromisoformat(m['timestamp'].replace('Z', '')) > cutoff_time
+                m
+                for m in self.metrics_history
+                if datetime.fromisoformat(m["timestamp"].replace("Z", ""))
+                > cutoff_time
             ]
-            
+
             # Save to file
-            with open('docker-performance-metrics.json', 'w') as f:
-                json.dump({
-                    "current": metrics_data,
-                    "history": self.metrics_history[-100:],  # Keep last 100 entries
-                    "alerts": [asdict(a) for a in self.alerts[-50:]]  # Keep last 50 alerts
-                }, f, indent=2)
-                
+            with open("docker-performance-metrics.json", "w") as f:
+                json.dump(
+                    {
+                        "current": metrics_data,
+                        "history": self.metrics_history[
+                            -100:
+                        ],  # Keep last 100 entries
+                        "alerts": [
+                            asdict(a) for a in self.alerts[-50:]
+                        ],  # Keep last 50 alerts
+                    },
+                    f,
+                    indent=2,
+                )
+
         except Exception as e:
             logger.error(f"Error saving metrics: {e}")
 
     def monitor_loop(self):
         """Main monitoring loop"""
         logger.info("Starting Docker performance monitoring...")
-        
+
         while self.running:
             try:
                 # Get system metrics
@@ -368,43 +443,47 @@ class DockerPerformanceMonitor:
                 if not system_metrics:
                     time.sleep(self.monitor_interval)
                     continue
-                
+
                 # Get container metrics
                 container_metrics = []
                 containers = self.client.containers.list()
-                
+
                 for container in containers:
                     # Filter by configured services
-                    if container.name in self.config['services_to_monitor']:
+                    if container.name in self.config["services_to_monitor"]:
                         metrics = self.get_container_metrics(container)
                         if metrics:
                             container_metrics.append(metrics)
-                
+
                 # Check for alerts
-                alerts = self.check_performance_alerts(container_metrics, system_metrics)
-                
+                alerts = self.check_performance_alerts(
+                    container_metrics, system_metrics
+                )
+
                 # Generate recommendations
                 recommendations = self.generate_optimization_recommendations(
                     container_metrics, system_metrics
                 )
-                
+
                 # Log recommendations
                 if recommendations:
                     logger.info("Performance recommendations:")
                     for rec in recommendations:
                         logger.info(f"  - {rec}")
-                
+
                 # Save metrics
                 self.save_metrics_to_file(container_metrics, system_metrics)
-                
+
                 # Print summary
-                logger.info(f"Monitored {len(container_metrics)} containers. "
-                          f"System: CPU {system_metrics.cpu_percent:.1f}%, "
-                          f"Memory {system_metrics.memory_percent:.1f}%, "
-                          f"Disk {system_metrics.disk_usage_percent:.1f}%")
-                
+                logger.info(
+                    f"Monitored {len(container_metrics)} containers. "
+                    f"System: CPU {system_metrics.cpu_percent:.1f}%, "
+                    f"Memory {system_metrics.memory_percent:.1f}%, "
+                    f"Disk {system_metrics.disk_usage_percent:.1f}%"
+                )
+
                 time.sleep(self.monitor_interval)
-                
+
             except Exception as e:
                 logger.error(f"Error in monitoring loop: {e}")
                 time.sleep(self.monitor_interval)
@@ -414,9 +493,11 @@ class DockerPerformanceMonitor:
         if self.running:
             logger.warning("Monitoring is already running")
             return
-        
+
         self.running = True
-        self.monitor_thread = threading.Thread(target=self.monitor_loop, daemon=True)
+        self.monitor_thread = threading.Thread(
+            target=self.monitor_loop, daemon=True
+        )
         self.monitor_thread.start()
         logger.info("Docker performance monitoring started")
 
@@ -424,7 +505,7 @@ class DockerPerformanceMonitor:
         """Stop the monitoring process"""
         if not self.running:
             return
-        
+
         self.running = False
         if self.monitor_thread:
             self.monitor_thread.join(timeout=5)
@@ -434,58 +515,71 @@ class DockerPerformanceMonitor:
         """Generate a comprehensive performance report"""
         if not self.metrics_history:
             return {"error": "No metrics data available"}
-        
+
         latest_metrics = self.metrics_history[-1]
-        
+
         # Calculate averages over last hour
         hour_ago = datetime.utcnow() - timedelta(hours=1)
         recent_metrics = [
-            m for m in self.metrics_history
-            if datetime.fromisoformat(m['timestamp'].replace('Z', '')) > hour_ago
+            m
+            for m in self.metrics_history
+            if datetime.fromisoformat(m["timestamp"].replace("Z", ""))
+            > hour_ago
         ]
-        
+
         if recent_metrics:
-            avg_system_cpu = sum(m['system']['cpu_percent'] for m in recent_metrics) / len(recent_metrics)
-            avg_system_memory = sum(m['system']['memory_percent'] for m in recent_metrics) / len(recent_metrics)
+            avg_system_cpu = sum(
+                m["system"]["cpu_percent"] for m in recent_metrics
+            ) / len(recent_metrics)
+            avg_system_memory = sum(
+                m["system"]["memory_percent"] for m in recent_metrics
+            ) / len(recent_metrics)
         else:
-            avg_system_cpu = latest_metrics['system']['cpu_percent']
-            avg_system_memory = latest_metrics['system']['memory_percent']
-        
+            avg_system_cpu = latest_metrics["system"]["cpu_percent"]
+            avg_system_memory = latest_metrics["system"]["memory_percent"]
+
         return {
             "report_time": datetime.utcnow().isoformat(),
-            "monitoring_duration_hours": len(self.metrics_history) * self.monitor_interval / 3600,
+            "monitoring_duration_hours": len(self.metrics_history)
+            * self.monitor_interval
+            / 3600,
             "current_metrics": latest_metrics,
             "hourly_averages": {
                 "system_cpu_percent": round(avg_system_cpu, 2),
-                "system_memory_percent": round(avg_system_memory, 2)
+                "system_memory_percent": round(avg_system_memory, 2),
             },
             "recent_alerts": [asdict(a) for a in self.alerts[-10:]],
-            "total_alerts": len(self.alerts)
+            "total_alerts": len(self.alerts),
         }
 
 
 def main():
     """Main function"""
     import argparse
-    
+
     parser = argparse.ArgumentParser(description="Docker Performance Monitor")
-    parser.add_argument("--config", default="monitor-config.json", 
-                       help="Configuration file path")
-    parser.add_argument("--daemon", action="store_true", 
-                       help="Run as daemon")
-    parser.add_argument("--report", action="store_true", 
-                       help="Generate and display performance report")
-    
+    parser.add_argument(
+        "--config",
+        default="monitor-config.json",
+        help="Configuration file path",
+    )
+    parser.add_argument("--daemon", action="store_true", help="Run as daemon")
+    parser.add_argument(
+        "--report",
+        action="store_true",
+        help="Generate and display performance report",
+    )
+
     args = parser.parse_args()
-    
+
     monitor = DockerPerformanceMonitor(args.config)
-    
+
     if args.report:
         # Generate and display report
         report = monitor.get_performance_report()
         print(json.dumps(report, indent=2))
         return
-    
+
     if args.daemon:
         # Run as daemon
         monitor.start_monitoring()
@@ -499,11 +593,11 @@ def main():
         monitor.start_monitoring()
         time.sleep(monitor.monitor_interval + 5)  # Wait for one cycle
         monitor.stop_monitoring()
-        
+
         # Display report
         report = monitor.get_performance_report()
         print(json.dumps(report, indent=2))
 
 
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
diff --git a/auto_generate_video_fold6/monitoring/health-check/health_monitor.py b/auto_generate_video_fold6/monitoring/health-check/health_monitor.py
index 3d91d66..1b1e308 100644
--- a/auto_generate_video_fold6/monitoring/health-check/health_monitor.py
+++ b/auto_generate_video_fold6/monitoring/health-check/health_monitor.py
@@ -19,7 +19,12 @@ import docker
 import redis
 import sqlalchemy as sa
 from sqlalchemy import create_engine
-from prometheus_client import CollectorRegistry, Gauge, Counter, start_http_server
+from prometheus_client import (
+    CollectorRegistry,
+    Gauge,
+    Counter,
+    start_http_server,
+)
 import smtplib
 from email.mime.text import MIMEText
 from email.mime.multipart import MIMEMultipart
@@ -53,7 +58,9 @@ class SystemMetrics:
 class HealthMonitor:
     """å¥åº·æª¢æŸ¥ç›£æ§å™¨"""
 
-    def __init__(self, config_path: str = "./monitoring/health-check/config.yaml"):
+    def __init__(
+        self, config_path: str = "./monitoring/health-check/config.yaml"
+    ):
         self.config_path = Path(config_path)
         self.config = self._load_config()
         self.logger = self._setup_logging()
@@ -106,7 +113,12 @@ class HealthMonitor:
     def _get_default_config(self) -> Dict[str, Any]:
         """ç²å–é è¨­é…ç½®"""
         return {
-            "monitoring": {"interval": 30, "timeout": 10, "retries": 3, "alert_threshold": 3},
+            "monitoring": {
+                "interval": 30,
+                "timeout": 10,
+                "retries": 3,
+                "alert_threshold": 3,
+            },
             "services": [
                 {
                     "name": "api-gateway",
@@ -163,7 +175,10 @@ class HealthMonitor:
                     "from": "health@auto-video-system.com",
                     "to": ["ops@auto-video-system.com"],
                 },
-                "webhook": {"enabled": True, "url": "http://alertmanager:9093/api/v1/alerts"},
+                "webhook": {
+                    "enabled": True,
+                    "url": "http://alertmanager:9093/api/v1/alerts",
+                },
             },
         }
 
@@ -172,7 +187,10 @@ class HealthMonitor:
         logging.basicConfig(
             level=logging.INFO,
             format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
-            handlers=[logging.FileHandler("./logs/health_monitor.log"), logging.StreamHandler()],
+            handlers=[
+                logging.FileHandler("./logs/health_monitor.log"),
+                logging.StreamHandler(),
+            ],
         )
         return logging.getLogger(__name__)
 
@@ -205,13 +223,21 @@ class HealthMonitor:
             self.docker_client = docker.from_env()
 
             # Redis é€£æ¥
-            redis_config = next((s for s in self.config["services"] if s["name"] == "redis"), None)
+            redis_config = next(
+                (s for s in self.config["services"] if s["name"] == "redis"),
+                None,
+            )
             if redis_config:
                 self.redis_client = redis.from_url(redis_config["connection"])
 
             # è³‡æ–™åº«é€£æ¥
             db_config = next(
-                (s for s in self.config["services"] if s["name"] == "postgresql"), None
+                (
+                    s
+                    for s in self.config["services"]
+                    if s["name"] == "postgresql"
+                ),
+                None,
             )
             if db_config:
                 self.db_engine = create_engine(db_config["connection"])
@@ -224,7 +250,9 @@ class HealthMonitor:
         check_tasks = []
 
         for service_config in self.config["services"]:
-            task = asyncio.create_task(self._check_service_health(service_config))
+            task = asyncio.create_task(
+                self._check_service_health(service_config)
+            )
             check_tasks.append(task)
 
         # ç³»çµ±æŒ‡æ¨™æª¢æŸ¥
@@ -253,7 +281,9 @@ class HealthMonitor:
         # è©•ä¼°æ•´é«”ç³»çµ±å¥åº·
         await self._evaluate_system_health(health_statuses)
 
-    async def _check_service_health(self, service_config: Dict[str, Any]) -> HealthStatus:
+    async def _check_service_health(
+        self, service_config: Dict[str, Any]
+    ) -> HealthStatus:
         """æª¢æŸ¥å–®å€‹æœå‹™å¥åº·ç‹€æ…‹"""
         service_name = service_config["name"]
         service_type = service_config["type"]
@@ -285,7 +315,9 @@ class HealthMonitor:
             self.health_gauge.labels(service_name=service_name).set(
                 1 if status.status == "healthy" else 0
             )
-            self.response_time_gauge.labels(service_name=service_name).set(response_time)
+            self.response_time_gauge.labels(service_name=service_name).set(
+                response_time
+            )
 
             return status
 
@@ -301,7 +333,9 @@ class HealthMonitor:
                 error_message=str(e),
             )
 
-    async def _check_http_service(self, service_config: Dict[str, Any]) -> HealthStatus:
+    async def _check_http_service(
+        self, service_config: Dict[str, Any]
+    ) -> HealthStatus:
         """æª¢æŸ¥ HTTP æœå‹™"""
         url = service_config["url"]
         timeout = self.config["monitoring"]["timeout"]
@@ -317,7 +351,9 @@ class HealthMonitor:
                             response_time=0,  # å°‡åœ¨ä¸Šå±¤è¨­å®š
                             timestamp=datetime.now(),
                             details=response_data,
-                            dependencies=service_config.get("dependencies", []),
+                            dependencies=service_config.get(
+                                "dependencies", []
+                            ),
                         )
                     else:
                         return HealthStatus(
@@ -326,7 +362,9 @@ class HealthMonitor:
                             response_time=0,
                             timestamp=datetime.now(),
                             details={"http_status": response.status},
-                            dependencies=service_config.get("dependencies", []),
+                            dependencies=service_config.get(
+                                "dependencies", []
+                            ),
                             error_message=f"HTTP {response.status}",
                         )
             except asyncio.TimeoutError:
@@ -340,7 +378,9 @@ class HealthMonitor:
                     error_message="Request timeout",
                 )
 
-    async def _check_database_service(self, service_config: Dict[str, Any]) -> HealthStatus:
+    async def _check_database_service(
+        self, service_config: Dict[str, Any]
+    ) -> HealthStatus:
         """æª¢æŸ¥è³‡æ–™åº«æœå‹™"""
         try:
             if self.db_engine:
@@ -370,11 +410,15 @@ class HealthMonitor:
                 error_message=str(e),
             )
 
-    async def _check_redis_service(self, service_config: Dict[str, Any]) -> HealthStatus:
+    async def _check_redis_service(
+        self, service_config: Dict[str, Any]
+    ) -> HealthStatus:
         """æª¢æŸ¥ Redis æœå‹™"""
         try:
             if self.redis_client:
-                await asyncio.get_event_loop().run_in_executor(None, self.redis_client.ping)
+                await asyncio.get_event_loop().run_in_executor(
+                    None, self.redis_client.ping
+                )
 
                 return HealthStatus(
                     service_name=service_config["name"],
@@ -398,15 +442,21 @@ class HealthMonitor:
                 error_message=str(e),
             )
 
-    async def _check_docker_service(self, service_config: Dict[str, Any]) -> HealthStatus:
+    async def _check_docker_service(
+        self, service_config: Dict[str, Any]
+    ) -> HealthStatus:
         """æª¢æŸ¥ Docker å®¹å™¨æœå‹™"""
         try:
-            container_name = service_config.get("container_name", service_config["name"])
+            container_name = service_config.get(
+                "container_name", service_config["name"]
+            )
 
             if self.docker_client:
                 container = self.docker_client.containers.get(container_name)
 
-                status = "healthy" if container.status == "running" else "unhealthy"
+                status = (
+                    "healthy" if container.status == "running" else "unhealthy"
+                )
 
                 return HealthStatus(
                     service_name=service_config["name"],
@@ -449,7 +499,10 @@ class HealthMonitor:
 
             # ç¶²è·¯ I/O
             network = psutil.net_io_counters()
-            network_io = {"bytes_sent": network.bytes_sent, "bytes_recv": network.bytes_recv}
+            network_io = {
+                "bytes_sent": network.bytes_sent,
+                "bytes_recv": network.bytes_recv,
+            }
 
             # æ´»èºé€£æ¥æ•¸
             connections = len(psutil.net_connections())
@@ -506,13 +559,19 @@ class HealthMonitor:
 
             if failure_count >= alert_threshold:
                 last_alert = previous_state.get("last_alert")
-                if not last_alert or (current_time - last_alert).seconds > 300:  # 5åˆ†é˜é–“éš”
+                if (
+                    not last_alert or (current_time - last_alert).seconds > 300
+                ):  # 5åˆ†é˜é–“éš”
                     await self._send_alert(health_status, failure_count)
-                    self.service_states[service_name]["last_alert"] = current_time
+                    self.service_states[service_name]["last_alert"] = (
+                        current_time
+                    )
 
     async def _check_dependencies(self, health_statuses: List[HealthStatus]):
         """æª¢æŸ¥æœå‹™ä¾è³´é—œä¿‚"""
-        status_map = {status.service_name: status for status in health_statuses}
+        status_map = {
+            status.service_name: status for status in health_statuses
+        }
 
         for status in health_statuses:
             for dependency in status.dependencies:
@@ -522,17 +581,24 @@ class HealthMonitor:
                         service_name=status.service_name, dependency=dependency
                     ).set(1 if dep_status.status == "healthy" else 0)
 
-    async def _evaluate_system_health(self, health_statuses: List[HealthStatus]):
+    async def _evaluate_system_health(
+        self, health_statuses: List[HealthStatus]
+    ):
         """è©•ä¼°æ•´é«”ç³»çµ±å¥åº·"""
         total_services = len(health_statuses)
-        healthy_services = sum(1 for s in health_statuses if s.status == "healthy")
+        healthy_services = sum(
+            1 for s in health_statuses if s.status == "healthy"
+        )
         critical_services = [
             s
             for s in health_statuses
-            if s.status != "healthy" and self._is_critical_service(s.service_name)
+            if s.status != "healthy"
+            and self._is_critical_service(s.service_name)
         ]
 
-        system_health_score = healthy_services / total_services if total_services > 0 else 0
+        system_health_score = (
+            healthy_services / total_services if total_services > 0 else 0
+        )
 
         # è¨˜éŒ„ç³»çµ±å¥åº·ç‹€æ…‹
         self.logger.info(
@@ -542,11 +608,16 @@ class HealthMonitor:
 
         # å¦‚æœæœ‰é—œéµæœå‹™æ•…éšœï¼Œç™¼é€ç³»çµ±ç´šå‘Šè­¦
         if critical_services:
-            await self._send_system_alert(critical_services, system_health_score)
+            await self._send_system_alert(
+                critical_services, system_health_score
+            )
 
     def _is_critical_service(self, service_name: str) -> bool:
         """æª¢æŸ¥æ˜¯å¦ç‚ºé—œéµæœå‹™"""
-        service_config = next((s for s in self.config["services"] if s["name"] == service_name), {})
+        service_config = next(
+            (s for s in self.config["services"] if s["name"] == service_name),
+            {},
+        )
         return service_config.get("critical", False)
 
     async def _update_system_metrics(self, metrics: SystemMetrics):
@@ -554,7 +625,9 @@ class HealthMonitor:
         # å¯ä»¥åœ¨é€™è£¡æ·»åŠ ç³»çµ±æŒ‡æ¨™çš„ Prometheus æ›´æ–°
         pass
 
-    async def _send_alert(self, health_status: HealthStatus, failure_count: int):
+    async def _send_alert(
+        self, health_status: HealthStatus, failure_count: int
+    ):
         """ç™¼é€å‘Šè­¦"""
         alert_data = {
             "service": health_status.service_name,
@@ -578,7 +651,9 @@ class HealthMonitor:
         if self.config["alerts"]["webhook"]["enabled"]:
             await self._send_webhook_alert(alert_data)
 
-        self.logger.warning(f"Alert sent for {health_status.service_name}: {health_status.status}")
+        self.logger.warning(
+            f"Alert sent for {health_status.service_name}: {health_status.status}"
+        )
 
     async def _send_recovery_alert(self, health_status: HealthStatus):
         """ç™¼é€æ¢å¾©å‘Šè­¦"""
@@ -591,12 +666,16 @@ class HealthMonitor:
 
         self.logger.info(f"Service recovered: {health_status.service_name}")
 
-    async def _send_system_alert(self, critical_services: List[HealthStatus], health_score: float):
+    async def _send_system_alert(
+        self, critical_services: List[HealthStatus], health_score: float
+    ):
         """ç™¼é€ç³»çµ±ç´šå‘Šè­¦"""
         system_alert = {
             "alert_type": "system_health",
             "health_score": health_score,
-            "critical_services_down": [s.service_name for s in critical_services],
+            "critical_services_down": [
+                s.service_name for s in critical_services
+            ],
             "timestamp": datetime.now().isoformat(),
         }
 
@@ -615,9 +694,13 @@ class HealthMonitor:
         try:
             webhook_url = self.config["alerts"]["webhook"]["url"]
             async with aiohttp.ClientSession() as session:
-                async with session.post(webhook_url, json=alert_data) as response:
+                async with session.post(
+                    webhook_url, json=alert_data
+                ) as response:
                     if response.status != 200:
-                        self.logger.error(f"Webhook alert failed: {response.status}")
+                        self.logger.error(
+                            f"Webhook alert failed: {response.status}"
+                        )
         except Exception as e:
             self.logger.error(f"Failed to send webhook alert: {e}")
 
diff --git a/auto_generate_video_fold6/monitoring/health_monitor.py b/auto_generate_video_fold6/monitoring/health_monitor.py
index 2e85fbe..a8482bd 100755
--- a/auto_generate_video_fold6/monitoring/health_monitor.py
+++ b/auto_generate_video_fold6/monitoring/health_monitor.py
@@ -94,7 +94,8 @@ class HealthMonitor:
                     data = json.load(f)
 
                 self.services = [
-                    ServiceCheck(**service_data) for service_data in data.get("services", [])
+                    ServiceCheck(**service_data)
+                    for service_data in data.get("services", [])
                 ]
             else:
                 # ä½¿ç”¨é è¨­é…ç½®
@@ -125,12 +126,24 @@ class HealthMonitor:
                 auto_restart=True,
                 restart_command="bash scripts/start_backend.sh",
             ),
-            ServiceCheck(name="redis", type="tcp", target="localhost:6379", timeout=5, interval=60),
             ServiceCheck(
-                name="disk_space", type="custom", target="check_disk_space", interval=300  # 5åˆ†é˜
+                name="redis",
+                type="tcp",
+                target="localhost:6379",
+                timeout=5,
+                interval=60,
             ),
             ServiceCheck(
-                name="memory_usage", type="custom", target="check_memory_usage", interval=60
+                name="disk_space",
+                type="custom",
+                target="check_disk_space",
+                interval=300,  # 5åˆ†é˜
+            ),
+            ServiceCheck(
+                name="memory_usage",
+                type="custom",
+                target="check_memory_usage",
+                interval=60,
             ),
         ]
 
@@ -138,7 +151,9 @@ class HealthMonitor:
 
         # ä¿å­˜é è¨­é…ç½®
         try:
-            config_data = {"services": [asdict(service) for service in self.services]}
+            config_data = {
+                "services": [asdict(service) for service in self.services]
+            }
 
             config_path = Path(self.config_file)
             config_path.parent.mkdir(parents=True, exist_ok=True)
@@ -217,7 +232,9 @@ class HealthMonitor:
 
             # å˜—è©¦å»ºç«‹ TCP é€£æ¥
             future = asyncio.open_connection(host, port)
-            reader, writer = await asyncio.wait_for(future, timeout=service.timeout)
+            reader, writer = await asyncio.wait_for(
+                future, timeout=service.timeout
+            )
 
             writer.close()
             await writer.wait_closed()
@@ -250,7 +267,9 @@ class HealthMonitor:
                 timestamp=datetime.now(),
             )
 
-    async def _check_custom_service(self, service: ServiceCheck) -> HealthResult:
+    async def _check_custom_service(
+        self, service: ServiceCheck
+    ) -> HealthResult:
         """æª¢æŸ¥è‡ªå®šç¾©æœå‹™"""
         start_time = time.time()
 
@@ -262,7 +281,9 @@ class HealthMonitor:
             else:
                 # åŸ·è¡Œè‡ªå®šç¾©å‘½ä»¤
                 process = await asyncio.create_subprocess_shell(
-                    service.target, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
+                    service.target,
+                    stdout=asyncio.subprocess.PIPE,
+                    stderr=asyncio.subprocess.PIPE,
                 )
 
                 stdout, stderr = await asyncio.wait_for(
@@ -277,7 +298,8 @@ class HealthMonitor:
                 else:
                     status = HealthStatus.CRITICAL
                     message = (
-                        stderr.decode().strip() or f"å‘½ä»¤åŸ·è¡Œå¤±æ•— (é€€å‡ºä»£ç¢¼: {process.returncode})"
+                        stderr.decode().strip()
+                        or f"å‘½ä»¤åŸ·è¡Œå¤±æ•— (é€€å‡ºä»£ç¢¼: {process.returncode})"
                     )
 
                 return HealthResult(
@@ -306,7 +328,9 @@ class HealthMonitor:
                 timestamp=datetime.now(),
             )
 
-    async def _check_disk_space(self, service: ServiceCheck, start_time: float) -> HealthResult:
+    async def _check_disk_space(
+        self, service: ServiceCheck, start_time: float
+    ) -> HealthResult:
         """æª¢æŸ¥ç£ç›¤ç©ºé–“"""
         try:
             disk_usage = psutil.disk_usage("/")
@@ -347,7 +371,9 @@ class HealthMonitor:
                 timestamp=datetime.now(),
             )
 
-    async def _check_memory_usage(self, service: ServiceCheck, start_time: float) -> HealthResult:
+    async def _check_memory_usage(
+        self, service: ServiceCheck, start_time: float
+    ) -> HealthResult:
         """æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨"""
         try:
             memory = psutil.virtual_memory()
@@ -409,7 +435,10 @@ class HealthMonitor:
             )
 
         # é‡è©¦æ©Ÿåˆ¶
-        if result.status in [HealthStatus.CRITICAL, HealthStatus.DOWN] and service.retries > 0:
+        if (
+            result.status in [HealthStatus.CRITICAL, HealthStatus.DOWN]
+            and service.retries > 0
+        ):
             for retry in range(service.retries):
                 logger.info(f"é‡è©¦æª¢æŸ¥ {service.name} (ç¬¬ {retry + 1} æ¬¡)")
                 await asyncio.sleep(2)  # ç­‰å¾…2ç§’å†é‡è©¦
@@ -421,7 +450,10 @@ class HealthMonitor:
                 elif service.type == "custom":
                     retry_result = await self._check_custom_service(service)
 
-                if retry_result.status not in [HealthStatus.CRITICAL, HealthStatus.DOWN]:
+                if retry_result.status not in [
+                    HealthStatus.CRITICAL,
+                    HealthStatus.DOWN,
+                ]:
                     result = retry_result
                     result.message += f" (é‡è©¦ {retry + 1} æ¬¡å¾ŒæˆåŠŸ)"
                     break
@@ -453,7 +485,9 @@ class HealthMonitor:
             if process.returncode == 0:
                 logger.info(f"æœå‹™ {service.name} é‡å•ŸæˆåŠŸ")
             else:
-                logger.error(f"æœå‹™ {service.name} é‡å•Ÿå¤±æ•—: {stderr.decode()}")
+                logger.error(
+                    f"æœå‹™ {service.name} é‡å•Ÿå¤±æ•—: {stderr.decode()}"
+                )
 
         except Exception as e:
             logger.error(f"é‡å•Ÿæœå‹™ {service.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
@@ -469,7 +503,9 @@ class HealthMonitor:
 
         for i, result in enumerate(results):
             if isinstance(result, Exception):
-                logger.error(f"æª¢æŸ¥æœå‹™ {self.services[i].name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {result}")
+                logger.error(
+                    f"æª¢æŸ¥æœå‹™ {self.services[i].name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {result}"
+                )
                 result = HealthResult(
                     service=self.services[i].name,
                     status=HealthStatus.UNKNOWN,
@@ -489,19 +525,27 @@ class HealthMonitor:
             latest_file = self.monitor_dir / "latest_results.json"
             results_data = {
                 "timestamp": datetime.now().isoformat(),
-                "results": {name: result.to_dict() for name, result in self.results.items()},
+                "results": {
+                    name: result.to_dict()
+                    for name, result in self.results.items()
+                },
             }
 
             async with aiofiles.open(latest_file, "w", encoding="utf-8") as f:
-                await f.write(json.dumps(results_data, indent=2, ensure_ascii=False))
+                await f.write(
+                    json.dumps(results_data, indent=2, ensure_ascii=False)
+                )
 
             # ä¿å­˜æ­·å²è¨˜éŒ„
             history_file = (
-                self.monitor_dir / f"health_history_{datetime.now().strftime('%Y%m%d')}.jsonl"
+                self.monitor_dir
+                / f"health_history_{datetime.now().strftime('%Y%m%d')}.jsonl"
             )
 
             async with aiofiles.open(history_file, "a", encoding="utf-8") as f:
-                await f.write(json.dumps(results_data, ensure_ascii=False) + "\n")
+                await f.write(
+                    json.dumps(results_data, ensure_ascii=False) + "\n"
+                )
 
             logger.debug("å¥åº·æª¢æŸ¥çµæœå·²ä¿å­˜")
 
@@ -531,7 +575,9 @@ class HealthMonitor:
         status_counts = {}
         for status in HealthStatus:
             status_counts[status.value] = sum(
-                1 for result in self.results.values() if result.status == status
+                1
+                for result in self.results.values()
+                if result.status == status
             )
 
         return {
@@ -539,7 +585,9 @@ class HealthMonitor:
             "total_services": len(self.services),
             "status_counts": status_counts,
             "last_check": datetime.now().isoformat(),
-            "services": {name: result.to_dict() for name, result in self.results.items()},
+            "services": {
+                name: result.to_dict() for name, result in self.results.items()
+            },
         }
 
     async def run_continuous_monitoring(self):
@@ -560,11 +608,18 @@ class HealthMonitor:
 
                 # å¦‚æœæœ‰åš´é‡å•é¡Œï¼Œè¨˜éŒ„è©³ç´°ä¿¡æ¯
                 for name, result in self.results.items():
-                    if result.status in [HealthStatus.CRITICAL, HealthStatus.DOWN]:
-                        logger.warning(f"æœå‹™ {name} ç‹€æ…‹ç•°å¸¸: {result.message}")
+                    if result.status in [
+                        HealthStatus.CRITICAL,
+                        HealthStatus.DOWN,
+                    ]:
+                        logger.warning(
+                            f"æœå‹™ {name} ç‹€æ…‹ç•°å¸¸: {result.message}"
+                        )
 
                 # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥ (ä½¿ç”¨æœ€å°é–“éš”)
-                min_interval = min(service.interval for service in self.services)
+                min_interval = min(
+                    service.interval for service in self.services
+                )
                 await asyncio.sleep(min_interval)
 
             except Exception as e:
diff --git a/auto_generate_video_fold6/monitoring/intelligent_cost_optimizer.py b/auto_generate_video_fold6/monitoring/intelligent_cost_optimizer.py
index a0161f9..336a5c9 100644
--- a/auto_generate_video_fold6/monitoring/intelligent_cost_optimizer.py
+++ b/auto_generate_video_fold6/monitoring/intelligent_cost_optimizer.py
@@ -132,7 +132,9 @@ class CostDataCollector:
 
             for record_json in daily_records:
                 record_data = json.loads(record_json)
-                record_data["timestamp"] = datetime.fromisoformat(record_data["timestamp"])
+                record_data["timestamp"] = datetime.fromisoformat(
+                    record_data["timestamp"]
+                )
                 record_data["category"] = CostCategory(record_data["category"])
 
                 record = CostRecord(**record_data)
@@ -165,7 +167,9 @@ class CostPredictor:
         end_date = datetime.utcnow()
         start_date = end_date - timedelta(days=90)  # 90å¤©æ­·å²æ•¸æ“š
 
-        cost_data = await self.cost_collector.get_cost_data(start_date, end_date)
+        cost_data = await self.cost_collector.get_cost_data(
+            start_date, end_date
+        )
 
         if len(cost_data) < 30:  # éœ€è¦è‡³å°‘30å€‹æ•¸æ“šé»
             logger.warning("æ­·å²æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•è¨“ç·´é æ¸¬æ¨¡å‹")
@@ -179,11 +183,15 @@ class CostPredictor:
 
         logger.info(f"å®Œæˆ {len(categories)} å€‹é¡åˆ¥çš„é æ¸¬æ¨¡å‹è¨“ç·´")
 
-    async def _train_category_model(self, cost_data: List[CostRecord], category: CostCategory):
+    async def _train_category_model(
+        self, cost_data: List[CostRecord], category: CostCategory
+    ):
         """ç‚ºç‰¹å®šé¡åˆ¥è¨“ç·´é æ¸¬æ¨¡å‹"""
 
         # ç¯©é¸é¡åˆ¥æ•¸æ“š
-        category_data = [record for record in cost_data if record.category == category]
+        category_data = [
+            record for record in cost_data if record.category == category
+        ]
 
         if len(category_data) < 10:
             return
@@ -205,9 +213,15 @@ class CostPredictor:
 
         # å‰µå»ºæ™‚é–“åºåˆ—ç‰¹å¾µ
         df = df.sort_values("timestamp")
-        df["days_since_start"] = (df["timestamp"] - df["timestamp"].min()).dt.days
-        df["rolling_mean_7d"] = df["amount"].rolling(window=7, min_periods=1).mean()
-        df["rolling_std_7d"] = df["amount"].rolling(window=7, min_periods=1).std().fillna(0)
+        df["days_since_start"] = (
+            df["timestamp"] - df["timestamp"].min()
+        ).dt.days
+        df["rolling_mean_7d"] = (
+            df["amount"].rolling(window=7, min_periods=1).mean()
+        )
+        df["rolling_std_7d"] = (
+            df["amount"].rolling(window=7, min_periods=1).std().fillna(0)
+        )
 
         # æº–å‚™ç‰¹å¾µçŸ©é™£
         features = [
@@ -225,7 +239,10 @@ class CostPredictor:
 
         # è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹
         model = RandomForestRegressor(
-            n_estimators=100, max_depth=10, random_state=42, min_samples_split=5
+            n_estimators=100,
+            max_depth=10,
+            random_state=42,
+            min_samples_split=5,
         )
 
         model.fit(X, y)
@@ -235,7 +252,9 @@ class CostPredictor:
 
         # è¨ˆç®—ç‰¹å¾µé‡è¦æ€§
         feature_importance = dict(zip(features, model.feature_importances_))
-        logger.info(f"{category.value} æ¨¡å‹è¨“ç·´å®Œæˆï¼Œç‰¹å¾µé‡è¦æ€§: {feature_importance}")
+        logger.info(
+            f"{category.value} æ¨¡å‹è¨“ç·´å®Œæˆï¼Œç‰¹å¾µé‡è¦æ€§: {feature_importance}"
+        )
 
     async def predict_future_costs(
         self, category: CostCategory, days_ahead: int = 30
@@ -248,7 +267,10 @@ class CostPredictor:
         model = self.models[category]
 
         # ç”Ÿæˆæœªä¾†æ™‚é–“é»çš„ç‰¹å¾µ
-        future_dates = [datetime.utcnow() + timedelta(days=i) for i in range(1, days_ahead + 1)]
+        future_dates = [
+            datetime.utcnow() + timedelta(days=i)
+            for i in range(1, days_ahead + 1)
+        ]
 
         predictions = []
 
@@ -277,7 +299,10 @@ class CostPredictor:
         # è¨ˆç®—ç½®ä¿¡å€é–“ (ä½¿ç”¨æ¨¹çš„æ¨™æº–å·®)
         # é€™æ˜¯ç°¡åŒ–çš„ç½®ä¿¡å€é–“è¨ˆç®—
         std_dev = np.std(predictions)
-        confidence_interval = (total_predicted - 1.96 * std_dev, total_predicted + 1.96 * std_dev)
+        confidence_interval = (
+            total_predicted - 1.96 * std_dev,
+            total_predicted + 1.96 * std_dev,
+        )
 
         # åˆ†æè¶‹åŠ¿
         trend = self._analyze_trend(predictions)
@@ -311,7 +336,9 @@ class CostPredictor:
         else:
             return "stable"
 
-    async def _analyze_cost_factors(self, category: CostCategory) -> Dict[str, float]:
+    async def _analyze_cost_factors(
+        self, category: CostCategory
+    ) -> Dict[str, float]:
         """åˆ†ææˆæœ¬å½±éŸ¿å› ç´ """
 
         if category not in self.models:
@@ -336,7 +363,9 @@ class CostPredictor:
 class CostOptimizer:
     """æˆæœ¬å„ªåŒ–å™¨"""
 
-    def __init__(self, cost_collector: CostDataCollector, cost_predictor: CostPredictor):
+    def __init__(
+        self, cost_collector: CostDataCollector, cost_predictor: CostPredictor
+    ):
         self.cost_collector = cost_collector
         self.cost_predictor = cost_predictor
         self.optimization_rules = {}
@@ -403,7 +432,9 @@ class CostOptimizer:
         # ç²å–æœ€è¿‘30å¤©çš„æˆæœ¬æ•¸æ“š
         end_date = datetime.utcnow()
         start_date = end_date - timedelta(days=30)
-        cost_data = await self.cost_collector.get_cost_data(start_date, end_date)
+        cost_data = await self.cost_collector.get_cost_data(
+            start_date, end_date
+        )
 
         # æŒ‰é¡åˆ¥åˆ†ææˆæœ¬
         category_costs = {}
@@ -418,8 +449,10 @@ class CostOptimizer:
             avg_daily_cost = total_cost / 30
 
             if category in self.optimization_rules:
-                category_recommendations = await self._generate_category_recommendations(
-                    category, total_cost, avg_daily_cost, strategy
+                category_recommendations = (
+                    await self._generate_category_recommendations(
+                        category, total_cost, avg_daily_cost, strategy
+                    )
                 )
                 recommendations.extend(category_recommendations)
 
@@ -442,7 +475,9 @@ class CostOptimizer:
 
         for rule_name, rule_config in rules.items():
             # è¨ˆç®—æ½›åœ¨ç¯€çœ
-            potential_savings = total_cost * rule_config["potential_savings_rate"]
+            potential_savings = (
+                total_cost * rule_config["potential_savings_rate"]
+            )
 
             # æ ¹æ“šç­–ç•¥èª¿æ•´å»ºè­°å„ªå…ˆç´š
             if strategy == OptimizationStrategy.COST_FIRST:
@@ -469,7 +504,9 @@ class CostOptimizer:
 
         return recommendations
 
-    def _assess_risk_level(self, rule_name: str, category: CostCategory) -> str:
+    def _assess_risk_level(
+        self, rule_name: str, category: CostCategory
+    ) -> str:
         """è©•ä¼°é¢¨éšªç­‰ç´š"""
         high_risk_rules = ["spot_instances", "aggressive_caching"]
         medium_risk_rules = ["auto_scaling", "model_selection"]
@@ -486,7 +523,9 @@ class CostOptimizer:
         effort_to_days = {"low": 3, "medium": 7, "high": 14}
         return effort_to_days.get(implementation_effort, 7)
 
-    def _generate_action_items(self, rule_name: str, category: CostCategory) -> List[str]:
+    def _generate_action_items(
+        self, rule_name: str, category: CostCategory
+    ) -> List[str]:
         """ç”Ÿæˆè¡Œå‹•é …ç›®"""
         action_templates = {
             "model_selection": [
@@ -494,12 +533,18 @@ class CostOptimizer:
                 "å¯¦æ–½å‹•æ…‹æ¨¡å‹é¸æ“‡é‚è¼¯",
                 "è¨­ç½®æ¨¡å‹æ€§èƒ½ç›£æ§",
             ],
-            "batch_processing": ["è¨­è¨ˆæ‰¹é‡è™•ç†API", "å¯¦æ–½è«‹æ±‚èšåˆé‚è¼¯", "å„ªåŒ–æ‰¹è™•ç†èª¿åº¦"],
+            "batch_processing": [
+                "è¨­è¨ˆæ‰¹é‡è™•ç†API",
+                "å¯¦æ–½è«‹æ±‚èšåˆé‚è¼¯",
+                "å„ªåŒ–æ‰¹è™•ç†èª¿åº¦",
+            ],
             "caching": ["è¨­è¨ˆç·©å­˜ç­–ç•¥", "å¯¦æ–½ç·©å­˜å¤±æ•ˆæ©Ÿåˆ¶", "ç›£æ§ç·©å­˜å‘½ä¸­ç‡"],
             "auto_scaling": ["é…ç½®HPAè¦å‰‡", "è¨­ç½®æ€§èƒ½é–¾å€¼", "å¯¦æ–½é æ¸¬æ€§æ“´å±•"],
         }
 
-        return action_templates.get(rule_name, ["åˆ†æç•¶å‰é…ç½®", "åˆ¶å®šå„ªåŒ–è¨ˆåŠƒ", "å¯¦æ–½ä¸¦ç›£æ§"])
+        return action_templates.get(
+            rule_name, ["åˆ†æç•¶å‰é…ç½®", "åˆ¶å®šå„ªåŒ–è¨ˆåŠƒ", "å¯¦æ–½ä¸¦ç›£æ§"]
+        )
 
 
 class CostAnomalyDetector:
@@ -509,13 +554,17 @@ class CostAnomalyDetector:
         self.cost_collector = cost_collector
         self.anomaly_threshold = 2.0  # æ¨™æº–å·®å€æ•¸
 
-    async def detect_anomalies(self, lookback_days: int = 7) -> List[Dict[str, Any]]:
+    async def detect_anomalies(
+        self, lookback_days: int = 7
+    ) -> List[Dict[str, Any]]:
         """æª¢æ¸¬æˆæœ¬ç•°å¸¸"""
 
         end_date = datetime.utcnow()
         start_date = end_date - timedelta(days=lookback_days)
 
-        cost_data = await self.cost_collector.get_cost_data(start_date, end_date)
+        cost_data = await self.cost_collector.get_cost_data(
+            start_date, end_date
+        )
 
         # æŒ‰æœå‹™åˆ†çµ„
         service_costs = {}
@@ -529,7 +578,9 @@ class CostAnomalyDetector:
         anomalies = []
 
         for service, costs in service_costs.items():
-            service_anomalies = await self._detect_service_anomalies(service, costs)
+            service_anomalies = await self._detect_service_anomalies(
+                service, costs
+            )
             anomalies.extend(service_anomalies)
 
         return anomalies
@@ -598,7 +649,9 @@ class BudgetManager:
         budget_key = f"budget:{period}:{budget_config['category']}"
         self.redis_client.set(budget_key, json.dumps(budget_config))
 
-        logger.info(f"è¨­ç½® {period} é ç®—: ${amount} ({budget_config['category']})")
+        logger.info(
+            f"è¨­ç½® {period} é ç®—: ${amount} ({budget_config['category']})"
+        )
 
     async def check_budget_status(self) -> List[Dict[str, Any]]:
         """æª¢æŸ¥é ç®—ç‹€æ…‹"""
@@ -612,7 +665,9 @@ class BudgetManager:
             # è¨ˆç®—ç•¶å‰æœŸé–“çš„å¯¦éš›æˆæœ¬
             actual_cost = await self._calculate_period_cost(
                 budget_data["period"],
-                CostCategory(budget_data["category"]) if budget_data["category"] != "all" else None,
+                CostCategory(budget_data["category"])
+                if budget_data["category"] != "all"
+                else None,
             )
 
             # è¨ˆç®—ä½¿ç”¨ç‡
@@ -623,7 +678,10 @@ class BudgetManager:
             for threshold in budget_data["alert_thresholds"]:
                 if usage_rate >= threshold:
                     alerts.append(
-                        {"threshold": threshold, "message": f"é ç®—ä½¿ç”¨ç‡é”åˆ° {threshold*100:.0f}%"}
+                        {
+                            "threshold": threshold,
+                            "message": f"é ç®—ä½¿ç”¨ç‡é”åˆ° {threshold * 100:.0f}%",
+                        }
                     )
 
             status = {
@@ -649,17 +707,25 @@ class BudgetManager:
         end_date = datetime.utcnow()
 
         if period == "daily":
-            start_date = end_date.replace(hour=0, minute=0, second=0, microsecond=0)
+            start_date = end_date.replace(
+                hour=0, minute=0, second=0, microsecond=0
+            )
         elif period == "weekly":
             days_since_monday = end_date.weekday()
             start_date = end_date - timedelta(days=days_since_monday)
-            start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
+            start_date = start_date.replace(
+                hour=0, minute=0, second=0, microsecond=0
+            )
         elif period == "monthly":
-            start_date = end_date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
+            start_date = end_date.replace(
+                day=1, hour=0, minute=0, second=0, microsecond=0
+            )
         else:
             raise ValueError(f"ä¸æ”¯æŒçš„æœŸé–“é¡å‹: {period}")
 
-        cost_data = await self.cost_collector.get_cost_data(start_date, end_date, category=category)
+        cost_data = await self.cost_collector.get_cost_data(
+            start_date, end_date, category=category
+        )
 
         return sum(record.amount for record in cost_data)
 
@@ -683,7 +749,9 @@ class IntelligentCostOptimizer:
     def __init__(self):
         self.cost_collector = CostDataCollector()
         self.cost_predictor = CostPredictor(self.cost_collector)
-        self.cost_optimizer = CostOptimizer(self.cost_collector, self.cost_predictor)
+        self.cost_optimizer = CostOptimizer(
+            self.cost_collector, self.cost_predictor
+        )
         self.anomaly_detector = CostAnomalyDetector(self.cost_collector)
         self.budget_manager = BudgetManager(self.cost_collector)
 
@@ -708,7 +776,11 @@ class IntelligentCostOptimizer:
             predictions = {}
             for category in CostCategory:
                 try:
-                    prediction = await self.cost_predictor.predict_future_costs(category, 30)
+                    prediction = (
+                        await self.cost_predictor.predict_future_costs(
+                            category, 30
+                        )
+                    )
                     predictions[category.value] = asdict(prediction)
                 except Exception as e:
                     logger.warning(f"ç„¡æ³•é æ¸¬ {category.value} çš„æˆæœ¬: {e}")
@@ -723,7 +795,9 @@ class IntelligentCostOptimizer:
             budget_status = await self.budget_manager.check_budget_status()
 
             # 5. è¨ˆç®—ç¸½é«”ç¯€çœæ½›åŠ›
-            total_potential_savings = sum(rec.potential_savings for rec in recommendations)
+            total_potential_savings = sum(
+                rec.potential_savings for rec in recommendations
+            )
 
             analysis_result = {
                 "timestamp": datetime.utcnow().isoformat(),
@@ -735,7 +809,11 @@ class IntelligentCostOptimizer:
                 "summary": {
                     "total_recommendations": len(recommendations),
                     "high_priority_recommendations": len(
-                        [r for r in recommendations if r.potential_savings > 100]
+                        [
+                            r
+                            for r in recommendations
+                            if r.potential_savings > 100
+                        ]
                     ),
                     "anomalies_detected": len(anomalies),
                     "budgets_exceeded": len(
@@ -744,7 +822,9 @@ class IntelligentCostOptimizer:
                 },
             }
 
-            logger.info(f"ç¶œåˆåˆ†æå®Œæˆ - æ½›åœ¨ç¯€çœ: ${total_potential_savings:.2f}")
+            logger.info(
+                f"ç¶œåˆåˆ†æå®Œæˆ - æ½›åœ¨ç¯€çœ: ${total_potential_savings:.2f}"
+            )
             return analysis_result
 
         except Exception as e:
diff --git a/auto_generate_video_fold6/monitoring/logging/performance_logger.py b/auto_generate_video_fold6/monitoring/logging/performance_logger.py
index 7ae82c1..66638f4 100644
--- a/auto_generate_video_fold6/monitoring/logging/performance_logger.py
+++ b/auto_generate_video_fold6/monitoring/logging/performance_logger.py
@@ -18,72 +18,83 @@ from collections import deque
 import uuid
 
 # ä¸Šä¸‹æ–‡è®Šæ•¸
-correlation_id_context: ContextVar[Optional[str]] = ContextVar("correlation_id", default=None)
-request_id_context: ContextVar[Optional[str]] = ContextVar("request_id", default=None)
+correlation_id_context: ContextVar[Optional[str]] = ContextVar(
+    "correlation_id", default=None
+)
+request_id_context: ContextVar[Optional[str]] = ContextVar(
+    "request_id", default=None
+)
+
 
 class PerformanceLogger:
     """é«˜æ•ˆèƒ½çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„å™¨"""
-    
-    def __init__(self, 
-                 service_name: str,
-                 buffer_size: int = 1000,
-                 flush_interval: float = 1.0,
-                 enable_async: bool = True):
+
+    def __init__(
+        self,
+        service_name: str,
+        buffer_size: int = 1000,
+        flush_interval: float = 1.0,
+        enable_async: bool = True,
+    ):
         self.service_name = service_name
         self.buffer_size = buffer_size
         self.flush_interval = flush_interval
         self.enable_async = enable_async
-        
+
         # é«˜æ•ˆèƒ½ç·©è¡å€
         self.log_buffer = deque(maxlen=buffer_size)
         self.buffer_lock = threading.Lock()
-        
+
         # ç•°æ­¥è™•ç†
         if enable_async:
             self.log_queue = queue.Queue(maxsize=buffer_size * 2)
-            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
+            self.worker_thread = threading.Thread(
+                target=self._worker_loop, daemon=True
+            )
             self.worker_thread.start()
-        
+
         # æ¨™æº–æ—¥èªŒè¨˜éŒ„å™¨ä½œç‚ºå¾Œå‚™
         self.stdlib_logger = logging.getLogger(service_name)
         if not self.stdlib_logger.handlers:
             handler = logging.StreamHandler()
             formatter = logging.Formatter(
-                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
             )
             handler.setFormatter(formatter)
             self.stdlib_logger.addHandler(handler)
             self.stdlib_logger.setLevel(logging.INFO)
-        
+
         # æ•ˆèƒ½æŒ‡æ¨™
         self.stats = {
             "logs_processed": 0,
             "total_processing_time": 0.0,
             "avg_processing_time": 0.0,
             "buffer_utilization": 0.0,
-            "last_flush_time": time.time()
+            "last_flush_time": time.time(),
         }
-    
-    def _create_log_entry(self, level: str, message: str, **kwargs) -> Dict[str, Any]:
+
+    def _create_log_entry(
+        self, level: str, message: str, **kwargs
+    ) -> Dict[str, Any]:
         """å‰µå»ºé«˜æ•ˆèƒ½æ—¥èªŒæ¢ç›®"""
         timestamp = datetime.now(timezone.utc).isoformat()
-        
+
         entry = {
             "timestamp": timestamp,
             "level": level,
             "service": self.service_name,
             "message": message,
             "correlation_id": correlation_id_context.get(),
-            "request_id": request_id_context.get()
+            "request_id": request_id_context.get(),
         }
-        
+
         # åªæ·»åŠ æœ‰å€¼çš„æ¬„ä½ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
         for key, value in kwargs.items():
             if value is not None:
                 entry[key] = value
-        
+
         return entry
-    
+
     def _worker_loop(self):
         """ç•°æ­¥å·¥ä½œç·šç¨‹"""
         while True:
@@ -97,24 +108,26 @@ class PerformanceLogger:
                         entries.append(entry)
                 except queue.Empty:
                     pass
-                
+
                 if entries:
                     self._flush_entries(entries)
-                    
+
             except Exception as e:
                 # éŒ¯èª¤è™•ç†ï¼Œé¿å…å·¥ä½œç·šç¨‹å´©æ½°
                 self.stdlib_logger.error(f"Log worker error: {e}")
                 time.sleep(0.1)
-    
+
     def _flush_entries(self, entries: list):
         """æ‰¹é‡åˆ·æ–°æ—¥èªŒæ¢ç›®"""
         start_time = time.perf_counter()
-        
+
         for entry in entries:
             # è¼¸å‡ºç‚º JSON æ ¼å¼
-            json_line = json.dumps(entry, separators=(',', ':'), ensure_ascii=False)
+            json_line = json.dumps(
+                entry, separators=(",", ":"), ensure_ascii=False
+            )
             print(json_line, flush=True)
-        
+
         # æ›´æ–°çµ±è¨ˆä¿¡æ¯
         processing_time = time.perf_counter() - start_time
         self.stats["logs_processed"] += len(entries)
@@ -123,11 +136,11 @@ class PerformanceLogger:
             self.stats["total_processing_time"] / self.stats["logs_processed"]
         )
         self.stats["last_flush_time"] = time.time()
-    
+
     def log(self, level: str, message: str, **kwargs):
         """é«˜æ•ˆèƒ½æ—¥èªŒè¨˜éŒ„"""
         entry = self._create_log_entry(level, message, **kwargs)
-        
+
         if self.enable_async:
             try:
                 self.log_queue.put(entry, block=False)
@@ -138,27 +151,27 @@ class PerformanceLogger:
             # åŒæ­¥æ¨¡å¼ï¼šä½¿ç”¨ç·©è¡å€
             with self.buffer_lock:
                 self.log_buffer.append(entry)
-                
+
                 if len(self.log_buffer) >= self.buffer_size:
                     self._flush_entries(list(self.log_buffer))
                     self.log_buffer.clear()
-    
+
     def info(self, message: str, **kwargs):
         """INFO ç´šåˆ¥æ—¥èªŒ"""
         self.log("INFO", message, **kwargs)
-    
+
     def error(self, message: str, **kwargs):
         """ERROR ç´šåˆ¥æ—¥èªŒ"""
         self.log("ERROR", message, **kwargs)
-    
+
     def warning(self, message: str, **kwargs):
         """WARNING ç´šåˆ¥æ—¥èªŒ"""
         self.log("WARNING", message, **kwargs)
-    
+
     def debug(self, message: str, **kwargs):
         """DEBUG ç´šåˆ¥æ—¥èªŒ"""
         self.log("DEBUG", message, **kwargs)
-    
+
     def flush(self):
         """å¼·åˆ¶åˆ·æ–°ç·©è¡å€"""
         if not self.enable_async:
@@ -166,197 +179,216 @@ class PerformanceLogger:
                 if self.log_buffer:
                     self._flush_entries(list(self.log_buffer))
                     self.log_buffer.clear()
-    
+
     def get_stats(self) -> Dict[str, Any]:
         """ç²å–æ•ˆèƒ½çµ±è¨ˆ"""
         with self.buffer_lock:
-            self.stats["buffer_utilization"] = len(self.log_buffer) / self.buffer_size
-        
+            self.stats["buffer_utilization"] = (
+                len(self.log_buffer) / self.buffer_size
+            )
+
         return self.stats.copy()
 
+
 class LogContext:
     """é«˜æ•ˆèƒ½æ—¥èªŒä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
-    
+
     def __init__(self, correlation_id: str = None, request_id: str = None):
         self.correlation_id = correlation_id or str(uuid.uuid4())
         self.request_id = request_id or str(uuid.uuid4())
         self.tokens = []
-    
+
     def __enter__(self):
         self.tokens.append(correlation_id_context.set(self.correlation_id))
         self.tokens.append(request_id_context.set(self.request_id))
         return self
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         for token in reversed(self.tokens):
             token.var.reset(token)
 
+
 # è£é£¾å™¨ç”¨æ–¼å‡½æ•¸æ•ˆèƒ½æ¸¬é‡
-def measure_performance(logger: PerformanceLogger = None, operation_name: str = None):
+def measure_performance(
+    logger: PerformanceLogger = None, operation_name: str = None
+):
     """å‡½æ•¸æ•ˆèƒ½æ¸¬é‡è£é£¾å™¨"""
+
     def decorator(func):
         from functools import wraps
-        
+
         @wraps(func)
         async def async_wrapper(*args, **kwargs):
             nonlocal logger
             if not logger:
                 logger = get_performance_logger(func.__module__)
-            
+
             name = operation_name or f"{func.__module__}.{func.__name__}"
             start_time = time.perf_counter()
-            
+
             try:
                 result = await func(*args, **kwargs)
                 duration = (time.perf_counter() - start_time) * 1000
-                
+
                 logger.info(
                     f"Operation completed: {name}",
                     operation=name,
                     duration_ms=duration,
-                    status="success"
+                    status="success",
                 )
-                
+
                 return result
-                
+
             except Exception as e:
                 duration = (time.perf_counter() - start_time) * 1000
-                
+
                 logger.error(
                     f"Operation failed: {name}",
                     operation=name,
                     duration_ms=duration,
                     status="error",
                     error_type=type(e).__name__,
-                    error_message=str(e)
+                    error_message=str(e),
                 )
                 raise
-        
+
         @wraps(func)
         def sync_wrapper(*args, **kwargs):
             nonlocal logger
             if not logger:
                 logger = get_performance_logger(func.__module__)
-            
+
             name = operation_name or f"{func.__module__}.{func.__name__}"
             start_time = time.perf_counter()
-            
+
             try:
                 result = func(*args, **kwargs)
                 duration = (time.perf_counter() - start_time) * 1000
-                
+
                 logger.info(
                     f"Operation completed: {name}",
                     operation=name,
                     duration_ms=duration,
-                    status="success"
+                    status="success",
                 )
-                
+
                 return result
-                
+
             except Exception as e:
                 duration = (time.perf_counter() - start_time) * 1000
-                
+
                 logger.error(
                     f"Operation failed: {name}",
                     operation=name,
                     duration_ms=duration,
                     status="error",
                     error_type=type(e).__name__,
-                    error_message=str(e)
+                    error_message=str(e),
                 )
                 raise
-        
+
         if asyncio.iscoroutinefunction(func):
             return async_wrapper
         else:
             return sync_wrapper
-    
+
     return decorator
 
+
 # å…¨åŸŸè¨˜éŒ„å™¨ç®¡ç†
 _performance_loggers: Dict[str, PerformanceLogger] = {}
 _loggers_lock = threading.Lock()
 
+
 def get_performance_logger(service_name: str, **kwargs) -> PerformanceLogger:
     """ç²å–æˆ–å‰µå»ºé«˜æ•ˆèƒ½æ—¥èªŒè¨˜éŒ„å™¨"""
     with _loggers_lock:
         if service_name not in _performance_loggers:
             _performance_loggers[service_name] = PerformanceLogger(
-                service_name=service_name,
-                **kwargs
+                service_name=service_name, **kwargs
             )
         return _performance_loggers[service_name]
 
+
 def set_correlation_id(correlation_id: str):
     """è¨­å®šé—œè¯ID"""
     correlation_id_context.set(correlation_id)
 
+
 def set_request_id(request_id: str):
     """è¨­å®šè«‹æ±‚ID"""
     request_id_context.set(request_id)
 
+
 def get_correlation_id() -> Optional[str]:
     """ç²å–é—œè¯ID"""
     return correlation_id_context.get()
 
+
 def get_request_id() -> Optional[str]:
     """ç²å–è«‹æ±‚ID"""
     return request_id_context.get()
 
+
 @contextmanager
 def performance_context(correlation_id: str = None, request_id: str = None):
     """æ•ˆèƒ½ç›£æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
     with LogContext(correlation_id, request_id):
         yield
 
+
 # æ‰¹é‡æ—¥èªŒè™•ç†å™¨
 class BatchLogProcessor:
     """æ‰¹é‡æ—¥èªŒè™•ç†å™¨ï¼Œç”¨æ–¼é«˜ååé‡å ´æ™¯"""
-    
+
     def __init__(self, batch_size: int = 100, flush_interval: float = 5.0):
         self.batch_size = batch_size
         self.flush_interval = flush_interval
         self.batch = []
         self.last_flush = time.time()
         self.lock = threading.Lock()
-    
+
     def add_log(self, entry: Dict[str, Any]):
         """æ·»åŠ æ—¥èªŒæ¢ç›®åˆ°æ‰¹æ¬¡"""
         with self.lock:
             self.batch.append(entry)
-            
+
             # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
-            if (len(self.batch) >= self.batch_size or 
-                time.time() - self.last_flush >= self.flush_interval):
+            if (
+                len(self.batch) >= self.batch_size
+                or time.time() - self.last_flush >= self.flush_interval
+            ):
                 self._flush_batch()
-    
+
     def _flush_batch(self):
         """åˆ·æ–°æ‰¹æ¬¡"""
         if not self.batch:
             return
-        
+
         # æ‰¹é‡è¼¸å‡º
         for entry in self.batch:
-            json_line = json.dumps(entry, separators=(',', ':'), ensure_ascii=False)
+            json_line = json.dumps(
+                entry, separators=(",", ":"), ensure_ascii=False
+            )
             print(json_line, flush=True)
-        
+
         self.batch.clear()
         self.last_flush = time.time()
-    
+
     def flush(self):
         """å¼·åˆ¶åˆ·æ–°"""
         with self.lock:
             self._flush_batch()
 
+
 # æ•ˆèƒ½ç›£æ§å·¥å…·
 class PerformanceMonitor:
     """æ•ˆèƒ½ç›£æ§å·¥å…·"""
-    
+
     def __init__(self):
         self.metrics = {}
         self.lock = threading.Lock()
-    
+
     def record_timing(self, operation: str, duration_ms: float):
         """è¨˜éŒ„æ“ä½œæ™‚é–“"""
         with self.lock:
@@ -364,35 +396,38 @@ class PerformanceMonitor:
                 self.metrics[operation] = {
                     "count": 0,
                     "total_time": 0.0,
-                    "min_time": float('inf'),
+                    "min_time": float("inf"),
                     "max_time": 0.0,
-                    "recent_times": deque(maxlen=100)
+                    "recent_times": deque(maxlen=100),
                 }
-            
+
             metric = self.metrics[operation]
             metric["count"] += 1
             metric["total_time"] += duration_ms
             metric["min_time"] = min(metric["min_time"], duration_ms)
             metric["max_time"] = max(metric["max_time"], duration_ms)
             metric["recent_times"].append(duration_ms)
-    
+
     def get_stats(self) -> Dict[str, Any]:
         """ç²å–æ•ˆèƒ½çµ±è¨ˆ"""
         with self.lock:
             stats = {}
             for operation, metric in self.metrics.items():
                 avg_time = metric["total_time"] / metric["count"]
-                recent_avg = sum(metric["recent_times"]) / len(metric["recent_times"])
-                
+                recent_avg = sum(metric["recent_times"]) / len(
+                    metric["recent_times"]
+                )
+
                 stats[operation] = {
                     "count": metric["count"],
                     "avg_time_ms": avg_time,
                     "min_time_ms": metric["min_time"],
                     "max_time_ms": metric["max_time"],
-                    "recent_avg_ms": recent_avg
+                    "recent_avg_ms": recent_avg,
                 }
-            
+
             return stats
 
+
 # å…¨åŸŸæ•ˆèƒ½ç›£æ§å™¨
-performance_monitor = PerformanceMonitor()
\ No newline at end of file
+performance_monitor = PerformanceMonitor()
diff --git a/auto_generate_video_fold6/monitoring/logging/structured_logger.py b/auto_generate_video_fold6/monitoring/logging/structured_logger.py
index 4b7250d..393eba3 100644
--- a/auto_generate_video_fold6/monitoring/logging/structured_logger.py
+++ b/auto_generate_video_fold6/monitoring/logging/structured_logger.py
@@ -17,10 +17,18 @@ import structlog
 from pythonjsonlogger import jsonlogger
 
 # ä¸Šä¸‹æ–‡è®Šæ•¸ï¼Œç”¨æ–¼è¿½è¹¤è«‹æ±‚
-request_id_context: ContextVar[Optional[str]] = ContextVar("request_id", default=None)
-user_id_context: ContextVar[Optional[str]] = ContextVar("user_id", default=None)
-trace_id_context: ContextVar[Optional[str]] = ContextVar("trace_id", default=None)
-correlation_id_context: ContextVar[Optional[str]] = ContextVar("correlation_id", default=None)
+request_id_context: ContextVar[Optional[str]] = ContextVar(
+    "request_id", default=None
+)
+user_id_context: ContextVar[Optional[str]] = ContextVar(
+    "user_id", default=None
+)
+trace_id_context: ContextVar[Optional[str]] = ContextVar(
+    "trace_id", default=None
+)
+correlation_id_context: ContextVar[Optional[str]] = ContextVar(
+    "correlation_id", default=None
+)
 
 
 class LogLevel(Enum):
@@ -49,7 +57,7 @@ class EventType(Enum):
 @dataclass
 class LogEvent:
     """æ—¥èªŒäº‹ä»¶æ•¸æ“šçµæ§‹"""
-    
+
     timestamp: str
     level: str
     service: str
@@ -66,7 +74,7 @@ class LogEvent:
     exception: Optional[str] = None
     stack_trace: Optional[str] = None
     additional_fields: Optional[Dict[str, Any]] = None
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
         result = asdict(self)
@@ -81,19 +89,19 @@ def add_context_processor(logger, method_name, event_dict):
     # æ·»åŠ é—œè¯ID
     if correlation_id_context.get():
         event_dict["correlation_id"] = correlation_id_context.get()
-    
+
     # æ·»åŠ è«‹æ±‚ID
     if request_id_context.get():
         event_dict["request_id"] = request_id_context.get()
-    
+
     # æ·»åŠ ç”¨æˆ¶ID
     if user_id_context.get():
         event_dict["user_id"] = user_id_context.get()
-    
+
     # æ·»åŠ è¿½è¸ªID
     if trace_id_context.get():
         event_dict["trace_id"] = trace_id_context.get()
-    
+
     return event_dict
 
 
@@ -126,7 +134,7 @@ structlog.configure(
         structlog.stdlib.PositionalArgumentsFormatter(),
         structlog.processors.StackInfoRenderer(),
         structlog.processors.format_exc_info,
-        structlog.processors.JSONRenderer()
+        structlog.processors.JSONRenderer(),
     ],
     context_class=dict,
     logger_factory=structlog.stdlib.LoggerFactory(),
@@ -198,7 +206,8 @@ class StructuredLogger:
             )
         else:
             formatter = logging.Formatter(
-                "%(asctime)s - %(name)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
+                "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
+                datefmt="%Y-%m-%d %H:%M:%S",
             )
 
         # æ§åˆ¶å°è™•ç†å™¨
@@ -271,29 +280,49 @@ class StructuredLogger:
             **kwargs,
         )
 
-    def debug(self, message: str, event_type: EventType = EventType.SYSTEM, **kwargs):
+    def debug(
+        self, message: str, event_type: EventType = EventType.SYSTEM, **kwargs
+    ):
         """è¨˜éŒ„ DEBUG ç´šåˆ¥æ—¥èªŒ"""
-        event = self._create_log_event(LogLevel.DEBUG, event_type, message, **kwargs)
+        event = self._create_log_event(
+            LogLevel.DEBUG, event_type, message, **kwargs
+        )
         self.struct_logger.debug(message, **event.to_dict())
 
-    def info(self, message: str, event_type: EventType = EventType.SYSTEM, **kwargs):
+    def info(
+        self, message: str, event_type: EventType = EventType.SYSTEM, **kwargs
+    ):
         """è¨˜éŒ„ INFO ç´šåˆ¥æ—¥èªŒ"""
-        event = self._create_log_event(LogLevel.INFO, event_type, message, **kwargs)
+        event = self._create_log_event(
+            LogLevel.INFO, event_type, message, **kwargs
+        )
         self.struct_logger.info(message, **event.to_dict())
 
-    def warning(self, message: str, event_type: EventType = EventType.SYSTEM, **kwargs):
+    def warning(
+        self, message: str, event_type: EventType = EventType.SYSTEM, **kwargs
+    ):
         """è¨˜éŒ„ WARNING ç´šåˆ¥æ—¥èªŒ"""
-        event = self._create_log_event(LogLevel.WARNING, event_type, message, **kwargs)
+        event = self._create_log_event(
+            LogLevel.WARNING, event_type, message, **kwargs
+        )
         self.struct_logger.warning(message, **event.to_dict())
 
-    def error(self, message: str, event_type: EventType = EventType.ERROR, **kwargs):
+    def error(
+        self, message: str, event_type: EventType = EventType.ERROR, **kwargs
+    ):
         """è¨˜éŒ„ ERROR ç´šåˆ¥æ—¥èªŒ"""
-        event = self._create_log_event(LogLevel.ERROR, event_type, message, **kwargs)
+        event = self._create_log_event(
+            LogLevel.ERROR, event_type, message, **kwargs
+        )
         self.struct_logger.error(message, **event.to_dict())
 
-    def critical(self, message: str, event_type: EventType = EventType.ERROR, **kwargs):
+    def critical(
+        self, message: str, event_type: EventType = EventType.ERROR, **kwargs
+    ):
         """è¨˜éŒ„ CRITICAL ç´šåˆ¥æ—¥èªŒ"""
-        event = self._create_log_event(LogLevel.CRITICAL, event_type, message, **kwargs)
+        event = self._create_log_event(
+            LogLevel.CRITICAL, event_type, message, **kwargs
+        )
         self.struct_logger.critical(message, **event.to_dict())
 
     def log_request(
@@ -319,7 +348,9 @@ class StructuredLogger:
             **kwargs,
         )
 
-    def log_error(self, error: Exception, context: Dict[str, Any] = None, **kwargs):
+    def log_error(
+        self, error: Exception, context: Dict[str, Any] = None, **kwargs
+    ):
         """è¨˜éŒ„éŒ¯èª¤"""
         error_details = {
             "type": type(error).__name__,
@@ -354,15 +385,25 @@ class StructuredLogger:
             "resource": resource,
         }
 
-        level = LogLevel.WARNING if severity in ["low", "medium"] else LogLevel.ERROR
+        level = (
+            LogLevel.WARNING
+            if severity in ["low", "medium"]
+            else LogLevel.ERROR
+        )
 
         if level == LogLevel.ERROR:
             self.error(
-                event_description, event_type=EventType.SECURITY, extra=security_data, **kwargs
+                event_description,
+                event_type=EventType.SECURITY,
+                extra=security_data,
+                **kwargs,
             )
         else:
             self.warning(
-                event_description, event_type=EventType.SECURITY, extra=security_data, **kwargs
+                event_description,
+                event_type=EventType.SECURITY,
+                extra=security_data,
+                **kwargs,
             )
 
     def log_performance(
@@ -385,11 +426,23 @@ class StructuredLogger:
         message = f"Performance: {operation} took {duration_ms:.2f}ms"
 
         if level == LogLevel.WARNING:
-            self.warning(message, event_type=EventType.PERFORMANCE, extra=perf_data, **kwargs)
+            self.warning(
+                message,
+                event_type=EventType.PERFORMANCE,
+                extra=perf_data,
+                **kwargs,
+            )
         else:
-            self.info(message, event_type=EventType.PERFORMANCE, extra=perf_data, **kwargs)
+            self.info(
+                message,
+                event_type=EventType.PERFORMANCE,
+                extra=perf_data,
+                **kwargs,
+            )
 
-    def log_business_event(self, event_name: str, metrics: Dict[str, Any], **kwargs):
+    def log_business_event(
+        self, event_name: str, metrics: Dict[str, Any], **kwargs
+    ):
         """è¨˜éŒ„æ¥­å‹™äº‹ä»¶"""
         self.info(
             f"Business event: {event_name}",
@@ -407,10 +460,17 @@ class StructuredLogger:
         **kwargs,
     ):
         """è¨˜éŒ„å¯©è¨ˆäº‹ä»¶"""
-        audit_data = {"action": action, "resource": resource, "details": details or {}}
+        audit_data = {
+            "action": action,
+            "resource": resource,
+            "details": details or {},
+        }
 
         self.info(
-            f"Audit: {action} on {resource}", event_type=EventType.AUDIT, extra=audit_data, **kwargs
+            f"Audit: {action} on {resource}",
+            event_type=EventType.AUDIT,
+            extra=audit_data,
+            **kwargs,
         )
 
 
@@ -419,7 +479,10 @@ _global_loggers: Dict[str, StructuredLogger] = {}
 
 
 def get_logger(
-    service_name: str, version: str = "1.0.0", environment: str = None, **kwargs
+    service_name: str,
+    version: str = "1.0.0",
+    environment: str = None,
+    **kwargs,
 ) -> StructuredLogger:
     """ç²å–æˆ–å‰µå»ºæ—¥èªŒè¨˜éŒ„å™¨"""
     environment = environment or os.getenv("ENVIRONMENT", "production")
@@ -428,7 +491,10 @@ def get_logger(
 
     if logger_key not in _global_loggers:
         _global_loggers[logger_key] = StructuredLogger(
-            service_name=service_name, version=version, environment=environment, **kwargs
+            service_name=service_name,
+            version=version,
+            environment=environment,
+            **kwargs,
         )
 
     return _global_loggers[logger_key]
@@ -438,7 +504,13 @@ def get_logger(
 class LogContext:
     """æ—¥èªŒä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
 
-    def __init__(self, request_id: str = None, user_id: str = None, trace_id: str = None, correlation_id: str = None):
+    def __init__(
+        self,
+        request_id: str = None,
+        user_id: str = None,
+        trace_id: str = None,
+        correlation_id: str = None,
+    ):
         self.request_id = request_id
         self.user_id = user_id
         self.trace_id = trace_id
@@ -463,7 +535,9 @@ class LogContext:
 
 # è£é£¾å™¨
 def log_function_call(
-    logger: StructuredLogger = None, include_args: bool = False, include_result: bool = False
+    logger: StructuredLogger = None,
+    include_args: bool = False,
+    include_result: bool = False,
 ):
     """å‡½æ•¸èª¿ç”¨æ—¥èªŒè£é£¾å™¨"""
 
@@ -494,7 +568,9 @@ def log_function_call(
                 if include_result:
                     log_data["result"] = str(result)
 
-                logger.debug(f"Function completed: {func_name}", extra=log_data)
+                logger.debug(
+                    f"Function completed: {func_name}", extra=log_data
+                )
                 return result
 
             except Exception as e:
@@ -530,7 +606,9 @@ def log_function_call(
                 if include_result:
                     log_data["result"] = str(result)
 
-                logger.debug(f"Function completed: {func_name}", extra=log_data)
+                logger.debug(
+                    f"Function completed: {func_name}", extra=log_data
+                )
                 return result
 
             except Exception as e:
diff --git a/auto_generate_video_fold6/monitoring/metrics/optimized_metrics_collector.py b/auto_generate_video_fold6/monitoring/metrics/optimized_metrics_collector.py
index 249ad08..5e6c9c7 100644
--- a/auto_generate_video_fold6/monitoring/metrics/optimized_metrics_collector.py
+++ b/auto_generate_video_fold6/monitoring/metrics/optimized_metrics_collector.py
@@ -16,60 +16,70 @@ import statistics
 import asyncio
 from concurrent.futures import ThreadPoolExecutor
 
+
 class MetricType(Enum):
     """æŒ‡æ¨™é¡å‹"""
+
     COUNTER = "counter"
     GAUGE = "gauge"
     HISTOGRAM = "histogram"
     SUMMARY = "summary"
 
+
 @dataclass
 class MetricEntry:
     """é«˜æ•ˆèƒ½æŒ‡æ¨™æ¢ç›®"""
+
     name: str
     value: Union[int, float]
     labels: Dict[str, str]
     timestamp: float
     metric_type: MetricType
 
+
 class OptimizedMetricsCollector:
     """å„ªåŒ–çš„æŒ‡æ¨™æ”¶é›†å™¨"""
-    
-    def __init__(self, 
-                 buffer_size: int = 10000,
-                 flush_interval: float = 5.0,
-                 enable_sampling: bool = True,
-                 sampling_rate: float = 0.1):
+
+    def __init__(
+        self,
+        buffer_size: int = 10000,
+        flush_interval: float = 5.0,
+        enable_sampling: bool = True,
+        sampling_rate: float = 0.1,
+    ):
         self.buffer_size = buffer_size
         self.flush_interval = flush_interval
         self.enable_sampling = enable_sampling
         self.sampling_rate = sampling_rate
-        
+
         # é«˜æ•ˆèƒ½ç·©è¡å€
         self.metrics_buffer = deque(maxlen=buffer_size)
         self.buffer_lock = threading.RLock()
-        
+
         # èšåˆæŒ‡æ¨™å¿«å–
         self.aggregated_metrics = defaultdict(list)
         self.last_aggregation = time.time()
-        
+
         # æ•ˆèƒ½çµ±è¨ˆ
         self.performance_stats = {
             "metrics_processed": 0,
             "buffer_flushes": 0,
             "processing_time_ms": 0.0,
             "last_flush_time": time.time(),
-            "buffer_utilization": 0.0
+            "buffer_utilization": 0.0,
         }
-        
+
         # ç•°æ­¥è™•ç†å™¨
-        self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="metrics")
-        
+        self.executor = ThreadPoolExecutor(
+            max_workers=2, thread_name_prefix="metrics"
+        )
+
         # é–‹å§‹å¾Œå°è™•ç†
         self._start_background_processing()
-    
+
     def _start_background_processing(self):
         """é–‹å§‹å¾Œå°æŒ‡æ¨™è™•ç†"""
+
         def background_worker():
             while True:
                 try:
@@ -78,139 +88,159 @@ class OptimizedMetricsCollector:
                     self._aggregate_metrics()
                 except Exception as e:
                     print(f"Background metrics processing error: {e}")
-        
+
         worker_thread = threading.Thread(target=background_worker, daemon=True)
         worker_thread.start()
-    
-    def record_metric(self, 
-                     name: str, 
-                     value: Union[int, float], 
-                     labels: Optional[Dict[str, str]] = None,
-                     metric_type: MetricType = MetricType.GAUGE) -> bool:
+
+    def record_metric(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+        metric_type: MetricType = MetricType.GAUGE,
+    ) -> bool:
         """é«˜æ•ˆèƒ½æŒ‡æ¨™è¨˜éŒ„"""
         start_time = time.perf_counter()
-        
+
         # æ¡æ¨£å„ªåŒ–
         if self.enable_sampling and metric_type != MetricType.COUNTER:
             import random
+
             if random.random() > self.sampling_rate:
                 return False
-        
+
         # å‰µå»ºæŒ‡æ¨™æ¢ç›®
         entry = MetricEntry(
             name=name,
             value=value,
             labels=labels or {},
             timestamp=time.time(),
-            metric_type=metric_type
+            metric_type=metric_type,
         )
-        
+
         # ç·šç¨‹å®‰å…¨åœ°æ·»åŠ åˆ°ç·©è¡å€
         with self.buffer_lock:
             self.metrics_buffer.append(entry)
-            
+
             # æ›´æ–°çµ±è¨ˆ
             self.performance_stats["metrics_processed"] += 1
             processing_time = (time.perf_counter() - start_time) * 1000
             self.performance_stats["processing_time_ms"] += processing_time
-            
+
             # æª¢æŸ¥æ˜¯å¦éœ€è¦ç«‹å³åˆ·æ–°
             if len(self.metrics_buffer) >= self.buffer_size * 0.9:
                 self.executor.submit(self._flush_metrics)
-        
+
         return True
-    
+
     def _flush_metrics(self):
         """åˆ·æ–°æŒ‡æ¨™ç·©è¡å€"""
         with self.buffer_lock:
             if not self.metrics_buffer:
                 return
-            
+
             # è¤‡è£½ç·©è¡å€å…§å®¹
             metrics_to_flush = list(self.metrics_buffer)
             self.metrics_buffer.clear()
-        
+
         # æ‰¹é‡è™•ç†æŒ‡æ¨™
         self._process_metrics_batch(metrics_to_flush)
-        
+
         # æ›´æ–°çµ±è¨ˆ
         self.performance_stats["buffer_flushes"] += 1
         self.performance_stats["last_flush_time"] = time.time()
-    
+
     def _process_metrics_batch(self, metrics: List[MetricEntry]):
         """æ‰¹é‡è™•ç†æŒ‡æ¨™"""
         start_time = time.perf_counter()
-        
+
         # æŒ‰æŒ‡æ¨™åç¨±åˆ†çµ„
         grouped_metrics = defaultdict(list)
         for metric in metrics:
             grouped_metrics[metric.name].append(metric)
-        
+
         # è™•ç†æ¯çµ„æŒ‡æ¨™
         for name, metric_group in grouped_metrics.items():
             self._process_metric_group(name, metric_group)
-        
+
         processing_time = (time.perf_counter() - start_time) * 1000
         print(f"Processed {len(metrics)} metrics in {processing_time:.2f}ms")
-    
+
     def _process_metric_group(self, name: str, metrics: List[MetricEntry]):
         """è™•ç†å–®å€‹æŒ‡æ¨™çµ„"""
         if not metrics:
             return
-        
+
         first_metric = metrics[0]
-        
+
         if first_metric.metric_type == MetricType.COUNTER:
             # è¨ˆæ•¸å™¨ï¼šç´¯åŠ å€¼
             total_value = sum(m.value for m in metrics)
-            self._emit_metric(name, total_value, first_metric.labels, "counter")
-            
+            self._emit_metric(
+                name, total_value, first_metric.labels, "counter"
+            )
+
         elif first_metric.metric_type == MetricType.GAUGE:
             # æ¸¬é‡å™¨ï¼šä½¿ç”¨æœ€æ–°å€¼
             latest_metric = max(metrics, key=lambda m: m.timestamp)
-            self._emit_metric(name, latest_metric.value, latest_metric.labels, "gauge")
-            
+            self._emit_metric(
+                name, latest_metric.value, latest_metric.labels, "gauge"
+            )
+
         elif first_metric.metric_type == MetricType.HISTOGRAM:
             # ç›´æ–¹åœ–ï¼šçµ±è¨ˆåˆ†ä½ˆ
-            values = [m.value for m in metrics] 
+            values = [m.value for m in metrics]
             histogram_data = self._calculate_histogram(values)
             self._emit_histogram(name, histogram_data, first_metric.labels)
-            
+
         elif first_metric.metric_type == MetricType.SUMMARY:
             # æ‘˜è¦ï¼šè¨ˆç®—åˆ†ä½æ•¸
             values = [m.value for m in metrics]
             summary_data = self._calculate_summary(values)
             self._emit_summary(name, summary_data, first_metric.labels)
-    
+
     def _calculate_histogram(self, values: List[float]) -> Dict[str, Any]:
         """è¨ˆç®—ç›´æ–¹åœ–çµ±è¨ˆ"""
         if not values:
             return {}
-        
+
         # å®šç¾©æ¡¶é‚Šç•Œ
-        buckets = [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, float('inf')]
+        buckets = [
+            0.005,
+            0.01,
+            0.025,
+            0.05,
+            0.1,
+            0.25,
+            0.5,
+            1.0,
+            2.5,
+            5.0,
+            10.0,
+            float("inf"),
+        ]
         bucket_counts = [0] * len(buckets)
-        
+
         # è¨ˆç®—æ¯å€‹æ¡¶çš„è¨ˆæ•¸
         for value in values:
             for i, bucket in enumerate(buckets):
                 if value <= bucket:
                     bucket_counts[i] += 1
-        
+
         return {
             "count": len(values),
             "sum": sum(values),
-            "buckets": dict(zip([str(b) for b in buckets], bucket_counts))
+            "buckets": dict(zip([str(b) for b in buckets], bucket_counts)),
         }
-    
+
     def _calculate_summary(self, values: List[float]) -> Dict[str, Any]:
         """è¨ˆç®—æ‘˜è¦çµ±è¨ˆ"""
         if not values:
             return {}
-        
+
         sorted_values = sorted(values)
         count = len(sorted_values)
-        
+
         return {
             "count": count,
             "sum": sum(values),
@@ -218,175 +248,237 @@ class OptimizedMetricsCollector:
                 "0.5": sorted_values[int(count * 0.5)],
                 "0.9": sorted_values[int(count * 0.9)],
                 "0.95": sorted_values[int(count * 0.95)],
-                "0.99": sorted_values[int(count * 0.99)] if count > 1 else sorted_values[0]
-            }
+                "0.99": sorted_values[int(count * 0.99)]
+                if count > 1
+                else sorted_values[0],
+            },
         }
-    
-    def _emit_metric(self, name: str, value: Union[int, float], labels: Dict[str, str], metric_type: str):
+
+    def _emit_metric(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Dict[str, str],
+        metric_type: str,
+    ):
         """è¼¸å‡º Prometheus æ ¼å¼æŒ‡æ¨™"""
-        labels_str = ",".join([f'{k}="{v}"' for k, v in labels.items()]) if labels else ""
+        labels_str = (
+            ",".join([f'{k}="{v}"' for k, v in labels.items()])
+            if labels
+            else ""
+        )
         labels_part = f"{{{labels_str}}}" if labels_str else ""
-        
+
         # è¼¸å‡º Prometheus æ ¼å¼
         print(f"# TYPE {name} {metric_type}")
         print(f"{name}{labels_part} {value} {int(time.time() * 1000)}")
-    
-    def _emit_histogram(self, name: str, histogram_data: Dict[str, Any], labels: Dict[str, str]):
+
+    def _emit_histogram(
+        self, name: str, histogram_data: Dict[str, Any], labels: Dict[str, str]
+    ):
         """è¼¸å‡ºç›´æ–¹åœ–æŒ‡æ¨™"""
-        labels_str = ",".join([f'{k}="{v}"' for k, v in labels.items()]) if labels else ""
+        labels_str = (
+            ",".join([f'{k}="{v}"' for k, v in labels.items()])
+            if labels
+            else ""
+        )
         base_labels = f"{{{labels_str}}}" if labels_str else ""
-        
+
         print(f"# TYPE {name} histogram")
-        
+
         # è¼¸å‡ºæ¡¶è¨ˆæ•¸
         for bucket, count in histogram_data["buckets"].items():
-            bucket_labels = f"{{{labels_str},le=\"{bucket}\"}}" if labels_str else f'{{le="{bucket}"}}'
+            bucket_labels = (
+                f'{{{labels_str},le="{bucket}"}}'
+                if labels_str
+                else f'{{le="{bucket}"}}'
+            )
             print(f"{name}_bucket{bucket_labels} {count}")
-        
+
         # è¼¸å‡ºç¸½è¨ˆæ•¸å’Œç¸½å’Œ
         print(f"{name}_count{base_labels} {histogram_data['count']}")
         print(f"{name}_sum{base_labels} {histogram_data['sum']}")
-    
-    def _emit_summary(self, name: str, summary_data: Dict[str, Any], labels: Dict[str, str]):
+
+    def _emit_summary(
+        self, name: str, summary_data: Dict[str, Any], labels: Dict[str, str]
+    ):
         """è¼¸å‡ºæ‘˜è¦æŒ‡æ¨™"""
-        labels_str = ",".join([f'{k}="{v}"' for k, v in labels.items()]) if labels else ""
+        labels_str = (
+            ",".join([f'{k}="{v}"' for k, v in labels.items()])
+            if labels
+            else ""
+        )
         base_labels = f"{{{labels_str}}}" if labels_str else ""
-        
+
         print(f"# TYPE {name} summary")
-        
+
         # è¼¸å‡ºåˆ†ä½æ•¸
         for quantile, value in summary_data["quantiles"].items():
-            quantile_labels = f"{{{labels_str},quantile=\"{quantile}\"}}" if labels_str else f'{{quantile="{quantile}"}}'
+            quantile_labels = (
+                f'{{{labels_str},quantile="{quantile}"}}'
+                if labels_str
+                else f'{{quantile="{quantile}"}}'
+            )
             print(f"{name}{quantile_labels} {value}")
-        
+
         # è¼¸å‡ºç¸½è¨ˆæ•¸å’Œç¸½å’Œ
         print(f"{name}_count{base_labels} {summary_data['count']}")
         print(f"{name}_sum{base_labels} {summary_data['sum']}")
-    
+
     def _aggregate_metrics(self):
         """èšåˆæŒ‡æ¨™ä»¥æé«˜æŸ¥è©¢æ•ˆèƒ½"""
         current_time = time.time()
-        
+
         # æ¯åˆ†é˜åŸ·è¡Œä¸€æ¬¡èšåˆ
         if current_time - self.last_aggregation < 60:
             return
-        
+
         with self.buffer_lock:
             # é€™è£¡å¯ä»¥å¯¦ä½œæ›´è¤‡é›œçš„èšåˆé‚è¼¯
             # ä¾‹å¦‚ï¼šé è¨ˆç®—å¸¸ç”¨æŸ¥è©¢çš„çµæœ
             self.last_aggregation = current_time
-    
+
     def get_performance_stats(self) -> Dict[str, Any]:
         """ç²å–æ•ˆèƒ½çµ±è¨ˆ"""
         with self.buffer_lock:
             current_utilization = len(self.metrics_buffer) / self.buffer_size
             self.performance_stats["buffer_utilization"] = current_utilization
-            
+
             stats = self.performance_stats.copy()
-            
+
             # è¨ˆç®—å¹³å‡è™•ç†æ™‚é–“
             if stats["metrics_processed"] > 0:
                 stats["avg_processing_time_ms"] = (
                     stats["processing_time_ms"] / stats["metrics_processed"]
                 )
-            
+
             return stats
-    
+
     def collect_metrics(self) -> Dict[str, Any]:
         """æ”¶é›†æ‰€æœ‰æŒ‡æ¨™çµ±è¨ˆ"""
         return {
             "timestamp": datetime.utcnow().isoformat(),
             "performance": self.get_performance_stats(),
             "buffer_size": len(self.metrics_buffer),
-            "aggregated_metrics_count": len(self.aggregated_metrics)
+            "aggregated_metrics_count": len(self.aggregated_metrics),
         }
-    
+
     # ä¾¿æ·æ–¹æ³•
-    def increment_counter(self, name: str, value: Union[int, float] = 1, labels: Optional[Dict[str, str]] = None):
+    def increment_counter(
+        self,
+        name: str,
+        value: Union[int, float] = 1,
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """å¢åŠ è¨ˆæ•¸å™¨"""
         return self.record_metric(name, value, labels, MetricType.COUNTER)
-    
-    def set_gauge(self, name: str, value: Union[int, float], labels: Optional[Dict[str, str]] = None):
+
+    def set_gauge(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """è¨­å®šæ¸¬é‡å™¨å€¼"""
         return self.record_metric(name, value, labels, MetricType.GAUGE)
-    
-    def observe_histogram(self, name: str, value: Union[int, float], labels: Optional[Dict[str, str]] = None):
+
+    def observe_histogram(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """è§€å¯Ÿç›´æ–¹åœ–å€¼"""
         return self.record_metric(name, value, labels, MetricType.HISTOGRAM)
-    
-    def observe_summary(self, name: str, value: Union[int, float], labels: Optional[Dict[str, str]] = None):
+
+    def observe_summary(
+        self,
+        name: str,
+        value: Union[int, float],
+        labels: Optional[Dict[str, str]] = None,
+    ):
         """è§€å¯Ÿæ‘˜è¦å€¼"""
         return self.record_metric(name, value, labels, MetricType.SUMMARY)
 
+
 # é«˜æ•ˆèƒ½æ¥­å‹™æŒ‡æ¨™ç®¡ç†å™¨
 class BusinessMetricsManager:
     """æ¥­å‹™æŒ‡æ¨™ç®¡ç†å™¨"""
-    
+
     def __init__(self):
         self.collector = OptimizedMetricsCollector(
             buffer_size=5000,
             flush_interval=10.0,
-            sampling_rate=0.2  # æ¥­å‹™æŒ‡æ¨™æ¡æ¨£ç‡è¼ƒé«˜
+            sampling_rate=0.2,  # æ¥­å‹™æŒ‡æ¨™æ¡æ¨£ç‡è¼ƒé«˜
         )
-        
+
         # é å®šç¾©æ¥­å‹™æŒ‡æ¨™
         self.business_metrics_definitions = {
             "video_generation_count": {
                 "type": MetricType.COUNTER,
                 "description": "Total video generations",
-                "labels": ["status", "platform", "user_tier"]
+                "labels": ["status", "platform", "user_tier"],
             },
             "user_engagement_rate": {
                 "type": MetricType.GAUGE,
                 "description": "User engagement rate",
-                "labels": ["platform", "content_type"]
+                "labels": ["platform", "content_type"],
             },
             "processing_duration": {
                 "type": MetricType.HISTOGRAM,
                 "description": "Processing time distribution",
-                "labels": ["operation", "service"]
+                "labels": ["operation", "service"],
             },
             "api_response_time": {
                 "type": MetricType.SUMMARY,
                 "description": "API response time summary",
-                "labels": ["endpoint", "method", "status_code"]
-            }
+                "labels": ["endpoint", "method", "status_code"],
+            },
         }
-    
-    def record_video_generation(self, status: str, platform: str, user_tier: str = "free"):
+
+    def record_video_generation(
+        self, status: str, platform: str, user_tier: str = "free"
+    ):
         """è¨˜éŒ„å½±ç‰‡ç”ŸæˆæŒ‡æ¨™"""
         return self.collector.increment_counter(
             "video_generation_count",
             labels={
                 "status": status,
                 "platform": platform,
-                "user_tier": user_tier
-            }
+                "user_tier": user_tier,
+            },
         )
-    
-    def record_user_engagement(self, platform: str, engagement_rate: float, content_type: str = "video"):
+
+    def record_user_engagement(
+        self,
+        platform: str,
+        engagement_rate: float,
+        content_type: str = "video",
+    ):
         """è¨˜éŒ„ç”¨æˆ¶åƒèˆ‡åº¦"""
         return self.collector.set_gauge(
             "user_engagement_rate",
             engagement_rate,
-            labels={
-                "platform": platform,
-                "content_type": content_type
-            }
+            labels={"platform": platform, "content_type": content_type},
         )
-    
-    def record_processing_time(self, operation: str, service: str, duration_ms: float):
+
+    def record_processing_time(
+        self, operation: str, service: str, duration_ms: float
+    ):
         """è¨˜éŒ„è™•ç†æ™‚é–“"""
         return self.collector.observe_histogram(
             "processing_duration",
             duration_ms / 1000,  # è½‰æ›ç‚ºç§’
-            labels={
-                "operation": operation,
-                "service": service
-            }
+            labels={"operation": operation, "service": service},
         )
-    
-    def record_api_response_time(self, endpoint: str, method: str, status_code: int, response_time_ms: float):
+
+    def record_api_response_time(
+        self,
+        endpoint: str,
+        method: str,
+        status_code: int,
+        response_time_ms: float,
+    ):
         """è¨˜éŒ„ API å›æ‡‰æ™‚é–“"""
         return self.collector.observe_summary(
             "api_response_time",
@@ -394,81 +486,104 @@ class BusinessMetricsManager:
             labels={
                 "endpoint": endpoint,
                 "method": method,
-                "status_code": str(status_code)
-            }
+                "status_code": str(status_code),
+            },
         )
-    
+
     def get_business_metrics_summary(self) -> Dict[str, Any]:
         """ç²å–æ¥­å‹™æŒ‡æ¨™æ‘˜è¦"""
         return self.collector.collect_metrics()
 
+
 # å…¨åŸŸå¯¦ä¾‹
 optimized_collector = OptimizedMetricsCollector()
 business_metrics_manager = BusinessMetricsManager()
 
+
 # æ•ˆèƒ½æ¸¬é‡è£é£¾å™¨
-def measure_execution_time(metric_name: str = None, labels: Dict[str, str] = None):
+def measure_execution_time(
+    metric_name: str = None, labels: Dict[str, str] = None
+):
     """æ¸¬é‡å‡½æ•¸åŸ·è¡Œæ™‚é–“çš„è£é£¾å™¨"""
+
     def decorator(func):
         from functools import wraps
-        
+
         @wraps(func)
         async def async_wrapper(*args, **kwargs):
             name = metric_name or f"{func.__module__}.{func.__name__}"
             start_time = time.perf_counter()
-            
+
             try:
                 result = await func(*args, **kwargs)
                 duration_ms = (time.perf_counter() - start_time) * 1000
-                
+
                 optimized_collector.observe_histogram(
                     f"function_execution_time",
                     duration_ms / 1000,
-                    labels={**(labels or {}), "function": name, "status": "success"}
+                    labels={
+                        **(labels or {}),
+                        "function": name,
+                        "status": "success",
+                    },
                 )
-                
+
                 return result
-                
+
             except Exception as e:
                 duration_ms = (time.perf_counter() - start_time) * 1000
-                
+
                 optimized_collector.observe_histogram(
                     f"function_execution_time",
                     duration_ms / 1000,
-                    labels={**(labels or {}), "function": name, "status": "error", "error_type": type(e).__name__}
+                    labels={
+                        **(labels or {}),
+                        "function": name,
+                        "status": "error",
+                        "error_type": type(e).__name__,
+                    },
                 )
                 raise
-        
+
         @wraps(func)
         def sync_wrapper(*args, **kwargs):
             name = metric_name or f"{func.__module__}.{func.__name__}"
             start_time = time.perf_counter()
-            
+
             try:
                 result = func(*args, **kwargs)
                 duration_ms = (time.perf_counter() - start_time) * 1000
-                
+
                 optimized_collector.observe_histogram(
                     f"function_execution_time",
                     duration_ms / 1000,
-                    labels={**(labels or {}), "function": name, "status": "success"}
+                    labels={
+                        **(labels or {}),
+                        "function": name,
+                        "status": "success",
+                    },
                 )
-                
+
                 return result
-                
+
             except Exception as e:
                 duration_ms = (time.perf_counter() - start_time) * 1000
-                
+
                 optimized_collector.observe_histogram(
                     f"function_execution_time",
                     duration_ms / 1000,
-                    labels={**(labels or {}), "function": name, "status": "error", "error_type": type(e).__name__}
+                    labels={
+                        **(labels or {}),
+                        "function": name,
+                        "status": "error",
+                        "error_type": type(e).__name__,
+                    },
                 )
                 raise
-        
+
         if asyncio.iscoroutinefunction(func):
             return async_wrapper
         else:
             return sync_wrapper
-    
-    return decorator
\ No newline at end of file
+
+    return decorator
diff --git a/auto_generate_video_fold6/monitoring/middleware/correlation_middleware.py b/auto_generate_video_fold6/monitoring/middleware/correlation_middleware.py
index 49dc0aa..8088adc 100644
--- a/auto_generate_video_fold6/monitoring/middleware/correlation_middleware.py
+++ b/auto_generate_video_fold6/monitoring/middleware/correlation_middleware.py
@@ -14,55 +14,72 @@ from datetime import datetime
 # ä½¿ç”¨ç›¸å°å°å…¥çš„æ›¿ä»£æ–¹æ¡ˆ
 import os
 import sys
+
 sys.path.append(os.path.dirname(os.path.dirname(__file__)))
 
 try:
-    from logging.performance_logger import get_performance_logger, set_correlation_id, set_request_id
+    from logging.performance_logger import (
+        get_performance_logger,
+        set_correlation_id,
+        set_request_id,
+    )
+
     logger = get_performance_logger(__name__)
 except ImportError:
     import logging
+
     logger = logging.getLogger(__name__)
+
     # å®šç¾©æ›¿ä»£å‡½æ•¸
     def set_correlation_id(correlation_id: str):
         pass
+
     def set_request_id(request_id: str):
         pass
 
+
 # å…¨åŸŸä¸Šä¸‹æ–‡è®Šæ•¸
-correlation_id_context: ContextVar[Optional[str]] = ContextVar("correlation_id", default=None)
-trace_id_context: ContextVar[Optional[str]] = ContextVar("trace_id", default=None)
-span_id_context: ContextVar[Optional[str]] = ContextVar("span_id", default=None)
+correlation_id_context: ContextVar[Optional[str]] = ContextVar(
+    "correlation_id", default=None
+)
+trace_id_context: ContextVar[Optional[str]] = ContextVar(
+    "trace_id", default=None
+)
+span_id_context: ContextVar[Optional[str]] = ContextVar(
+    "span_id", default=None
+)
+
 
 class CorrelationMiddleware:
     """é—œè¯IDå’Œåˆ†ä½ˆå¼è¿½è¸ªä¸­é–“ä»¶"""
-    
+
     def __init__(self, app, service_name: str = "unknown"):
         self.app = app
         self.service_name = service_name
         self.logger = get_logger(f"{service_name}.correlation")
-    
+
     async def __call__(self, scope, receive, send):
         if scope["type"] != "http":
             await self.app(scope, receive, send)
             return
-        
+
         # æå–æˆ–ç”Ÿæˆé—œè¯ID
         headers = dict(scope.get("headers", []))
         correlation_id = self._extract_correlation_id(headers)
         trace_id = self._extract_trace_id(headers)
         span_id = self._generate_span_id()
         request_id = self._extract_request_id(headers)
-        
+
         # è¨­å®šä¸Šä¸‹æ–‡
         correlation_id_context.set(correlation_id)
         trace_id_context.set(trace_id)
         span_id_context.set(span_id)
-        
+
         # è¨­å®šçµæ§‹åŒ–æ—¥èªŒä¸Šä¸‹æ–‡
         set_correlation_id(correlation_id)
         if request_id:
             set_request_id(request_id)
-        
+
         # è¨˜éŒ„è«‹æ±‚é–‹å§‹
         start_time = time.time()
         self.logger.info(
@@ -75,27 +92,31 @@ class CorrelationMiddleware:
             method=scope["method"],
             path=scope["path"],
             user_agent=headers.get(b"user-agent", b"").decode(),
-            client_ip=self._get_client_ip(scope, headers)
+            client_ip=self._get_client_ip(scope, headers),
         )
-        
+
         # åŒ…è£ send ä»¥æ·»åŠ é—œè¯é ­éƒ¨
         async def send_wrapper(message):
             if message["type"] == "http.response.start":
                 # æ·»åŠ é—œè¯é ­éƒ¨åˆ°å›æ‡‰
                 response_headers = list(message.get("headers", []))
-                response_headers.extend([
-                    [b"x-correlation-id", correlation_id.encode()],
-                    [b"x-trace-id", trace_id.encode()],
-                    [b"x-span-id", span_id.encode()],
-                ])
-                
+                response_headers.extend(
+                    [
+                        [b"x-correlation-id", correlation_id.encode()],
+                        [b"x-trace-id", trace_id.encode()],
+                        [b"x-span-id", span_id.encode()],
+                    ]
+                )
+
                 if request_id:
-                    response_headers.append([b"x-request-id", request_id.encode()])
-                
+                    response_headers.append(
+                        [b"x-request-id", request_id.encode()]
+                    )
+
                 message["headers"] = response_headers
-            
+
             await send(message)
-        
+
         try:
             await self.app(scope, receive, send_wrapper)
         except Exception as e:
@@ -107,7 +128,7 @@ class CorrelationMiddleware:
                 span_id=span_id,
                 request_id=request_id,
                 exception=e,
-                duration_ms=(time.time() - start_time) * 1000
+                duration_ms=(time.time() - start_time) * 1000,
             )
             raise
         finally:
@@ -119,9 +140,9 @@ class CorrelationMiddleware:
                 trace_id=trace_id,
                 span_id=span_id,
                 request_id=request_id,
-                duration_ms=duration * 1000
+                duration_ms=duration * 1000,
             )
-    
+
     def _extract_correlation_id(self, headers: Dict[bytes, bytes]) -> str:
         """æå–æˆ–ç”Ÿæˆé—œè¯ID"""
         # æª¢æŸ¥å¤šç¨®å¯èƒ½çš„é ­éƒ¨åç¨±
@@ -130,18 +151,18 @@ class CorrelationMiddleware:
             b"x-correlation-id",
             b"correlation-id",
             b"x-request-id",
-            b"request-id"
+            b"request-id",
         ]
-        
+
         for header_name in correlation_headers:
             if header_name in headers:
                 correlation_id = headers[header_name].decode()
                 if correlation_id and len(correlation_id) > 0:
                     return correlation_id
-        
+
         # ç”Ÿæˆæ–°çš„é—œè¯ID
         return str(uuid.uuid4())
-    
+
     def _extract_trace_id(self, headers: Dict[bytes, bytes]) -> str:
         """æå–æˆ–ç”Ÿæˆè¿½è¸ªID"""
         # æª¢æŸ¥åˆ†ä½ˆå¼è¿½è¸ªé ­éƒ¨
@@ -149,9 +170,9 @@ class CorrelationMiddleware:
             b"x-trace-id",
             b"trace-id",
             b"x-b3-traceid",  # Zipkin B3
-            b"traceparent",   # W3C Trace Context
+            b"traceparent",  # W3C Trace Context
         ]
-        
+
         for header_name in trace_headers:
             if header_name in headers:
                 trace_value = headers[header_name].decode()
@@ -162,39 +183,40 @@ class CorrelationMiddleware:
                         if len(parts) >= 2:
                             return parts[1]
                     return trace_value
-        
+
         # ç”Ÿæˆæ–°çš„è¿½è¸ªID
         return str(uuid.uuid4()).replace("-", "")[:16]
-    
-    def _extract_request_id(self, headers: Dict[bytes, bytes]) -> Optional[str]:
+
+    def _extract_request_id(
+        self, headers: Dict[bytes, bytes]
+    ) -> Optional[str]:
         """æå–è«‹æ±‚ID"""
-        request_id_headers = [
-            b"x-request-id",
-            b"request-id"
-        ]
-        
+        request_id_headers = [b"x-request-id", b"request-id"]
+
         for header_name in request_id_headers:
             if header_name in headers:
                 request_id = headers[header_name].decode()
                 if request_id and len(request_id) > 0:
                     return request_id
-        
+
         return None
-    
+
     def _generate_span_id(self) -> str:
         """ç”Ÿæˆæ–°çš„ Span ID"""
         return str(uuid.uuid4()).replace("-", "")[:8]
-    
-    def _get_client_ip(self, scope: Dict[str, Any], headers: Dict[bytes, bytes]) -> str:
+
+    def _get_client_ip(
+        self, scope: Dict[str, Any], headers: Dict[bytes, bytes]
+    ) -> str:
         """ç²å–å®¢æˆ¶ç«¯IPåœ°å€"""
         # æª¢æŸ¥ä»£ç†é ­éƒ¨
         proxy_headers = [
             b"x-forwarded-for",
             b"x-real-ip",
             b"x-client-ip",
-            b"cf-connecting-ip"  # Cloudflare
+            b"cf-connecting-ip",  # Cloudflare
         ]
-        
+
         for header_name in proxy_headers:
             if header_name in headers:
                 ip_value = headers[header_name].decode()
@@ -203,32 +225,33 @@ class CorrelationMiddleware:
                     if "," in ip_value:
                         ip_value = ip_value.split(",")[0].strip()
                     return ip_value
-        
+
         # å¾ scope ç²å–å®¢æˆ¶ç«¯åœ°å€
         client = scope.get("client")
         if client:
             return client[0]
-        
+
         return "unknown"
 
+
 class DistributedTracingContext:
     """åˆ†ä½ˆå¼è¿½è¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
-    
+
     def __init__(self, operation_name: str, service_name: str = "unknown"):
         self.operation_name = operation_name
         self.service_name = service_name
         self.start_time = None
         self.span_id = None
         self.parent_span_id = None
-        
+
     def __enter__(self):
         self.start_time = time.time()
         self.parent_span_id = span_id_context.get()
         self.span_id = str(uuid.uuid4()).replace("-", "")[:8]
-        
+
         # è¨­å®šæ–°çš„ span context
         span_id_context.set(self.span_id)
-        
+
         # è¨˜éŒ„ span é–‹å§‹
         logger.info(
             f"Span started: {self.operation_name}",
@@ -237,14 +260,14 @@ class DistributedTracingContext:
             span_id=self.span_id,
             parent_span_id=self.parent_span_id,
             service=self.service_name,
-            operation=self.operation_name
+            operation=self.operation_name,
         )
-        
+
         return self
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         duration = time.time() - self.start_time
-        
+
         # è¨˜éŒ„ span çµæŸ
         if exc_type is None:
             logger.info(
@@ -256,7 +279,7 @@ class DistributedTracingContext:
                 service=self.service_name,
                 operation=self.operation_name,
                 duration_ms=duration * 1000,
-                status="success"
+                status="success",
             )
         else:
             logger.error(
@@ -269,58 +292,69 @@ class DistributedTracingContext:
                 operation=self.operation_name,
                 duration_ms=duration * 1000,
                 status="error",
-                exception=exc_val
+                exception=exc_val,
             )
-        
+
         # æ¢å¾©çˆ¶ span context
         span_id_context.set(self.parent_span_id)
 
+
 # ä¾¿æ·å‡½æ•¸
 def get_correlation_id() -> Optional[str]:
     """ç²å–ç•¶å‰é—œè¯ID"""
     return correlation_id_context.get()
 
+
 def get_trace_id() -> Optional[str]:
     """ç²å–ç•¶å‰è¿½è¸ªID"""
     return trace_id_context.get()
 
+
 def get_span_id() -> Optional[str]:
     """ç²å–ç•¶å‰ Span ID"""
     return span_id_context.get()
 
+
 def create_child_span(operation_name: str, service_name: str = "unknown"):
     """å‰µå»ºå­ Span ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
     return DistributedTracingContext(operation_name, service_name)
 
-def trace_function(operation_name: Optional[str] = None, service_name: str = "unknown"):
+
+def trace_function(
+    operation_name: Optional[str] = None, service_name: str = "unknown"
+):
     """å‡½æ•¸è¿½è¸ªè£é£¾å™¨"""
+
     def decorator(func):
         from functools import wraps
         import asyncio
-        
-        actual_operation_name = operation_name or f"{func.__module__}.{func.__name__}"
-        
+
+        actual_operation_name = (
+            operation_name or f"{func.__module__}.{func.__name__}"
+        )
+
         @wraps(func)
         async def async_wrapper(*args, **kwargs):
             with create_child_span(actual_operation_name, service_name):
                 return await func(*args, **kwargs)
-        
+
         @wraps(func)
         def sync_wrapper(*args, **kwargs):
             with create_child_span(actual_operation_name, service_name):
                 return func(*args, **kwargs)
-        
+
         if asyncio.iscoroutinefunction(func):
             return async_wrapper
         else:
             return sync_wrapper
-    
+
     return decorator
 
+
 # åˆ†ä½ˆå¼è¿½è¸ªè¼”åŠ©é¡åˆ¥
 class TraceEvent:
     """è¿½è¸ªäº‹ä»¶"""
-    
+
     def __init__(self, event_name: str, **attributes):
         self.event_name = event_name
         self.timestamp = datetime.utcnow()
@@ -328,7 +362,7 @@ class TraceEvent:
         self.correlation_id = get_correlation_id()
         self.trace_id = get_trace_id()
         self.span_id = get_span_id()
-    
+
     def log_event(self):
         """è¨˜éŒ„è¿½è¸ªäº‹ä»¶"""
         logger.info(
@@ -338,11 +372,12 @@ class TraceEvent:
             span_id=self.span_id,
             event_name=self.event_name,
             timestamp=self.timestamp.isoformat(),
-            **self.attributes
+            **self.attributes,
         )
 
+
 def log_trace_event(event_name: str, **attributes):
     """è¨˜éŒ„è¿½è¸ªäº‹ä»¶çš„ä¾¿æ·å‡½æ•¸"""
     event = TraceEvent(event_name, **attributes)
     event.log_event()
-    return event
\ No newline at end of file
+    return event
diff --git a/auto_generate_video_fold6/monitoring/middleware/health_check_middleware.py b/auto_generate_video_fold6/monitoring/middleware/health_check_middleware.py
index 767ebce..fb1e588 100644
--- a/auto_generate_video_fold6/monitoring/middleware/health_check_middleware.py
+++ b/auto_generate_video_fold6/monitoring/middleware/health_check_middleware.py
@@ -16,12 +16,14 @@ from enum import Enum
 # Database connectivity checks
 try:
     import psycopg2
+
     POSTGRES_AVAILABLE = True
 except ImportError:
     POSTGRES_AVAILABLE = False
 
 try:
     import redis
+
     REDIS_AVAILABLE = True
 except ImportError:
     REDIS_AVAILABLE = False
@@ -30,35 +32,41 @@ from ..logging.structured_logger import get_logger
 
 logger = get_logger(__name__)
 
+
 class HealthStatus(Enum):
     """å¥åº·ç‹€æ…‹æšèˆ‰"""
+
     HEALTHY = "healthy"
     DEGRADED = "degraded"
     UNHEALTHY = "unhealthy"
 
+
 @dataclass
 class HealthCheckResult:
     """å¥åº·æª¢æŸ¥çµæœ"""
+
     name: str
     status: HealthStatus
     timestamp: datetime
     duration_ms: float
     message: str = ""
     details: Dict[str, Any] = None
-    
+
     def __post_init__(self):
         if self.details is None:
             self.details = {}
 
+
 @dataclass
 class SystemHealth:
     """ç³»çµ±æ•´é«”å¥åº·ç‹€æ…‹"""
+
     status: HealthStatus
     timestamp: datetime
     checks: List[HealthCheckResult]
     uptime_seconds: float
     version: str = "1.0.0"
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
         return {
@@ -72,27 +80,27 @@ class SystemHealth:
                     "timestamp": check.timestamp.isoformat(),
                     "duration_ms": check.duration_ms,
                     "message": check.message,
-                    "details": check.details
+                    "details": check.details,
                 }
                 for check in self.checks
-            }
+            },
         }
 
+
 class HealthChecker:
     """å¥åº·æª¢æŸ¥åŸ·è¡Œå™¨"""
-    
+
     def __init__(self):
         self.start_time = time.time()
         self.checks: Dict[str, Callable] = {}
         self.last_results: Dict[str, HealthCheckResult] = {}
-        
-    def register_check(self, name: str, check_func: Callable, timeout: float = 5.0):
+
+    def register_check(
+        self, name: str, check_func: Callable, timeout: float = 5.0
+    ):
         """è¨»å†Šå¥åº·æª¢æŸ¥å‡½æ•¸"""
-        self.checks[name] = {
-            'func': check_func,
-            'timeout': timeout
-        }
-        
+        self.checks[name] = {"func": check_func, "timeout": timeout}
+
     async def run_check(self, name: str) -> HealthCheckResult:
         """åŸ·è¡Œå–®å€‹å¥åº·æª¢æŸ¥"""
         if name not in self.checks:
@@ -101,31 +109,32 @@ class HealthChecker:
                 status=HealthStatus.UNHEALTHY,
                 timestamp=datetime.utcnow(),
                 duration_ms=0,
-                message=f"Check '{name}' not registered"
+                message=f"Check '{name}' not registered",
             )
-        
+
         check_config = self.checks[name]
         start_time = time.time()
-        
+
         try:
             # åŸ·è¡Œå¥åº·æª¢æŸ¥ï¼Œå¸¶è¶…æ™‚æ§åˆ¶
             result = await asyncio.wait_for(
-                check_config['func'](),
-                timeout=check_config['timeout']
+                check_config["func"](), timeout=check_config["timeout"]
             )
-            
+
             duration_ms = (time.time() - start_time) * 1000
-            
+
             if isinstance(result, HealthCheckResult):
                 result.duration_ms = duration_ms
                 return result
             elif isinstance(result, bool):
                 return HealthCheckResult(
                     name=name,
-                    status=HealthStatus.HEALTHY if result else HealthStatus.UNHEALTHY,
+                    status=HealthStatus.HEALTHY
+                    if result
+                    else HealthStatus.UNHEALTHY,
                     timestamp=datetime.utcnow(),
                     duration_ms=duration_ms,
-                    message="OK" if result else "Check failed"
+                    message="OK" if result else "Check failed",
                 )
             else:  # Assume dict or other data
                 return HealthCheckResult(
@@ -134,9 +143,11 @@ class HealthChecker:
                     timestamp=datetime.utcnow(),
                     duration_ms=duration_ms,
                     message="OK",
-                    details=result if isinstance(result, dict) else {"result": str(result)}
+                    details=result
+                    if isinstance(result, dict)
+                    else {"result": str(result)},
                 )
-                
+
         except asyncio.TimeoutError:
             duration_ms = (time.time() - start_time) * 1000
             return HealthCheckResult(
@@ -144,9 +155,9 @@ class HealthChecker:
                 status=HealthStatus.UNHEALTHY,
                 timestamp=datetime.utcnow(),
                 duration_ms=duration_ms,
-                message=f"Check timeout after {check_config['timeout']}s"
+                message=f"Check timeout after {check_config['timeout']}s",
             )
-            
+
         except Exception as e:
             duration_ms = (time.time() - start_time) * 1000
             return HealthCheckResult(
@@ -154,21 +165,21 @@ class HealthChecker:
                 status=HealthStatus.UNHEALTHY,
                 timestamp=datetime.utcnow(),
                 duration_ms=duration_ms,
-                message=f"Check failed: {str(e)}"
+                message=f"Check failed: {str(e)}",
             )
-    
+
     async def run_all_checks(self) -> SystemHealth:
         """åŸ·è¡Œæ‰€æœ‰å¥åº·æª¢æŸ¥"""
         results = []
-        
+
         # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰æª¢æŸ¥
-        check_tasks = [
-            self.run_check(name) for name in self.checks.keys()
-        ]
-        
+        check_tasks = [self.run_check(name) for name in self.checks.keys()]
+
         if check_tasks:
-            results = await asyncio.gather(*check_tasks, return_exceptions=True)
-            
+            results = await asyncio.gather(
+                *check_tasks, return_exceptions=True
+            )
+
             # è™•ç†ç•°å¸¸çµæœ
             processed_results = []
             for i, result in enumerate(results):
@@ -180,37 +191,43 @@ class HealthChecker:
                             status=HealthStatus.UNHEALTHY,
                             timestamp=datetime.utcnow(),
                             duration_ms=0,
-                            message=f"Exception: {str(result)}"
+                            message=f"Exception: {str(result)}",
                         )
                     )
                 else:
                     processed_results.append(result)
-            
+
             results = processed_results
-        
+
         # å„²å­˜çµæœä¾›å¾ŒçºŒä½¿ç”¨
         for result in results:
             self.last_results[result.name] = result
-        
+
         # è¨ˆç®—æ•´é«”å¥åº·ç‹€æ…‹
         overall_status = self._calculate_overall_status(results)
         uptime = time.time() - self.start_time
-        
+
         return SystemHealth(
             status=overall_status,
             timestamp=datetime.utcnow(),
             checks=results,
-            uptime_seconds=uptime
+            uptime_seconds=uptime,
         )
-    
-    def _calculate_overall_status(self, results: List[HealthCheckResult]) -> HealthStatus:
+
+    def _calculate_overall_status(
+        self, results: List[HealthCheckResult]
+    ) -> HealthStatus:
         """è¨ˆç®—æ•´é«”å¥åº·ç‹€æ…‹"""
         if not results:
             return HealthStatus.HEALTHY
-        
-        unhealthy_count = sum(1 for r in results if r.status == HealthStatus.UNHEALTHY)
-        degraded_count = sum(1 for r in results if r.status == HealthStatus.DEGRADED)
-        
+
+        unhealthy_count = sum(
+            1 for r in results if r.status == HealthStatus.UNHEALTHY
+        )
+        degraded_count = sum(
+            1 for r in results if r.status == HealthStatus.DEGRADED
+        )
+
         if unhealthy_count > 0:
             # å¦‚æœæœ‰è¶…éä¸€åŠçš„æª¢æŸ¥å¤±æ•—ï¼Œç³»çµ±ä¸å¥åº·
             if unhealthy_count > len(results) // 2:
@@ -222,13 +239,14 @@ class HealthChecker:
         else:
             return HealthStatus.HEALTHY
 
+
 # é å®šç¾©å¥åº·æª¢æŸ¥å‡½æ•¸
 async def check_database_connectivity(
-    host: str = "localhost", 
-    port: int = 5432, 
+    host: str = "localhost",
+    port: int = 5432,
     database: str = "postgres",
-    user: str = "postgres", 
-    password: str = "password"
+    user: str = "postgres",
+    password: str = "password",
 ) -> HealthCheckResult:
     """æª¢æŸ¥ PostgreSQL è³‡æ–™åº«é€£æ¥"""
     if not POSTGRES_AVAILABLE:
@@ -237,9 +255,9 @@ async def check_database_connectivity(
             status=HealthStatus.UNHEALTHY,
             timestamp=datetime.utcnow(),
             duration_ms=0,
-            message="psycopg2 not available"
+            message="psycopg2 not available",
         )
-    
+
     try:
         start_time = time.time()
         conn = psycopg2.connect(
@@ -248,25 +266,25 @@ async def check_database_connectivity(
             database=database,
             user=user,
             password=password,
-            connect_timeout=3
+            connect_timeout=3,
         )
-        
+
         with conn.cursor() as cur:
             cur.execute("SELECT 1")
             result = cur.fetchone()
-        
+
         conn.close()
         duration_ms = (time.time() - start_time) * 1000
-        
+
         return HealthCheckResult(
             name="database",
             status=HealthStatus.HEALTHY,
             timestamp=datetime.utcnow(),
             duration_ms=duration_ms,
             message="Database connection successful",
-            details={"query_result": result[0] if result else None}
+            details={"query_result": result[0] if result else None},
         )
-        
+
     except Exception as e:
         duration_ms = (time.time() - start_time) * 1000
         return HealthCheckResult(
@@ -274,14 +292,15 @@ async def check_database_connectivity(
             status=HealthStatus.UNHEALTHY,
             timestamp=datetime.utcnow(),
             duration_ms=duration_ms,
-            message=f"Database connection failed: {str(e)}"
+            message=f"Database connection failed: {str(e)}",
         )
 
+
 async def check_redis_connectivity(
-    host: str = "localhost", 
-    port: int = 6379, 
+    host: str = "localhost",
+    port: int = 6379,
     db: int = 0,
-    password: str = None
+    password: str = None,
 ) -> HealthCheckResult:
     """æª¢æŸ¥ Redis é€£æ¥"""
     if not REDIS_AVAILABLE:
@@ -290,9 +309,9 @@ async def check_redis_connectivity(
             status=HealthStatus.UNHEALTHY,
             timestamp=datetime.utcnow(),
             duration_ms=0,
-            message="redis library not available"
+            message="redis library not available",
         )
-    
+
     try:
         start_time = time.time()
         r = redis.Redis(
@@ -301,15 +320,15 @@ async def check_redis_connectivity(
             db=db,
             password=password,
             socket_timeout=3,
-            socket_connect_timeout=3
+            socket_connect_timeout=3,
         )
-        
+
         # æ¸¬è©¦é€£æ¥
         ping_result = r.ping()
-        info = r.info('memory')
-        
+        info = r.info("memory")
+
         duration_ms = (time.time() - start_time) * 1000
-        
+
         return HealthCheckResult(
             name="redis",
             status=HealthStatus.HEALTHY,
@@ -318,11 +337,11 @@ async def check_redis_connectivity(
             message="Redis connection successful",
             details={
                 "ping": ping_result,
-                "used_memory": info.get('used_memory', 0),
-                "max_memory": info.get('maxmemory', 0)
-            }
+                "used_memory": info.get("used_memory", 0),
+                "max_memory": info.get("maxmemory", 0),
+            },
         )
-        
+
     except Exception as e:
         duration_ms = (time.time() - start_time) * 1000
         return HealthCheckResult(
@@ -330,20 +349,23 @@ async def check_redis_connectivity(
             status=HealthStatus.UNHEALTHY,
             timestamp=datetime.utcnow(),
             duration_ms=duration_ms,
-            message=f"Redis connection failed: {str(e)}"
+            message=f"Redis connection failed: {str(e)}",
         )
 
-async def check_disk_space(threshold_percent: float = 90.0) -> HealthCheckResult:
+
+async def check_disk_space(
+    threshold_percent: float = 90.0,
+) -> HealthCheckResult:
     """æª¢æŸ¥ç£ç¢Ÿç©ºé–“"""
     import shutil
-    
+
     try:
         start_time = time.time()
         total, used, free = shutil.disk_usage("/")
-        
+
         used_percent = (used / total) * 100
         duration_ms = (time.time() - start_time) * 1000
-        
+
         if used_percent > threshold_percent:
             status = HealthStatus.UNHEALTHY
             message = f"Disk usage critical: {used_percent:.1f}%"
@@ -353,7 +375,7 @@ async def check_disk_space(threshold_percent: float = 90.0) -> HealthCheckResult
         else:
             status = HealthStatus.HEALTHY
             message = f"Disk usage normal: {used_percent:.1f}%"
-        
+
         return HealthCheckResult(
             name="disk_space",
             status=status,
@@ -364,30 +386,33 @@ async def check_disk_space(threshold_percent: float = 90.0) -> HealthCheckResult
                 "total_bytes": total,
                 "used_bytes": used,
                 "free_bytes": free,
-                "used_percent": used_percent
-            }
+                "used_percent": used_percent,
+            },
         )
-        
+
     except Exception as e:
         return HealthCheckResult(
             name="disk_space",
             status=HealthStatus.UNHEALTHY,
             timestamp=datetime.utcnow(),
             duration_ms=0,
-            message=f"Disk space check failed: {str(e)}"
+            message=f"Disk space check failed: {str(e)}",
         )
 
-async def check_memory_usage(threshold_percent: float = 90.0) -> HealthCheckResult:
+
+async def check_memory_usage(
+    threshold_percent: float = 90.0,
+) -> HealthCheckResult:
     """æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨é‡"""
     import psutil
-    
+
     try:
         start_time = time.time()
         memory = psutil.virtual_memory()
-        
+
         used_percent = memory.percent
         duration_ms = (time.time() - start_time) * 1000
-        
+
         if used_percent > threshold_percent:
             status = HealthStatus.UNHEALTHY
             message = f"Memory usage critical: {used_percent:.1f}%"
@@ -397,7 +422,7 @@ async def check_memory_usage(threshold_percent: float = 90.0) -> HealthCheckResu
         else:
             status = HealthStatus.HEALTHY
             message = f"Memory usage normal: {used_percent:.1f}%"
-        
+
         return HealthCheckResult(
             name="memory_usage",
             status=status,
@@ -408,17 +433,17 @@ async def check_memory_usage(threshold_percent: float = 90.0) -> HealthCheckResu
                 "total_bytes": memory.total,
                 "available_bytes": memory.available,
                 "used_bytes": memory.used,
-                "used_percent": used_percent
-            }
+                "used_percent": used_percent,
+            },
         )
-        
+
     except ImportError:
         return HealthCheckResult(
             name="memory_usage",
             status=HealthStatus.UNHEALTHY,
             timestamp=datetime.utcnow(),
             duration_ms=0,
-            message="psutil library not available"
+            message="psutil library not available",
         )
     except Exception as e:
         return HealthCheckResult(
@@ -426,67 +451,78 @@ async def check_memory_usage(threshold_percent: float = 90.0) -> HealthCheckResu
             status=HealthStatus.UNHEALTHY,
             timestamp=datetime.utcnow(),
             duration_ms=0,
-            message=f"Memory check failed: {str(e)}"
+            message=f"Memory check failed: {str(e)}",
         )
 
+
 # å…¨åŸŸå¥åº·æª¢æŸ¥å™¨å¯¦ä¾‹
 health_checker = HealthChecker()
 
+
 class HealthCheckMiddleware:
     """å¥åº·æª¢æŸ¥ä¸­é–“ä»¶"""
-    
+
     def __init__(self, app, health_endpoint: str = "/health"):
         self.app = app
         self.health_endpoint = health_endpoint
         self.detailed_endpoint = f"{health_endpoint}/detailed"
         self.logger = get_logger("health_check")
-        
+
         # è¨»å†Šé è¨­å¥åº·æª¢æŸ¥
         self._register_default_checks()
-    
+
     def _register_default_checks(self):
         """è¨»å†Šé è¨­å¥åº·æª¢æŸ¥"""
         # ç£ç¢Ÿç©ºé–“æª¢æŸ¥
         health_checker.register_check("disk_space", check_disk_space)
-        
+
         # è¨˜æ†¶é«”ä½¿ç”¨æª¢æŸ¥ (å¦‚æœ psutil å¯ç”¨)
         try:
             import psutil
+
             health_checker.register_check("memory_usage", check_memory_usage)
         except ImportError:
             pass
-        
+
         # è³‡æ–™åº«æª¢æŸ¥ (å¦‚æœé…ç½®å¯ç”¨)
         if POSTGRES_AVAILABLE:
-            health_checker.register_check("database", check_database_connectivity)
-        
-        # Redis æª¢æŸ¥ (å¦‚æœé…ç½®å¯ç”¨)  
+            health_checker.register_check(
+                "database", check_database_connectivity
+            )
+
+        # Redis æª¢æŸ¥ (å¦‚æœé…ç½®å¯ç”¨)
         if REDIS_AVAILABLE:
             health_checker.register_check("redis", check_redis_connectivity)
-    
+
     async def __call__(self, scope, receive, send):
         if scope["type"] != "http":
             await self.app(scope, receive, send)
             return
-        
+
         path = scope["path"]
-        
+
         # å¥åº·æª¢æŸ¥ç«¯é»
         if path == self.health_endpoint:
-            await self._handle_health_check(scope, receive, send, detailed=False)
+            await self._handle_health_check(
+                scope, receive, send, detailed=False
+            )
             return
         elif path == self.detailed_endpoint:
-            await self._handle_health_check(scope, receive, send, detailed=True)
+            await self._handle_health_check(
+                scope, receive, send, detailed=True
+            )
             return
-        
+
         # æ­£å¸¸è«‹æ±‚è™•ç†
         await self.app(scope, receive, send)
-    
-    async def _handle_health_check(self, scope, receive, send, detailed: bool = False):
+
+    async def _handle_health_check(
+        self, scope, receive, send, detailed: bool = False
+    ):
         """è™•ç†å¥åº·æª¢æŸ¥è«‹æ±‚"""
         try:
             system_health = await health_checker.run_all_checks()
-            
+
             if detailed:
                 response_data = system_health.to_dict()
             else:
@@ -494,9 +530,9 @@ class HealthCheckMiddleware:
                 response_data = {
                     "status": system_health.status.value,
                     "timestamp": system_health.timestamp.isoformat(),
-                    "uptime_seconds": system_health.uptime_seconds
+                    "uptime_seconds": system_health.uptime_seconds,
                 }
-            
+
             # æ ¹æ“šå¥åº·ç‹€æ…‹è¨­å®š HTTP ç‹€æ…‹ç¢¼
             if system_health.status == HealthStatus.HEALTHY:
                 status_code = 200
@@ -504,66 +540,75 @@ class HealthCheckMiddleware:
                 status_code = 200  # ä»å¯æœå‹™ä½†æœ‰è­¦å‘Š
             else:
                 status_code = 503  # æœå‹™ä¸å¯ç”¨
-            
+
             response_body = json.dumps(response_data, indent=2).encode()
-            
-            await send({
-                "type": "http.response.start",
-                "status": status_code,
-                "headers": [
-                    [b"content-type", b"application/json"],
-                    [b"content-length", str(len(response_body)).encode()],
-                ],
-            })
-            
-            await send({
-                "type": "http.response.body",
-                "body": response_body,
-            })
-            
+
+            await send(
+                {
+                    "type": "http.response.start",
+                    "status": status_code,
+                    "headers": [
+                        [b"content-type", b"application/json"],
+                        [b"content-length", str(len(response_body)).encode()],
+                    ],
+                }
+            )
+
+            await send(
+                {
+                    "type": "http.response.body",
+                    "body": response_body,
+                }
+            )
+
             # è¨˜éŒ„å¥åº·æª¢æŸ¥
             self.logger.info(
                 f"Health check completed: {system_health.status.value}",
                 status=system_health.status.value,
                 checks_count=len(system_health.checks),
                 uptime_seconds=system_health.uptime_seconds,
-                detailed=detailed
+                detailed=detailed,
             )
-            
+
         except Exception as e:
             # å¥åº·æª¢æŸ¥æœ¬èº«å¤±æ•—
             error_response = {
                 "status": "unhealthy",
                 "timestamp": datetime.utcnow().isoformat(),
-                "error": str(e)
+                "error": str(e),
             }
-            
+
             response_body = json.dumps(error_response).encode()
-            
-            await send({
-                "type": "http.response.start",
-                "status": 500,
-                "headers": [
-                    [b"content-type", b"application/json"],
-                    [b"content-length", str(len(response_body)).encode()],
-                ],
-            })
-            
-            await send({
-                "type": "http.response.body",
-                "body": response_body,
-            })
-            
-            self.logger.error(
-                "Health check middleware failed",
-                exception=e
+
+            await send(
+                {
+                    "type": "http.response.start",
+                    "status": 500,
+                    "headers": [
+                        [b"content-type", b"application/json"],
+                        [b"content-length", str(len(response_body)).encode()],
+                    ],
+                }
             )
 
+            await send(
+                {
+                    "type": "http.response.body",
+                    "body": response_body,
+                }
+            )
+
+            self.logger.error("Health check middleware failed", exception=e)
+
+
 # ä¾¿æ·å‡½æ•¸
-def register_health_check(name: str, check_func: Callable, timeout: float = 5.0):
+def register_health_check(
+    name: str, check_func: Callable, timeout: float = 5.0
+):
     """è¨»å†Šå¥åº·æª¢æŸ¥çš„ä¾¿æ·å‡½æ•¸"""
     health_checker.register_check(name, check_func, timeout)
 
+
 async def get_system_health() -> SystemHealth:
     """ç²å–ç³»çµ±å¥åº·ç‹€æ…‹çš„ä¾¿æ·å‡½æ•¸"""
-    return await health_checker.run_all_checks()
\ No newline at end of file
+    return await health_checker.run_all_checks()
diff --git a/auto_generate_video_fold6/monitoring/middleware/performance_middleware.py b/auto_generate_video_fold6/monitoring/middleware/performance_middleware.py
index 74215f9..728a505 100644
--- a/auto_generate_video_fold6/monitoring/middleware/performance_middleware.py
+++ b/auto_generate_video_fold6/monitoring/middleware/performance_middleware.py
@@ -16,222 +16,319 @@ import json
 
 # Prometheus metrics (optional dependency)
 try:
-    from prometheus_client import Counter, Histogram, Gauge, Summary, start_http_server
+    from prometheus_client import (
+        Counter,
+        Histogram,
+        Gauge,
+        Summary,
+        start_http_server,
+    )
+
     PROMETHEUS_AVAILABLE = True
 except ImportError:
     PROMETHEUS_AVAILABLE = False
 
-from ..logging.structured_logger import get_logger, set_correlation_id, set_request_id
+from ..logging.structured_logger import (
+    get_logger,
+    set_correlation_id,
+    set_request_id,
+)
 
 logger = get_logger(__name__)
 
+
 class PerformanceMetrics:
     """æ•ˆèƒ½æŒ‡æ¨™æ”¶é›†å™¨"""
-    
+
     def __init__(self):
         # Request metrics
-        self.request_count = Counter(
-            'http_requests_total',
-            'Total HTTP requests',
-            ['method', 'endpoint', 'status', 'service']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.request_duration = Histogram(
-            'http_request_duration_seconds',
-            'HTTP request duration in seconds',
-            ['method', 'endpoint', 'service']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.request_size = Summary(
-            'http_request_size_bytes',
-            'HTTP request size in bytes',
-            ['method', 'endpoint', 'service']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.response_size = Summary(
-            'http_response_size_bytes',
-            'HTTP response size in bytes',
-            ['method', 'endpoint', 'service']
-        ) if PROMETHEUS_AVAILABLE else None
-        
+        self.request_count = (
+            Counter(
+                "http_requests_total",
+                "Total HTTP requests",
+                ["method", "endpoint", "status", "service"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.request_duration = (
+            Histogram(
+                "http_request_duration_seconds",
+                "HTTP request duration in seconds",
+                ["method", "endpoint", "service"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.request_size = (
+            Summary(
+                "http_request_size_bytes",
+                "HTTP request size in bytes",
+                ["method", "endpoint", "service"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.response_size = (
+            Summary(
+                "http_response_size_bytes",
+                "HTTP response size in bytes",
+                ["method", "endpoint", "service"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
         # Business metrics
-        self.video_generation_duration = Histogram(
-            'video_generation_duration_seconds',
-            'Video generation duration in seconds',
-            ['status', 'video_type']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.video_generation_total = Counter(
-            'video_generation_total',
-            'Total video generations',
-            ['status', 'video_type', 'platform']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.trend_analysis_duration = Histogram(
-            'trend_analysis_duration_seconds',
-            'Trend analysis duration in seconds',
-            ['source', 'accuracy']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.trend_analysis_total = Counter(
-            'trend_analysis_total',
-            'Total trend analyses',
-            ['source', 'status', 'accuracy']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.social_publish_total = Counter(
-            'social_publish_total',
-            'Total social media publishes',
-            ['platform', 'status', 'content_type']
-        ) if PROMETHEUS_AVAILABLE else None
-        
+        self.video_generation_duration = (
+            Histogram(
+                "video_generation_duration_seconds",
+                "Video generation duration in seconds",
+                ["status", "video_type"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.video_generation_total = (
+            Counter(
+                "video_generation_total",
+                "Total video generations",
+                ["status", "video_type", "platform"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.trend_analysis_duration = (
+            Histogram(
+                "trend_analysis_duration_seconds",
+                "Trend analysis duration in seconds",
+                ["source", "accuracy"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.trend_analysis_total = (
+            Counter(
+                "trend_analysis_total",
+                "Total trend analyses",
+                ["source", "status", "accuracy"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.social_publish_total = (
+            Counter(
+                "social_publish_total",
+                "Total social media publishes",
+                ["platform", "status", "content_type"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
         # System metrics
-        self.active_connections = Gauge(
-            'active_connections',
-            'Number of active connections',
-            ['service']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.queue_size = Gauge(
-            'queue_size',
-            'Current queue size',
-            ['queue_name', 'service']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.cache_hits = Counter(
-            'cache_hits_total',
-            'Total cache hits',
-            ['cache_type', 'service']
-        ) if PROMETHEUS_AVAILABLE else None
-        
-        self.cache_misses = Counter(
-            'cache_misses_total',
-            'Total cache misses',
-            ['cache_type', 'service']
-        ) if PROMETHEUS_AVAILABLE else None
-        
+        self.active_connections = (
+            Gauge(
+                "active_connections",
+                "Number of active connections",
+                ["service"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.queue_size = (
+            Gauge(
+                "queue_size", "Current queue size", ["queue_name", "service"]
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.cache_hits = (
+            Counter(
+                "cache_hits_total",
+                "Total cache hits",
+                ["cache_type", "service"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
+        self.cache_misses = (
+            Counter(
+                "cache_misses_total",
+                "Total cache misses",
+                ["cache_type", "service"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
         # Application errors
-        self.application_errors = Counter(
-            'application_errors_total',
-            'Total application errors',
-            ['service', 'error_type', 'endpoint']
-        ) if PROMETHEUS_AVAILABLE else None
-        
+        self.application_errors = (
+            Counter(
+                "application_errors_total",
+                "Total application errors",
+                ["service", "error_type", "endpoint"],
+            )
+            if PROMETHEUS_AVAILABLE
+            else None
+        )
+
         # In-memory metrics for fallback
         self._fallback_metrics = defaultdict(list)
         self._metrics_lock = threading.Lock()
-        
+
         # Performance tracking
         self._request_history = deque(maxlen=1000)
         self._performance_stats = {
-            'avg_response_time': 0.0,
-            'request_rate': 0.0,
-            'error_rate': 0.0,
-            'last_updated': datetime.utcnow()
+            "avg_response_time": 0.0,
+            "request_rate": 0.0,
+            "error_rate": 0.0,
+            "last_updated": datetime.utcnow(),
         }
 
-    def record_request(self, method: str, endpoint: str, status_code: int, 
-                      duration: float, service: str, request_size: int = 0, 
-                      response_size: int = 0):
+    def record_request(
+        self,
+        method: str,
+        endpoint: str,
+        status_code: int,
+        duration: float,
+        service: str,
+        request_size: int = 0,
+        response_size: int = 0,
+    ):
         """è¨˜éŒ„ HTTP è«‹æ±‚æŒ‡æ¨™"""
         status = str(status_code)
-        
+
         if PROMETHEUS_AVAILABLE and self.request_count:
             self.request_count.labels(
-                method=method, endpoint=endpoint, status=status, service=service
+                method=method,
+                endpoint=endpoint,
+                status=status,
+                service=service,
             ).inc()
-            
+
             self.request_duration.labels(
                 method=method, endpoint=endpoint, service=service
             ).observe(duration)
-            
+
             if request_size > 0:
                 self.request_size.labels(
                     method=method, endpoint=endpoint, service=service
                 ).observe(request_size)
-            
+
             if response_size > 0:
                 self.response_size.labels(
                     method=method, endpoint=endpoint, service=service
                 ).observe(response_size)
-        
+
         # Fallback metrics
         with self._metrics_lock:
-            self._fallback_metrics['requests'].append({
-                'timestamp': datetime.utcnow().isoformat(),
-                'method': method,
-                'endpoint': endpoint,
-                'status': status,
-                'duration': duration,
-                'service': service
-            })
-            
+            self._fallback_metrics["requests"].append(
+                {
+                    "timestamp": datetime.utcnow().isoformat(),
+                    "method": method,
+                    "endpoint": endpoint,
+                    "status": status,
+                    "duration": duration,
+                    "service": service,
+                }
+            )
+
             # Update request history for performance stats
-            self._request_history.append({
-                'timestamp': time.time(),
-                'duration': duration,
-                'status_code': status_code
-            })
-            
+            self._request_history.append(
+                {
+                    "timestamp": time.time(),
+                    "duration": duration,
+                    "status_code": status_code,
+                }
+            )
+
             self._update_performance_stats()
 
-    def record_video_generation(self, duration: float, status: str, 
-                               video_type: str, platform: str = 'unknown'):
+    def record_video_generation(
+        self,
+        duration: float,
+        status: str,
+        video_type: str,
+        platform: str = "unknown",
+    ):
         """è¨˜éŒ„å½±ç‰‡ç”ŸæˆæŒ‡æ¨™"""
         if PROMETHEUS_AVAILABLE and self.video_generation_duration:
             self.video_generation_duration.labels(
                 status=status, video_type=video_type
             ).observe(duration)
-            
+
             self.video_generation_total.labels(
                 status=status, video_type=video_type, platform=platform
             ).inc()
-        
+
         with self._metrics_lock:
-            self._fallback_metrics['video_generation'].append({
-                'timestamp': datetime.utcnow().isoformat(),
-                'duration': duration,
-                'status': status,
-                'video_type': video_type,
-                'platform': platform
-            })
-
-    def record_trend_analysis(self, duration: float, source: str, 
-                             status: str, accuracy: str = 'unknown'):
+            self._fallback_metrics["video_generation"].append(
+                {
+                    "timestamp": datetime.utcnow().isoformat(),
+                    "duration": duration,
+                    "status": status,
+                    "video_type": video_type,
+                    "platform": platform,
+                }
+            )
+
+    def record_trend_analysis(
+        self,
+        duration: float,
+        source: str,
+        status: str,
+        accuracy: str = "unknown",
+    ):
         """è¨˜éŒ„è¶¨å‹¢åˆ†ææŒ‡æ¨™"""
         if PROMETHEUS_AVAILABLE and self.trend_analysis_duration:
             self.trend_analysis_duration.labels(
                 source=source, accuracy=accuracy
             ).observe(duration)
-            
+
             self.trend_analysis_total.labels(
                 source=source, status=status, accuracy=accuracy
             ).inc()
-        
+
         with self._metrics_lock:
-            self._fallback_metrics['trend_analysis'].append({
-                'timestamp': datetime.utcnow().isoformat(),
-                'duration': duration,
-                'source': source,
-                'status': status,
-                'accuracy': accuracy
-            })
-
-    def record_social_publish(self, platform: str, status: str, 
-                             content_type: str = 'video'):
+            self._fallback_metrics["trend_analysis"].append(
+                {
+                    "timestamp": datetime.utcnow().isoformat(),
+                    "duration": duration,
+                    "source": source,
+                    "status": status,
+                    "accuracy": accuracy,
+                }
+            )
+
+    def record_social_publish(
+        self, platform: str, status: str, content_type: str = "video"
+    ):
         """è¨˜éŒ„ç¤¾äº¤åª’é«”ç™¼å¸ƒæŒ‡æ¨™"""
         if PROMETHEUS_AVAILABLE and self.social_publish_total:
             self.social_publish_total.labels(
                 platform=platform, status=status, content_type=content_type
             ).inc()
-        
+
         with self._metrics_lock:
-            self._fallback_metrics['social_publish'].append({
-                'timestamp': datetime.utcnow().isoformat(),
-                'platform': platform,
-                'status': status,
-                'content_type': content_type
-            })
+            self._fallback_metrics["social_publish"].append(
+                {
+                    "timestamp": datetime.utcnow().isoformat(),
+                    "platform": platform,
+                    "status": status,
+                    "content_type": content_type,
+                }
+            )
 
     def record_error(self, service: str, error_type: str, endpoint: str):
         """è¨˜éŒ„æ‡‰ç”¨ç¨‹å¼éŒ¯èª¤"""
@@ -239,27 +336,35 @@ class PerformanceMetrics:
             self.application_errors.labels(
                 service=service, error_type=error_type, endpoint=endpoint
             ).inc()
-        
+
         with self._metrics_lock:
-            self._fallback_metrics['errors'].append({
-                'timestamp': datetime.utcnow().isoformat(),
-                'service': service,
-                'error_type': error_type,
-                'endpoint': endpoint
-            })
+            self._fallback_metrics["errors"].append(
+                {
+                    "timestamp": datetime.utcnow().isoformat(),
+                    "service": service,
+                    "error_type": error_type,
+                    "endpoint": endpoint,
+                }
+            )
 
     def record_cache_event(self, cache_type: str, service: str, hit: bool):
         """è¨˜éŒ„å¿«å–äº‹ä»¶"""
         if PROMETHEUS_AVAILABLE:
             if hit and self.cache_hits:
-                self.cache_hits.labels(cache_type=cache_type, service=service).inc()
+                self.cache_hits.labels(
+                    cache_type=cache_type, service=service
+                ).inc()
             elif not hit and self.cache_misses:
-                self.cache_misses.labels(cache_type=cache_type, service=service).inc()
+                self.cache_misses.labels(
+                    cache_type=cache_type, service=service
+                ).inc()
 
     def update_queue_size(self, queue_name: str, service: str, size: int):
         """æ›´æ–°éšŠåˆ—å¤§å°"""
         if PROMETHEUS_AVAILABLE and self.queue_size:
-            self.queue_size.labels(queue_name=queue_name, service=service).set(size)
+            self.queue_size.labels(queue_name=queue_name, service=service).set(
+                size
+            )
 
     def update_active_connections(self, service: str, count: int):
         """æ›´æ–°æ´»èºé€£æ¥æ•¸"""
@@ -270,24 +375,34 @@ class PerformanceMetrics:
         """æ›´æ–°æ•ˆèƒ½çµ±è¨ˆ"""
         if not self._request_history:
             return
-        
+
         now = time.time()
-        recent_requests = [r for r in self._request_history if now - r['timestamp'] < 300]  # Last 5 minutes
-        
+        recent_requests = [
+            r for r in self._request_history if now - r["timestamp"] < 300
+        ]  # Last 5 minutes
+
         if recent_requests:
             # Average response time
-            avg_duration = sum(r['duration'] for r in recent_requests) / len(recent_requests)
-            self._performance_stats['avg_response_time'] = avg_duration
-            
+            avg_duration = sum(r["duration"] for r in recent_requests) / len(
+                recent_requests
+            )
+            self._performance_stats["avg_response_time"] = avg_duration
+
             # Request rate (requests per second)
-            time_span = max(now - recent_requests[0]['timestamp'], 1)
-            self._performance_stats['request_rate'] = len(recent_requests) / time_span
-            
+            time_span = max(now - recent_requests[0]["timestamp"], 1)
+            self._performance_stats["request_rate"] = (
+                len(recent_requests) / time_span
+            )
+
             # Error rate
-            error_count = sum(1 for r in recent_requests if r['status_code'] >= 400)
-            self._performance_stats['error_rate'] = error_count / len(recent_requests) * 100
-            
-            self._performance_stats['last_updated'] = datetime.utcnow()
+            error_count = sum(
+                1 for r in recent_requests if r["status_code"] >= 400
+            )
+            self._performance_stats["error_rate"] = (
+                error_count / len(recent_requests) * 100
+            )
+
+            self._performance_stats["last_updated"] = datetime.utcnow()
 
     def get_performance_stats(self) -> Dict[str, Any]:
         """ç²å–æ•ˆèƒ½çµ±è¨ˆ"""
@@ -299,104 +414,110 @@ class PerformanceMetrics:
         with self._metrics_lock:
             # Return recent metrics (last hour)
             cutoff_time = datetime.utcnow() - timedelta(hours=1)
-            
+
             filtered_metrics = {}
             for metric_type, metrics in self._fallback_metrics.items():
                 filtered_metrics[metric_type] = [
-                    m for m in metrics 
-                    if datetime.fromisoformat(m['timestamp']) > cutoff_time
+                    m
+                    for m in metrics
+                    if datetime.fromisoformat(m["timestamp"]) > cutoff_time
                 ]
-            
+
             return filtered_metrics
 
+
 # Global metrics instance
 metrics = PerformanceMetrics()
 
+
 class PerformanceMiddleware:
     """æ•ˆèƒ½ç›£æ§ä¸­é–“ä»¶"""
-    
+
     def __init__(self, app, service_name: str = "unknown"):
         self.app = app
         self.service_name = service_name
         self.logger = get_logger(f"{service_name}.performance")
-    
+
     async def __call__(self, scope, receive, send):
         if scope["type"] != "http":
             await self.app(scope, receive, send)
             return
-        
+
         # Extract request information
         method = scope["method"]
         path = scope["path"]
-        
+
         # Generate correlation ID for request tracking
         import uuid
+
         correlation_id = str(uuid.uuid4())
         set_correlation_id(correlation_id)
-        
+
         # Extract request ID from headers if present
         headers = dict(scope.get("headers", []))
         request_id = headers.get(b"x-request-id", b"").decode()
         if request_id:
             set_request_id(request_id)
-        
+
         start_time = time.time()
         request_size = 0
         response_size = 0
         status_code = 500  # Default to error
-        
+
         # Calculate request size
         if "content-length" in headers:
             try:
                 request_size = int(headers[b"content-length"].decode())
             except (ValueError, AttributeError):
                 pass
-        
+
         # Wrap send to capture response size and status
         async def send_wrapper(message):
             nonlocal response_size, status_code
-            
+
             if message["type"] == "http.response.start":
                 status_code = message["status"]
                 response_headers = dict(message.get("headers", []))
                 if b"content-length" in response_headers:
                     try:
-                        response_size = int(response_headers[b"content-length"].decode())
+                        response_size = int(
+                            response_headers[b"content-length"].decode()
+                        )
                     except (ValueError, AttributeError):
                         pass
-            
+
             elif message["type"] == "http.response.body":
                 if "body" in message:
                     response_size += len(message["body"])
-            
+
             await send(message)
-        
+
         try:
             # Process request
             await self.app(scope, receive, send_wrapper)
-            
+
         except Exception as e:
             # Record error
             metrics.record_error(
                 service=self.service_name,
                 error_type=type(e).__name__,
-                endpoint=path
+                endpoint=path,
             )
-            
+
             self.logger.error(
                 f"Request failed: {method} {path}",
                 exception=e,
                 correlation_id=correlation_id,
                 request_id=request_id,
-                duration_ms=(time.time() - start_time) * 1000
+                duration_ms=(time.time() - start_time) * 1000,
             )
-            
+
             raise
-        
+
         finally:
             # Record metrics
             duration = time.time() - start_time
-            
+
             metrics.record_request(
                 method=method,
                 endpoint=path,
@@ -404,9 +525,9 @@ class PerformanceMiddleware:
                 duration=duration,
                 service=self.service_name,
                 request_size=request_size,
-                response_size=response_size
+                response_size=response_size,
             )
-            
+
             # Log request completion
             log_level = "warning" if status_code >= 400 else "info"
             getattr(self.logger, log_level)(
@@ -418,105 +539,121 @@ class PerformanceMiddleware:
                 request_size=request_size,
                 response_size=response_size,
                 correlation_id=correlation_id,
-                request_id=request_id
+                request_id=request_id,
             )
 
+
 @asynccontextmanager
 async def performance_context(operation_name: str, **labels):
     """æ•ˆèƒ½ç›£æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
     start_time = time.time()
     operation_logger = get_logger(f"performance.{operation_name}")
-    
+
     operation_logger.info(f"Operation started: {operation_name}", **labels)
-    
+
     try:
         yield
         duration = time.time() - start_time
         operation_logger.info(
             f"Operation completed: {operation_name}",
             duration_ms=duration * 1000,
-            **labels
+            **labels,
         )
-        
+
     except Exception as e:
         duration = time.time() - start_time
         operation_logger.error(
             f"Operation failed: {operation_name}",
             exception=e,
             duration_ms=duration * 1000,
-            **labels
+            **labels,
         )
         raise
 
+
 def performance_tracker(operation_type: str = "unknown"):
     """æ•ˆèƒ½è¿½è¹¤è£é£¾å™¨"""
+
     def decorator(func):
         from functools import wraps
         import asyncio
-        
+
         @wraps(func)
         async def async_wrapper(*args, **kwargs):
             async with performance_context(
                 operation_name=func.__name__,
                 operation_type=operation_type,
-                function=f"{func.__module__}.{func.__name__}"
+                function=f"{func.__module__}.{func.__name__}",
             ):
                 return await func(*args, **kwargs)
-        
+
         @wraps(func)
         def sync_wrapper(*args, **kwargs):
             import time
+
             start_time = time.time()
-            
+
             try:
                 result = func(*args, **kwargs)
                 duration = time.time() - start_time
-                
+
                 logger.info(
                     f"Function completed: {func.__name__}",
                     operation_type=operation_type,
                     function=f"{func.__module__}.{func.__name__}",
-                    duration_ms=duration * 1000
+                    duration_ms=duration * 1000,
                 )
-                
+
                 return result
-                
+
             except Exception as e:
                 duration = time.time() - start_time
-                
+
                 logger.error(
                     f"Function failed: {func.__name__}",
                     exception=e,
                     operation_type=operation_type,
                     function=f"{func.__module__}.{func.__name__}",
-                    duration_ms=duration * 1000
+                    duration_ms=duration * 1000,
                 )
                 raise
-        
+
         if asyncio.iscoroutinefunction(func):
             return async_wrapper
         else:
             return sync_wrapper
-    
+
     return decorator
 
+
 # Utility functions for recording specific business metrics
-def record_video_generation(duration: float, status: str, video_type: str, platform: str = 'unknown'):
+def record_video_generation(
+    duration: float, status: str, video_type: str, platform: str = "unknown"
+):
     """è¨˜éŒ„å½±ç‰‡ç”ŸæˆæŒ‡æ¨™çš„ä¾¿æ·å‡½æ•¸"""
-    return metrics.record_video_generation(duration, status, video_type, platform)
+    return metrics.record_video_generation(
+        duration, status, video_type, platform
+    )
 
-def record_trend_analysis(duration: float, source: str, status: str, accuracy: str = 'unknown'):
+
+def record_trend_analysis(
+    duration: float, source: str, status: str, accuracy: str = "unknown"
+):
     """è¨˜éŒ„è¶¨å‹¢åˆ†ææŒ‡æ¨™çš„ä¾¿æ·å‡½æ•¸"""
     return metrics.record_trend_analysis(duration, source, status, accuracy)
 
-def record_social_publish(platform: str, status: str, content_type: str = 'video'):
+
+def record_social_publish(
+    platform: str, status: str, content_type: str = "video"
+):
     """è¨˜éŒ„ç¤¾äº¤åª’é«”ç™¼å¸ƒæŒ‡æ¨™çš„ä¾¿æ·å‡½æ•¸"""
     return metrics.record_social_publish(platform, status, content_type)
 
+
 def start_metrics_server(port: int = 8000):
     """å•Ÿå‹• Prometheus æŒ‡æ¨™ä¼ºæœå™¨"""
     if PROMETHEUS_AVAILABLE:
         start_http_server(port)
         logger.info(f"Prometheus metrics server started on port {port}")
     else:
-        logger.warning("Prometheus not available, metrics server not started")
\ No newline at end of file
+        logger.warning("Prometheus not available, metrics server not started")
diff --git a/auto_generate_video_fold6/monitoring/middleware/prometheus_middleware.py b/auto_generate_video_fold6/monitoring/middleware/prometheus_middleware.py
index 808345e..bedbcdf 100644
--- a/auto_generate_video_fold6/monitoring/middleware/prometheus_middleware.py
+++ b/auto_generate_video_fold6/monitoring/middleware/prometheus_middleware.py
@@ -5,7 +5,13 @@ Prometheus æŒ‡æ¨™æ”¶é›†ä¸­ä»‹è»Ÿé«”
 
 import time
 from typing import Callable
-from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
+from prometheus_client import (
+    Counter,
+    Histogram,
+    Gauge,
+    generate_latest,
+    CONTENT_TYPE_LATEST,
+)
 from fastapi import FastAPI, Request, Response
 from fastapi.responses import PlainTextResponse
 import psutil
@@ -49,7 +55,9 @@ class PrometheusMiddleware:
         )
 
         self.active_requests = Gauge(
-            "http_requests_active", "Number of active HTTP requests", ["service"]
+            "http_requests_active",
+            "Number of active HTTP requests",
+            ["service"],
         )
 
         # æ¥­å‹™æŒ‡æ¨™
@@ -67,7 +75,9 @@ class PrometheusMiddleware:
         )
 
         self.ai_request_count = Counter(
-            "ai_requests_total", "Total AI service requests", ["request_type", "status", "service"]
+            "ai_requests_total",
+            "Total AI service requests",
+            ["request_type", "status", "service"],
         )
 
         self.ai_request_duration = Histogram(
@@ -78,30 +88,52 @@ class PrometheusMiddleware:
         )
 
         # ç³»çµ±æŒ‡æ¨™
-        self.memory_usage = Gauge("memory_usage_bytes", "Memory usage in bytes", ["service"])
+        self.memory_usage = Gauge(
+            "memory_usage_bytes", "Memory usage in bytes", ["service"]
+        )
 
-        self.cpu_usage = Gauge("cpu_usage_percent", "CPU usage percentage", ["service"])
+        self.cpu_usage = Gauge(
+            "cpu_usage_percent", "CPU usage percentage", ["service"]
+        )
 
         # è³‡æ–™åº«æŒ‡æ¨™
         self.db_connections = Gauge(
-            "database_connections_active", "Number of active database connections", ["service"]
+            "database_connections_active",
+            "Number of active database connections",
+            ["service"],
         )
 
         self.db_query_duration = Histogram(
             "database_query_duration_seconds",
             "Database query duration in seconds",
             ["operation", "service"],
-            buckets=(0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, float("inf")),
+            buckets=(
+                0.001,
+                0.005,
+                0.01,
+                0.05,
+                0.1,
+                0.25,
+                0.5,
+                1.0,
+                2.5,
+                5.0,
+                float("inf"),
+            ),
         )
 
         # å¿«å–æŒ‡æ¨™
         self.cache_operations = Counter(
-            "cache_operations_total", "Total cache operations", ["operation", "result", "service"]
+            "cache_operations_total",
+            "Total cache operations",
+            ["operation", "result", "service"],
         )
 
         # éŒ¯èª¤æŒ‡æ¨™
         self.error_count = Counter(
-            "application_errors_total", "Total application errors", ["error_type", "service"]
+            "application_errors_total",
+            "Total application errors",
+            ["error_type", "service"],
         )
 
         # å•Ÿå‹•èƒŒæ™¯ä»»å‹™æ”¶é›†ç³»çµ±æŒ‡æ¨™
@@ -113,7 +145,9 @@ class PrometheusMiddleware:
             try:
                 # è¨˜æ†¶é«”ä½¿ç”¨é‡
                 memory_info = psutil.virtual_memory()
-                self.memory_usage.labels(service=self.app_name).set(memory_info.used)
+                self.memory_usage.labels(service=self.app_name).set(
+                    memory_info.used
+                )
 
                 # CPU ä½¿ç”¨ç‡
                 cpu_percent = psutil.cpu_percent(interval=1)
@@ -124,7 +158,9 @@ class PrometheusMiddleware:
                 print(f"Error collecting system metrics: {e}")
                 await asyncio.sleep(60)
 
-    async def __call__(self, request: Request, call_next: Callable) -> Response:
+    async def __call__(
+        self, request: Request, call_next: Callable
+    ) -> Response:
         """ä¸­ä»‹è»Ÿé«”ä¸»è¦é‚è¼¯"""
         # è¨˜éŒ„è«‹æ±‚é–‹å§‹æ™‚é–“
         start_time = time.time()
@@ -146,7 +182,9 @@ class PrometheusMiddleware:
 
         except Exception as e:
             # è¨˜éŒ„éŒ¯èª¤
-            self.error_count.labels(error_type=type(e).__name__, service=self.app_name).inc()
+            self.error_count.labels(
+                error_type=type(e).__name__, service=self.app_name
+            ).inc()
 
             status_code = 500
             raise
@@ -160,7 +198,10 @@ class PrometheusMiddleware:
 
             # è¨˜éŒ„æŒ‡æ¨™
             self.request_count.labels(
-                method=method, endpoint=endpoint, status_code=status_code, service=self.app_name
+                method=method,
+                endpoint=endpoint,
+                status_code=status_code,
+                service=self.app_name,
             ).inc()
 
             self.request_duration.labels(
@@ -179,7 +220,9 @@ class PrometheusMiddleware:
 
         # UUID æ¨¡å¼
         path = re.sub(
-            r"/[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}", "/{uuid}", path
+            r"/[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}",
+            "/{uuid}",
+            path,
         )
 
         # æ•¸å­— ID æ¨¡å¼
@@ -198,30 +241,36 @@ class PrometheusMiddleware:
 
         return path
 
-    def record_video_generation(self, status: str, platform: str, duration: float = None):
+    def record_video_generation(
+        self, status: str, platform: str, duration: float = None
+    ):
         """è¨˜éŒ„å½±ç‰‡ç”ŸæˆæŒ‡æ¨™"""
         self.video_generation_count.labels(
             status=status, platform=platform, service=self.app_name
         ).inc()
 
         if duration is not None:
-            self.video_generation_duration.labels(platform=platform, service=self.app_name).observe(
-                duration
-            )
+            self.video_generation_duration.labels(
+                platform=platform, service=self.app_name
+            ).observe(duration)
 
-    def record_ai_request(self, request_type: str, status: str, duration: float):
+    def record_ai_request(
+        self, request_type: str, status: str, duration: float
+    ):
         """è¨˜éŒ„ AI è«‹æ±‚æŒ‡æ¨™"""
         self.ai_request_count.labels(
             request_type=request_type, status=status, service=self.app_name
         ).inc()
 
-        self.ai_request_duration.labels(request_type=request_type, service=self.app_name).observe(
-            duration
-        )
+        self.ai_request_duration.labels(
+            request_type=request_type, service=self.app_name
+        ).observe(duration)
 
     def record_db_query(self, operation: str, duration: float):
         """è¨˜éŒ„è³‡æ–™åº«æŸ¥è©¢æŒ‡æ¨™"""
-        self.db_query_duration.labels(operation=operation, service=self.app_name).observe(duration)
+        self.db_query_duration.labels(
+            operation=operation, service=self.app_name
+        ).observe(duration)
 
     def record_cache_operation(self, operation: str, result: str):
         """è¨˜éŒ„å¿«å–æ“ä½œæŒ‡æ¨™"""
@@ -231,7 +280,9 @@ class PrometheusMiddleware:
 
     def record_error(self, error_type: str):
         """è¨˜éŒ„éŒ¯èª¤æŒ‡æ¨™"""
-        self.error_count.labels(error_type=error_type, service=self.app_name).inc()
+        self.error_count.labels(
+            error_type=error_type, service=self.app_name
+        ).inc()
 
 
 def setup_metrics_endpoint(app: FastAPI, middleware: PrometheusMiddleware):
@@ -245,7 +296,11 @@ def setup_metrics_endpoint(app: FastAPI, middleware: PrometheusMiddleware):
     @app.get("/health")
     async def health_check():
         """å¥åº·æª¢æŸ¥ç«¯é»"""
-        return {"status": "healthy", "service": middleware.app_name, "timestamp": time.time()}
+        return {
+            "status": "healthy",
+            "service": middleware.app_name,
+            "timestamp": time.time(),
+        }
 
 
 # ä½¿ç”¨ç¯„ä¾‹
diff --git a/auto_generate_video_fold6/monitoring/report_generator.py b/auto_generate_video_fold6/monitoring/report_generator.py
index 5ceb403..f02ba6e 100755
--- a/auto_generate_video_fold6/monitoring/report_generator.py
+++ b/auto_generate_video_fold6/monitoring/report_generator.py
@@ -66,8 +66,7 @@ class ReportGenerator:
         template_dir = Path(__file__).parent / "templates"
         template_dir.mkdir(exist_ok=True)
         self.jinja_env = Environment(
-            loader=FileSystemLoader(str(template_dir)),
-            autoescape=True
+            loader=FileSystemLoader(str(template_dir)), autoescape=True
         )
 
         # ç¢ºä¿æ¨¡æ¿æ–‡ä»¶å­˜åœ¨
@@ -265,7 +264,9 @@ class ReportGenerator:
 
         return await self._generate_report(config)
 
-    async def generate_monthly_report(self, year: int = None, month: int = None) -> str:
+    async def generate_monthly_report(
+        self, year: int = None, month: int = None
+    ) -> str:
         """ç”Ÿæˆæœˆå ±å‘Š"""
         if year is None or month is None:
             today = date.today()
@@ -311,7 +312,9 @@ class ReportGenerator:
         else:
             raise ValueError(f"ä¸æ”¯æ´çš„è¼¸å‡ºæ ¼å¼: {config.output_format}")
 
-    async def _collect_report_data(self, config: ReportConfig) -> Dict[str, Any]:
+    async def _collect_report_data(
+        self, config: ReportConfig
+    ) -> Dict[str, Any]:
         """æ”¶é›†å ±å‘Šæ•¸æ“š"""
         data = {
             "period": {
@@ -327,7 +330,9 @@ class ReportGenerator:
 
         return data
 
-    async def _get_generation_metrics(self, config: ReportConfig) -> GenerationMetrics:
+    async def _get_generation_metrics(
+        self, config: ReportConfig
+    ) -> GenerationMetrics:
         """ç²å–ç”ŸæˆæŒ‡æ¨™"""
         # æ¨¡æ“¬æ•¸æ“š - åœ¨å¯¦éš›å¯¦ä½œä¸­æ‡‰è©²å¾è³‡æ–™åº«æŸ¥è©¢
         total_videos = 45
@@ -340,7 +345,11 @@ class ReportGenerator:
             failed_generations=failed,
             total_cost=67.50,
             average_cost_per_video=1.50,
-            platforms_distribution={"tiktok": 20, "instagram": 15, "youtube": 10},
+            platforms_distribution={
+                "tiktok": 20,
+                "instagram": 15,
+                "youtube": 10,
+            },
             quality_distribution={"high": 15, "medium": 25, "low": 5},
             generation_time_stats={"avg": 180, "min": 120, "max": 300},
         )
@@ -352,7 +361,9 @@ class ReportGenerator:
 
         try:
             if config.report_type == "daily":
-                summary = await self.cost_tracker.get_daily_summary(config.start_date)
+                summary = await self.cost_tracker.get_daily_summary(
+                    config.start_date
+                )
                 return {
                     "total_cost": summary.total_cost,
                     "api_calls_count": summary.api_calls_count,
@@ -361,7 +372,10 @@ class ReportGenerator:
                     "budget_status": {
                         "limit": summary.budget_limit,
                         "remaining": summary.budget_remaining,
-                        "usage_percent": (summary.total_cost / summary.budget_limit) * 100,
+                        "usage_percent": (
+                            summary.total_cost / summary.budget_limit
+                        )
+                        * 100,
                     },
                 }
             else:
@@ -382,16 +396,26 @@ class ReportGenerator:
         return {
             "total_cost": 67.50,
             "api_calls_count": 450,
-            "providers_breakdown": {"openai": 35.20, "stability_ai": 22.80, "elevenlabs": 9.50},
+            "providers_breakdown": {
+                "openai": 35.20,
+                "stability_ai": 22.80,
+                "elevenlabs": 9.50,
+            },
             "operations_breakdown": {
                 "text_generation": 35.20,
                 "image_generation": 22.80,
                 "voice_synthesis": 9.50,
             },
-            "budget_status": {"limit": 100.0, "remaining": 32.50, "usage_percent": 67.5},
+            "budget_status": {
+                "limit": 100.0,
+                "remaining": 32.50,
+                "usage_percent": 67.5,
+            },
         }
 
-    async def _get_performance_metrics(self, config: ReportConfig) -> Dict[str, Any]:
+    async def _get_performance_metrics(
+        self, config: ReportConfig
+    ) -> Dict[str, Any]:
         """ç²å–æ•ˆèƒ½æŒ‡æ¨™"""
         # æ¨¡æ“¬æ•ˆèƒ½æ•¸æ“š
         return {
@@ -399,10 +423,16 @@ class ReportGenerator:
             "success_rate": 93.3,  # ç™¾åˆ†æ¯”
             "error_rate": 6.7,
             "throughput": 15,  # å½±ç‰‡/å°æ™‚
-            "resource_usage": {"cpu_avg": 65, "memory_avg": 78, "disk_usage": 45},
+            "resource_usage": {
+                "cpu_avg": 65,
+                "memory_avg": 78,
+                "disk_usage": 45,
+            },
         }
 
-    async def _get_daily_breakdown(self, config: ReportConfig) -> List[Dict[str, Any]]:
+    async def _get_daily_breakdown(
+        self, config: ReportConfig
+    ) -> List[Dict[str, Any]]:
         """ç²å–æ¯æ—¥æ˜ç´°"""
         breakdown = []
         current_date = config.start_date
@@ -416,7 +446,9 @@ class ReportGenerator:
                     "success_rate": 95.0,
                     "total_cost": 22.50,
                     "avg_cost": 1.50,
-                    "success_rate_class": "status-success" if 95.0 >= 90 else "status-warning",
+                    "success_rate_class": "status-success"
+                    if 95.0 >= 90
+                    else "status-warning",
                 }
             )
             current_date += timedelta(days=1)
@@ -477,7 +509,12 @@ class ReportGenerator:
             return {"title": "æˆæœ¬åˆ†å¸ƒ", "path": "", "error": "ç„¡æ•¸æ“š"}
 
         plt.figure(figsize=(10, 6))
-        plt.pie(providers.values(), labels=providers.keys(), autopct="%1.1f%%", startangle=90)
+        plt.pie(
+            providers.values(),
+            labels=providers.keys(),
+            autopct="%1.1f%%",
+            startangle=90,
+        )
         plt.title("API ä¾›æ‡‰å•†æˆæœ¬åˆ†å¸ƒ")
 
         chart_path = output_dir / "cost_distribution.png"
@@ -511,7 +548,10 @@ class ReportGenerator:
         plt.savefig(chart_path, dpi=300, bbox_inches="tight")
         plt.close()
 
-        return {"title": "æ¯æ—¥æˆæœ¬è¶¨å‹¢", "path": str(chart_path.relative_to(self.reports_dir))}
+        return {
+            "title": "æ¯æ—¥æˆæœ¬è¶¨å‹¢",
+            "path": str(chart_path.relative_to(self.reports_dir)),
+        }
 
     async def _create_generation_stats_chart(
         self, metrics: GenerationMetrics, output_dir: Path
@@ -522,14 +562,23 @@ class ReportGenerator:
         colors = ["#28a745", "#dc3545"]
 
         plt.figure(figsize=(8, 6))
-        plt.pie(sizes, labels=labels, colors=colors, autopct="%1.1f%%", startangle=90)
+        plt.pie(
+            sizes,
+            labels=labels,
+            colors=colors,
+            autopct="%1.1f%%",
+            startangle=90,
+        )
         plt.title("å½±ç‰‡ç”ŸæˆæˆåŠŸç‡")
 
         chart_path = output_dir / "generation_success_rate.png"
         plt.savefig(chart_path, dpi=300, bbox_inches="tight")
         plt.close()
 
-        return {"title": "å½±ç‰‡ç”ŸæˆæˆåŠŸç‡", "path": str(chart_path.relative_to(self.reports_dir))}
+        return {
+            "title": "å½±ç‰‡ç”ŸæˆæˆåŠŸç‡",
+            "path": str(chart_path.relative_to(self.reports_dir)),
+        }
 
     async def _create_platform_distribution_chart(
         self, platforms: Dict[str, int], output_dir: Path
@@ -539,7 +588,11 @@ class ReportGenerator:
             return {"title": "å¹³å°åˆ†å¸ƒ", "path": "", "error": "ç„¡æ•¸æ“š"}
 
         plt.figure(figsize=(10, 6))
-        plt.bar(platforms.keys(), platforms.values(), color=["#1f77b4", "#ff7f0e", "#2ca02c"])
+        plt.bar(
+            platforms.keys(),
+            platforms.values(),
+            color=["#1f77b4", "#ff7f0e", "#2ca02c"],
+        )
         plt.title("å„å¹³å°å½±ç‰‡æ•¸é‡åˆ†å¸ƒ")
         plt.xlabel("å¹³å°")
         plt.ylabel("å½±ç‰‡æ•¸é‡")
@@ -557,7 +610,10 @@ class ReportGenerator:
         }
 
     async def _prepare_template_data(
-        self, data: Dict[str, Any], charts: List[Dict[str, str]], config: ReportConfig
+        self,
+        data: Dict[str, Any],
+        charts: List[Dict[str, str]],
+        config: ReportConfig,
     ) -> Dict[str, Any]:
         """æº–å‚™æ¨¡æ¿æ•¸æ“š"""
         metrics = data["generation_metrics"]
@@ -571,11 +627,12 @@ class ReportGenerator:
                 "status_class": "status-success",
             },
             {
-                "value": f"{(metrics.successful_generations/metrics.total_videos)*100:.1f}%",
+                "value": f"{(metrics.successful_generations / metrics.total_videos) * 100:.1f}%",
                 "label": "æˆåŠŸç‡",
                 "status_class": (
                     "status-success"
-                    if metrics.successful_generations / metrics.total_videos > 0.9
+                    if metrics.successful_generations / metrics.total_videos
+                    > 0.9
                     else "status-warning"
                 ),
             },
@@ -600,7 +657,9 @@ class ReportGenerator:
                     {
                         "category": provider,
                         "amount": amount,
-                        "percentage": (amount / total_cost) * 100 if total_cost > 0 else 0,
+                        "percentage": (amount / total_cost) * 100
+                        if total_cost > 0
+                        else 0,
                         "trend": "â†—ï¸",
                         "trend_class": "status-success",
                     }
@@ -612,7 +671,11 @@ class ReportGenerator:
             "generation_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
             "key_metrics": key_metrics,
             "cost_analysis": config.include_cost_analysis,
-            "cost_charts": [chart for chart in charts if "cost" in chart.get("title", "").lower()],
+            "cost_charts": [
+                chart
+                for chart in charts
+                if "cost" in chart.get("title", "").lower()
+            ],
             "generation_charts": [
                 chart
                 for chart in charts
@@ -623,15 +686,15 @@ class ReportGenerator:
             "detailed_data": data["daily_breakdown"],
         }
 
-    async def _render_html_report(self, template_data: Dict[str, Any], config: ReportConfig) -> str:
+    async def _render_html_report(
+        self, template_data: Dict[str, Any], config: ReportConfig
+    ) -> str:
         """æ¸²æŸ“ HTML å ±å‘Š"""
         template = self.jinja_env.get_template("report.html")
         html_content = template.render(**template_data)
 
         # ä¿å­˜å ±å‘Š
-        report_filename = (
-            f"{config.report_type}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
-        )
+        report_filename = f"{config.report_type}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
         report_path = self.reports_dir / report_filename
 
         with open(report_path, "w", encoding="utf-8") as f:
@@ -640,16 +703,18 @@ class ReportGenerator:
         logger.info(f"HTML å ±å‘Šå·²ç”Ÿæˆ: {report_path}")
         return str(report_path)
 
-    async def _render_json_report(self, template_data: Dict[str, Any], config: ReportConfig) -> str:
+    async def _render_json_report(
+        self, template_data: Dict[str, Any], config: ReportConfig
+    ) -> str:
         """æ¸²æŸ“ JSON å ±å‘Š"""
         # ä¿å­˜å ±å‘Š
-        report_filename = (
-            f"{config.report_type}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
-        )
+        report_filename = f"{config.report_type}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
         report_path = self.reports_dir / report_filename
 
         with open(report_path, "w", encoding="utf-8") as f:
-            json.dump(template_data, f, indent=2, ensure_ascii=False, default=str)
+            json.dump(
+                template_data, f, indent=2, ensure_ascii=False, default=str
+            )
 
         logger.info(f"JSON å ±å‘Šå·²ç”Ÿæˆ: {report_path}")
         return str(report_path)
diff --git a/auto_generate_video_fold6/monitoring/tracing/opentelemetry_middleware.py b/auto_generate_video_fold6/monitoring/tracing/opentelemetry_middleware.py
index caad384..970969a 100644
--- a/auto_generate_video_fold6/monitoring/tracing/opentelemetry_middleware.py
+++ b/auto_generate_video_fold6/monitoring/tracing/opentelemetry_middleware.py
@@ -9,7 +9,9 @@ from typing import Callable, Dict, Any
 from fastapi import FastAPI, Request, Response
 from opentelemetry import trace, baggage
 from opentelemetry.exporter.jaeger.thrift import JaegerExporter
-from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
+from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import (
+    OTLPSpanExporter,
+)
 from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
 from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
 from opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor
@@ -47,7 +49,9 @@ class OpenTelemetryMiddleware:
                 ResourceAttributes.SERVICE_NAME: service_name,
                 ResourceAttributes.SERVICE_VERSION: service_version,
                 ResourceAttributes.DEPLOYMENT_ENVIRONMENT: environment,
-                ResourceAttributes.SERVICE_INSTANCE_ID: os.getenv("HOSTNAME", "unknown"),
+                ResourceAttributes.SERVICE_INSTANCE_ID: os.getenv(
+                    "HOSTNAME", "unknown"
+                ),
             }
         )
 
@@ -71,7 +75,9 @@ class OpenTelemetryMiddleware:
                 agent_host_name=jaeger_endpoint.split("://")[1].split(":")[0],
                 agent_port=int(jaeger_endpoint.split(":")[-1]),
             )
-            tracer_provider.add_span_processor(BatchSpanProcessor(jaeger_exporter))
+            tracer_provider.add_span_processor(
+                BatchSpanProcessor(jaeger_exporter)
+            )
 
         # OTLP å°å‡ºå™¨ (å„ªå…ˆä½¿ç”¨)
         otlp_endpoint = otlp_endpoint or os.getenv(
@@ -79,8 +85,12 @@ class OpenTelemetryMiddleware:
         )
         if otlp_endpoint:
             try:
-                otlp_exporter = OTLPSpanExporter(endpoint=otlp_endpoint, insecure=True)
-                tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
+                otlp_exporter = OTLPSpanExporter(
+                    endpoint=otlp_endpoint, insecure=True
+                )
+                tracer_provider.add_span_processor(
+                    BatchSpanProcessor(otlp_exporter)
+                )
                 logger.info(f"OTLP exporter configured: {otlp_endpoint}")
             except Exception as e:
                 logger.warning(f"Failed to setup OTLP exporter: {e}")
@@ -115,7 +125,9 @@ class OpenTelemetryMiddleware:
         # æ·»åŠ è‡ªè¨‚ä¸­ä»‹è»Ÿé«”
         app.middleware("http")(self._custom_middleware)
 
-    async def _custom_middleware(self, request: Request, call_next: Callable) -> Response:
+    async def _custom_middleware(
+        self, request: Request, call_next: Callable
+    ) -> Response:
         """è‡ªè¨‚è¿½è¹¤ä¸­ä»‹è»Ÿé«”"""
         # æå–ä¸Šæ¸¸è¿½è¹¤ä¸Šä¸‹æ–‡
         headers = dict(request.headers)
@@ -123,7 +135,9 @@ class OpenTelemetryMiddleware:
 
         # å‰µå»ºæ–°çš„ span
         with self.tracer.start_as_current_span(
-            f"{request.method} {request.url.path}", context=context, kind=trace.SpanKind.SERVER
+            f"{request.method} {request.url.path}",
+            context=context,
+            kind=trace.SpanKind.SERVER,
         ) as span:
             # è¨­å®šåŸºæœ¬å±¬æ€§
             span.set_attributes(
@@ -133,7 +147,9 @@ class OpenTelemetryMiddleware:
                     SpanAttributes.HTTP_SCHEME: request.url.scheme,
                     SpanAttributes.HTTP_HOST: request.url.hostname,
                     SpanAttributes.HTTP_TARGET: request.url.path,
-                    SpanAttributes.USER_AGENT_ORIGINAL: request.headers.get("user-agent", ""),
+                    SpanAttributes.USER_AGENT_ORIGINAL: request.headers.get(
+                        "user-agent", ""
+                    ),
                 }
             )
 
@@ -144,9 +160,15 @@ class OpenTelemetryMiddleware:
                 baggage.set_baggage("user.id", user_id)
 
             # æ·»åŠ è«‹æ±‚ ID å’Œé—œè¯ ID
-            request_id = request.headers.get("x-request-id") or self._generate_request_id()
-            correlation_id = request.headers.get("x-correlation-id") or self._generate_correlation_id()
-            
+            request_id = (
+                request.headers.get("x-request-id")
+                or self._generate_request_id()
+            )
+            correlation_id = (
+                request.headers.get("x-correlation-id")
+                or self._generate_correlation_id()
+            )
+
             span.set_attribute("request.id", request_id)
             span.set_attribute("correlation.id", correlation_id)
             baggage.set_baggage("request.id", request_id)
@@ -166,7 +188,8 @@ class OpenTelemetryMiddleware:
                 span.set_attributes(
                     {
                         SpanAttributes.HTTP_STATUS_CODE: response.status_code,
-                        "http.response.duration_ms": (time.time() - start_time) * 1000,
+                        "http.response.duration_ms": (time.time() - start_time)
+                        * 1000,
                     }
                 )
 
@@ -174,8 +197,12 @@ class OpenTelemetryMiddleware:
                 for key, value in headers_to_inject.items():
                     response.headers[key] = value
 
-                response.headers["x-trace-id"] = format(span.get_span_context().trace_id, "032x")
-                response.headers["x-span-id"] = format(span.get_span_context().span_id, "016x")
+                response.headers["x-trace-id"] = format(
+                    span.get_span_context().trace_id, "032x"
+                )
+                response.headers["x-span-id"] = format(
+                    span.get_span_context().span_id, "016x"
+                )
                 response.headers["x-request-id"] = request_id
                 response.headers["x-correlation-id"] = correlation_id
 
@@ -202,7 +229,9 @@ class OpenTelemetryMiddleware:
                 import jwt
 
                 token = auth_header[7:]
-                payload = jwt.decode(token, options={"verify_signature": False})
+                payload = jwt.decode(
+                    token, options={"verify_signature": False}
+                )
                 return payload.get("sub") or payload.get("user_id")
             except Exception:
                 pass
@@ -213,21 +242,27 @@ class OpenTelemetryMiddleware:
     def _generate_request_id(self) -> str:
         """ç”Ÿæˆè«‹æ±‚ ID"""
         import uuid
+
         return str(uuid.uuid4())
-    
+
     def _generate_correlation_id(self) -> str:
         """ç”Ÿæˆé—œè¯ ID"""
         import uuid
+
         return str(uuid.uuid4())
 
-    def create_span(self, name: str, attributes: Dict[str, Any] = None) -> trace.Span:
+    def create_span(
+        self, name: str, attributes: Dict[str, Any] = None
+    ) -> trace.Span:
         """å‰µå»ºæ–°çš„ span"""
         span = self.tracer.start_span(name)
         if attributes:
             span.set_attributes(attributes)
         return span
 
-    def add_span_event(self, span: trace.Span, name: str, attributes: Dict[str, Any] = None):
+    def add_span_event(
+        self, span: trace.Span, name: str, attributes: Dict[str, Any] = None
+    ):
         """æ·»åŠ  span äº‹ä»¶"""
         span.add_event(name, attributes or {})
 
@@ -286,7 +321,9 @@ def trace_function(operation_name: str = None):
                     return result
                 except Exception as e:
                     span.record_exception(e)
-                    span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
+                    span.set_status(
+                        trace.Status(trace.StatusCode.ERROR, str(e))
+                    )
                     raise
 
         @wraps(func)
@@ -308,7 +345,9 @@ def trace_function(operation_name: str = None):
                     return result
                 except Exception as e:
                     span.record_exception(e)
-                    span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
+                    span.set_status(
+                        trace.Status(trace.StatusCode.ERROR, str(e))
+                    )
                     raise
 
         import asyncio
@@ -352,12 +391,16 @@ class DatabaseTracing:
 
                         # è¨˜éŒ„çµæœçµ±è¨ˆ
                         if hasattr(result, "rowcount"):
-                            span.set_attribute("db.rows_affected", result.rowcount)
+                            span.set_attribute(
+                                "db.rows_affected", result.rowcount
+                            )
 
                         return result
                     except Exception as e:
                         span.record_exception(e)
-                        span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
+                        span.set_status(
+                            trace.Status(trace.StatusCode.ERROR, str(e))
+                        )
                         raise
 
             return wrapper
@@ -398,13 +441,16 @@ class HTTPClientTracing:
 
                         if hasattr(response, "status_code"):
                             span.set_attribute(
-                                SpanAttributes.HTTP_STATUS_CODE, response.status_code
+                                SpanAttributes.HTTP_STATUS_CODE,
+                                response.status_code,
                             )
 
                         return response
                     except Exception as e:
                         span.record_exception(e)
-                        span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
+                        span.set_status(
+                            trace.Status(trace.StatusCode.ERROR, str(e))
+                        )
                         raise
 
             return wrapper
diff --git a/auto_generate_video_fold6/performance/api/response-optimization.py b/auto_generate_video_fold6/performance/api/response-optimization.py
index ce2cf95..3e1e913 100644
--- a/auto_generate_video_fold6/performance/api/response-optimization.py
+++ b/auto_generate_video_fold6/performance/api/response-optimization.py
@@ -39,7 +39,9 @@ class PerformanceMiddleware(BaseHTTPMiddleware):
 
         # è¨˜éŒ„è«‹æ±‚é–‹å§‹
         if self.enable_detailed_logging:
-            logger.info(f"[{request_id}] {request.method} {request.url.path} started")
+            logger.info(
+                f"[{request_id}] {request.method} {request.url.path} started"
+            )
 
         try:
             response = await call_next(request)
@@ -71,7 +73,9 @@ class PerformanceMiddleware(BaseHTTPMiddleware):
             )
             raise
 
-    def _collect_metrics(self, request: Request, response: Response, process_time: float):
+    def _collect_metrics(
+        self, request: Request, response: Response, process_time: float
+    ):
         """æ”¶é›†æ•ˆèƒ½æŒ‡æ¨™"""
         metric = {
             "method": request.method,
@@ -90,7 +94,9 @@ class PerformanceMiddleware(BaseHTTPMiddleware):
 class CompressionMiddleware(BaseHTTPMiddleware):
     """æ™ºæ…§å£“ç¸®ä¸­ä»‹è»Ÿé«”"""
 
-    def __init__(self, app, minimum_size: int = 1024, compression_level: int = 6):
+    def __init__(
+        self, app, minimum_size: int = 1024, compression_level: int = 6
+    ):
         super().__init__(app)
         self.minimum_size = minimum_size
         self.compression_level = compression_level
@@ -137,7 +143,9 @@ class CompressionMiddleware(BaseHTTPMiddleware):
             return response
 
         # åŸ·è¡Œ gzip å£“ç¸®
-        compressed_body = gzip.compress(body, compresslevel=self.compression_level)
+        compressed_body = gzip.compress(
+            body, compresslevel=self.compression_level
+        )
 
         # æ›´æ–°æ¨™é ­
         response.headers["content-encoding"] = "gzip"
@@ -219,7 +227,12 @@ class CacheMiddleware(BaseHTTPMiddleware):
             return False
 
         # å¯å¿«å–çš„è·¯å¾‘
-        cacheable_paths = ["/api/trends", "/api/user/profile", "/api/analytics", "/api/models/list"]
+        cacheable_paths = [
+            "/api/trends",
+            "/api/user/profile",
+            "/api/analytics",
+            "/api/models/list",
+        ]
 
         return any(path in request.url.path for path in cacheable_paths)
 
@@ -240,7 +253,9 @@ class CacheMiddleware(BaseHTTPMiddleware):
                 "headers": dict(response.headers),
             }
 
-            await self.cache_manager.set(cache_key, cache_data, self.default_ttl)
+            await self.cache_manager.set(
+                cache_key, cache_data, self.default_ttl
+            )
             logger.debug(f"Cached response for key {cache_key[:16]}")
 
         except Exception as e:
@@ -340,12 +355,15 @@ def create_optimized_app(
     # æ•ˆèƒ½ç›£æ§ä¸­ä»‹è»Ÿé«”
     if enable_performance_logging:
         app.add_middleware(
-            PerformanceMiddleware, enable_detailed_logging=enable_performance_logging
+            PerformanceMiddleware,
+            enable_detailed_logging=enable_performance_logging,
         )
 
     # å¿«å–ä¸­ä»‹è»Ÿé«”
     if cache_manager:
-        app.add_middleware(CacheMiddleware, cache_manager=cache_manager, default_ttl=300)
+        app.add_middleware(
+            CacheMiddleware, cache_manager=cache_manager, default_ttl=300
+        )
 
     # é€Ÿç‡é™åˆ¶ä¸­ä»‹è»Ÿé«”
     if rate_limiter:
@@ -353,7 +371,9 @@ def create_optimized_app(
 
     # å£“ç¸®ä¸­ä»‹è»Ÿé«”
     if enable_compression:
-        app.add_middleware(CompressionMiddleware, minimum_size=1024, compression_level=6)
+        app.add_middleware(
+            CompressionMiddleware, minimum_size=1024, compression_level=6
+        )
 
     return app
 
@@ -388,7 +408,9 @@ def async_cache(ttl: int = 300):
 
             # æ¸…ç†éæœŸå¿«å–
             if len(cache) > 1000:  # é™åˆ¶å¿«å–å¤§å°
-                expired_keys = [k for k, t in cache_times.items() if now - t > ttl]
+                expired_keys = [
+                    k for k, t in cache_times.items() if now - t > ttl
+                ]
                 for k in expired_keys:
                     cache.pop(k, None)
                     cache_times.pop(k, None)
@@ -420,7 +442,9 @@ def batch_processor(batch_size: int = 10, timeout: float = 1.0):
                 pending_requests.clear()
             else:
                 # è¨­ç½®è¶…æ™‚è™•ç†
-                asyncio.create_task(_timeout_handler(func, pending_requests, timeout))
+                asyncio.create_task(
+                    _timeout_handler(func, pending_requests, timeout)
+                )
 
             return await future
 
@@ -461,7 +485,9 @@ def batch_processor(batch_size: int = 10, timeout: float = 1.0):
 # ä½¿ç”¨ç¯„ä¾‹
 if __name__ == "__main__":
     # å‰µå»ºå„ªåŒ–çš„æ‡‰ç”¨
-    app = create_optimized_app(enable_compression=True, enable_performance_logging=True)
+    app = create_optimized_app(
+        enable_compression=True, enable_performance_logging=True
+    )
 
     @app.get("/api/health")
     async def health_check():
@@ -472,6 +498,9 @@ if __name__ == "__main__":
     async def test_cache():
         # æ¨¡æ“¬æ…¢é€Ÿæ“ä½œ
         await asyncio.sleep(1)
-        return {"data": "cached_result", "timestamp": datetime.now().isoformat()}
+        return {
+            "data": "cached_result",
+            "timestamp": datetime.now().isoformat(),
+        }
 
     logger.info("API optimization configured successfully")
diff --git a/auto_generate_video_fold6/performance/benchmarking/performance-tests.py b/auto_generate_video_fold6/performance/benchmarking/performance-tests.py
index 4bbf3cb..ad4d5eb 100644
--- a/auto_generate_video_fold6/performance/benchmarking/performance-tests.py
+++ b/auto_generate_video_fold6/performance/benchmarking/performance-tests.py
@@ -73,7 +73,9 @@ class PerformanceBenchmark:
 
             request_start = time.time()
             try:
-                async with session.get(f"{self.base_url}{endpoint}", headers=headers) as response:
+                async with session.get(
+                    f"{self.base_url}{endpoint}", headers=headers
+                ) as response:
                     await response.text()
                     request_time = time.time() - request_start
                     response_times.append(request_time)
@@ -122,13 +124,17 @@ class PerformanceBenchmark:
             successful_requests=successful_requests,
             failed_requests=failed_requests,
             duration_seconds=total_time,
-            requests_per_second=total_requests / total_time if total_time > 0 else 0,
+            requests_per_second=total_requests / total_time
+            if total_time > 0
+            else 0,
             average_response_time=avg_response_time,
             min_response_time=min_response_time,
             max_response_time=max_response_time,
             percentile_95=percentile_95,
             percentile_99=percentile_99,
-            error_rate=failed_requests / total_requests if total_requests > 0 else 0,
+            error_rate=failed_requests / total_requests
+            if total_requests > 0
+            else 0,
             timestamp=datetime.now(),
         )
 
@@ -182,12 +188,16 @@ class PerformanceBenchmark:
             failed_requests=failed_queries,
             duration_seconds=total_time,
             requests_per_second=total_queries / total_time,
-            average_response_time=statistics.mean(query_times) if query_times else 0,
+            average_response_time=statistics.mean(query_times)
+            if query_times
+            else 0,
             min_response_time=min(query_times) if query_times else 0,
             max_response_time=max(query_times) if query_times else 0,
             percentile_95=np.percentile(query_times, 95) if query_times else 0,
             percentile_99=np.percentile(query_times, 99) if query_times else 0,
-            error_rate=failed_queries / total_queries if total_queries > 0 else 0,
+            error_rate=failed_queries / total_queries
+            if total_queries > 0
+            else 0,
             timestamp=datetime.now(),
         )
 
@@ -252,12 +262,20 @@ class PerformanceBenchmark:
             failed_requests=failed_operations,
             duration_seconds=total_time,
             requests_per_second=total_operations / total_time,
-            average_response_time=statistics.mean(operation_times) if operation_times else 0,
+            average_response_time=statistics.mean(operation_times)
+            if operation_times
+            else 0,
             min_response_time=min(operation_times) if operation_times else 0,
             max_response_time=max(operation_times) if operation_times else 0,
-            percentile_95=np.percentile(operation_times, 95) if operation_times else 0,
-            percentile_99=np.percentile(operation_times, 99) if operation_times else 0,
-            error_rate=failed_operations / total_operations if total_operations > 0 else 0,
+            percentile_95=np.percentile(operation_times, 95)
+            if operation_times
+            else 0,
+            percentile_99=np.percentile(operation_times, 99)
+            if operation_times
+            else 0,
+            error_rate=failed_operations / total_operations
+            if total_operations > 0
+            else 0,
             timestamp=datetime.now(),
         )
 
@@ -315,7 +333,9 @@ class PerformanceBenchmark:
                 "min": min(cpu_usage) if cpu_usage else 0,
             },
             "memory": {
-                "average": statistics.mean(memory_usage) if memory_usage else 0,
+                "average": statistics.mean(memory_usage)
+                if memory_usage
+                else 0,
                 "max": max(memory_usage) if memory_usage else 0,
                 "min": min(memory_usage) if memory_usage else 0,
             },
@@ -340,7 +360,9 @@ class PerformanceBenchmark:
         print(f"   Failed: {result.failed_requests}")
         print(f"   Duration: {result.duration_seconds:.2f}s")
         print(f"   RPS: {result.requests_per_second:.2f}")
-        print(f"   Avg Response Time: {result.average_response_time * 1000:.2f}ms")
+        print(
+            f"   Avg Response Time: {result.average_response_time * 1000:.2f}ms"
+        )
         print(f"   95th Percentile: {result.percentile_95 * 1000:.2f}ms")
         print(f"   99th Percentile: {result.percentile_99 * 1000:.2f}ms")
         print(f"   Error Rate: {result.error_rate * 100:.2f}%")
@@ -351,7 +373,9 @@ class PerformanceBenchmark:
             "generated_at": datetime.now().isoformat(),
             "test_summary": {
                 "total_tests": len(self.results),
-                "tests_completed": len([r for r in self.results if r.error_rate < 0.05]),
+                "tests_completed": len(
+                    [r for r in self.results if r.error_rate < 0.05]
+                ),
             },
             "results": [],
         }
@@ -365,7 +389,8 @@ class PerformanceBenchmark:
                     "failed_requests": result.failed_requests,
                     "duration_seconds": result.duration_seconds,
                     "requests_per_second": result.requests_per_second,
-                    "average_response_time_ms": result.average_response_time * 1000,
+                    "average_response_time_ms": result.average_response_time
+                    * 1000,
                     "percentile_95_ms": result.percentile_95 * 1000,
                     "percentile_99_ms": result.percentile_99 * 1000,
                     "error_rate_percent": result.error_rate * 100,
diff --git a/auto_generate_video_fold6/performance/caching/cache-strategies.py b/auto_generate_video_fold6/performance/caching/cache-strategies.py
index 3301a39..a05de5a 100644
--- a/auto_generate_video_fold6/performance/caching/cache-strategies.py
+++ b/auto_generate_video_fold6/performance/caching/cache-strategies.py
@@ -39,7 +39,9 @@ class CacheManager:
         """ç²å–å¿«å–å€¼"""
         try:
             if self._is_circuit_breaker_open():
-                logger.warning(f"Circuit breaker open, skipping cache get for {key}")
+                logger.warning(
+                    f"Circuit breaker open, skipping cache get for {key}"
+                )
                 return default
 
             value = await self.redis.get(key)
@@ -151,14 +153,19 @@ class CacheManager:
 
     def _is_circuit_breaker_open(self) -> bool:
         """æª¢æŸ¥æ–·è·¯å™¨ç‹€æ…‹"""
-        if self.circuit_breaker_failures < self.config.circuit_breaker_threshold:
+        if (
+            self.circuit_breaker_failures
+            < self.config.circuit_breaker_threshold
+        ):
             return False
 
         if self.circuit_breaker_last_failure is None:
             return False
 
         # 30ç§’å¾Œé‡è©¦
-        return (datetime.now() - self.circuit_breaker_last_failure).seconds < 30
+        return (
+            datetime.now() - self.circuit_breaker_last_failure
+        ).seconds < 30
 
     def _handle_failure(self):
         """è™•ç†å¤±æ•—"""
@@ -254,16 +261,22 @@ def cache_result(ttl: int = 3600, key_prefix: str = None):
     return decorator
 
 
-def _generate_cache_key(func: Callable, prefix: str, args: tuple, kwargs: dict) -> str:
+def _generate_cache_key(
+    func: Callable, prefix: str, args: tuple, kwargs: dict
+) -> str:
     """ç”Ÿæˆå¿«å–éµ"""
     func_name = f"{func.__module__}.{func.__name__}"
 
     # æ’é™¤ç‰¹æ®Šåƒæ•¸
-    filtered_kwargs = {k: v for k, v in kwargs.items() if k not in ["cache_manager", "db"]}
+    filtered_kwargs = {
+        k: v for k, v in kwargs.items() if k not in ["cache_manager", "db"]
+    }
 
     # ç”Ÿæˆåƒæ•¸é›œæ¹Š
     args_str = ":".join(str(arg) for arg in args)
-    kwargs_str = ":".join(f"{k}={v}" for k, v in sorted(filtered_kwargs.items()))
+    kwargs_str = ":".join(
+        f"{k}={v}" for k, v in sorted(filtered_kwargs.items())
+    )
 
     content = f"{func_name}:{args_str}:{kwargs_str}"
     hash_key = hashlib.sha256(content.encode()).hexdigest()
@@ -282,7 +295,9 @@ class SessionCache:
 
     async def create_session(self, user_id: int, session_data: dict) -> str:
         """å‰µå»ºç”¨æˆ¶æœƒè©±"""
-        session_id = CacheKeyGenerator.generate_hash(user_id, datetime.now().isoformat())
+        session_id = CacheKeyGenerator.generate_hash(
+            user_id, datetime.now().isoformat()
+        )
         session_key = CacheKeyGenerator.user_session(user_id)
 
         session_info = {
@@ -355,7 +370,9 @@ class ModelCache:
         self.cache = cache_manager
         self.inference_ttl = 3600  # 1å°æ™‚
 
-    async def cache_inference_result(self, model_id: int, input_data: dict, result: Any) -> bool:
+    async def cache_inference_result(
+        self, model_id: int, input_data: dict, result: Any
+    ) -> bool:
         """å¿«å–æ¨è«–çµæœ"""
         input_hash = CacheKeyGenerator.generate_hash(str(input_data))
         key = CacheKeyGenerator.inference_result(model_id, input_hash)
@@ -369,7 +386,9 @@ class ModelCache:
 
         return await self.cache.set(key, cache_data, self.inference_ttl)
 
-    async def get_cached_inference(self, model_id: int, input_data: dict) -> Optional[Any]:
+    async def get_cached_inference(
+        self, model_id: int, input_data: dict
+    ) -> Optional[Any]:
         """ç²å–å¿«å–çš„æ¨è«–çµæœ"""
         input_hash = CacheKeyGenerator.generate_hash(str(input_data))
         key = CacheKeyGenerator.inference_result(model_id, input_hash)
@@ -409,7 +428,9 @@ async def example_usage():
 
     # ä½¿ç”¨é€Ÿç‡é™åˆ¶
     rate_limit = RateLimitCache(cache_manager)
-    allowed, count = await rate_limit.check_rate_limit(123, "api/upload", 10, 60)
+    allowed, count = await rate_limit.check_rate_limit(
+        123, "api/upload", 10, 60
+    )
     print(f"å…è¨±è«‹æ±‚: {allowed}, ç•¶å‰è¨ˆæ•¸: {count}")
 
 
diff --git a/auto_generate_video_fold6/pyproject.toml b/auto_generate_video_fold6/pyproject.toml
index db9f914..0fb2be7 100644
--- a/auto_generate_video_fold6/pyproject.toml
+++ b/auto_generate_video_fold6/pyproject.toml
@@ -182,20 +182,6 @@ skips = ["*_test.py", "test_*.py"]
 [tool.ruff]
 target-version = "py311"
 line-length = 79
-select = [
-    "E",  # pycodestyle errors
-    "W",  # pycodestyle warnings
-    "F",  # pyflakes
-    "I",  # isort
-    "B",  # flake8-bugbear
-    "C4", # flake8-comprehensions
-    "UP", # pyupgrade
-]
-ignore = [
-    "E501",  # line too long, handled by black
-    "B008",  # do not perform function calls in argument defaults
-    "C901",  # too complex
-]
 exclude = [
     ".bzr",
     ".direnv",
@@ -219,11 +205,27 @@ exclude = [
     "frontend",
 ]
 
-[tool.ruff.per-file-ignores]
+[tool.ruff.lint]
+select = [
+    "E",  # pycodestyle errors
+    "W",  # pycodestyle warnings
+    "F",  # pyflakes
+    "I",  # isort
+    "B",  # flake8-bugbear
+    "C4", # flake8-comprehensions
+    "UP", # pyupgrade
+]
+ignore = [
+    "E501",  # line too long, handled by black
+    "B008",  # do not perform function calls in argument defaults
+    "C901",  # too complex
+]
+
+[tool.ruff.lint.per-file-ignores]
 "__init__.py" = ["F401"]
 "services/*/tests/*" = ["B011", "B018"]
 
-[tool.ruff.mccabe]
+[tool.ruff.lint.mccabe]
 max-complexity = 10
 
 # Flake8 é…ç½®
diff --git a/auto_generate_video_fold6/scripts/auto_trends_video.py b/auto_generate_video_fold6/scripts/auto_trends_video.py
index 0f30a25..f89e96c 100644
--- a/auto_generate_video_fold6/scripts/auto_trends_video.py
+++ b/auto_generate_video_fold6/scripts/auto_trends_video.py
@@ -38,7 +38,8 @@ except ImportError:
 
 # è¨­ç½®æ—¥èªŒ
 logging.basicConfig(
-    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+    level=logging.INFO,
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
 )
 logger = logging.getLogger(__name__)
 
@@ -52,7 +53,9 @@ class AutoTrendsVideoGenerator:
             if mode:
                 config_manager.set_mode(mode)
             self.config = self._load_unified_config()
-            logger.info(f"ä½¿ç”¨çµ±ä¸€é…ç½®ç®¡ç†å™¨ï¼Œç•¶å‰æ¨¡å¼: {config_manager.current_mode}")
+            logger.info(
+                f"ä½¿ç”¨çµ±ä¸€é…ç½®ç®¡ç†å™¨ï¼Œç•¶å‰æ¨¡å¼: {config_manager.current_mode}"
+            )
         else:
             self.config = self._load_legacy_config(config_file)
             logger.info("ä½¿ç”¨å‚³çµ±é…ç½®æ–¹å¼")
@@ -61,7 +64,9 @@ class AutoTrendsVideoGenerator:
         self.services = self._setup_services()
 
         # è¨­ç½®è¼¸å‡ºç›®éŒ„
-        self.output_dir = Path(self.config.get("output_dir", "./generated_videos"))
+        self.output_dir = Path(
+            self.config.get("output_dir", "./generated_videos")
+        )
         self.output_dir.mkdir(parents=True, exist_ok=True)
 
         # å½±ç‰‡ç”Ÿæˆè¨­å®š
@@ -91,24 +96,38 @@ class AutoTrendsVideoGenerator:
             self.cost_tracker = None
             self.budget_controller = None
 
-        logger.info(f"å½±ç‰‡ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œæ¯æ—¥é™åˆ¶: {self.video_configs['max_videos_per_run']}")
+        logger.info(
+            f"å½±ç‰‡ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œæ¯æ—¥é™åˆ¶: {self.video_configs['max_videos_per_run']}"
+        )
 
     def _setup_services(self) -> dict:
         """è¨­ç½®æœå‹™é…ç½®"""
         if CONFIG_MANAGER_AVAILABLE:
             return {
-                "trend_service": get_config("services.trend_service.url", "http://localhost:8001"),
-                "video_service": get_config("services.video_service.url", "http://localhost:8002"),
-                "ai_service": get_config("services.ai_service.url", "http://localhost:8003"),
+                "trend_service": get_config(
+                    "services.trend_service.url", "http://localhost:8001"
+                ),
+                "video_service": get_config(
+                    "services.video_service.url", "http://localhost:8002"
+                ),
+                "ai_service": get_config(
+                    "services.ai_service.url", "http://localhost:8003"
+                ),
                 "social_service": get_config(
                     "services.social_service.url", "http://localhost:8004"
                 ),
             }
         else:
             return {
-                "trend_service": self.config.get("trend_service_url", "http://localhost:8001"),
-                "video_service": self.config.get("video_service_url", "http://localhost:8002"),
-                "ai_service": self.config.get("ai_service_url", "http://localhost:8003"),
+                "trend_service": self.config.get(
+                    "trend_service_url", "http://localhost:8001"
+                ),
+                "video_service": self.config.get(
+                    "video_service_url", "http://localhost:8002"
+                ),
+                "ai_service": self.config.get(
+                    "ai_service_url", "http://localhost:8003"
+                ),
             }
 
     def _setup_video_configs(self) -> dict:
@@ -116,16 +135,25 @@ class AutoTrendsVideoGenerator:
         if CONFIG_MANAGER_AVAILABLE:
             generation_config = get_config("generation", {})
             return {
-                "max_videos_per_run": generation_config.get("daily_video_limit", 5),
+                "max_videos_per_run": generation_config.get(
+                    "daily_video_limit", 5
+                ),
                 "batch_size": generation_config.get("batch_size", 1),
-                "max_concurrent_jobs": generation_config.get("max_concurrent_jobs", 2),
-                "video_duration": generation_config.get("duration_range", [30, 60])[0],  # å–æœ€å°å€¼
+                "max_concurrent_jobs": generation_config.get(
+                    "max_concurrent_jobs", 2
+                ),
+                "video_duration": generation_config.get(
+                    "duration_range", [30, 60]
+                )[0],  # å–æœ€å°å€¼
                 "platforms": generation_config.get("platforms", ["tiktok"]),
                 "categories": get_config(
-                    "content.content_categories", ["technology", "entertainment", "lifestyle"]
+                    "content.content_categories",
+                    ["technology", "entertainment", "lifestyle"],
                 ),
                 "languages": [get_config("content.language", "zh-TW")],
-                "quality_preset": generation_config.get("quality_preset", "medium"),
+                "quality_preset": generation_config.get(
+                    "quality_preset", "medium"
+                ),
             }
         else:
             return {
@@ -134,7 +162,9 @@ class AutoTrendsVideoGenerator:
                 "max_concurrent_jobs": 2,
                 "video_duration": self.config.get("video_duration", 30),
                 "platforms": self.config.get("target_platforms", ["tiktok"]),
-                "categories": self.config.get("categories", ["technology", "entertainment"]),
+                "categories": self.config.get(
+                    "categories", ["technology", "entertainment"]
+                ),
                 "languages": self.config.get("languages", ["zh-TW"]),
                 "quality_preset": "medium",
             }
@@ -148,11 +178,17 @@ class AutoTrendsVideoGenerator:
     def _load_unified_config(self) -> dict:
         """è¼‰å…¥çµ±ä¸€é…ç½®"""
         return {
-            "output_dir": get_config("storage.output_dir", "./generated_videos"),
-            "quality_threshold": get_config("generation.quality_threshold", 0.7),
+            "output_dir": get_config(
+                "storage.output_dir", "./generated_videos"
+            ),
+            "quality_threshold": get_config(
+                "generation.quality_threshold", 0.7
+            ),
             "schedule_interval": 1800,  # é»˜èª30åˆ†é˜
             "daily_budget": get_config("cost_control.daily_budget_usd", 50.0),
-            "stop_on_budget_exceeded": get_config("cost_control.stop_on_budget_exceeded", True),
+            "stop_on_budget_exceeded": get_config(
+                "cost_control.stop_on_budget_exceeded", True
+            ),
         }
 
     def _load_legacy_config(self, config_file: str) -> dict:
@@ -200,12 +236,18 @@ class AutoTrendsVideoGenerator:
             logger.info(f"æ‰¾åˆ° {len(trending_keywords)} å€‹ç†±é–€é—œéµå­—")
 
             # 2. é¸æ“‡æœ€ä½³é—œéµå­—
-            selected_keywords = await self._select_best_keywords(trending_keywords)
+            selected_keywords = await self._select_best_keywords(
+                trending_keywords
+            )
 
-            logger.info(f"é¸æ“‡äº† {len(selected_keywords)} å€‹é—œéµå­—é€²è¡Œå½±ç‰‡ç”Ÿæˆ")
+            logger.info(
+                f"é¸æ“‡äº† {len(selected_keywords)} å€‹é—œéµå­—é€²è¡Œå½±ç‰‡ç”Ÿæˆ"
+            )
 
             # 3. æ‰¹æ¬¡ç”Ÿæˆå½±ç‰‡
-            generation_results = await self._batch_generate_videos(selected_keywords)
+            generation_results = await self._batch_generate_videos(
+                selected_keywords
+            )
 
             # 4. è™•ç†çµæœ
             await self._process_results(generation_results)
@@ -228,7 +270,10 @@ class AutoTrendsVideoGenerator:
             # 2. æª¢æŸ¥é ç®—ç‹€æ…‹ (ä½¿ç”¨æ–°çš„é ç®—æ§åˆ¶å™¨)
             if COST_MONITORING_AVAILABLE and self.budget_controller:
                 estimated_cost = self._estimate_batch_cost()
-                can_proceed, message = await self.budget_controller.pre_operation_check(
+                (
+                    can_proceed,
+                    message,
+                ) = await self.budget_controller.pre_operation_check(
                     "batch_generation", estimated_cost
                 )
                 if not can_proceed:
@@ -238,9 +283,13 @@ class AutoTrendsVideoGenerator:
             else:
                 # èˆŠç‰ˆé ç®—æª¢æŸ¥
                 if CONFIG_MANAGER_AVAILABLE:
-                    daily_budget = get_config("cost_control.daily_budget_usd", 50.0)
+                    daily_budget = get_config(
+                        "cost_control.daily_budget_usd", 50.0
+                    )
                     if self.cost_tracker["daily_cost"] >= daily_budget:
-                        logger.info(f"å·²é”æ¯æ—¥é ç®—é™åˆ¶ (${daily_budget})ï¼Œè·³éç”Ÿæˆ")
+                        logger.info(
+                            f"å·²é”æ¯æ—¥é ç®—é™åˆ¶ (${daily_budget})ï¼Œè·³éç”Ÿæˆ"
+                        )
                         return False
 
             # 3. æª¢æŸ¥æ¯æ—¥é™åˆ¶
@@ -278,7 +327,8 @@ class AutoTrendsVideoGenerator:
 
                 estimated_per_video = 0.11  # ä¿å®ˆä¼°è¨ˆ
                 planned_videos = min(
-                    batch_size, max_videos - self.cost_tracker["videos_generated_today"]
+                    batch_size,
+                    max_videos - self.cost_tracker["videos_generated_today"],
                 )
 
                 return max(0, planned_videos * estimated_per_video)
@@ -293,7 +343,11 @@ class AutoTrendsVideoGenerator:
         """æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹"""
         try:
             healthy_services = 0
-            required_services = ["trend_service", "ai_service", "video_service"]
+            required_services = [
+                "trend_service",
+                "ai_service",
+                "video_service",
+            ]
 
             for service_name in required_services:
                 service_url = self.services.get(service_name)
@@ -310,17 +364,23 @@ class AutoTrendsVideoGenerator:
                                 healthy_services += 1
                                 logger.debug(f"{service_name} å¥åº·ç‹€æ…‹æ­£å¸¸")
                             else:
-                                logger.warning(f"{service_name} å¥åº·æª¢æŸ¥å¤±æ•—: {resp.status}")
+                                logger.warning(
+                                    f"{service_name} å¥åº·æª¢æŸ¥å¤±æ•—: {resp.status}"
+                                )
 
                 except Exception as e:
                     logger.warning(f"{service_name} é€£æ¥å¤±æ•—: {e}")
 
             # è‡³å°‘éœ€è¦2å€‹æœå‹™æ­£å¸¸é‹è¡Œ
             if healthy_services >= 2:
-                logger.info(f"æœå‹™å¥åº·æª¢æŸ¥é€šé ({healthy_services}/{len(required_services)})")
+                logger.info(
+                    f"æœå‹™å¥åº·æª¢æŸ¥é€šé ({healthy_services}/{len(required_services)})"
+                )
                 return True
             else:
-                logger.error(f"æœå‹™å¥åº·æª¢æŸ¥å¤±æ•— ({healthy_services}/{len(required_services)})")
+                logger.error(
+                    f"æœå‹™å¥åº·æª¢æŸ¥å¤±æ•— ({healthy_services}/{len(required_services)})"
+                )
                 return False
 
         except Exception as e:
@@ -335,17 +395,23 @@ class AutoTrendsVideoGenerator:
             # å¾å¤šå€‹é¡åˆ¥ç²å–é—œéµå­—
             for category in self.video_configs["categories"]:
                 async with aiohttp.ClientSession() as session:
-                    url = f"{self.services['trend_service']}/api/trends/keywords"
+                    url = (
+                        f"{self.services['trend_service']}/api/trends/keywords"
+                    )
                     params = {"category": category, "geo": "TW"}
 
                     async with session.get(url, params=params) as resp:
                         if resp.status == 200:
                             data = await resp.json()
                             keywords = data.get("keywords", [])
-                            logger.info(f"å¾ {category} é¡åˆ¥ç²å–åˆ° {len(keywords)} å€‹é—œéµå­—")
+                            logger.info(
+                                f"å¾ {category} é¡åˆ¥ç²å–åˆ° {len(keywords)} å€‹é—œéµå­—"
+                            )
                             all_keywords.extend(keywords)
                         else:
-                            logger.warning(f"ç²å– {category} é¡åˆ¥é—œéµå­—å¤±æ•—: {resp.status}")
+                            logger.warning(
+                                f"ç²å– {category} é¡åˆ¥é—œéµå­—å¤±æ•—: {resp.status}"
+                            )
 
             return all_keywords
 
@@ -405,7 +471,10 @@ class AutoTrendsVideoGenerator:
 
             # ç¶œåˆè©•åˆ†
             final_score = (
-                traffic_score * 0.4 + category_score * 0.3 + length_score * 0.2 + time_score * 0.1
+                traffic_score * 0.4
+                + category_score * 0.3
+                + length_score * 0.2
+                + time_score * 0.1
             )
 
             return final_score
@@ -428,21 +497,35 @@ class AutoTrendsVideoGenerator:
             # åˆ†æ‰¹è™•ç†
             for i in range(0, len(keywords), batch_size):
                 batch = keywords[i : i + batch_size]
-                logger.info(f"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}, åŒ…å« {len(batch)} å€‹é—œéµå­—")
+                logger.info(
+                    f"è™•ç†æ‰¹æ¬¡ {i // batch_size + 1}, åŒ…å« {len(batch)} å€‹é—œéµå­—"
+                )
 
                 # æª¢æŸ¥æ˜¯å¦é”åˆ°æ¯æ—¥é™åˆ¶
                 if CONFIG_MANAGER_AVAILABLE:
                     daily_limit = get_config("generation.daily_video_limit", 5)
-                    if self.cost_tracker["videos_generated_today"] >= daily_limit:
+                    if (
+                        self.cost_tracker["videos_generated_today"]
+                        >= daily_limit
+                    ):
                         logger.info(f"å·²é”æ¯æ—¥é™åˆ¶ ({daily_limit})ï¼Œåœæ­¢ç”Ÿæˆ")
                         break
 
                 # æª¢æŸ¥é ç®—é™åˆ¶
                 if CONFIG_MANAGER_AVAILABLE:
-                    daily_budget = get_config("cost_control.daily_budget_usd", 50.0)
-                    stop_on_budget = get_config("cost_control.stop_on_budget_exceeded", True)
-                    if stop_on_budget and self.cost_tracker["daily_cost"] >= daily_budget:
-                        logger.info(f"å·²é”é ç®—é™åˆ¶ (${daily_budget})ï¼Œåœæ­¢ç”Ÿæˆ")
+                    daily_budget = get_config(
+                        "cost_control.daily_budget_usd", 50.0
+                    )
+                    stop_on_budget = get_config(
+                        "cost_control.stop_on_budget_exceeded", True
+                    )
+                    if (
+                        stop_on_budget
+                        and self.cost_tracker["daily_cost"] >= daily_budget
+                    ):
+                        logger.info(
+                            f"å·²é”é ç®—é™åˆ¶ (${daily_budget})ï¼Œåœæ­¢ç”Ÿæˆ"
+                        )
                         break
 
                 # æ‰¹æ¬¡å…§ä¸¦è¡Œè™•ç†
@@ -459,14 +542,17 @@ class AutoTrendsVideoGenerator:
                         return await task
 
                 batch_results = await asyncio.gather(
-                    *[bounded_task(task) for task in tasks], return_exceptions=True
+                    *[bounded_task(task) for task in tasks],
+                    return_exceptions=True,
                 )
 
                 results.extend(batch_results)
 
                 # æ›´æ–°æˆæœ¬è¿½è¹¤
                 successful_count = sum(
-                    1 for r in batch_results if isinstance(r, dict) and r.get("status") == "success"
+                    1
+                    for r in batch_results
+                    if isinstance(r, dict) and r.get("status") == "success"
                 )
                 self.cost_tracker["videos_generated_today"] += successful_count
 
@@ -524,7 +610,9 @@ class AutoTrendsVideoGenerator:
                                 success=True,
                                 metadata={
                                     "keyword": keyword,
-                                    "video_duration": self.video_configs["video_duration"],
+                                    "video_duration": self.video_configs[
+                                        "video_duration"
+                                    ],
                                 },
                             )
 
@@ -538,19 +626,33 @@ class AutoTrendsVideoGenerator:
                                 success=True,
                                 metadata={
                                     "keyword": keyword,
-                                    "language": self.video_configs["languages"][0],
+                                    "language": self.video_configs[
+                                        "languages"
+                                    ][0],
                                 },
                             )
 
-                        return {"keyword": keyword, "status": "success", "result": result}
+                        return {
+                            "keyword": keyword,
+                            "status": "success",
+                            "result": result,
+                        }
                     else:
                         error_msg = f"å½±ç‰‡ç”Ÿæˆå¤±æ•—: {resp.status}"
                         logger.error(error_msg)
-                        return {"keyword": keyword, "status": "error", "error": error_msg}
+                        return {
+                            "keyword": keyword,
+                            "status": "error",
+                            "error": error_msg,
+                        }
 
         except Exception as e:
             logger.error(f"ç”Ÿæˆå½±ç‰‡ '{keyword_data.get('keyword')}' å¤±æ•—: {e}")
-            return {"keyword": keyword_data.get("keyword"), "status": "error", "error": str(e)}
+            return {
+                "keyword": keyword_data.get("keyword"),
+                "status": "error",
+                "error": str(e),
+            }
 
     async def _generate_script(self, keyword_data: dict) -> str:
         """ç”Ÿæˆå½±ç‰‡è…³æœ¬"""
@@ -583,11 +685,15 @@ class AutoTrendsVideoGenerator:
                                 tokens_used=500,  # ä¼°ç®—å€¼
                                 request_id=f"script_{keyword}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                                 success=True,
-                                metadata={"keyword": keyword, "category": category},
+                                metadata={
+                                    "keyword": keyword,
+                                    "category": category,
+                                },
                             )
 
                         return result.get(
-                            "script", f"æ¢ç´¢ {keyword} çš„ç²¾å½©ä¸–ç•Œï¼é€™å€‹è©±é¡Œæ­£åœ¨çˆ†ç´…ä¸­..."
+                            "script",
+                            f"æ¢ç´¢ {keyword} çš„ç²¾å½©ä¸–ç•Œï¼é€™å€‹è©±é¡Œæ­£åœ¨çˆ†ç´…ä¸­...",
                         )
                     else:
                         # è¿½è¹¤å¤±æ•—çš„ API å‘¼å«
@@ -598,11 +704,16 @@ class AutoTrendsVideoGenerator:
                                 operation_type="text_generation",
                                 tokens_used=0,
                                 success=False,
-                                metadata={"keyword": keyword, "error": f"HTTP {resp.status}"},
+                                metadata={
+                                    "keyword": keyword,
+                                    "error": f"HTTP {resp.status}",
+                                },
                             )
 
                         # å‚™ç”¨è…³æœ¬
-                        return self._generate_fallback_script(keyword, category)
+                        return self._generate_fallback_script(
+                            keyword, category
+                        )
 
         except Exception as e:
             logger.error(f"ç”Ÿæˆè…³æœ¬å¤±æ•—: {e}")
@@ -625,7 +736,9 @@ class AutoTrendsVideoGenerator:
         """å„²å­˜å½±ç‰‡çµæœ"""
         try:
             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-            result_file = self.output_dir / f"{keyword}_{timestamp}_result.json"
+            result_file = (
+                self.output_dir / f"{keyword}_{timestamp}_result.json"
+            )
 
             with open(result_file, "w", encoding="utf-8") as f:
                 json.dump(result, f, ensure_ascii=False, indent=2)
@@ -639,9 +752,15 @@ class AutoTrendsVideoGenerator:
         """è™•ç†ç”Ÿæˆçµæœ"""
         try:
             successful = [
-                r for r in results if isinstance(r, dict) and r.get("status") == "success"
+                r
+                for r in results
+                if isinstance(r, dict) and r.get("status") == "success"
+            ]
+            failed = [
+                r
+                for r in results
+                if isinstance(r, dict) and r.get("status") == "error"
             ]
-            failed = [r for r in results if isinstance(r, dict) and r.get("status") == "error"]
             exceptions = [r for r in results if isinstance(r, Exception)]
 
             logger.info(f"ç”Ÿæˆçµæœçµ±è¨ˆ:")
@@ -656,9 +775,13 @@ class AutoTrendsVideoGenerator:
                 "successful": len(successful),
                 "failed": len(failed),
                 "exceptions": len(exceptions),
-                "success_rate": len(successful) / len(results) if results else 0,
+                "success_rate": len(successful) / len(results)
+                if results
+                else 0,
                 "successful_keywords": [r["keyword"] for r in successful],
-                "failed_keywords": [r["keyword"] for r in failed if "keyword" in r],
+                "failed_keywords": [
+                    r["keyword"] for r in failed if "keyword" in r
+                ],
             }
 
             summary_file = (
@@ -736,11 +859,15 @@ class AutoTrendsVideoGenerator:
         """ç²å–æˆæœ¬æ‘˜è¦"""
         return {
             "daily_cost": self.cost_tracker["daily_cost"],
-            "videos_generated_today": self.cost_tracker["videos_generated_today"],
+            "videos_generated_today": self.cost_tracker[
+                "videos_generated_today"
+            ],
             "api_calls_count": self.cost_tracker["api_calls_count"],
             "daily_budget": self.config.get("daily_budget", 50.0),
             "budget_remaining": max(
-                0, self.config.get("daily_budget", 50.0) - self.cost_tracker["daily_cost"]
+                0,
+                self.config.get("daily_budget", 50.0)
+                - self.cost_tracker["daily_cost"],
             ),
             "videos_remaining": max(
                 0,
@@ -752,16 +879,33 @@ class AutoTrendsVideoGenerator:
 
 async def main():
     """ä¸»å‡½æ•¸"""
-    parser = argparse.ArgumentParser(description="è‡ªå‹• Google Trends å½±ç‰‡ç”Ÿæˆå™¨")
+    parser = argparse.ArgumentParser(
+        description="è‡ªå‹• Google Trends å½±ç‰‡ç”Ÿæˆå™¨"
+    )
     parser.add_argument("--config", "-c", help="é…ç½®æª”æ¡ˆè·¯å¾‘")
     parser.add_argument("--mode", "-m", help="é‹è¡Œæ¨¡å¼ (startup/enterprise)")
-    parser.add_argument("--schedule", "-s", action="store_true", help="å•Ÿå‹•æ’ç¨‹æ¨¡å¼")
-    parser.add_argument("--once", "-o", action="store_true", help="åŸ·è¡Œä¸€æ¬¡ç”Ÿæˆ")
+    parser.add_argument(
+        "--schedule", "-s", action="store_true", help="å•Ÿå‹•æ’ç¨‹æ¨¡å¼"
+    )
+    parser.add_argument(
+        "--once", "-o", action="store_true", help="åŸ·è¡Œä¸€æ¬¡ç”Ÿæˆ"
+    )
     parser.add_argument("--status", action="store_true", help="é¡¯ç¤ºç•¶å‰ç‹€æ…‹")
-    parser.add_argument("--cost-summary", action="store_true", help="é¡¯ç¤ºæˆæœ¬æ‘˜è¦")
-    parser.add_argument("--budget-status", action="store_true", help="é¡¯ç¤ºé ç®—ç‹€æ…‹")
-    parser.add_argument("--cost-report", action="store_true", help="ç”Ÿæˆæˆæœ¬å ±å‘Š")
-    parser.add_argument("--export-costs", type=int, metavar="DAYS", help="åŒ¯å‡ºæŒ‡å®šå¤©æ•¸çš„æˆæœ¬æ•¸æ“š")
+    parser.add_argument(
+        "--cost-summary", action="store_true", help="é¡¯ç¤ºæˆæœ¬æ‘˜è¦"
+    )
+    parser.add_argument(
+        "--budget-status", action="store_true", help="é¡¯ç¤ºé ç®—ç‹€æ…‹"
+    )
+    parser.add_argument(
+        "--cost-report", action="store_true", help="ç”Ÿæˆæˆæœ¬å ±å‘Š"
+    )
+    parser.add_argument(
+        "--export-costs",
+        type=int,
+        metavar="DAYS",
+        help="åŒ¯å‡ºæŒ‡å®šå¤©æ•¸çš„æˆæœ¬æ•¸æ“š",
+    )
 
     args = parser.parse_args()
 
@@ -786,14 +930,20 @@ async def main():
 
     if args.budget_status:
         if COST_MONITORING_AVAILABLE and generator.budget_controller:
-            budget_status = await generator.budget_controller.check_budget_status()
+            budget_status = (
+                await generator.budget_controller.check_budget_status()
+            )
             print("\n=== é ç®—ç‹€æ…‹ ===")
             print(f"ç•¶å‰æˆæœ¬: ${budget_status['current_cost']:.4f}")
             print(f"é ç®—é™åˆ¶: ${budget_status['budget_limit']:.2f}")
             print(f"å‰©é¤˜é ç®—: ${budget_status['budget_remaining']:.2f}")
             print(f"ä½¿ç”¨ç‡: {budget_status['usage_percentage']:.1f}%")
-            print(f"è¶…å‡ºé ç®—: {'æ˜¯' if budget_status['is_over_budget'] else 'å¦'}")
-            print(f"å¯ç¹¼çºŒæ“ä½œ: {'æ˜¯' if budget_status['can_continue'] else 'å¦'}")
+            print(
+                f"è¶…å‡ºé ç®—: {'æ˜¯' if budget_status['is_over_budget'] else 'å¦'}"
+            )
+            print(
+                f"å¯ç¹¼çºŒæ“ä½œ: {'æ˜¯' if budget_status['can_continue'] else 'å¦'}"
+            )
         else:
             print("æˆæœ¬ç›£æ§ç³»çµ±ä¸å¯ç”¨")
         return
@@ -812,7 +962,9 @@ async def main():
             if weekly_report["daily_stats"]:
                 print("\næ¯æ—¥çµ±è¨ˆ:")
                 for date_str, stats in weekly_report["daily_stats"].items():
-                    print(f"  {date_str}: ${stats['cost']:.4f} ({stats['calls']} æ¬¡å‘¼å«)")
+                    print(
+                        f"  {date_str}: ${stats['cost']:.4f} ({stats['calls']} æ¬¡å‘¼å«)"
+                    )
         else:
             print("æˆæœ¬ç›£æ§ç³»çµ±ä¸å¯ç”¨")
         return
@@ -820,7 +972,9 @@ async def main():
     if args.export_costs:
         if COST_MONITORING_AVAILABLE and generator.cost_tracker:
             print(f"\n=== åŒ¯å‡º {args.export_costs} å¤©æˆæœ¬æ•¸æ“š ===")
-            export_file = await generator.cost_tracker.export_cost_data(args.export_costs)
+            export_file = await generator.cost_tracker.export_cost_data(
+                args.export_costs
+            )
             print(f"æˆæœ¬æ•¸æ“šå·²åŒ¯å‡ºè‡³: {export_file}")
         else:
             print("æˆæœ¬ç›£æ§ç³»çµ±ä¸å¯ç”¨")
diff --git a/auto_generate_video_fold6/scripts/backup/backup_system.py b/auto_generate_video_fold6/scripts/backup/backup_system.py
index 5e13e89..e7b7b15 100644
--- a/auto_generate_video_fold6/scripts/backup/backup_system.py
+++ b/auto_generate_video_fold6/scripts/backup/backup_system.py
@@ -94,7 +94,10 @@ class BackupSystem:
                 "retention_policy": {"daily": 7, "weekly": 4, "monthly": 12},
             },
             "storage": {
-                "local": {"enabled": True, "path": "/var/backups/auto-video/local"},
+                "local": {
+                    "enabled": True,
+                    "path": "/var/backups/auto-video/local",
+                },
                 "s3": {
                     "enabled": False,
                     "bucket": "auto-video-backups",
@@ -117,7 +120,10 @@ class BackupSystem:
         logging.basicConfig(
             level=logging.INFO,
             format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
-            handlers=[logging.FileHandler("./logs/backup_system.log"), logging.StreamHandler()],
+            handlers=[
+                logging.FileHandler("./logs/backup_system.log"),
+                logging.StreamHandler(),
+            ],
         )
         return logging.getLogger(__name__)
 
@@ -145,10 +151,14 @@ class BackupSystem:
                 "s3",
                 region_name=s3_config.get("region"),
                 aws_access_key_id=os.getenv(
-                    s3_config.get("access_key", "").replace("${", "").replace("}", "")
+                    s3_config.get("access_key", "")
+                    .replace("${", "")
+                    .replace("}", "")
                 ),
                 aws_secret_access_key=os.getenv(
-                    s3_config.get("secret_key", "").replace("${", "").replace("}", "")
+                    s3_config.get("secret_key", "")
+                    .replace("${", "")
+                    .replace("}", "")
                 ),
             )
         return None
@@ -248,7 +258,9 @@ class BackupSystem:
         except Exception:
             return False
 
-    async def _backup_database(self, job: BackupJob, start_time: datetime) -> BackupResult:
+    async def _backup_database(
+        self, job: BackupJob, start_time: datetime
+    ) -> BackupResult:
         """å‚™ä»½è³‡æ–™åº«"""
         timestamp = start_time.strftime("%Y%m%d_%H%M%S")
         backup_filename = f"{job.name}_{timestamp}.sql"
@@ -279,7 +291,9 @@ class BackupSystem:
                 dump_cmd.extend(["--compress=9"])
 
             process = await asyncio.create_subprocess_exec(
-                *dump_cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
+                *dump_cmd,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE,
             )
 
             stdout, stderr = await process.communicate()
@@ -317,7 +331,9 @@ class BackupSystem:
                 backup_path.unlink()
             raise e
 
-    async def _backup_files(self, job: BackupJob, start_time: datetime) -> BackupResult:
+    async def _backup_files(
+        self, job: BackupJob, start_time: datetime
+    ) -> BackupResult:
         """å‚™ä»½æª”æ¡ˆç³»çµ±"""
         timestamp = start_time.strftime("%Y%m%d_%H%M%S")
         backup_filename = f"{job.name}_{timestamp}.tar"
@@ -363,7 +379,9 @@ class BackupSystem:
                 backup_path.unlink()
             raise e
 
-    async def _backup_container(self, job: BackupJob, start_time: datetime) -> BackupResult:
+    async def _backup_container(
+        self, job: BackupJob, start_time: datetime
+    ) -> BackupResult:
         """å‚™ä»½ Docker å®¹å™¨"""
         timestamp = start_time.strftime("%Y%m%d_%H%M%S")
         backup_filename = f"{job.name}_{timestamp}.tar"
@@ -385,14 +403,19 @@ class BackupSystem:
                 )
 
                 gzip_process = await asyncio.create_subprocess_exec(
-                    "gzip", "-c", stdin=process.stdout, stdout=open(backup_path, "wb")
+                    "gzip",
+                    "-c",
+                    stdin=process.stdout,
+                    stdout=open(backup_path, "wb"),
                 )
 
                 await process.wait()
                 await gzip_process.wait()
             else:
                 with open(backup_path, "wb") as f:
-                    process = await asyncio.create_subprocess_exec(*export_cmd, stdout=f)
+                    process = await asyncio.create_subprocess_exec(
+                        *export_cmd, stdout=f
+                    )
                     await process.wait()
 
             if job.encryption:
@@ -422,7 +445,9 @@ class BackupSystem:
                 backup_path.unlink()
             raise e
 
-    async def _backup_configuration(self, job: BackupJob, start_time: datetime) -> BackupResult:
+    async def _backup_configuration(
+        self, job: BackupJob, start_time: datetime
+    ) -> BackupResult:
         """å‚™ä»½é…ç½®æª”æ¡ˆ"""
         timestamp = start_time.strftime("%Y%m%d_%H%M%S")
         backup_filename = f"{job.name}_{timestamp}.tar.gz"
@@ -513,7 +538,9 @@ class BackupSystem:
             raise Exception("Backup file not found")
 
         # é‡æ–°è¨ˆç®—æ ¡é©—å’Œ
-        current_checksum = await self._calculate_checksum(Path(result.backup_path))
+        current_checksum = await self._calculate_checksum(
+            Path(result.backup_path)
+        )
 
         if current_checksum != result.checksum:
             raise Exception("Backup verification failed: checksum mismatch")
@@ -570,9 +597,13 @@ class BackupSystem:
                 await process.wait()
 
                 if process.returncode == 0:
-                    self.logger.info(f"Backup synced to remote: {backup_file.name}")
+                    self.logger.info(
+                        f"Backup synced to remote: {backup_file.name}"
+                    )
                 else:
-                    raise Exception(f"rsync failed with code {process.returncode}")
+                    raise Exception(
+                        f"rsync failed with code {process.returncode}"
+                    )
 
             except Exception as e:
                 self.logger.error(f"Failed to sync to remote: {e}")
@@ -593,12 +624,15 @@ class BackupSystem:
 
                 async with aiohttp.ClientSession() as session:
                     async with session.post(
-                        "http://alertmanager:9093/api/v1/alerts", json=[alert_data]
+                        "http://alertmanager:9093/api/v1/alerts",
+                        json=[alert_data],
                     ) as response:
                         if response.status == 200:
                             self.logger.info("Backup alert sent successfully")
                         else:
-                            self.logger.error(f"Failed to send backup alert: {response.status}")
+                            self.logger.error(
+                                f"Failed to send backup alert: {response.status}"
+                            )
 
             except Exception as e:
                 self.logger.error(f"Failed to send backup alert: {e}")
@@ -608,7 +642,9 @@ class BackupSystem:
         self.logger.info(f"Starting disaster recovery: {recovery_plan}")
 
         # è¼‰å…¥æ¢å¾©è¨ˆåŠƒ
-        plan_path = Path(f"./scripts/backup/recovery_plans/{recovery_plan}.yaml")
+        plan_path = Path(
+            f"./scripts/backup/recovery_plans/{recovery_plan}.yaml"
+        )
 
         if not plan_path.exists():
             raise Exception(f"Recovery plan not found: {recovery_plan}")
diff --git a/auto_generate_video_fold6/scripts/backup_manager.py b/auto_generate_video_fold6/scripts/backup_manager.py
index 5dea99f..a5dfa8d 100755
--- a/auto_generate_video_fold6/scripts/backup_manager.py
+++ b/auto_generate_video_fold6/scripts/backup_manager.py
@@ -89,7 +89,10 @@ class PostgreSQLBackup:
                     pg_dump_cmd,
                     stdout=f,
                     stderr=subprocess.PIPE,
-                    env={**os.environ, "PGPASSWORD": self.db_config.get("password", "")},
+                    env={
+                        **os.environ,
+                        "PGPASSWORD": self.db_config.get("password", ""),
+                    },
                 )
 
                 _, stderr = process.communicate()
@@ -101,7 +104,9 @@ class PostgreSQLBackup:
             file_size = os.path.getsize(backup_file)
             checksum = await self._calculate_checksum(backup_file)
 
-            logger.info(f"PostgreSQL å‚™ä»½å®Œæˆ: {backup_file} ({file_size} bytes)")
+            logger.info(
+                f"PostgreSQL å‚™ä»½å®Œæˆ: {backup_file} ({file_size} bytes)"
+            )
 
             return {
                 "success": True,
@@ -137,7 +142,10 @@ class PostgreSQLBackup:
                     stdin=f,
                     stdout=subprocess.PIPE,
                     stderr=subprocess.PIPE,
-                    env={**os.environ, "PGPASSWORD": self.db_config.get("password", "")},
+                    env={
+                        **os.environ,
+                        "PGPASSWORD": self.db_config.get("password", ""),
+                    },
                 )
 
                 stdout, stderr = process.communicate()
@@ -189,14 +197,20 @@ class RedisBackup:
                 await asyncio.sleep(1)
 
             # è¤‡è£½ RDB æª”æ¡ˆ
-            redis_dump_path = self.redis_config.get("dump_dir", "/var/lib/redis/dump.rdb")
+            redis_dump_path = self.redis_config.get(
+                "dump_dir", "/var/lib/redis/dump.rdb"
+            )
             if os.path.exists(redis_dump_path):
-                subprocess.run(["cp", redis_dump_path, backup_file], check=True)
+                subprocess.run(
+                    ["cp", redis_dump_path, backup_file], check=True
+                )
 
                 file_size = os.path.getsize(backup_file)
                 checksum = await self._calculate_checksum(backup_file)
 
-                logger.info(f"Redis å‚™ä»½å®Œæˆ: {backup_file} ({file_size} bytes)")
+                logger.info(
+                    f"Redis å‚™ä»½å®Œæˆ: {backup_file} ({file_size} bytes)"
+                )
 
                 return {
                     "success": True,
@@ -228,7 +242,10 @@ class FileSystemBackup:
         self.config = config
 
     async def create_backup(
-        self, source_paths: List[str], backup_path: str, exclude_patterns: List[str] = None
+        self,
+        source_paths: List[str],
+        backup_path: str,
+        exclude_patterns: List[str] = None,
     ) -> Dict[str, Any]:
         """å‰µå»ºæª”æ¡ˆç³»çµ±å‚™ä»½"""
 
@@ -245,7 +262,9 @@ class FileSystemBackup:
                         tar.add(
                             source_path,
                             arcname=os.path.basename(source_path),
-                            exclude=self._create_exclude_filter(exclude_patterns or []),
+                            exclude=self._create_exclude_filter(
+                                exclude_patterns or []
+                            ),
                         )
 
             file_size = os.path.getsize(backup_file)
@@ -299,10 +318,14 @@ class S3BackupStorage:
         )
         self.bucket_name = self.s3_config.get("bucket_name")
 
-    async def upload_backup(self, local_file: str, s3_key: str) -> Dict[str, Any]:
+    async def upload_backup(
+        self, local_file: str, s3_key: str
+    ) -> Dict[str, Any]:
         """ä¸Šå‚³å‚™ä»½åˆ° S3"""
         try:
-            logger.info(f"ä¸Šå‚³å‚™ä»½åˆ° S3: {local_file} -> s3://{self.bucket_name}/{s3_key}")
+            logger.info(
+                f"ä¸Šå‚³å‚™ä»½åˆ° S3: {local_file} -> s3://{self.bucket_name}/{s3_key}"
+            )
 
             # ä¸Šå‚³æª”æ¡ˆ
             self.s3_client.upload_file(local_file, self.bucket_name, s3_key)
@@ -314,7 +337,10 @@ class S3BackupStorage:
                 Tagging={
                     "TagSet": [
                         {"Key": "BackupType", "Value": "AutoVideoSystem"},
-                        {"Key": "CreatedDate", "Value": datetime.utcnow().isoformat()},
+                        {
+                            "Key": "CreatedDate",
+                            "Value": datetime.utcnow().isoformat(),
+                        },
                     ]
                 },
             )
@@ -326,10 +352,14 @@ class S3BackupStorage:
             logger.error(f"S3 ä¸Šå‚³å¤±æ•—: {e}")
             return {"success": False, "error": str(e)}
 
-    async def download_backup(self, s3_key: str, local_file: str) -> Dict[str, Any]:
+    async def download_backup(
+        self, s3_key: str, local_file: str
+    ) -> Dict[str, Any]:
         """å¾ S3 ä¸‹è¼‰å‚™ä»½"""
         try:
-            logger.info(f"å¾ S3 ä¸‹è¼‰å‚™ä»½: s3://{self.bucket_name}/{s3_key} -> {local_file}")
+            logger.info(
+                f"å¾ S3 ä¸‹è¼‰å‚™ä»½: s3://{self.bucket_name}/{s3_key} -> {local_file}"
+            )
 
             self.s3_client.download_file(self.bucket_name, s3_key, local_file)
 
@@ -380,7 +410,9 @@ class BackupManager:
             "s3": {
                 "access_key_id": os.getenv("AWS_ACCESS_KEY_ID", ""),
                 "secret_access_key": os.getenv("AWS_SECRET_ACCESS_KEY", ""),
-                "bucket_name": os.getenv("S3_BACKUP_BUCKET", "auto-video-backups"),
+                "bucket_name": os.getenv(
+                    "S3_BACKUP_BUCKET", "auto-video-backups"
+                ),
                 "region": "us-west-2",
             },
             "backup": {
@@ -424,7 +456,12 @@ class BackupManager:
 
             # 3. æª”æ¡ˆç³»çµ±å‚™ä»½
             logger.info("åŸ·è¡Œæª”æ¡ˆç³»çµ±å‚™ä»½...")
-            filesystem_paths = ["./config", "./services", "./monitoring", "./scripts"]
+            filesystem_paths = [
+                "./config",
+                "./services",
+                "./monitoring",
+                "./scripts",
+            ]
             exclude_patterns = ["__pycache__", ".git", "node_modules", "*.log"]
             fs_result = await self.filesystem_backup.create_backup(
                 filesystem_paths, backup_path, exclude_patterns
@@ -439,10 +476,16 @@ class BackupManager:
                 for component, result in backup_results["components"].items():
                     if result.get("success") and "backup_file" in result:
                         backup_file = result["backup_file"]
-                        s3_key = f"backups/{job_id}/{os.path.basename(backup_file)}"
+                        s3_key = (
+                            f"backups/{job_id}/{os.path.basename(backup_file)}"
+                        )
 
-                        s3_result = await self.s3_storage.upload_backup(backup_file, s3_key)
-                        s3_results.append({"component": component, "s3_result": s3_result})
+                        s3_result = await self.s3_storage.upload_backup(
+                            backup_file, s3_key
+                        )
+                        s3_results.append(
+                            {"component": component, "s3_result": s3_result}
+                        )
 
                 backup_results["s3_uploads"] = s3_results
 
@@ -473,7 +516,9 @@ class BackupManager:
                 file_path = os.path.join(backup_path, filename)
                 if os.path.isfile(file_path):
                     # æª¢æŸ¥æª”æ¡ˆä¿®æ”¹æ™‚é–“
-                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
+                    file_time = datetime.fromtimestamp(
+                        os.path.getmtime(file_path)
+                    )
                     if file_time < cutoff_date:
                         os.remove(file_path)
                         logger.info(f"åˆªé™¤éæœŸå‚™ä»½: {filename}")
@@ -481,7 +526,9 @@ class BackupManager:
         except Exception as e:
             logger.error(f"æ¸…ç†å‚™ä»½å¤±æ•—: {e}")
 
-    async def verify_backup_integrity(self, backup_file: str, expected_checksum: str) -> bool:
+    async def verify_backup_integrity(
+        self, backup_file: str, expected_checksum: str
+    ) -> bool:
         """é©—è­‰å‚™ä»½å®Œæ•´æ€§"""
 
         try:
@@ -520,9 +567,13 @@ async def main():
         # é¡¯ç¤ºå„çµ„ä»¶å‚™ä»½çµæœ
         for component, details in result["components"].items():
             if details.get("success"):
-                print(f"  âœ… {component}: {details.get('size_bytes', 0)} bytes")
+                print(
+                    f"  âœ… {component}: {details.get('size_bytes', 0)} bytes"
+                )
             else:
-                print(f"  âŒ {component}: {details.get('error', 'Unknown error')}")
+                print(
+                    f"  âŒ {component}: {details.get('error', 'Unknown error')}"
+                )
     else:
         print(f"âŒ å‚™ä»½å¤±æ•—: {result.get('error', 'Unknown error')}")
 
diff --git a/auto_generate_video_fold6/scripts/config-validator.py b/auto_generate_video_fold6/scripts/config-validator.py
index 16c2566..f4afd13 100644
--- a/auto_generate_video_fold6/scripts/config-validator.py
+++ b/auto_generate_video_fold6/scripts/config-validator.py
@@ -16,70 +16,107 @@ from jsonschema import validate, ValidationError
 
 class ConfigValidator:
     """é…ç½®é©—è­‰å™¨"""
-    
+
     def __init__(self, config_dir: str = None):
         self.config_dir = Path(config_dir or "config")
         self.schemas = {}
         self.validation_results = {}
         self.load_schemas()
-    
+
     def load_schemas(self):
         """è¼‰å…¥é…ç½®æ¨¡å¼å®šç¾©"""
         self.schemas = {
             "base": {
                 "type": "object",
-                "required": ["generation", "ai_services", "cost_control", "resources"],
+                "required": [
+                    "generation",
+                    "ai_services",
+                    "cost_control",
+                    "resources",
+                ],
                 "properties": {
                     "generation": {
                         "type": "object",
                         "required": ["daily_video_limit", "platforms"],
                         "properties": {
-                            "daily_video_limit": {"type": "integer", "minimum": 1},
-                            "platforms": {"type": "array", "items": {"type": "string"}},
-                            "max_duration_seconds": {"type": "integer", "minimum": 10}
-                        }
+                            "daily_video_limit": {
+                                "type": "integer",
+                                "minimum": 1,
+                            },
+                            "platforms": {
+                                "type": "array",
+                                "items": {"type": "string"},
+                            },
+                            "max_duration_seconds": {
+                                "type": "integer",
+                                "minimum": 10,
+                            },
+                        },
                     },
                     "ai_services": {
                         "type": "object",
                         "required": ["text_generation", "image_generation"],
                         "properties": {
                             "text_generation": {
-                                "type": "object", 
+                                "type": "object",
                                 "required": ["provider"],
                                 "properties": {
-                                    "provider": {"type": "string", "enum": ["openai", "gemini", "claude"]},
+                                    "provider": {
+                                        "type": "string",
+                                        "enum": ["openai", "gemini", "claude"],
+                                    },
                                     "model": {"type": "string"},
-                                    "max_tokens": {"type": "integer", "minimum": 100}
-                                }
+                                    "max_tokens": {
+                                        "type": "integer",
+                                        "minimum": 100,
+                                    },
+                                },
                             },
                             "image_generation": {
                                 "type": "object",
                                 "required": ["provider"],
                                 "properties": {
-                                    "provider": {"type": "string", "enum": ["dalle", "midjourney", "stable-diffusion"]},
+                                    "provider": {
+                                        "type": "string",
+                                        "enum": [
+                                            "dalle",
+                                            "midjourney",
+                                            "stable-diffusion",
+                                        ],
+                                    },
                                     "resolution": {"type": "string"},
-                                    "quality": {"type": "string", "enum": ["standard", "high", "premium"]}
-                                }
-                            }
-                        }
+                                    "quality": {
+                                        "type": "string",
+                                        "enum": [
+                                            "standard",
+                                            "high",
+                                            "premium",
+                                        ],
+                                    },
+                                },
+                            },
+                        },
                     },
                     "cost_control": {
                         "type": "object",
                         "required": ["daily_budget_usd"],
                         "properties": {
-                            "daily_budget_usd": {"type": "number", "minimum": 0},
-                            "api_rate_limits": {"type": "object"}
-                        }
+                            "daily_budget_usd": {
+                                "type": "number",
+                                "minimum": 0,
+                            },
+                            "api_rate_limits": {"type": "object"},
+                        },
                     },
                     "resources": {
                         "type": "object",
                         "properties": {
                             "max_memory_usage": {"type": "string"},
                             "max_cpu_cores": {"type": "integer", "minimum": 1},
-                            "disk_space_limit": {"type": "string"}
-                        }
-                    }
-                }
+                            "disk_space_limit": {"type": "string"},
+                        },
+                    },
+                },
             },
             "auth": {
                 "type": "object",
@@ -87,52 +124,73 @@ class ConfigValidator:
                 "properties": {
                     "jwt": {
                         "type": "object",
-                        "required": ["secret_key", "algorithm", "expire_minutes"],
+                        "required": [
+                            "secret_key",
+                            "algorithm",
+                            "expire_minutes",
+                        ],
                         "properties": {
                             "secret_key": {"type": "string", "minLength": 32},
-                            "algorithm": {"type": "string", "enum": ["HS256", "RS256"]},
-                            "expire_minutes": {"type": "integer", "minimum": 15}
-                        }
+                            "algorithm": {
+                                "type": "string",
+                                "enum": ["HS256", "RS256"],
+                            },
+                            "expire_minutes": {
+                                "type": "integer",
+                                "minimum": 15,
+                            },
+                        },
                     },
                     "oauth": {
                         "type": "object",
                         "properties": {
                             "google": {"type": "object"},
                             "github": {"type": "object"},
-                            "discord": {"type": "object"}
-                        }
+                            "discord": {"type": "object"},
+                        },
                     },
                     "security": {
                         "type": "object",
                         "properties": {
-                            "password_min_length": {"type": "integer", "minimum": 8},
-                            "max_login_attempts": {"type": "integer", "minimum": 3},
-                            "lockout_duration_minutes": {"type": "integer", "minimum": 5}
-                        }
-                    }
-                }
-            }
+                            "password_min_length": {
+                                "type": "integer",
+                                "minimum": 8,
+                            },
+                            "max_login_attempts": {
+                                "type": "integer",
+                                "minimum": 3,
+                            },
+                            "lockout_duration_minutes": {
+                                "type": "integer",
+                                "minimum": 5,
+                            },
+                        },
+                    },
+                },
+            },
         }
-    
-    def validate_config_file(self, config_file: Path) -> Tuple[bool, List[str]]:
+
+    def validate_config_file(
+        self, config_file: Path
+    ) -> Tuple[bool, List[str]]:
         """é©—è­‰å–®å€‹é…ç½®æ–‡ä»¶"""
         errors = []
-        
+
         if not config_file.exists():
             return False, [f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}"]
-        
+
         try:
-            with open(config_file, 'r', encoding='utf-8') as f:
+            with open(config_file, "r", encoding="utf-8") as f:
                 config_data = json.load(f)
         except json.JSONDecodeError as e:
             return False, [f"JSON æ ¼å¼éŒ¯èª¤: {e}"]
         except Exception as e:
             return False, [f"è®€å–æ–‡ä»¶å¤±æ•—: {e}"]
-        
+
         # æ ¹æ“šæ–‡ä»¶åé¸æ“‡æ¨¡å¼
-        config_type = config_file.stem.replace('-config', '')
+        config_type = config_file.stem.replace("-config", "")
         schema = self.schemas.get(config_type)
-        
+
         if schema:
             try:
                 validate(instance=config_data, schema=schema)
@@ -140,48 +198,59 @@ class ConfigValidator:
                 errors.append(f"æ¨¡å¼é©—è­‰å¤±æ•—: {e.message}")
             except Exception as e:
                 errors.append(f"é©—è­‰éç¨‹éŒ¯èª¤: {e}")
-        
+
         # è‡ªå®šç¾©é©—è­‰è¦å‰‡
         custom_errors = self._custom_validation(config_data, config_type)
         errors.extend(custom_errors)
-        
+
         return len(errors) == 0, errors
-    
-    def _custom_validation(self, config_data: Dict, config_type: str) -> List[str]:
+
+    def _custom_validation(
+        self, config_data: Dict, config_type: str
+    ) -> List[str]:
         """è‡ªå®šç¾©é©—è­‰è¦å‰‡"""
         errors = []
-        
+
         if config_type == "base":
             # æª¢æŸ¥å¹³å°é…ç½®
             platforms = config_data.get("generation", {}).get("platforms", [])
             valid_platforms = ["tiktok", "youtube", "instagram", "twitter"]
-            
+
             for platform in platforms:
                 if platform not in valid_platforms:
                     errors.append(f"ä¸æ”¯æ´çš„å¹³å°: {platform}")
-            
+
             # æª¢æŸ¥é ç®—å’Œé™åˆ¶çš„åˆç†æ€§
-            daily_limit = config_data.get("generation", {}).get("daily_video_limit", 0)
-            daily_budget = config_data.get("cost_control", {}).get("daily_budget_usd", 0)
-            
+            daily_limit = config_data.get("generation", {}).get(
+                "daily_video_limit", 0
+            )
+            daily_budget = config_data.get("cost_control", {}).get(
+                "daily_budget_usd", 0
+            )
+
             estimated_cost_per_video = 0.50  # å‡è¨­æ¯å€‹å½±ç‰‡æˆæœ¬
-            if daily_budget > 0 and daily_limit * estimated_cost_per_video > daily_budget:
-                errors.append(f"æ¯æ—¥å½±ç‰‡é™åˆ¶({daily_limit})å¯èƒ½è¶…å‡ºé ç®—({daily_budget} USD)")
-        
+            if (
+                daily_budget > 0
+                and daily_limit * estimated_cost_per_video > daily_budget
+            ):
+                errors.append(
+                    f"æ¯æ—¥å½±ç‰‡é™åˆ¶({daily_limit})å¯èƒ½è¶…å‡ºé ç®—({daily_budget} USD)"
+                )
+
         elif config_type == "auth":
             # æª¢æŸ¥ JWT å¯†é‘°å¼·åº¦
             jwt_secret = config_data.get("jwt", {}).get("secret_key", "")
             if len(jwt_secret) < 32:
                 errors.append("JWT å¯†é‘°é•·åº¦æ‡‰è‡³å°‘ 32 å­—ç¬¦")
-            
+
             # æª¢æŸ¥å®‰å…¨è¨­ç½®
             security = config_data.get("security", {})
             password_min_length = security.get("password_min_length", 0)
             if password_min_length < 8:
                 errors.append("å¯†ç¢¼æœ€å°é•·åº¦æ‡‰è‡³å°‘ 8 å­—ç¬¦")
-        
+
         return errors
-    
+
     def validate_all_configs(self) -> Dict[str, Any]:
         """é©—è­‰æ‰€æœ‰é…ç½®æ–‡ä»¶"""
         results = {
@@ -189,86 +258,102 @@ class ConfigValidator:
             "total_files": 0,
             "valid_files": 0,
             "invalid_files": 0,
-            "files": {}
+            "files": {},
         }
-        
+
         config_files = list(self.config_dir.glob("*-config.json"))
         results["total_files"] = len(config_files)
-        
+
         for config_file in config_files:
             is_valid, errors = self.validate_config_file(config_file)
-            
+
             results["files"][config_file.name] = {
                 "valid": is_valid,
                 "errors": errors,
                 "size": config_file.stat().st_size,
-                "modified": datetime.fromtimestamp(config_file.stat().st_mtime).isoformat()
+                "modified": datetime.fromtimestamp(
+                    config_file.stat().st_mtime
+                ).isoformat(),
             }
-            
+
             if is_valid:
                 results["valid_files"] += 1
             else:
                 results["invalid_files"] += 1
-        
+
         self.validation_results = results
         return results
-    
+
     def check_config_consistency(self) -> Dict[str, Any]:
         """æª¢æŸ¥é…ç½®é–“çš„ä¸€è‡´æ€§"""
         consistency_issues = []
-        
+
         # è¼‰å…¥ä¸»è¦é…ç½®æ–‡ä»¶
         base_config = self._load_config("base-config.json")
         auth_config = self._load_config("auth-config.json")
         enterprise_config = self._load_config("enterprise-config.json")
-        
+
         if not all([base_config, auth_config]):
             return {"issues": ["ç„¡æ³•è¼‰å…¥å¿…è¦çš„é…ç½®æ–‡ä»¶"]}
-        
+
         # æª¢æŸ¥æœå‹™ç«¯å£æ˜¯å¦è¡çª
         ports_used = set()
         for config_name, config_data in [
-            ("base", base_config), 
-            ("auth", auth_config), 
-            ("enterprise", enterprise_config)
+            ("base", base_config),
+            ("auth", auth_config),
+            ("enterprise", enterprise_config),
         ]:
             if config_data and "services" in config_data:
                 for service, service_config in config_data["services"].items():
                     if "port" in service_config:
                         port = service_config["port"]
                         if port in ports_used:
-                            consistency_issues.append(f"ç«¯å£è¡çª: {port} åœ¨å¤šå€‹æœå‹™ä¸­ä½¿ç”¨")
+                            consistency_issues.append(
+                                f"ç«¯å£è¡çª: {port} åœ¨å¤šå€‹æœå‹™ä¸­ä½¿ç”¨"
+                            )
                         else:
                             ports_used.add(port)
-        
+
         # æª¢æŸ¥è³‡æºé™åˆ¶çš„ä¸€è‡´æ€§
         if base_config and enterprise_config:
-            base_memory = base_config.get("resources", {}).get("max_memory_usage")
-            enterprise_memory = enterprise_config.get("resources", {}).get("max_memory_usage")
-            
-            if base_memory and enterprise_memory and base_memory != enterprise_memory:
-                consistency_issues.append(f"è¨˜æ†¶é«”é™åˆ¶ä¸ä¸€è‡´: base({base_memory}) vs enterprise({enterprise_memory})")
-        
+            base_memory = base_config.get("resources", {}).get(
+                "max_memory_usage"
+            )
+            enterprise_memory = enterprise_config.get("resources", {}).get(
+                "max_memory_usage"
+            )
+
+            if (
+                base_memory
+                and enterprise_memory
+                and base_memory != enterprise_memory
+            ):
+                consistency_issues.append(
+                    f"è¨˜æ†¶é«”é™åˆ¶ä¸ä¸€è‡´: base({base_memory}) vs enterprise({enterprise_memory})"
+                )
+
         return {
             "timestamp": datetime.now().isoformat(),
             "issues_count": len(consistency_issues),
-            "issues": consistency_issues
+            "issues": consistency_issues,
         }
-    
+
     def _load_config(self, filename: str) -> Dict[str, Any]:
         """è¼‰å…¥é…ç½®æ–‡ä»¶"""
         config_path = self.config_dir / filename
-        
+
         if not config_path.exists():
             return None
-        
+
         try:
-            with open(config_path, 'r', encoding='utf-8') as f:
+            with open(config_path, "r", encoding="utf-8") as f:
                 return json.load(f)
         except:
             return None
-    
-    def generate_config_template(self, config_type: str, output_file: str = None) -> str:
+
+    def generate_config_template(
+        self, config_type: str, output_file: str = None
+    ) -> str:
         """ç”Ÿæˆé…ç½®æ¨¡æ¿"""
         templates = {
             "base": {
@@ -276,86 +361,85 @@ class ConfigValidator:
                     "daily_video_limit": 50,
                     "platforms": ["tiktok", "youtube", "instagram"],
                     "max_duration_seconds": 300,
-                    "quality": "high"
+                    "quality": "high",
                 },
                 "ai_services": {
                     "text_generation": {
                         "provider": "gemini",
                         "model": "gemini-pro",
-                        "max_tokens": 2000
+                        "max_tokens": 2000,
                     },
                     "image_generation": {
                         "provider": "stable-diffusion",
                         "resolution": "1024x1024",
-                        "quality": "high"
+                        "quality": "high",
                     },
                     "voice_cloning": {
                         "provider": "elevenlabs",
                         "voice_stability": 0.8,
-                        "voice_similarity": 0.8
-                    }
+                        "voice_similarity": 0.8,
+                    },
                 },
                 "cost_control": {
                     "daily_budget_usd": 25.0,
                     "api_rate_limits": {
                         "gemini_requests_per_hour": 100,
                         "dalle_requests_per_hour": 50,
-                        "elevenlabs_requests_per_hour": 200
-                    }
+                        "elevenlabs_requests_per_hour": 200,
+                    },
                 },
                 "resources": {
                     "max_memory_usage": "4GB",
                     "max_cpu_cores": 4,
-                    "disk_space_limit": "100GB"
+                    "disk_space_limit": "100GB",
                 },
                 "scheduling": {
                     "enabled": True,
-                    "work_hours": {
-                        "start": "09:00",
-                        "end": "18:00"
-                    },
-                    "auto_publish": False
-                }
+                    "work_hours": {"start": "09:00", "end": "18:00"},
+                    "auto_publish": False,
+                },
             },
             "auth": {
                 "jwt": {
                     "secret_key": "your-super-secret-jwt-key-32chars-min",
                     "algorithm": "HS256",
-                    "expire_minutes": 60
+                    "expire_minutes": 60,
                 },
                 "oauth": {
                     "google": {
                         "client_id": "your-google-client-id",
-                        "client_secret": "your-google-client-secret"
+                        "client_secret": "your-google-client-secret",
                     },
                     "github": {
                         "client_id": "your-github-client-id",
-                        "client_secret": "your-github-client-secret"
-                    }
+                        "client_secret": "your-github-client-secret",
+                    },
                 },
                 "security": {
                     "password_min_length": 8,
                     "max_login_attempts": 5,
                     "lockout_duration_minutes": 15,
-                    "require_email_verification": True
-                }
-            }
+                    "require_email_verification": True,
+                },
+            },
         }
-        
+
         template = templates.get(config_type)
         if not template:
             raise ValueError(f"ä¸æ”¯æ´çš„é…ç½®é¡å‹: {config_type}")
-        
+
         if output_file is None:
-            output_file = self.config_dir / f"{config_type}-config.template.json"
+            output_file = (
+                self.config_dir / f"{config_type}-config.template.json"
+            )
         else:
             output_file = Path(output_file)
-        
-        with open(output_file, 'w', encoding='utf-8') as f:
+
+        with open(output_file, "w", encoding="utf-8") as f:
             json.dump(template, f, indent=2, ensure_ascii=False)
-        
+
         return str(output_file)
-    
+
     def sync_configs_with_environment(self) -> Dict[str, Any]:
         """åŒæ­¥é…ç½®èˆ‡ç’°å¢ƒè®Šæ•¸"""
         env_mappings = {
@@ -363,61 +447,67 @@ class ConfigValidator:
             "DAILY_VIDEO_LIMIT": "base.generation.daily_video_limit",
             "DAILY_BUDGET": "base.cost_control.daily_budget_usd",
             "AI_TEXT_PROVIDER": "base.ai_services.text_generation.provider",
-            "AI_IMAGE_PROVIDER": "base.ai_services.image_generation.provider"
+            "AI_IMAGE_PROVIDER": "base.ai_services.image_generation.provider",
         }
-        
+
         sync_results = {
             "timestamp": datetime.now().isoformat(),
             "synced_vars": [],
             "missing_vars": [],
-            "conflicts": []
+            "conflicts": [],
         }
-        
+
         for env_var, config_path in env_mappings.items():
             env_value = os.getenv(env_var)
-            
+
             if env_value:
-                config_file, config_key = config_path.split('.', 1)
+                config_file, config_key = config_path.split(".", 1)
                 config_data = self._load_config(f"{config_file}-config.json")
-                
+
                 if config_data:
                     # ç²å–é…ç½®ä¸­çš„ç•¶å‰å€¼
-                    current_value = self._get_nested_value(config_data, config_key)
-                    
+                    current_value = self._get_nested_value(
+                        config_data, config_key
+                    )
+
                     if str(current_value) != env_value:
-                        sync_results["conflicts"].append({
-                            "env_var": env_var,
-                            "env_value": env_value,
-                            "config_value": current_value,
-                            "config_path": config_path
-                        })
+                        sync_results["conflicts"].append(
+                            {
+                                "env_var": env_var,
+                                "env_value": env_value,
+                                "config_value": current_value,
+                                "config_path": config_path,
+                            }
+                        )
                     else:
                         sync_results["synced_vars"].append(env_var)
                 else:
-                    sync_results["missing_vars"].append(f"é…ç½®æ–‡ä»¶ {config_file}-config.json ä¸å­˜åœ¨")
+                    sync_results["missing_vars"].append(
+                        f"é…ç½®æ–‡ä»¶ {config_file}-config.json ä¸å­˜åœ¨"
+                    )
             else:
                 sync_results["missing_vars"].append(env_var)
-        
+
         return sync_results
-    
+
     def _get_nested_value(self, data: Dict, key_path: str):
         """ç²å–åµŒå¥—å­—å…¸å€¼"""
-        keys = key_path.split('.')
+        keys = key_path.split(".")
         current = data
-        
+
         try:
             for key in keys:
                 current = current[key]
             return current
         except (KeyError, TypeError):
             return None
-    
+
     def generate_report(self, output_file: str = None) -> str:
         """ç”Ÿæˆå®Œæ•´çš„é…ç½®é©—è­‰å ±å‘Š"""
         validation_results = self.validate_all_configs()
         consistency_results = self.check_config_consistency()
         sync_results = self.sync_configs_with_environment()
-        
+
         report = {
             "timestamp": datetime.now().isoformat(),
             "summary": {
@@ -425,95 +515,103 @@ class ConfigValidator:
                 "valid_files": validation_results["valid_files"],
                 "invalid_files": validation_results["invalid_files"],
                 "consistency_issues": consistency_results["issues_count"],
-                "sync_conflicts": len(sync_results.get("conflicts", []))
+                "sync_conflicts": len(sync_results.get("conflicts", [])),
             },
             "validation": validation_results,
             "consistency": consistency_results,
             "environment_sync": sync_results,
             "recommendations": self._generate_recommendations(
                 validation_results, consistency_results, sync_results
-            )
+            ),
         }
-        
+
         if output_file is None:
             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
             output_file = f"config_validation_report_{timestamp}.json"
-        
+
         output_path = Path(output_file)
-        with open(output_path, 'w', encoding='utf-8') as f:
+        with open(output_path, "w", encoding="utf-8") as f:
             json.dump(report, f, indent=2, ensure_ascii=False)
-        
+
         return str(output_path)
-    
-    def _generate_recommendations(self, validation_results: Dict, 
-                                consistency_results: Dict, sync_results: Dict) -> List[str]:
+
+    def _generate_recommendations(
+        self,
+        validation_results: Dict,
+        consistency_results: Dict,
+        sync_results: Dict,
+    ) -> List[str]:
         """ç”Ÿæˆæ”¹é€²å»ºè­°"""
         recommendations = []
-        
+
         if validation_results["invalid_files"] > 0:
-            recommendations.append("ä¿®å¾©é…ç½®æ–‡ä»¶é©—è­‰éŒ¯èª¤ï¼Œç¢ºä¿æ‰€æœ‰é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¢º")
-        
+            recommendations.append(
+                "ä¿®å¾©é…ç½®æ–‡ä»¶é©—è­‰éŒ¯èª¤ï¼Œç¢ºä¿æ‰€æœ‰é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¢º"
+            )
+
         if consistency_results["issues_count"] > 0:
             recommendations.append("è§£æ±ºé…ç½®é–“çš„ä¸€è‡´æ€§å•é¡Œï¼Œé¿å…æœå‹™è¡çª")
-        
+
         if len(sync_results.get("conflicts", [])) > 0:
             recommendations.append("åŒæ­¥ç’°å¢ƒè®Šæ•¸èˆ‡é…ç½®æ–‡ä»¶ï¼Œç¢ºä¿éƒ¨ç½²ä¸€è‡´æ€§")
-        
+
         if len(sync_results.get("missing_vars", [])) > 0:
             recommendations.append("è¨­ç½®ç¼ºå¤±çš„ç’°å¢ƒè®Šæ•¸ï¼Œå®Œå–„éƒ¨ç½²é…ç½®")
-        
+
         if not recommendations:
             recommendations.append("é…ç½®ç³»çµ±é‹è¡Œè‰¯å¥½ï¼Œå»ºè­°å®šæœŸåŸ·è¡Œé©—è­‰æª¢æŸ¥")
-        
+
         return recommendations
-    
+
     def print_summary(self):
         """æ‰“å°é…ç½®é©—è­‰æ‘˜è¦"""
         if not self.validation_results:
             self.validate_all_configs()
-        
+
         results = self.validation_results
-        
-        print("\n" + "="*60)
+
+        print("\n" + "=" * 60)
         print("âš™ï¸  é…ç½®é©—è­‰æ‘˜è¦å ±å‘Š")
-        print("="*60)
+        print("=" * 60)
         print(f"ğŸ“„ ç¸½é…ç½®æ–‡ä»¶: {results['total_files']}")
         print(f"âœ… æœ‰æ•ˆæ–‡ä»¶: {results['valid_files']}")
         print(f"âŒ ç„¡æ•ˆæ–‡ä»¶: {results['invalid_files']}")
-        
-        if results['invalid_files'] > 0:
+
+        if results["invalid_files"] > 0:
             print("\nğŸš¨ é…ç½®éŒ¯èª¤:")
-            for filename, file_info in results['files'].items():
-                if not file_info['valid']:
+            for filename, file_info in results["files"].items():
+                if not file_info["valid"]:
                     print(f"  âŒ {filename}:")
-                    for error in file_info['errors']:
+                    for error in file_info["errors"]:
                         print(f"     - {error}")
-        
+
         consistency = self.check_config_consistency()
-        if consistency['issues_count'] > 0:
+        if consistency["issues_count"] > 0:
             print(f"\nâš ï¸  ä¸€è‡´æ€§å•é¡Œ ({consistency['issues_count']} å€‹):")
-            for issue in consistency['issues']:
+            for issue in consistency["issues"]:
                 print(f"  - {issue}")
-        
+
         sync = self.sync_configs_with_environment()
-        if sync.get('conflicts'):
+        if sync.get("conflicts"):
             print(f"\nğŸ”„ ç’°å¢ƒåŒæ­¥è¡çª ({len(sync['conflicts'])} å€‹):")
-            for conflict in sync['conflicts']:
-                print(f"  - {conflict['env_var']}: ç’°å¢ƒ({conflict['env_value']}) â‰  é…ç½®({conflict['config_value']})")
+            for conflict in sync["conflicts"]:
+                print(
+                    f"  - {conflict['env_var']}: ç’°å¢ƒ({conflict['env_value']}) â‰  é…ç½®({conflict['config_value']})"
+                )
 
 
 if __name__ == "__main__":
     import argparse
-    
+
     parser = argparse.ArgumentParser(description="é…ç½®é©—è­‰å’ŒåŒæ­¥å·¥å…·")
     parser.add_argument("--config-dir", default="config", help="é…ç½®æ–‡ä»¶ç›®éŒ„")
     parser.add_argument("--report", help="ç”Ÿæˆå ±å‘Šæ–‡ä»¶è·¯å¾‘")
     parser.add_argument("--template", help="ç”Ÿæˆé…ç½®æ¨¡æ¿ (base|auth)")
-    
+
     args = parser.parse_args()
-    
+
     validator = ConfigValidator(args.config_dir)
-    
+
     if args.template:
         try:
             template_file = validator.generate_config_template(args.template)
@@ -523,7 +621,7 @@ if __name__ == "__main__":
             sys.exit(1)
     else:
         validator.print_summary()
-        
+
         if args.report:
             report_file = validator.generate_report(args.report)
-            print(f"\nğŸ“Š è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_file}")
\ No newline at end of file
+            print(f"\nğŸ“Š è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_file}")
diff --git a/auto_generate_video_fold6/scripts/deploy_enterprise_system.py b/auto_generate_video_fold6/scripts/deploy_enterprise_system.py
index e4d8205..47f3fa3 100644
--- a/auto_generate_video_fold6/scripts/deploy_enterprise_system.py
+++ b/auto_generate_video_fold6/scripts/deploy_enterprise_system.py
@@ -54,8 +54,16 @@ class EnterpriseSystemDeployer:
                     "compliance-framework",
                     "security-scanner",
                 ],
-                "high_availability": {"enabled": True, "load_balancer": "haproxy", "replicas": 3},
-                "auto_scaling": {"enabled": True, "min_instances": 2, "max_instances": 10},
+                "high_availability": {
+                    "enabled": True,
+                    "load_balancer": "haproxy",
+                    "replicas": 3,
+                },
+                "auto_scaling": {
+                    "enabled": True,
+                    "min_instances": 2,
+                    "max_instances": 10,
+                },
             }
 
     async def deploy_enterprise_system(self) -> Dict[str, Any]:
@@ -154,15 +162,21 @@ class EnterpriseSystemDeployer:
 
             # è¨ˆç®—éƒ¨ç½²çµæœ
             total_duration = time.time() - start_time
-            deployment_success = self._evaluate_deployment_success(deployment_results)
+            deployment_success = self._evaluate_deployment_success(
+                deployment_results
+            )
 
             deployment_results.update(
                 {
                     "end_time": datetime.utcnow().isoformat(),
                     "duration_seconds": total_duration,
                     "deployment_success": deployment_success,
-                    "system_status": "OPERATIONAL" if deployment_success else "FAILED",
-                    "next_steps": self._generate_next_steps(deployment_results),
+                    "system_status": "OPERATIONAL"
+                    if deployment_success
+                    else "FAILED",
+                    "next_steps": self._generate_next_steps(
+                        deployment_results
+                    ),
                     "maintenance_schedule": self._generate_maintenance_schedule(),
                 }
             )
@@ -171,7 +185,9 @@ class EnterpriseSystemDeployer:
             await self._generate_deployment_summary(deployment_results)
 
             if deployment_success:
-                logger.info("ğŸ‰ ä¼æ¥­ç´šç³»çµ±éƒ¨ç½²å®Œæˆï¼ç³»çµ±å·²æº–å‚™å¥½ç‚ºç”¨æˆ¶æä¾›æœå‹™ã€‚")
+                logger.info(
+                    "ğŸ‰ ä¼æ¥­ç´šç³»çµ±éƒ¨ç½²å®Œæˆï¼ç³»çµ±å·²æº–å‚™å¥½ç‚ºç”¨æˆ¶æä¾›æœå‹™ã€‚"
+                )
             else:
                 logger.error("âŒ ç³»çµ±éƒ¨ç½²éç¨‹ä¸­é‡åˆ°å•é¡Œï¼Œè«‹æŸ¥çœ‹è©³ç´°æ—¥èªŒã€‚")
 
@@ -180,7 +196,11 @@ class EnterpriseSystemDeployer:
         except Exception as e:
             logger.error(f"ç³»çµ±éƒ¨ç½²å¤±æ•—: {e}")
             deployment_results.update(
-                {"error": str(e), "deployment_success": False, "system_status": "FAILED"}
+                {
+                    "error": str(e),
+                    "deployment_success": False,
+                    "system_status": "FAILED",
+                }
             )
             return deployment_results
 
@@ -275,11 +295,15 @@ class EnterpriseSystemDeployer:
 
             for service_name in services:
                 logger.info(f"éƒ¨ç½²æœå‹™: {service_name}")
-                service_result = await self._deploy_single_service(service_name)
+                service_result = await self._deploy_single_service(
+                    service_name
+                )
                 deployment_results[service_name] = service_result
 
             # é…ç½®æœå‹™é–“é€šä¿¡
-            inter_service_result = await self._configure_inter_service_communication()
+            inter_service_result = (
+                await self._configure_inter_service_communication()
+            )
 
             return {
                 "status": "SUCCESS",
@@ -400,7 +424,9 @@ class EnterpriseSystemDeployer:
             audit_logging_result = await self._configure_audit_logging()
 
             # é…ç½®è³‡æ–™ä¿ç•™æ”¿ç­–
-            retention_policies_result = await self._configure_retention_policies()
+            retention_policies_result = (
+                await self._configure_retention_policies()
+            )
 
             return {
                 "status": "SUCCESS",
@@ -416,8 +442,13 @@ class EnterpriseSystemDeployer:
     async def _configure_high_availability(self) -> Dict[str, Any]:
         """é…ç½®é«˜å¯ç”¨æ€§"""
         try:
-            if not self.config.get("high_availability", {}).get("enabled", False):
-                return {"status": "SKIPPED", "reason": "High availability disabled in config"}
+            if not self.config.get("high_availability", {}).get(
+                "enabled", False
+            ):
+                return {
+                    "status": "SKIPPED",
+                    "reason": "High availability disabled in config",
+                }
 
             # éƒ¨ç½²è² è¼‰å¹³è¡¡å™¨
             load_balancer_result = await self._deploy_load_balancer()
@@ -426,7 +457,9 @@ class EnterpriseSystemDeployer:
             replicas_result = await self._configure_service_replicas()
 
             # é…ç½®è³‡æ–™åº«ä¸»å¾è¤‡è£½
-            db_replication_result = await self._configure_database_replication()
+            db_replication_result = (
+                await self._configure_database_replication()
+            )
 
             return {
                 "status": "SUCCESS",
@@ -442,13 +475,18 @@ class EnterpriseSystemDeployer:
         """é…ç½®è‡ªå‹•æ“´å±•"""
         try:
             if not self.config.get("auto_scaling", {}).get("enabled", False):
-                return {"status": "SKIPPED", "reason": "Auto scaling disabled in config"}
+                return {
+                    "status": "SKIPPED",
+                    "reason": "Auto scaling disabled in config",
+                }
 
             # é…ç½® HPA (Horizontal Pod Autoscaler)
             hpa_result = await self._configure_horizontal_pod_autoscaler()
 
             # é…ç½®è³‡æºç›£æ§
-            resource_monitoring_result = await self._configure_resource_monitoring()
+            resource_monitoring_result = (
+                await self._configure_resource_monitoring()
+            )
 
             # é…ç½®æ“´å±•ç­–ç•¥
             scaling_policies_result = await self._configure_scaling_policies()
@@ -514,14 +552,22 @@ class EnterpriseSystemDeployer:
         try:
             # åŸ·è¡Œå®‰å…¨æƒæè…³æœ¬
             result = subprocess.run(
-                ["python", "scripts/security_scanner.py", "--severity", "medium", "--verbose"],
+                [
+                    "python",
+                    "scripts/security_scanner.py",
+                    "--severity",
+                    "medium",
+                    "--verbose",
+                ],
                 capture_output=True,
                 text=True,
                 timeout=1200,
             )
 
             return {
-                "status": "SUCCESS" if result.returncode <= 1 else "FAILED",  # å®‰å…¨æƒæå…è¨±è¼•å¾®å•é¡Œ
+                "status": "SUCCESS"
+                if result.returncode <= 1
+                else "FAILED",  # å®‰å…¨æƒæå…è¨±è¼•å¾®å•é¡Œ
                 "exit_code": result.returncode,
                 "stdout": result.stdout,
                 "stderr": result.stderr,
@@ -549,7 +595,9 @@ class EnterpriseSystemDeployer:
             return {
                 "status": "SUCCESS" if all_valid else "FAILED",
                 "validations": validations,
-                "failed_validations": [k for k, v in validations.items() if not v],
+                "failed_validations": [
+                    k for k, v in validations.items() if not v
+                ],
             }
 
         except Exception as e:
@@ -599,7 +647,9 @@ class EnterpriseSystemDeployer:
         dependencies = ["docker", "docker-compose", "python3", "pip"]
         for dep in dependencies:
             try:
-                subprocess.run([dep, "--version"], capture_output=True, check=True)
+                subprocess.run(
+                    [dep, "--version"], capture_output=True, check=True
+                )
             except (subprocess.CalledProcessError, FileNotFoundError):
                 return False
         return True
@@ -607,11 +657,17 @@ class EnterpriseSystemDeployer:
     # å…¶ä»–éƒ¨ç½²æ–¹æ³•çš„ç°¡åŒ–å¯¦ç¾
     async def _create_docker_networks(self) -> List[str]:
         """å‰µå»º Docker ç¶²è·¯"""
-        networks = ["auto-video-network", "monitoring-network", "backup-network"]
+        networks = [
+            "auto-video-network",
+            "monitoring-network",
+            "backup-network",
+        ]
         created = []
         for network_name in networks:
             try:
-                self.docker_client.networks.create(network_name, driver="bridge")
+                self.docker_client.networks.create(
+                    network_name, driver="bridge"
+                )
                 created.append(network_name)
             except Exception as e:
                 logger.warning(f"ç¶²è·¯å‰µå»ºå¤±æ•— {network_name}: {e}")
@@ -654,7 +710,9 @@ class EnterpriseSystemDeployer:
         """é…ç½®å¿«å–ç­–ç•¥"""
         return {"status": "SUCCESS", "policies_configured": 5}
 
-    async def _deploy_single_service(self, service_name: str) -> Dict[str, Any]:
+    async def _deploy_single_service(
+        self, service_name: str
+    ) -> Dict[str, Any]:
         """éƒ¨ç½²å–®å€‹æœå‹™"""
         return {"status": "SUCCESS", "service": service_name, "replicas": 3}
 
@@ -666,7 +724,12 @@ class EnterpriseSystemDeployer:
 
     def _evaluate_deployment_success(self, results: Dict[str, Any]) -> bool:
         """è©•ä¼°éƒ¨ç½²æˆåŠŸç‹€æ…‹"""
-        critical_components = ["infrastructure", "databases", "core_services", "final_validation"]
+        critical_components = [
+            "infrastructure",
+            "databases",
+            "core_services",
+            "final_validation",
+        ]
 
         for component in critical_components:
             if component not in results:
@@ -742,21 +805,29 @@ async def main():
     import argparse
 
     parser = argparse.ArgumentParser(description="ä¼æ¥­ç´šç³»çµ±éƒ¨ç½²")
-    parser.add_argument("--config", default="config/deployment-config.json", help="éƒ¨ç½²é…ç½®æª”æ¡ˆ")
+    parser.add_argument(
+        "--config",
+        default="config/deployment-config.json",
+        help="éƒ¨ç½²é…ç½®æª”æ¡ˆ",
+    )
     parser.add_argument(
         "--environment",
         choices=["development", "staging", "production"],
         default="production",
         help="éƒ¨ç½²ç’°å¢ƒ",
     )
-    parser.add_argument("--dry-run", action="store_true", help="æ¼”ç·´æ¨¡å¼ï¼ˆä¸å¯¦éš›éƒ¨ç½²ï¼‰")
+    parser.add_argument(
+        "--dry-run", action="store_true", help="æ¼”ç·´æ¨¡å¼ï¼ˆä¸å¯¦éš›éƒ¨ç½²ï¼‰"
+    )
     parser.add_argument("--verbose", action="store_true", help="è©³ç´°è¼¸å‡º")
 
     args = parser.parse_args()
 
     # è¨­ç½®æ—¥èªŒ
     log_level = logging.DEBUG if args.verbose else logging.INFO
-    logging.basicConfig(level=log_level, format="%(asctime)s - %(levelname)s - %(message)s")
+    logging.basicConfig(
+        level=log_level, format="%(asctime)s - %(levelname)s - %(message)s"
+    )
 
     if args.dry_run:
         logger.info("ğŸ” æ¼”ç·´æ¨¡å¼ï¼šå°‡æ¨¡æ“¬éƒ¨ç½²éç¨‹ï¼Œä¸æœƒå¯¦éš›ä¿®æ”¹ç³»çµ±")
@@ -766,13 +837,15 @@ async def main():
     results = await deployer.deploy_enterprise_system()
 
     # è¼¸å‡ºçµæœæ‘˜è¦
-    print(f"\n{'='*80}")
+    print(f"\n{'=' * 80}")
     print("ğŸš€ ä¼æ¥­ç´šç³»çµ±éƒ¨ç½²çµæœæ‘˜è¦")
-    print(f"{'='*80}")
+    print(f"{'=' * 80}")
     print(f"éƒ¨ç½²ç’°å¢ƒ: {results.get('environment', 'Unknown')}")
     print(f"éƒ¨ç½²æŒçºŒæ™‚é–“: {results.get('duration_seconds', 0):.2f} ç§’")
     print(f"ç³»çµ±ç‹€æ…‹: {results.get('system_status', 'Unknown')}")
-    print(f"éƒ¨ç½²æˆåŠŸ: {'âœ… æ˜¯' if results.get('deployment_success', False) else 'âŒ å¦'}")
+    print(
+        f"éƒ¨ç½²æˆåŠŸ: {'âœ… æ˜¯' if results.get('deployment_success', False) else 'âŒ å¦'}"
+    )
 
     if results.get("deployment_success", False):
         print(f"\nğŸ‰ æ­å–œï¼ä¼æ¥­ç´šè‡ªå‹•å½±ç‰‡ç”Ÿæˆç³»çµ±å·²æˆåŠŸéƒ¨ç½²ï¼")
@@ -790,7 +863,9 @@ async def main():
         for i, step in enumerate(results.get("next_steps", []), 1):
             print(f"{i}. {step}")
 
-        print(f"\nç³»çµ±å·²é”åˆ°ä¸–ç•Œç´šä¼æ¥­æ¨™æº–ï¼Œå¯èˆ‡ Netflixã€Spotifyã€Uber ç­‰é ‚ç´šæŠ€è¡“å…¬å¸åª²ç¾ï¼")
+        print(
+            f"\nç³»çµ±å·²é”åˆ°ä¸–ç•Œç´šä¼æ¥­æ¨™æº–ï¼Œå¯èˆ‡ Netflixã€Spotifyã€Uber ç­‰é ‚ç´šæŠ€è¡“å…¬å¸åª²ç¾ï¼"
+        )
         exit(0)
     else:
         print(f"\nâŒ éƒ¨ç½²éç¨‹ä¸­é‡åˆ°å•é¡Œï¼Œè«‹æª¢æŸ¥æ—¥èªŒä¸¦ä¿®å¾©å¾Œé‡æ–°éƒ¨ç½²ã€‚")
diff --git a/auto_generate_video_fold6/scripts/disaster_recovery.py b/auto_generate_video_fold6/scripts/disaster_recovery.py
index 77b0104..0c8ad4e 100755
--- a/auto_generate_video_fold6/scripts/disaster_recovery.py
+++ b/auto_generate_video_fold6/scripts/disaster_recovery.py
@@ -86,14 +86,18 @@ class HealthChecker:
 
         # æª¢æŸ¥å„å€‹æœå‹™
         for service_name, service_config in self.services.items():
-            service_health = await self._check_service_health(service_name, service_config)
+            service_health = await self._check_service_health(
+                service_name, service_config
+            )
             health_results["services"][service_name] = service_health
 
             if service_health["status"] == "critical":
                 health_results["critical_issues"].append(
                     {
                         "service": service_name,
-                        "issue": service_health.get("error", "Unknown critical issue"),
+                        "issue": service_health.get(
+                            "error", "Unknown critical issue"
+                        ),
                     }
                 )
                 health_results["overall_status"] = "critical"
@@ -101,7 +105,9 @@ class HealthChecker:
                 health_results["warnings"].append(
                     {
                         "service": service_name,
-                        "issue": service_health.get("error", "Unknown warning"),
+                        "issue": service_health.get(
+                            "error", "Unknown warning"
+                        ),
                     }
                 )
                 if health_results["overall_status"] == "healthy":
@@ -120,11 +126,16 @@ class HealthChecker:
             if service_type == "http":
                 return await self._check_http_service(service_name, config)
             elif service_type == "postgresql":
-                return await self._check_postgresql_service(service_name, config)
+                return await self._check_postgresql_service(
+                    service_name, config
+                )
             elif service_type == "redis":
                 return await self._check_redis_service(service_name, config)
             else:
-                return {"status": "unknown", "error": f"Unknown service type: {service_type}"}
+                return {
+                    "status": "unknown",
+                    "error": f"Unknown service type: {service_type}",
+                }
 
         except Exception as e:
             logger.error(f"Health check failed for {service_name}: {e}")
@@ -135,7 +146,9 @@ class HealthChecker:
     ) -> Dict[str, Any]:
         """æª¢æŸ¥ HTTP æœå‹™"""
 
-        url = config.get("health_url", f"http://localhost:{config.get('port', 8080)}/health")
+        url = config.get(
+            "health_url", f"http://localhost:{config.get('port', 8080)}/health"
+        )
         timeout = config.get("timeout", 5)
 
         try:
@@ -148,10 +161,18 @@ class HealthChecker:
                     "url": url,
                 }
             else:
-                return {"status": "warning", "error": f"HTTP {response.status_code}", "url": url}
+                return {
+                    "status": "warning",
+                    "error": f"HTTP {response.status_code}",
+                    "url": url,
+                }
 
         except requests.RequestException as e:
-            return {"status": "critical", "error": f"Connection failed: {str(e)}", "url": url}
+            return {
+                "status": "critical",
+                "error": f"Connection failed: {str(e)}",
+                "url": url,
+            }
 
     async def _check_postgresql_service(
         self, service_name: str, config: Dict[str, Any]
@@ -194,7 +215,10 @@ class HealthChecker:
 
             redis_client.ping()
 
-            return {"status": "healthy", "redis_version": redis_client.info()["redis_version"]}
+            return {
+                "status": "healthy",
+                "redis_version": redis_client.info()["redis_version"],
+            }
 
         except Exception as e:
             return {"status": "critical", "error": str(e)}
@@ -240,21 +264,27 @@ class FailoverManager:
                 continue
 
             # æª¢æŸ¥æœå‹™æ˜¯å¦æœ‰æ•…éšœè½‰ç§»é…ç½®
-            service_config = self.config.get("services", {}).get(service_name, {})
+            service_config = self.config.get("services", {}).get(
+                service_name, {}
+            )
             failover_config = service_config.get("failover", {})
 
             if failover_config.get("enabled", False):
                 logger.warning(f"å•Ÿå‹• {service_name} çš„æ•…éšœè½‰ç§»")
                 await self._execute_failover(service_name, failover_config)
 
-    async def _execute_failover(self, service_name: str, failover_config: Dict[str, Any]):
+    async def _execute_failover(
+        self, service_name: str, failover_config: Dict[str, Any]
+    ):
         """åŸ·è¡Œæ•…éšœè½‰ç§»"""
 
         failover_id = f"failover_{service_name}_{int(time.time())}"
         self.active_failovers[service_name] = failover_id
 
         try:
-            logger.info(f"é–‹å§‹åŸ·è¡Œ {service_name} æ•…éšœè½‰ç§» (ID: {failover_id})")
+            logger.info(
+                f"é–‹å§‹åŸ·è¡Œ {service_name} æ•…éšœè½‰ç§» (ID: {failover_id})"
+            )
 
             # 1. åœæ­¢æ•…éšœæœå‹™
             if failover_config.get("stop_primary", True):
@@ -267,18 +297,26 @@ class FailoverManager:
 
             # 3. æ›´æ–°æœå‹™ç™¼ç¾é…ç½®
             if failover_config.get("update_service_discovery", True):
-                await self._update_service_discovery(service_name, failover_config)
+                await self._update_service_discovery(
+                    service_name, failover_config
+                )
 
             # 4. é©—è­‰æ•…éšœè½‰ç§»
-            validation_result = await self._validate_failover(service_name, failover_config)
+            validation_result = await self._validate_failover(
+                service_name, failover_config
+            )
 
             if validation_result["success"]:
                 logger.info(f"{service_name} æ•…éšœè½‰ç§»æˆåŠŸå®Œæˆ")
 
                 # ç™¼é€é€šçŸ¥
-                await self._send_failover_notification(service_name, "success", failover_id)
+                await self._send_failover_notification(
+                    service_name, "success", failover_id
+                )
             else:
-                logger.error(f"{service_name} æ•…éšœè½‰ç§»å¤±æ•—: {validation_result['error']}")
+                logger.error(
+                    f"{service_name} æ•…éšœè½‰ç§»å¤±æ•—: {validation_result['error']}"
+                )
                 await self._rollback_failover(service_name, failover_config)
 
         except Exception as e:
@@ -306,7 +344,9 @@ class FailoverManager:
         except subprocess.CalledProcessError as e:
             logger.error(f"å•Ÿå‹•å‚™ç”¨æœå‹™ {backup_service} å¤±æ•—: {e}")
 
-    async def _update_service_discovery(self, service_name: str, failover_config: Dict[str, Any]):
+    async def _update_service_discovery(
+        self, service_name: str, failover_config: Dict[str, Any]
+    ):
         """æ›´æ–°æœå‹™ç™¼ç¾é…ç½®"""
         # é€™è£¡å¯¦ç¾æœå‹™ç™¼ç¾æ›´æ–°é‚è¼¯ï¼ˆå¦‚ Consul, etcd ç­‰ï¼‰
         logger.info(f"æ›´æ–° {service_name} çš„æœå‹™ç™¼ç¾é…ç½®")
@@ -330,12 +370,17 @@ class FailoverManager:
             if response.status_code == 200:
                 return {"success": True, "message": "æ•…éšœè½‰ç§»é©—è­‰æˆåŠŸ"}
             else:
-                return {"success": False, "error": f"é©—è­‰å¤±æ•—: HTTP {response.status_code}"}
+                return {
+                    "success": False,
+                    "error": f"é©—è­‰å¤±æ•—: HTTP {response.status_code}",
+                }
 
         except Exception as e:
             return {"success": False, "error": f"é©—è­‰éç¨‹éŒ¯èª¤: {str(e)}"}
 
-    async def _rollback_failover(self, service_name: str, failover_config: Dict[str, Any]):
+    async def _rollback_failover(
+        self, service_name: str, failover_config: Dict[str, Any]
+    ):
         """å›æ»¾æ•…éšœè½‰ç§»"""
         logger.warning(f"å›æ»¾ {service_name} çš„æ•…éšœè½‰ç§»")
 
@@ -343,7 +388,9 @@ class FailoverManager:
             # åœæ­¢å‚™ç”¨æœå‹™
             backup_service = failover_config.get("backup_service")
             if backup_service:
-                subprocess.run(["systemctl", "stop", backup_service], check=True)
+                subprocess.run(
+                    ["systemctl", "stop", backup_service], check=True
+                )
 
             # å˜—è©¦é‡æ–°å•Ÿå‹•åŸæœå‹™
             subprocess.run(["systemctl", "start", service_name], check=True)
@@ -353,7 +400,9 @@ class FailoverManager:
         except Exception as e:
             logger.error(f"{service_name} æ•…éšœè½‰ç§»å›æ»¾å¤±æ•—: {e}")
 
-    async def _send_failover_notification(self, service_name: str, status: str, failover_id: str):
+    async def _send_failover_notification(
+        self, service_name: str, status: str, failover_id: str
+    ):
         """ç™¼é€æ•…éšœè½‰ç§»é€šçŸ¥"""
 
         notification_config = self.config.get("notification", {})
@@ -362,18 +411,26 @@ class FailoverManager:
 
         # é›»å­éƒµä»¶é€šçŸ¥
         if notification_config.get("email", {}).get("enabled", False):
-            await self._send_email_notification(message, notification_config["email"])
+            await self._send_email_notification(
+                message, notification_config["email"]
+            )
 
         # Slack é€šçŸ¥
         if notification_config.get("slack", {}).get("enabled", False):
-            await self._send_slack_notification(message, notification_config["slack"])
+            await self._send_slack_notification(
+                message, notification_config["slack"]
+            )
 
-    async def _send_email_notification(self, message: str, email_config: Dict[str, Any]):
+    async def _send_email_notification(
+        self, message: str, email_config: Dict[str, Any]
+    ):
         """ç™¼é€é›»å­éƒµä»¶é€šçŸ¥"""
         # å¯¦ç¾é›»å­éƒµä»¶ç™¼é€é‚è¼¯
         logger.info(f"ç™¼é€é›»å­éƒµä»¶é€šçŸ¥: {message}")
 
-    async def _send_slack_notification(self, message: str, slack_config: Dict[str, Any]):
+    async def _send_slack_notification(
+        self, message: str, slack_config: Dict[str, Any]
+    ):
         """ç™¼é€ Slack é€šçŸ¥"""
         # å¯¦ç¾ Slack é€šçŸ¥é‚è¼¯
         logger.info(f"ç™¼é€ Slack é€šçŸ¥: {message}")
@@ -382,7 +439,9 @@ class FailoverManager:
 class DisasterRecoveryManager:
     """ç½é›£æ¢å¾©ç®¡ç†å™¨"""
 
-    def __init__(self, config_file: str = "config/disaster-recovery-config.json"):
+    def __init__(
+        self, config_file: str = "config/disaster-recovery-config.json"
+    ):
         self.config = self._load_config(config_file)
         self.health_checker = HealthChecker(self.config)
         self.failover_manager = FailoverManager(self.config)
@@ -439,7 +498,10 @@ class DisasterRecoveryManager:
                 },
             },
             "notification": {
-                "email": {"enabled": False, "recipients": ["admin@example.com"]},
+                "email": {
+                    "enabled": False,
+                    "recipients": ["admin@example.com"],
+                },
                 "slack": {"enabled": False, "webhook_url": ""},
             },
         }
@@ -457,10 +519,26 @@ class DisasterRecoveryManager:
             priority=1,  # æœ€é«˜å„ªå…ˆç´š
             dependencies=[],
             procedures=[
-                {"step": 1, "action": "stop_application_services", "timeout": 300},
-                {"step": 2, "action": "restore_database_backup", "timeout": 1800},
-                {"step": 3, "action": "verify_database_integrity", "timeout": 600},
-                {"step": 4, "action": "start_application_services", "timeout": 300},
+                {
+                    "step": 1,
+                    "action": "stop_application_services",
+                    "timeout": 300,
+                },
+                {
+                    "step": 2,
+                    "action": "restore_database_backup",
+                    "timeout": 1800,
+                },
+                {
+                    "step": 3,
+                    "action": "verify_database_integrity",
+                    "timeout": 600,
+                },
+                {
+                    "step": 4,
+                    "action": "start_application_services",
+                    "timeout": 300,
+                },
             ],
             validation_steps=[
                 {"check": "database_connectivity", "timeout": 30},
@@ -477,7 +555,11 @@ class DisasterRecoveryManager:
             priority=2,  # é«˜å„ªå…ˆç´š
             dependencies=["database_recovery"],
             procedures=[
-                {"step": 1, "action": "activate_backup_services", "timeout": 180},
+                {
+                    "step": 1,
+                    "action": "activate_backup_services",
+                    "timeout": 180,
+                },
                 {"step": 2, "action": "update_load_balancer", "timeout": 60},
                 {"step": 3, "action": "verify_service_health", "timeout": 120},
             ],
@@ -508,7 +590,8 @@ class DisasterRecoveryManager:
             initiated_at=datetime.utcnow(),
             status=RecoveryStatus.INITIATED,
             progress_percentage=0.0,
-            estimated_completion=datetime.utcnow() + timedelta(minutes=plan.target_rto),
+            estimated_completion=datetime.utcnow()
+            + timedelta(minutes=plan.target_rto),
             logs=[],
             metadata={"plan": asdict(plan)},
         )
@@ -524,7 +607,9 @@ class DisasterRecoveryManager:
             total_steps = len(plan.procedures)
             for i, procedure in enumerate(plan.procedures):
                 await self._execute_recovery_procedure(operation, procedure)
-                operation.progress_percentage = ((i + 1) / total_steps) * 80  # 80% for procedures
+                operation.progress_percentage = (
+                    (i + 1) / total_steps
+                ) * 80  # 80% for procedures
 
             # åŸ·è¡Œé©—è­‰æ­¥é©Ÿ
             total_validations = len(plan.validation_steps)
@@ -697,7 +782,9 @@ class DisasterRecoveryManager:
         logger.info("é©—è­‰æ ¸å¿ƒåŠŸèƒ½...")
         await asyncio.sleep(2)
 
-    async def get_recovery_status(self, operation_id: str) -> Optional[RecoveryOperation]:
+    async def get_recovery_status(
+        self, operation_id: str
+    ) -> Optional[RecoveryOperation]:
         """ç²å–æ¢å¾©æ“ä½œç‹€æ…‹"""
         return self.recovery_operations.get(operation_id)
 
@@ -722,7 +809,9 @@ async def main():
         print("æª¢æ¸¬åˆ°åš´é‡å•é¡Œï¼ŒåŸ·è¡Œæ¢å¾©è¨ˆåŠƒ...")
 
         # åŸ·è¡Œè³‡æ–™åº«æ¢å¾©
-        recovery_op = await dr_manager.execute_recovery_plan("database_recovery", "admin")
+        recovery_op = await dr_manager.execute_recovery_plan(
+            "database_recovery", "admin"
+        )
         print(f"æ¢å¾©æ“ä½œç‹€æ…‹: {recovery_op.status}")
 
     # é–‹å§‹ç›£æ§ï¼ˆé€™æœƒæŒçºŒé‹è¡Œï¼‰
diff --git a/auto_generate_video_fold6/scripts/fix_flake8.py b/auto_generate_video_fold6/scripts/fix_flake8.py
index 3deda6f..9056dbe 100644
--- a/auto_generate_video_fold6/scripts/fix_flake8.py
+++ b/auto_generate_video_fold6/scripts/fix_flake8.py
@@ -9,40 +9,43 @@ import subprocess
 import sys
 from pathlib import Path
 
+
 def fix_line_length(content, max_length=79):
     """ä¿®å¾©è¡Œé•·åº¦éé•·çš„å•é¡Œ"""
-    lines = content.split('\n')
+    lines = content.split("\n")
     fixed_lines = []
-    
+
     for line in lines:
         if len(line) <= max_length:
             fixed_lines.append(line)
             continue
-            
+
         # å¦‚æœæ˜¯å°å…¥èªå¥ï¼Œå˜—è©¦æ‹†åˆ†
-        if line.strip().startswith('from ') and 'import' in line:
-            if '(' not in line and ',' in line:
+        if line.strip().startswith("from ") and "import" in line:
+            if "(" not in line and "," in line:
                 # æ‹†åˆ†å¤šå€‹å°å…¥
-                parts = line.split('import', 1)
+                parts = line.split("import", 1)
                 if len(parts) == 2:
-                    prefix = parts[0] + 'import ('
-                    imports = [imp.strip() for imp in parts[1].split(',')]
+                    prefix = parts[0] + "import ("
+                    imports = [imp.strip() for imp in parts[1].split(",")]
                     fixed_lines.append(prefix)
                     for imp in imports:
                         fixed_lines.append(f"    {imp.strip()},")
-                    fixed_lines.append(')')
+                    fixed_lines.append(")")
                     continue
-        
+
         # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå˜—è©¦åœ¨é©ç•¶ä½ç½®æ›è¡Œ
         if '"' in line or "'" in line:
             # ç°¡å–®çš„å­—ç¬¦ä¸²æ‹†åˆ†
             indent = len(line) - len(line.lstrip())
             if len(line) > max_length:
                 # å°‹æ‰¾é©ç•¶çš„æ–·é»
-                for i in range(max_length-10, 40, -1):
-                    if line[i] in [' ', ',', '.', '(', ')']:
-                        fixed_lines.append(line[:i] + ' \\')
-                        fixed_lines.append(' ' * (indent + 4) + line[i:].lstrip())
+                for i in range(max_length - 10, 40, -1):
+                    if line[i] in [" ", ",", ".", "(", ")"]:
+                        fixed_lines.append(line[:i] + " \\")
+                        fixed_lines.append(
+                            " " * (indent + 4) + line[i:].lstrip()
+                        )
                         break
                 else:
                     fixed_lines.append(line)  # å¦‚æœæ‰¾ä¸åˆ°å¥½çš„æ–·é»ï¼Œä¿æŒåŸæ¨£
@@ -50,159 +53,175 @@ def fix_line_length(content, max_length=79):
                 fixed_lines.append(line)
         else:
             # å°æ–¼å…¶ä»–è¡Œï¼Œç°¡å–®è™•ç†
-            if len(line) > max_length and '(' in line:
+            if len(line) > max_length and "(" in line:
                 # å°‹æ‰¾æ‹¬è™Ÿå¾Œçš„ä½ç½®é€²è¡Œæ›è¡Œ
-                paren_pos = line.find('(') + 1
+                paren_pos = line.find("(") + 1
                 if paren_pos > 0 and paren_pos < max_length - 10:
                     before_paren = line[:paren_pos]
                     after_paren = line[paren_pos:]
                     indent = len(before_paren) - len(before_paren.lstrip()) + 4
                     fixed_lines.append(before_paren)
-                    fixed_lines.append(' ' * indent + after_paren)
+                    fixed_lines.append(" " * indent + after_paren)
                 else:
                     fixed_lines.append(line)
             else:
                 fixed_lines.append(line)
-    
-    return '\n'.join(fixed_lines)
+
+    return "\n".join(fixed_lines)
+
 
 def remove_unused_imports(content):
     """ç§»é™¤æ˜é¡¯æœªä½¿ç”¨çš„å°å…¥"""
-    lines = content.split('\n')
+    lines = content.split("\n")
     fixed_lines = []
-    
+
     # å¸¸è¦‹çš„æœªä½¿ç”¨å°å…¥æ¨¡å¼
     unused_patterns = [
-        r'^from abc import ABC, abstractmethod\s*$',
-        r'^import asyncio\s*$',
-        r'^import torch\s*$', 
-        r'^import torchaudio\s*$',
-        r'^import os\s*$',
-        r'^import tempfile\s*$',
-        r'^from typing import.*Optional.*$',
-        r'^from typing import.*Tuple.*$',
-        r'^from typing import.*Union.*$',
-        r'^from datetime import datetime\s*$',
+        r"^from abc import ABC, abstractmethod\s*$",
+        r"^import asyncio\s*$",
+        r"^import torch\s*$",
+        r"^import torchaudio\s*$",
+        r"^import os\s*$",
+        r"^import tempfile\s*$",
+        r"^from typing import.*Optional.*$",
+        r"^from typing import.*Tuple.*$",
+        r"^from typing import.*Union.*$",
+        r"^from datetime import datetime\s*$",
     ]
-    
+
     for line in lines:
         should_remove = False
         for pattern in unused_patterns:
             if re.match(pattern, line.strip()):
                 # ç°¡å–®æª¢æŸ¥ï¼šå¦‚æœå°å…¥çš„å…§å®¹åœ¨å¾ŒçºŒä»£ç¢¼ä¸­å‡ºç¾ï¼Œå‰‡ä¿ç•™
                 import_parts = []
-                if 'import' in line:
-                    if 'from' in line:
+                if "import" in line:
+                    if "from" in line:
                         # from xxx import yyy
-                        parts = line.split('import', 1)
+                        parts = line.split("import", 1)
                         if len(parts) == 2:
-                            import_parts = [p.strip() for p in parts[1].split(',')]
+                            import_parts = [
+                                p.strip() for p in parts[1].split(",")
+                            ]
                     else:
                         # import xxx
-                        import_parts = [line.replace('import', '').strip()]
-                
+                        import_parts = [line.replace("import", "").strip()]
+
                 # æª¢æŸ¥æ˜¯å¦åœ¨å…§å®¹ä¸­ä½¿ç”¨
-                rest_content = '\n'.join(lines[lines.index(line)+1:])
+                rest_content = "\n".join(lines[lines.index(line) + 1 :])
                 used = False
                 for part in import_parts:
-                    clean_part = part.split(' as ')[0] if ' as ' in part else part
+                    clean_part = (
+                        part.split(" as ")[0] if " as " in part else part
+                    )
                     if clean_part and clean_part in rest_content:
                         used = True
                         break
-                
+
                 if not used:
                     should_remove = True
                 break
-        
+
         if not should_remove:
             fixed_lines.append(line)
-    
-    return '\n'.join(fixed_lines)
+
+    return "\n".join(fixed_lines)
+
 
 def fix_bare_except(content):
     """ä¿®å¾©è£¸éœ²çš„ except èªå¥"""
-    content = re.sub(r'except\s*:', 'except Exception:', content)
+    content = re.sub(r"except\s*:", "except Exception:", content)
     return content
 
+
 def fix_trailing_whitespace(content):
     """ç§»é™¤è¡Œå°¾ç©ºç™½"""
-    lines = content.split('\n')
+    lines = content.split("\n")
     fixed_lines = [line.rstrip() for line in lines]
-    return '\n'.join(fixed_lines)
+    return "\n".join(fixed_lines)
+
 
 def fix_whitespace_before_colon(content):
     """ä¿®å¾©å†’è™Ÿå‰çš„ç©ºç™½"""
-    content = re.sub(r'\s+:', ':', content)
+    content = re.sub(r"\s+:", ":", content)
     return content
 
+
 def fix_python_file(file_path):
     """ä¿®å¾©å–®å€‹ Python æª”æ¡ˆ"""
     try:
-        with open(file_path, 'r', encoding='utf-8') as f:
+        with open(file_path, "r", encoding="utf-8") as f:
             content = f.read()
-        
+
         original_content = content
-        
+
         # æ‡‰ç”¨ä¿®å¾©
         content = remove_unused_imports(content)
         content = fix_line_length(content)
-        content = fix_bare_except(content) 
+        content = fix_bare_except(content)
         content = fix_trailing_whitespace(content)
         content = fix_whitespace_before_colon(content)
-        
+
         # åªæœ‰åœ¨å…§å®¹æœ‰è®ŠåŒ–æ™‚æ‰å¯«å…¥
         if content != original_content:
-            with open(file_path, 'w', encoding='utf-8') as f:
+            with open(file_path, "w", encoding="utf-8") as f:
                 f.write(content)
             print(f"âœ“ ä¿®å¾©: {file_path}")
             return True
-        
+
         return False
-        
+
     except Exception as e:
         print(f"âœ— éŒ¯èª¤è™•ç† {file_path}: {e}")
         return False
 
+
 def main():
     """ä¸»å‡½æ•¸"""
-    services_dir = Path('services')
+    services_dir = Path("services")
     if not services_dir.exists():
         print("éŒ¯èª¤: æ‰¾ä¸åˆ° services ç›®éŒ„")
         sys.exit(1)
-    
+
     fixed_count = 0
     total_count = 0
-    
+
     print("é–‹å§‹ä¿®å¾© Python æª”æ¡ˆ...")
-    
+
     # éæ­¸è™•ç†æ‰€æœ‰ Python æª”æ¡ˆ
-    for py_file in services_dir.rglob('*.py'):
+    for py_file in services_dir.rglob("*.py"):
         total_count += 1
         if fix_python_file(py_file):
             fixed_count += 1
-    
+
     print(f"\nä¿®å¾©å®Œæˆ!")
     print(f"ç¸½æª”æ¡ˆæ•¸: {total_count}")
     print(f"å·²ä¿®å¾©: {fixed_count}")
-    
+
     # é‹è¡Œ flake8 æª¢æŸ¥çµæœ
     print("\né‹è¡Œ flake8 æª¢æŸ¥...")
     try:
-        result = subprocess.run([
-            'flake8', 'services/', 
-            '--max-line-length=79',
-            '--count',
-            '--statistics'
-        ], capture_output=True, text=True)
-        
+        result = subprocess.run(
+            [
+                "flake8",
+                "services/",
+                "--max-line-length=79",
+                "--count",
+                "--statistics",
+            ],
+            capture_output=True,
+            text=True,
+        )
+
         if result.returncode == 0:
             print("âœ“ æ‰€æœ‰ flake8 æª¢æŸ¥é€šé!")
         else:
             print("é‚„æœ‰ä¸€äº›å•é¡Œéœ€è¦æ‰‹å‹•ä¿®å¾©:")
             print(result.stdout)
-            
+
     except FileNotFoundError:
         print("æ³¨æ„: ç„¡æ³•é‹è¡Œ flake8 æª¢æŸ¥ (æœªå®‰è£)")
 
-if __name__ == '__main__':
-    main()
\ No newline at end of file
+
+if __name__ == "__main__":
+    main()
diff --git a/auto_generate_video_fold6/scripts/generate-tdd-report.py b/auto_generate_video_fold6/scripts/generate-tdd-report.py
index 0d503f1..72dd9cd 100755
--- a/auto_generate_video_fold6/scripts/generate-tdd-report.py
+++ b/auto_generate_video_fold6/scripts/generate-tdd-report.py
@@ -13,27 +13,30 @@ from pathlib import Path
 from typing import Dict, List, Optional, Any
 import xml.etree.ElementTree as ET
 
+
 class TDDReportGenerator:
     """TDD æ¸¬è©¦å ±å‘Šç”Ÿæˆå™¨"""
-    
+
     def __init__(self):
         self.project_root = Path.cwd()
-        self.report_dir = self.project_root / 'tdd-reports'
-        self.coverage_dir = self.project_root / 'coverage-reports'
+        self.report_dir = self.project_root / "tdd-reports"
+        self.coverage_dir = self.project_root / "coverage-reports"
         self.timestamp = datetime.now().isoformat()
-        
+
         # ç¢ºä¿å ±å‘Šç›®éŒ„å­˜åœ¨
         self.report_dir.mkdir(exist_ok=True)
-        
+
         # TDD é…ç½®
         self.tdd_config = {
-            'coverage_threshold': 90,
-            'complexity_limit': 10,
-            'max_line_length': 88,
-            'required_test_patterns': ['test_*.py', '*_test.py', 'tests/']
+            "coverage_threshold": 90,
+            "complexity_limit": 10,
+            "max_line_length": 88,
+            "required_test_patterns": ["test_*.py", "*_test.py", "tests/"],
         }
-    
-    def run_command(self, cmd: str, cwd: Optional[Path] = None, timeout: int = 60) -> tuple:
+
+    def run_command(
+        self, cmd: str, cwd: Optional[Path] = None, timeout: int = 60
+    ) -> tuple:
         """å®‰å…¨åŸ·è¡Œå‘½ä»¤ä¸¦è¿”å›çµæœ"""
         try:
             result = subprocess.run(
@@ -41,26 +44,26 @@ class TDDReportGenerator:
                 cwd=cwd or self.project_root,
                 capture_output=True,
                 text=True,
-                timeout=timeout
+                timeout=timeout,
             )
             return result.returncode == 0, result.stdout, result.stderr
         except subprocess.TimeoutExpired:
             return False, "", f"å‘½ä»¤è¶…æ™‚: {cmd}"
         except Exception as e:
             return False, "", str(e)
-    
+
     def collect_git_info(self) -> Optional[Dict[str, Any]]:
         """æ”¶é›† Git è³‡è¨Š"""
         print("ğŸ“Š æ”¶é›† Git è³‡è¨Š...")
-        
+
         git_commands = {
-            'commit_hash': 'git rev-parse HEAD',
-            'commit_message': 'git log -1 --pretty=format:%s',
-            'branch': 'git rev-parse --abbrev-ref HEAD',
-            'author': 'git log -1 --pretty=format:%an',
-            'date': 'git log -1 --pretty=format:%ai'
+            "commit_hash": "git rev-parse HEAD",
+            "commit_message": "git log -1 --pretty=format:%s",
+            "branch": "git rev-parse --abbrev-ref HEAD",
+            "author": "git log -1 --pretty=format:%an",
+            "date": "git log -1 --pretty=format:%ai",
         }
-        
+
         git_info = {}
         for key, cmd in git_commands.items():
             success, stdout, stderr = self.run_command(cmd)
@@ -69,355 +72,393 @@ class TDDReportGenerator:
             else:
                 print(f"âš ï¸ ç„¡æ³•å–å¾— {key}: {stderr}")
                 return None
-        
+
         # æª¢æ¸¬ TDD éšæ®µ
-        commit_msg = git_info.get('commit_message', '')
-        tdd_phase = 'unknown'
-        if commit_msg.startswith('red:'):
-            tdd_phase = 'red'
-        elif commit_msg.startswith('green:'):
-            tdd_phase = 'green'
-        elif commit_msg.startswith('refactor:'):
-            tdd_phase = 'refactor'
-        
-        git_info.update({
-            'short_hash': git_info['commit_hash'][:8],
-            'tdd_phase': tdd_phase
-        })
-        
+        commit_msg = git_info.get("commit_message", "")
+        tdd_phase = "unknown"
+        if commit_msg.startswith("red:"):
+            tdd_phase = "red"
+        elif commit_msg.startswith("green:"):
+            tdd_phase = "green"
+        elif commit_msg.startswith("refactor:"):
+            tdd_phase = "refactor"
+
+        git_info.update(
+            {"short_hash": git_info["commit_hash"][:8], "tdd_phase": tdd_phase}
+        )
+
         return git_info
-    
+
     def collect_frontend_data(self) -> Dict[str, Any]:
         """æ”¶é›†å‰ç«¯æ¸¬è©¦æ•¸æ“š"""
         print("ğŸ“± æ”¶é›†å‰ç«¯æ¸¬è©¦æ•¸æ“š...")
-        
-        frontend_dir = self.project_root / 'frontend'
+
+        frontend_dir = self.project_root / "frontend"
         frontend_data = {
-            'exists': frontend_dir.exists(),
-            'tests_run': False,
-            'coverage': None,
-            'test_results': None,
-            'lint_results': None
+            "exists": frontend_dir.exists(),
+            "tests_run": False,
+            "coverage": None,
+            "test_results": None,
+            "lint_results": None,
         }
-        
-        if not frontend_data['exists']:
+
+        if not frontend_data["exists"]:
             return frontend_data
-        
+
         # æª¢æŸ¥ package.json
-        package_json = frontend_dir / 'package.json'
+        package_json = frontend_dir / "package.json"
         if package_json.exists():
             try:
                 with open(package_json) as f:
                     package_data = json.load(f)
-                frontend_data['scripts'] = package_data.get('scripts', {})
+                frontend_data["scripts"] = package_data.get("scripts", {})
             except Exception as e:
                 print(f"âš ï¸ ç„¡æ³•è®€å– package.json: {e}")
-        
+
         # æ”¶é›†æ¸¬è©¦è¦†è“‹ç‡
-        coverage_file = frontend_dir / 'coverage' / 'coverage-summary.json'
+        coverage_file = frontend_dir / "coverage" / "coverage-summary.json"
         if coverage_file.exists():
             try:
                 with open(coverage_file) as f:
                     coverage_data = json.load(f)
-                
-                frontend_data['coverage'] = {
-                    'statements': coverage_data['total']['statements']['pct'],
-                    'branches': coverage_data['total']['branches']['pct'],
-                    'functions': coverage_data['total']['functions']['pct'],
-                    'lines': coverage_data['total']['lines']['pct']
+
+                frontend_data["coverage"] = {
+                    "statements": coverage_data["total"]["statements"]["pct"],
+                    "branches": coverage_data["total"]["branches"]["pct"],
+                    "functions": coverage_data["total"]["functions"]["pct"],
+                    "lines": coverage_data["total"]["lines"]["pct"],
                 }
             except Exception as e:
                 print(f"âš ï¸ è¦†è“‹ç‡æ•¸æ“šè®€å–å¤±æ•—: {e}")
-        
+
         # å˜—è©¦åŸ·è¡Œæ¸¬è©¦
         os.chdir(frontend_dir)
         try:
             # æª¢æŸ¥æ˜¯å¦æœ‰ node_modules
-            if (frontend_dir / 'node_modules').exists():
+            if (frontend_dir / "node_modules").exists():
                 # åŸ·è¡Œæ¸¬è©¦
-                success, stdout, stderr = self.run_command('npm test -- --run --reporter=json')
+                success, stdout, stderr = self.run_command(
+                    "npm test -- --run --reporter=json"
+                )
                 if success:
-                    frontend_data['tests_run'] = True
-                    frontend_data['test_results'] = self.parse_test_output(stdout)
+                    frontend_data["tests_run"] = True
+                    frontend_data["test_results"] = self.parse_test_output(
+                        stdout
+                    )
                 else:
                     print(f"âš ï¸ å‰ç«¯æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {stderr}")
             else:
                 print("âš ï¸ å‰ç«¯ä¾è³´æœªå®‰è£ï¼Œè·³éæ¸¬è©¦åŸ·è¡Œ")
-                
+
         except Exception as e:
             print(f"å‰ç«¯æ¸¬è©¦åŸ·è¡ŒéŒ¯èª¤: {e}")
         finally:
             os.chdir(self.project_root)
-        
+
         return frontend_data
-    
+
     def collect_backend_data(self) -> Dict[str, Any]:
         """æ”¶é›†å¾Œç«¯æ¸¬è©¦æ•¸æ“š"""
         print("ğŸ”§ æ”¶é›†å¾Œç«¯æ¸¬è©¦æ•¸æ“š...")
-        
-        services_dir = self.project_root / 'services'
+
+        services_dir = self.project_root / "services"
         backend_data = {
-            'services': [],
-            'total_coverage': 0,
-            'total_tests': 0,
-            'total_services': 0
+            "services": [],
+            "total_coverage": 0,
+            "total_tests": 0,
+            "total_services": 0,
         }
-        
+
         if not services_dir.exists():
             return backend_data
-        
+
         # æ”¶é›†æ‰€æœ‰æœå‹™
         for service_dir in services_dir.iterdir():
-            if service_dir.is_dir() and not service_dir.name.startswith('.'):
+            if service_dir.is_dir() and not service_dir.name.startswith("."):
                 service_data = self.collect_service_data(service_dir)
-                backend_data['services'].append(service_data)
-        
+                backend_data["services"].append(service_data)
+
         # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
-        valid_services = [s for s in backend_data['services'] if s['coverage']]
-        backend_data['total_services'] = len(backend_data['services'])
-        
+        valid_services = [s for s in backend_data["services"] if s["coverage"]]
+        backend_data["total_services"] = len(backend_data["services"])
+
         if valid_services:
-            backend_data['total_coverage'] = sum(
-                s['coverage']['statements'] for s in valid_services
+            backend_data["total_coverage"] = sum(
+                s["coverage"]["statements"] for s in valid_services
             ) / len(valid_services)
-        
-        backend_data['total_tests'] = sum(
-            s['test_results']['total'] for s in backend_data['services']
-            if s['test_results']
+
+        backend_data["total_tests"] = sum(
+            s["test_results"]["total"]
+            for s in backend_data["services"]
+            if s["test_results"]
         )
-        
+
         return backend_data
-    
+
     def collect_service_data(self, service_dir: Path) -> Dict[str, Any]:
         """æ”¶é›†å–®å€‹æœå‹™çš„æ¸¬è©¦æ•¸æ“š"""
         service_name = service_dir.name
         service_data = {
-            'name': service_name,
-            'path': str(service_dir),
-            'exists': service_dir.exists(),
-            'has_tests': False,
-            'coverage': None,
-            'test_results': None,
-            'code_quality': None,
-            'requirements': None
+            "name": service_name,
+            "path": str(service_dir),
+            "exists": service_dir.exists(),
+            "has_tests": False,
+            "coverage": None,
+            "test_results": None,
+            "code_quality": None,
+            "requirements": None,
         }
-        
-        if not service_data['exists']:
+
+        if not service_data["exists"]:
             return service_data
-        
+
         # æª¢æŸ¥æ¸¬è©¦ç›®éŒ„
-        tests_dir = service_dir / 'tests'
-        service_data['has_tests'] = tests_dir.exists()
-        
+        tests_dir = service_dir / "tests"
+        service_data["has_tests"] = tests_dir.exists()
+
         # æª¢æŸ¥ requirements
-        for req_file in ['requirements.txt', 'requirements-dev.txt']:
+        for req_file in ["requirements.txt", "requirements-dev.txt"]:
             req_path = service_dir / req_file
             if req_path.exists():
-                if not service_data['requirements']:
-                    service_data['requirements'] = {}
+                if not service_data["requirements"]:
+                    service_data["requirements"] = {}
                 try:
                     with open(req_path) as f:
-                        service_data['requirements'][req_file] = f.read().splitlines()
+                        service_data["requirements"][req_file] = (
+                            f.read().splitlines()
+                        )
                 except Exception as e:
                     print(f"âš ï¸ ç„¡æ³•è®€å– {req_file}: {e}")
-        
-        if not service_data['has_tests']:
+
+        if not service_data["has_tests"]:
             return service_data
-        
+
         # åˆ‡æ›åˆ°æœå‹™ç›®éŒ„
         original_cwd = os.getcwd()
         os.chdir(service_dir)
-        
+
         try:
             # æ”¶é›†æ¸¬è©¦è¦†è“‹ç‡
             success, stdout, stderr = self.run_command(
-                'python -m pytest --cov=app --cov-report=json --cov-report=term-missing -q'
+                "python -m pytest --cov=app --cov-report=json --cov-report=term-missing -q"
             )
-            
+
             if success:
-                coverage_file = service_dir / 'coverage.json'
+                coverage_file = service_dir / "coverage.json"
                 if coverage_file.exists():
                     try:
                         with open(coverage_file) as f:
                             coverage_data = json.load(f)
-                        
-                        service_data['coverage'] = {
-                            'statements': round(coverage_data['totals']['percent_covered'] or 0),
-                            'missing': coverage_data['totals']['missing_lines'] or 0,
-                            'total': coverage_data['totals']['num_statements'] or 0
+
+                        service_data["coverage"] = {
+                            "statements": round(
+                                coverage_data["totals"]["percent_covered"] or 0
+                            ),
+                            "missing": coverage_data["totals"]["missing_lines"]
+                            or 0,
+                            "total": coverage_data["totals"]["num_statements"]
+                            or 0,
                         }
                     except Exception as e:
                         print(f"âš ï¸ {service_name} è¦†è“‹ç‡è§£æå¤±æ•—: {e}")
-            
+
             # æ”¶é›†æ¸¬è©¦çµæœ
-            success, stdout, stderr = self.run_command('python -m pytest --tb=no --quiet')
+            success, stdout, stderr = self.run_command(
+                "python -m pytest --tb=no --quiet"
+            )
             if success or stderr:  # pytest å¯èƒ½è¿”å›éé›¶ä½†æœ‰æœ‰æ•ˆè¼¸å‡º
-                service_data['test_results'] = self.parse_pytest_output(stdout + stderr)
-            
+                service_data["test_results"] = self.parse_pytest_output(
+                    stdout + stderr
+                )
+
             # æ”¶é›†ç¨‹å¼ç¢¼å“è³ª
             success, stdout, stderr = self.run_command(
-                f'flake8 app/ --max-complexity={self.tdd_config["complexity_limit"]} --statistics'
+                f"flake8 app/ --max-complexity={self.tdd_config['complexity_limit']} --statistics"
+            )
+            service_data["code_quality"] = self.parse_flake8_output(
+                stdout + stderr
             )
-            service_data['code_quality'] = self.parse_flake8_output(stdout + stderr)
-            
+
         except Exception as e:
             print(f"âš ï¸ {service_name} æ•¸æ“šæ”¶é›†éŒ¯èª¤: {e}")
         finally:
             os.chdir(original_cwd)
-        
+
         return service_data
-    
+
     def parse_test_output(self, output: str) -> Dict[str, int]:
         """è§£ææ¸¬è©¦è¼¸å‡º"""
-        results = {'total': 0, 'passed': 0, 'failed': 0, 'skipped': 0}
-        
+        results = {"total": 0, "passed": 0, "failed": 0, "skipped": 0}
+
         # ç°¡åŒ–çš„è§£æé‚è¼¯
-        lines = output.split('\n')
+        lines = output.split("\n")
         for line in lines:
-            if 'passed' in line.lower():
+            if "passed" in line.lower():
                 import re
-                match = re.search(r'(\d+)\s+passed', line)
+
+                match = re.search(r"(\d+)\s+passed", line)
                 if match:
-                    results['passed'] = int(match.group(1))
-            
-            if 'failed' in line.lower():
+                    results["passed"] = int(match.group(1))
+
+            if "failed" in line.lower():
                 import re
-                match = re.search(r'(\d+)\s+failed', line)
+
+                match = re.search(r"(\d+)\s+failed", line)
                 if match:
-                    results['failed'] = int(match.group(1))
-            
-            if 'skipped' in line.lower():
+                    results["failed"] = int(match.group(1))
+
+            if "skipped" in line.lower():
                 import re
-                match = re.search(r'(\d+)\s+skipped', line)
+
+                match = re.search(r"(\d+)\s+skipped", line)
                 if match:
-                    results['skipped'] = int(match.group(1))
-        
-        results['total'] = results['passed'] + results['failed'] + results['skipped']
+                    results["skipped"] = int(match.group(1))
+
+        results["total"] = (
+            results["passed"] + results["failed"] + results["skipped"]
+        )
         return results
-    
+
     def parse_pytest_output(self, output: str) -> Dict[str, int]:
         """è§£æ pytest è¼¸å‡º"""
-        results = {'total': 0, 'passed': 0, 'failed': 0, 'errors': 0}
-        
+        results = {"total": 0, "passed": 0, "failed": 0, "errors": 0}
+
         import re
-        
+
         # å°‹æ‰¾æ¸¬è©¦çµæœæ‘˜è¦
         summary_patterns = [
-            r'(\d+)\s+passed',
-            r'(\d+)\s+failed',
-            r'(\d+)\s+error',
-            r'(\d+)\s+skipped'
+            r"(\d+)\s+passed",
+            r"(\d+)\s+failed",
+            r"(\d+)\s+error",
+            r"(\d+)\s+skipped",
         ]
-        
+
         for pattern in summary_patterns:
             matches = re.findall(pattern, output, re.IGNORECASE)
             if matches:
                 count = int(matches[-1])  # å–æœ€å¾Œä¸€å€‹åŒ¹é…
-                if 'passed' in pattern:
-                    results['passed'] = count
-                elif 'failed' in pattern:
-                    results['failed'] = count
-                elif 'error' in pattern:
-                    results['errors'] = count
-        
-        results['total'] = results['passed'] + results['failed'] + results['errors']
+                if "passed" in pattern:
+                    results["passed"] = count
+                elif "failed" in pattern:
+                    results["failed"] = count
+                elif "error" in pattern:
+                    results["errors"] = count
+
+        results["total"] = (
+            results["passed"] + results["failed"] + results["errors"]
+        )
         return results
-    
+
     def parse_flake8_output(self, output: str) -> Dict[str, Any]:
         """è§£æ flake8 è¼¸å‡º"""
-        lines = output.strip().split('\n')
+        lines = output.strip().split("\n")
         issues = 0
         complexity_issues = 0
         error_types = {}
-        
+
         for line in lines:
-            if line.strip() and not line.startswith('Total'):
+            if line.strip() and not line.startswith("Total"):
                 issues += 1
-                
+
                 # æª¢æŸ¥è¤‡é›œåº¦å•é¡Œ
-                if 'C901' in line or 'too complex' in line:
+                if "C901" in line or "too complex" in line:
                     complexity_issues += 1
-                
+
                 # çµ±è¨ˆéŒ¯èª¤é¡å‹
                 import re
-                match = re.search(r'([A-Z]\d{3})', line)
+
+                match = re.search(r"([A-Z]\d{3})", line)
                 if match:
                     error_code = match.group(1)
-                    error_types[error_code] = error_types.get(error_code, 0) + 1
-        
+                    error_types[error_code] = (
+                        error_types.get(error_code, 0) + 1
+                    )
+
         return {
-            'total_issues': issues,
-            'complexity_issues': complexity_issues,
-            'error_types': error_types
+            "total_issues": issues,
+            "complexity_issues": complexity_issues,
+            "error_types": error_types,
         }
-    
+
     def check_tdd_compliance(self, data: Dict[str, Any]) -> Dict[str, Any]:
         """æª¢æŸ¥ TDD åˆè¦æ€§"""
-        frontend = data['frontend']
-        backend = data['backend']
-        
+        frontend = data["frontend"]
+        backend = data["backend"]
+
         compliance = {
-            'overall': False,
-            'coverage': False,
-            'test_existence': False,
-            'code_quality': False,
-            'tdd_workflow': False,
-            'score': 0,
-            'details': {}
+            "overall": False,
+            "coverage": False,
+            "test_existence": False,
+            "code_quality": False,
+            "tdd_workflow": False,
+            "score": 0,
+            "details": {},
         }
-        
+
         # æª¢æŸ¥è¦†è“‹ç‡ (40åˆ†)
-        avg_coverage = backend.get('total_coverage', 0)
-        frontend_coverage = frontend.get('coverage', {}).get('statements', 0) if frontend.get('coverage') else 0
-        
-        overall_coverage = (avg_coverage + frontend_coverage) / 2 if frontend_coverage else avg_coverage
-        compliance['coverage'] = overall_coverage >= self.tdd_config['coverage_threshold']
-        compliance['details']['coverage_score'] = overall_coverage
-        
+        avg_coverage = backend.get("total_coverage", 0)
+        frontend_coverage = (
+            frontend.get("coverage", {}).get("statements", 0)
+            if frontend.get("coverage")
+            else 0
+        )
+
+        overall_coverage = (
+            (avg_coverage + frontend_coverage) / 2
+            if frontend_coverage
+            else avg_coverage
+        )
+        compliance["coverage"] = (
+            overall_coverage >= self.tdd_config["coverage_threshold"]
+        )
+        compliance["details"]["coverage_score"] = overall_coverage
+
         # æª¢æŸ¥æ¸¬è©¦å­˜åœ¨æ€§ (30åˆ†)
-        has_backend_tests = any(s['has_tests'] for s in backend.get('services', []))
-        has_frontend_tests = frontend.get('tests_run', False)
-        compliance['test_existence'] = has_backend_tests or has_frontend_tests
-        
+        has_backend_tests = any(
+            s["has_tests"] for s in backend.get("services", [])
+        )
+        has_frontend_tests = frontend.get("tests_run", False)
+        compliance["test_existence"] = has_backend_tests or has_frontend_tests
+
         # æª¢æŸ¥ç¨‹å¼ç¢¼å“è³ª (20åˆ†)
         total_quality_issues = sum(
-            s.get('code_quality', {}).get('total_issues', 0) 
-            for s in backend.get('services', [])
+            s.get("code_quality", {}).get("total_issues", 0)
+            for s in backend.get("services", [])
         )
-        compliance['code_quality'] = total_quality_issues < 10
-        compliance['details']['quality_issues'] = total_quality_issues
-        
+        compliance["code_quality"] = total_quality_issues < 10
+        compliance["details"]["quality_issues"] = total_quality_issues
+
         # æª¢æŸ¥ TDD å·¥ä½œæµç¨‹ (10åˆ†)
-        git_info = data.get('git')
+        git_info = data.get("git")
         if git_info:
-            compliance['tdd_workflow'] = git_info.get('tdd_phase') != 'unknown'
-            compliance['details']['tdd_phase'] = git_info.get('tdd_phase')
-        
+            compliance["tdd_workflow"] = git_info.get("tdd_phase") != "unknown"
+            compliance["details"]["tdd_phase"] = git_info.get("tdd_phase")
+
         # è¨ˆç®—ç¸½åˆ†
         score = 0
-        if compliance['coverage']:
+        if compliance["coverage"]:
             score += 40
-        if compliance['test_existence']:
+        if compliance["test_existence"]:
             score += 30
-        if compliance['code_quality']:
+        if compliance["code_quality"]:
             score += 20
-        if compliance['tdd_workflow']:
+        if compliance["tdd_workflow"]:
             score += 10
-        
-        compliance['score'] = score
-        compliance['overall'] = score >= 80
-        
+
+        compliance["score"] = score
+        compliance["overall"] = score >= 80
+
         return compliance
-    
+
     def generate_html_report(self, data: Dict[str, Any]) -> Path:
         """ç”Ÿæˆ HTML å ±å‘Š"""
         print("ğŸ“ ç”Ÿæˆ HTML å ±å‘Š...")
-        
-        frontend = data['frontend']
-        backend = data['backend']
-        git = data['git']
+
+        frontend = data["frontend"]
+        backend = data["backend"]
+        git = data["git"]
         compliance = self.check_tdd_compliance(data)
-        
+
         # è®€å– HTML æ¨¡æ¿ï¼ˆé€™è£¡ç°¡åŒ–ç‚ºå…§åµŒï¼‰
         html_template = """
 <!DOCTYPE html>
@@ -544,158 +585,184 @@ class TDDReportGenerator:
     </div>
 </body>
 </html>"""
-        
+
         # æº–å‚™æ¨¡æ¿è®Šæ•¸
         template_vars = {
-            'timestamp': self.timestamp,
-            'compliance_score': compliance['score'],
-            'compliance_color': '#27ae60' if compliance['overall'] else '#e74c3c',
-            'compliance_class': 'success' if compliance['overall'] else 'error',
-            'compliance_status': 'âœ… é€šé' if compliance['overall'] else 'âŒ éœ€æ”¹é€²',
-            'coverage_class': 'success' if compliance['coverage'] else 'warning',
-            'coverage_status': 'âœ… é”æ¨™' if compliance['coverage'] else 'âš ï¸ ä¸è¶³',
-            'test_class': 'success' if compliance['test_existence'] else 'error',
-            'test_status': 'âœ… å­˜åœ¨' if compliance['test_existence'] else 'âŒ ç¼ºå¤±',
-            'quality_class': 'success' if compliance['code_quality'] else 'warning',
-            'quality_status': 'âœ… è‰¯å¥½' if compliance['code_quality'] else 'âš ï¸ æœ‰å•é¡Œ',
+            "timestamp": self.timestamp,
+            "compliance_score": compliance["score"],
+            "compliance_color": "#27ae60"
+            if compliance["overall"]
+            else "#e74c3c",
+            "compliance_class": "success"
+            if compliance["overall"]
+            else "error",
+            "compliance_status": "âœ… é€šé"
+            if compliance["overall"]
+            else "âŒ éœ€æ”¹é€²",
+            "coverage_class": "success"
+            if compliance["coverage"]
+            else "warning",
+            "coverage_status": "âœ… é”æ¨™"
+            if compliance["coverage"]
+            else "âš ï¸ ä¸è¶³",
+            "test_class": "success"
+            if compliance["test_existence"]
+            else "error",
+            "test_status": "âœ… å­˜åœ¨"
+            if compliance["test_existence"]
+            else "âŒ ç¼ºå¤±",
+            "quality_class": "success"
+            if compliance["code_quality"]
+            else "warning",
+            "quality_status": "âœ… è‰¯å¥½"
+            if compliance["code_quality"]
+            else "âš ï¸ æœ‰å•é¡Œ",
         }
-        
+
         # Git è³‡è¨Š
         if git:
-            template_vars['git_info'] = f"""
-                <p><strong>æäº¤:</strong> {git['short_hash']} - {git['commit_message']}</p>
-                <p><strong>åˆ†æ”¯:</strong> {git['branch']} | <strong>ä½œè€…:</strong> {git['author']}</p>
-                <p><strong>æ™‚é–“:</strong> {git['date']}</p>
-                <span class="tdd-phase phase-{git['tdd_phase']}">TDD {git['tdd_phase'].upper()} éšæ®µ</span>
+            template_vars["git_info"] = f"""
+                <p><strong>æäº¤:</strong> {git["short_hash"]} - {git["commit_message"]}</p>
+                <p><strong>åˆ†æ”¯:</strong> {git["branch"]} | <strong>ä½œè€…:</strong> {git["author"]}</p>
+                <p><strong>æ™‚é–“:</strong> {git["date"]}</p>
+                <span class="tdd-phase phase-{git["tdd_phase"]}">TDD {git["tdd_phase"].upper()} éšæ®µ</span>
             """
         else:
-            template_vars['git_info'] = '<p>âš ï¸ ç„¡æ³•å–å¾— Git è³‡è¨Š</p>'
-        
+            template_vars["git_info"] = "<p>âš ï¸ ç„¡æ³•å–å¾— Git è³‡è¨Š</p>"
+
         # å‰ç«¯å…§å®¹
-        if frontend['exists']:
+        if frontend["exists"]:
             frontend_html = []
-            if frontend.get('coverage'):
-                cov = frontend['coverage']
+            if frontend.get("coverage"):
+                cov = frontend["coverage"]
                 frontend_html.append(f"""
                     <div class="metric">
                         <span>èªå¥è¦†è“‹ç‡</span>
-                        <span class="metric-value {'success' if cov['statements'] >= 90 else 'warning'}">
-                            {cov['statements']}%
+                        <span class="metric-value {"success" if cov["statements"] >= 90 else "warning"}">
+                            {cov["statements"]}%
                         </span>
                     </div>
                     <div class="coverage-bar">
-                        <div class="coverage-fill" style="width: {cov['statements']}%"></div>
+                        <div class="coverage-fill" style="width: {cov["statements"]}%"></div>
                     </div>
                 """)
-            
-            if frontend.get('test_results'):
-                test = frontend['test_results']
+
+            if frontend.get("test_results"):
+                test = frontend["test_results"]
                 frontend_html.append(f"""
                     <div class="metric">
                         <span>é€šéæ¸¬è©¦</span>
-                        <span class="metric-value success">{test['passed']}</span>
+                        <span class="metric-value success">{test["passed"]}</span>
                     </div>
                     <div class="metric">
                         <span>å¤±æ•—æ¸¬è©¦</span>
-                        <span class="metric-value {'error' if test['failed'] > 0 else 'success'}">
-                            {test['failed']}
+                        <span class="metric-value {"error" if test["failed"] > 0 else "success"}">
+                            {test["failed"]}
                         </span>
                     </div>
                 """)
-            
-            template_vars['frontend_content'] = ''.join(frontend_html) or '<p>âš ï¸ æœªæ‰¾åˆ°æ¸¬è©¦æ•¸æ“š</p>'
+
+            template_vars["frontend_content"] = (
+                "".join(frontend_html) or "<p>âš ï¸ æœªæ‰¾åˆ°æ¸¬è©¦æ•¸æ“š</p>"
+            )
         else:
-            template_vars['frontend_content'] = '<p class="error">âŒ å‰ç«¯ç›®éŒ„ä¸å­˜åœ¨</p>'
-        
+            template_vars["frontend_content"] = (
+                '<p class="error">âŒ å‰ç«¯ç›®éŒ„ä¸å­˜åœ¨</p>'
+            )
+
         # å¾Œç«¯å…§å®¹
-        if backend['services']:
+        if backend["services"]:
             backend_html = []
             backend_html.append(f"""
                 <div class="metric">
                     <span>æœå‹™ç¸½æ•¸</span>
-                    <span class="metric-value">{backend['total_services']}</span>
+                    <span class="metric-value">{backend["total_services"]}</span>
                 </div>
                 <div class="metric">
                     <span>å¹³å‡è¦†è“‹ç‡</span>
-                    <span class="metric-value {'success' if backend['total_coverage'] >= 90 else 'warning'}">
-                        {backend['total_coverage']:.1f}%
+                    <span class="metric-value {"success" if backend["total_coverage"] >= 90 else "warning"}">
+                        {backend["total_coverage"]:.1f}%
                     </span>
                 </div>
                 <div class="metric">
                     <span>ç¸½æ¸¬è©¦æ•¸</span>
-                    <span class="metric-value">{backend['total_tests']}</span>
+                    <span class="metric-value">{backend["total_tests"]}</span>
                 </div>
             """)
-            
-            template_vars['backend_content'] = ''.join(backend_html)
+
+            template_vars["backend_content"] = "".join(backend_html)
         else:
-            template_vars['backend_content'] = '<p>âš ï¸ æœªæ‰¾åˆ°å¾Œç«¯æœå‹™</p>'
-        
+            template_vars["backend_content"] = "<p>âš ï¸ æœªæ‰¾åˆ°å¾Œç«¯æœå‹™</p>"
+
         # ç”Ÿæˆ HTML
         html_content = html_template.format(**template_vars)
-        
-        report_path = self.report_dir / 'index.html'
-        with open(report_path, 'w', encoding='utf-8') as f:
+
+        report_path = self.report_dir / "index.html"
+        with open(report_path, "w", encoding="utf-8") as f:
             f.write(html_content)
-        
+
         return report_path
-    
+
     def generate_json_report(self, data: Dict[str, Any]) -> Path:
         """ç”Ÿæˆ JSON å ±å‘Š"""
         print("ğŸ“„ ç”Ÿæˆ JSON å ±å‘Š...")
-        
+
         report_data = {
-            'timestamp': self.timestamp,
-            'version': '1.0.0',
-            'tdd_config': self.tdd_config,
-            'compliance': self.check_tdd_compliance(data),
-            **data
+            "timestamp": self.timestamp,
+            "version": "1.0.0",
+            "tdd_config": self.tdd_config,
+            "compliance": self.check_tdd_compliance(data),
+            **data,
         }
-        
-        report_path = self.report_dir / 'report.json'
-        with open(report_path, 'w', encoding='utf-8') as f:
+
+        report_path = self.report_dir / "report.json"
+        with open(report_path, "w", encoding="utf-8") as f:
             json.dump(report_data, f, indent=2, ensure_ascii=False)
-        
+
         return report_path
-    
+
     def generate(self):
         """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
         print("ğŸ§¬ é–‹å§‹ç”Ÿæˆ TDD æ¸¬è©¦å ±å‘Š...")
         print("=" * 50)
-        
+
         try:
             # æ”¶é›†æ‰€æœ‰æ•¸æ“š
             data = {
-                'git': self.collect_git_info(),
-                'frontend': self.collect_frontend_data(),
-                'backend': self.collect_backend_data()
+                "git": self.collect_git_info(),
+                "frontend": self.collect_frontend_data(),
+                "backend": self.collect_backend_data(),
             }
-            
+
             # ç”Ÿæˆå ±å‘Š
             html_path = self.generate_html_report(data)
             json_path = self.generate_json_report(data)
-            
+
             print("=" * 50)
             print("âœ… TDD æ¸¬è©¦å ±å‘Šç”Ÿæˆå®Œæˆï¼")
             print(f"ğŸ“„ HTML å ±å‘Š: {html_path}")
             print(f"ğŸ”§ JSON å ±å‘Š: {json_path}")
-            
+
             # é¡¯ç¤ºæ‘˜è¦
             compliance = self.check_tdd_compliance(data)
             print(f"ğŸ“Š TDD åˆè¦æ€§è©•åˆ†: {compliance['score']}/100")
-            print(f"ğŸ¯ æ•´é«”åˆè¦: {'âœ… é€šé' if compliance['overall'] else 'âŒ éœ€æ”¹é€²'}")
-            
+            print(
+                f"ğŸ¯ æ•´é«”åˆè¦: {'âœ… é€šé' if compliance['overall'] else 'âŒ éœ€æ”¹é€²'}"
+            )
+
             return True
-            
+
         except Exception as e:
             print(f"âŒ å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
             import traceback
+
             traceback.print_exc()
             return False
 
+
 def main():
     """ä¸»å‡½æ•¸"""
-    if len(sys.argv) > 1 and sys.argv[1] == '--help':
+    if len(sys.argv) > 1 and sys.argv[1] == "--help":
         print("""
 TDD æ¸¬è©¦å ±å‘Šç”Ÿæˆå·¥å…·
 
@@ -713,10 +780,11 @@ TDD æ¸¬è©¦å ±å‘Šç”Ÿæˆå·¥å…·
     - tdd-reports/report.json - JSON æ ¼å¼æ•¸æ“š
         """)
         return
-    
+
     generator = TDDReportGenerator()
     success = generator.generate()
     sys.exit(0 if success else 1)
 
+
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
diff --git a/auto_generate_video_fold6/scripts/integration_test.py b/auto_generate_video_fold6/scripts/integration_test.py
index 4882ea4..3b04020 100644
--- a/auto_generate_video_fold6/scripts/integration_test.py
+++ b/auto_generate_video_fold6/scripts/integration_test.py
@@ -49,7 +49,9 @@ class ServiceHealth:
 class SystemIntegrationTester:
     """ç³»çµ±æ•´åˆæ¸¬è©¦å™¨"""
 
-    def __init__(self, config_file: str = "config/integration-test-config.json"):
+    def __init__(
+        self, config_file: str = "config/integration-test-config.json"
+    ):
         self.config = self._load_config(config_file)
         self.docker_client = docker.from_env()
         self.test_results: List[TestResult] = []
@@ -120,7 +122,9 @@ class SystemIntegrationTester:
             }
         )
 
-        logger.info(f"âœ… ç³»çµ±æ•´åˆæ¸¬è©¦å®Œæˆï¼Œç‹€æ…‹: {test_results['overall_status']}")
+        logger.info(
+            f"âœ… ç³»çµ±æ•´åˆæ¸¬è©¦å®Œæˆï¼Œç‹€æ…‹: {test_results['overall_status']}"
+        )
         return test_results
 
     async def _test_infrastructure(self) -> Dict[str, Any]:
@@ -131,7 +135,9 @@ class SystemIntegrationTester:
         try:
             # æª¢æŸ¥ Docker å®¹å™¨ç‹€æ…‹
             containers = self.docker_client.containers.list()
-            running_services = [c.name for c in containers if c.status == "running"]
+            running_services = [
+                c.name for c in containers if c.status == "running"
+            ]
 
             # æª¢æŸ¥è³‡æ–™åº«é€£æ¥
             db_status = await self._check_database_connections()
@@ -162,7 +168,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     async def _test_service_health(self) -> Dict[str, Any]:
         """æ¸¬è©¦æœå‹™å¥åº·ç‹€æ…‹"""
@@ -174,9 +184,13 @@ class SystemIntegrationTester:
             health_results = {}
 
             # ä¸¦ç™¼æª¢æŸ¥æ‰€æœ‰æœå‹™
-            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
+            with concurrent.futures.ThreadPoolExecutor(
+                max_workers=10
+            ) as executor:
                 futures = {
-                    executor.submit(self._check_service_health, service): service
+                    executor.submit(
+                        self._check_service_health, service
+                    ): service
                     for service in services
                 }
 
@@ -185,7 +199,10 @@ class SystemIntegrationTester:
                     try:
                         health_results[service] = future.result()
                     except Exception as e:
-                        health_results[service] = {"status": "unhealthy", "error": str(e)}
+                        health_results[service] = {
+                            "status": "unhealthy",
+                            "error": str(e),
+                        }
 
             # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰æ ¸å¿ƒæœå‹™éƒ½å¥åº·
             healthy_services = [
@@ -204,7 +221,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     async def _test_api_integration(self) -> Dict[str, Any]:
         """æ¸¬è©¦ API æ•´åˆ"""
@@ -213,16 +234,32 @@ class SystemIntegrationTester:
 
         try:
             test_cases = [
-                {"method": "GET", "endpoint": "/health", "expected_status": 200},
-                {"method": "GET", "endpoint": "/api/v1/status", "expected_status": 200},
+                {
+                    "method": "GET",
+                    "endpoint": "/health",
+                    "expected_status": 200,
+                },
+                {
+                    "method": "GET",
+                    "endpoint": "/api/v1/status",
+                    "expected_status": 200,
+                },
                 {
                     "method": "POST",
                     "endpoint": "/api/v1/auth/login",
                     "data": {"username": "test", "password": "test"},
                     "expected_status": [200, 401],
                 },
-                {"method": "GET", "endpoint": "/api/v1/videos", "expected_status": [200, 401]},
-                {"method": "GET", "endpoint": "/api/v1/ai/models", "expected_status": [200, 401]},
+                {
+                    "method": "GET",
+                    "endpoint": "/api/v1/videos",
+                    "expected_status": [200, 401],
+                },
+                {
+                    "method": "GET",
+                    "endpoint": "/api/v1/ai/models",
+                    "expected_status": [200, 401],
+                },
             ]
 
             results = []
@@ -231,7 +268,9 @@ class SystemIntegrationTester:
                 results.append(result)
 
             passed_tests = [r for r in results if r["status"] == "PASS"]
-            overall_pass = len(passed_tests) >= len(test_cases) * 0.8  # 80% é€šéç‡
+            overall_pass = (
+                len(passed_tests) >= len(test_cases) * 0.8
+            )  # 80% é€šéç‡
 
             return {
                 "status": "PASS" if overall_pass else "FAIL",
@@ -243,7 +282,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     async def _test_authentication_flow(self) -> Dict[str, Any]:
         """æ¸¬è©¦èªè­‰æµç¨‹"""
@@ -281,7 +324,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     async def _test_cache_system(self) -> Dict[str, Any]:
         """æ¸¬è©¦å¿«å–ç³»çµ±"""
@@ -319,7 +366,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     async def _test_backup_recovery(self) -> Dict[str, Any]:
         """æ¸¬è©¦å‚™ä»½æ¢å¾©ç³»çµ±"""
@@ -352,7 +403,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     async def _test_compliance_framework(self) -> Dict[str, Any]:
         """æ¸¬è©¦åˆè¦æ€§æ¡†æ¶"""
@@ -370,7 +425,10 @@ class SystemIntegrationTester:
             retention_policy_result = await self._test_retention_policies()
 
             all_compliance_tests_passed = all(
-                [gdpr_result.get("status") == "PASS", audit_logging_result.get("status") == "PASS"]
+                [
+                    gdpr_result.get("status") == "PASS",
+                    audit_logging_result.get("status") == "PASS",
+                ]
             )
 
             return {
@@ -382,7 +440,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     async def _test_load_performance(self) -> Dict[str, Any]:
         """æ¸¬è©¦è² è¼‰æ€§èƒ½"""
@@ -395,13 +457,17 @@ class SystemIntegrationTester:
             duration = load_config.get("duration_seconds", 30)
 
             # åŸ·è¡Œè² è¼‰æ¸¬è©¦
-            performance_metrics = await self._execute_load_test(concurrent_users, duration)
+            performance_metrics = await self._execute_load_test(
+                concurrent_users, duration
+            )
 
             # è©•ä¼°æ€§èƒ½æŒ‡æ¨™
             performance_acceptable = (
                 performance_metrics.get("avg_response_time_ms", 9999) < 500
-                and performance_metrics.get("error_rate", 1.0) < 0.05  # <5% éŒ¯èª¤ç‡
-                and performance_metrics.get("throughput_rps", 0) > 100  # >100 RPS
+                and performance_metrics.get("error_rate", 1.0)
+                < 0.05  # <5% éŒ¯èª¤ç‡
+                and performance_metrics.get("throughput_rps", 0)
+                > 100  # >100 RPS
             )
 
             return {
@@ -412,7 +478,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     async def _test_disaster_recovery(self) -> Dict[str, Any]:
         """æ¸¬è©¦ç½é›£æ¢å¾©"""
@@ -427,7 +497,9 @@ class SystemIntegrationTester:
             auto_recovery_result = await self._test_auto_recovery()
 
             # æ¸¬è©¦æ•¸æ“šä¸€è‡´æ€§
-            data_consistency_result = await self._test_data_consistency_after_recovery()
+            data_consistency_result = (
+                await self._test_data_consistency_after_recovery()
+            )
 
             dr_tests_passed = all(
                 [
@@ -445,7 +517,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     async def _test_end_to_end_workflows(self) -> Dict[str, Any]:
         """æ¸¬è©¦ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹"""
@@ -454,7 +530,9 @@ class SystemIntegrationTester:
 
         try:
             # æ¸¬è©¦å½±ç‰‡ç”Ÿæˆå®Œæ•´æµç¨‹
-            video_generation_result = await self._test_video_generation_workflow()
+            video_generation_result = (
+                await self._test_video_generation_workflow()
+            )
 
             # æ¸¬è©¦ç”¨æˆ¶è¨»å†Šåˆ°ä½¿ç”¨çš„å®Œæ•´æµç¨‹
             user_journey_result = await self._test_complete_user_journey()
@@ -478,7 +556,11 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
     # è¼”åŠ©æ–¹æ³•å¯¦ç¾
     async def _check_database_connections(self) -> Dict[str, bool]:
@@ -536,7 +618,9 @@ class SystemIntegrationTester:
         except Exception as e:
             return {"status": "unhealthy", "error": str(e)}
 
-    async def _execute_api_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
+    async def _execute_api_test(
+        self, test_case: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """åŸ·è¡Œ API æ¸¬è©¦"""
         start_time = time.time()
         try:
@@ -546,9 +630,14 @@ class SystemIntegrationTester:
             if method == "GET":
                 response = requests.get(url, timeout=self.test_timeout)
             elif method == "POST":
-                response = requests.post(url, json=test_case.get("data"), timeout=self.test_timeout)
+                response = requests.post(
+                    url, json=test_case.get("data"), timeout=self.test_timeout
+                )
             else:
-                return {"status": "SKIP", "reason": f"ä¸æ”¯æŒçš„ HTTP æ–¹æ³•: {method}"}
+                return {
+                    "status": "SKIP",
+                    "reason": f"ä¸æ”¯æŒçš„ HTTP æ–¹æ³•: {method}",
+                }
 
             expected_status = test_case["expected_status"]
             if isinstance(expected_status, list):
@@ -565,9 +654,15 @@ class SystemIntegrationTester:
             }
 
         except Exception as e:
-            return {"status": "FAIL", "duration_seconds": time.time() - start_time, "error": str(e)}
+            return {
+                "status": "FAIL",
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+            }
 
-    def _generate_test_summary(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
+    def _generate_test_summary(
+        self, test_results: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """ç”Ÿæˆæ¸¬è©¦æ‘˜è¦"""
         total_tests = 0
         passed_tests = 0
@@ -650,7 +745,9 @@ class SystemIntegrationTester:
     async def _test_retention_policies(self) -> Dict[str, Any]:
         return {"status": "PASS", "message": "ä¿ç•™æ”¿ç­–æ¸¬è©¦é€šé"}
 
-    async def _execute_load_test(self, concurrent_users: int, duration: int) -> Dict[str, Any]:
+    async def _execute_load_test(
+        self, concurrent_users: int, duration: int
+    ) -> Dict[str, Any]:
         return {
             "avg_response_time_ms": 150,
             "error_rate": 0.02,
@@ -685,9 +782,13 @@ async def main():
 
     parser = argparse.ArgumentParser(description="ç³»çµ±æ•´åˆæ¸¬è©¦")
     parser.add_argument(
-        "--config", default="config/integration-test-config.json", help="é…ç½®æª”æ¡ˆè·¯å¾‘"
+        "--config",
+        default="config/integration-test-config.json",
+        help="é…ç½®æª”æ¡ˆè·¯å¾‘",
+    )
+    parser.add_argument(
+        "--output", default="test-results.json", help="çµæœè¼¸å‡ºæª”æ¡ˆ"
     )
-    parser.add_argument("--output", default="test-results.json", help="çµæœè¼¸å‡ºæª”æ¡ˆ")
     parser.add_argument("--verbose", action="store_true", help="è©³ç´°è¼¸å‡º")
 
     args = parser.parse_args()
@@ -695,7 +796,8 @@ async def main():
     # è¨­ç½®æ—¥èªŒ
     log_level = logging.DEBUG if args.verbose else logging.INFO
     logging.basicConfig(
-        level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+        level=log_level,
+        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
     )
 
     # åŸ·è¡Œæ¸¬è©¦
@@ -708,9 +810,9 @@ async def main():
 
     # è¼¸å‡ºæ‘˜è¦
     summary = results.get("summary", {})
-    print(f"\n{'='*50}")
+    print(f"\n{'=' * 50}")
     print("ğŸ” ç³»çµ±æ•´åˆæ¸¬è©¦çµæœæ‘˜è¦")
-    print(f"{'='*50}")
+    print(f"{'=' * 50}")
     print(f"ç¸½æ¸¬è©¦æ•¸: {summary.get('total_tests', 0)}")
     print(f"é€šéæ¸¬è©¦: {summary.get('passed_tests', 0)}")
     print(f"å¤±æ•—æ¸¬è©¦: {summary.get('failed_tests', 0)}")
@@ -718,7 +820,7 @@ async def main():
     print(f"å“è³ªç­‰ç´š: {summary.get('quality_grade', 'Unknown')}")
     print(f"æ•´é«”ç‹€æ…‹: {results.get('overall_status', 'Unknown')}")
     print(f"ç¸½æŒçºŒæ™‚é–“: {results.get('total_duration_seconds', 0):.2f} ç§’")
-    print(f"{'='*50}")
+    print(f"{'=' * 50}")
 
     if results.get("overall_status") == "PASS":
         print("âœ… æ‰€æœ‰æ•´åˆæ¸¬è©¦é€šéï¼ç³»çµ±å·²æº–å‚™å¥½é€²è¡Œç”Ÿç”¢éƒ¨ç½²ã€‚")
diff --git a/auto_generate_video_fold6/scripts/mock_services_e2e.py b/auto_generate_video_fold6/scripts/mock_services_e2e.py
index a38c08d..060ce46 100644
--- a/auto_generate_video_fold6/scripts/mock_services_e2e.py
+++ b/auto_generate_video_fold6/scripts/mock_services_e2e.py
@@ -25,7 +25,7 @@ MOCK_DATA = {
             "user_id": "e2e_test_user",
             "email": "e2e@test.com",
             "username": "e2e_tester",
-            "created_at": datetime.utcnow().isoformat()
+            "created_at": datetime.utcnow().isoformat(),
         }
     },
     "trends": [
@@ -36,7 +36,7 @@ MOCK_DATA = {
             "engagement_score": 0.855,
             "view_count": 1250000,
             "hashtags": ["#AI", "#technology", "#innovation"],
-            "created_at": datetime.utcnow().isoformat()
+            "created_at": datetime.utcnow().isoformat(),
         },
         {
             "id": 2,
@@ -45,7 +45,7 @@ MOCK_DATA = {
             "engagement_score": 0.782,
             "view_count": 890000,
             "hashtags": ["#ML", "#tutorial", "#coding"],
-            "created_at": datetime.utcnow().isoformat()
+            "created_at": datetime.utcnow().isoformat(),
         },
         {
             "id": 3,
@@ -54,27 +54,30 @@ MOCK_DATA = {
             "engagement_score": 0.921,
             "view_count": 2100000,
             "hashtags": ["#automation", "#business", "#tools"],
-            "created_at": datetime.utcnow().isoformat()
-        }
+            "created_at": datetime.utcnow().isoformat(),
+        },
     ],
     "workflows": {},
     "scheduled_tasks": {},
-    "publish_records": {}
+    "publish_records": {},
 }
 
+
 class MockTrendService:
     """è¶¨å‹¢æœå‹™æ¨¡æ“¬"""
-    
+
     async def handle_health(self, request):
-        return web.json_response({"status": "healthy", "service": "trend-service"})
-    
+        return web.json_response(
+            {"status": "healthy", "service": "trend-service"}
+        )
+
     async def handle_fetch_trends(self, request):
         data = await request.json()
         categories = data.get("categories", [])
         platforms = data.get("platforms", [])
         hours_back = data.get("hours_back", 24)
         min_engagement = data.get("min_engagement", 1000)
-        
+
         # æ ¹æ“šåƒæ•¸ç¯©é¸è¶¨å‹¢
         filtered_trends = []
         for trend in MOCK_DATA["trends"]:
@@ -83,45 +86,46 @@ class MockTrendService:
             if trend["view_count"] < min_engagement:
                 continue
             filtered_trends.append(trend)
-        
+
         # æ¨¡æ“¬è™•ç†å»¶é²
         await asyncio.sleep(0.5)
-        
-        return web.json_response({
-            "trends": filtered_trends,
-            "total_count": len(filtered_trends),
-            "categories_analyzed": categories or ["technology", "ai"],
-            "processing_time": 0.5
-        })
+
+        return web.json_response(
+            {
+                "trends": filtered_trends,
+                "total_count": len(filtered_trends),
+                "categories_analyzed": categories or ["technology", "ai"],
+                "processing_time": 0.5,
+            }
+        )
+
 
 class MockVideoService:
     """å½±ç‰‡æœå‹™æ¨¡æ“¬"""
-    
+
     async def handle_health(self, request):
-        return web.json_response({"status": "healthy", "service": "video-service"})
-    
+        return web.json_response(
+            {"status": "healthy", "service": "video-service"}
+        )
+
     async def handle_create_workflow(self, request):
         data = await request.json()
         user_id = data.get("user_id")
         trend_keywords = data.get("trend_keywords", [])
         video_count = data.get("video_count", 1)
-        
+
         logger.info(f"æ”¶åˆ°å·¥ä½œæµç¨‹å‰µå»ºè«‹æ±‚: {data}")
-        
+
         if not user_id:
-            return web.json_response(
-                {"error": "Missing user_id"}, 
-                status=400
-            )
-        
+            return web.json_response({"error": "Missing user_id"}, status=400)
+
         if video_count <= 0:
             return web.json_response(
-                {"error": "Invalid video_count"}, 
-                status=400
+                {"error": "Invalid video_count"}, status=400
             )
-        
+
         # trend_keywords å¯ä»¥ç‚ºç©ºï¼Œç³»çµ±æœƒè‡ªå‹•æŠ“å–
-        
+
         workflow_id = str(uuid.uuid4())
         workflow_data = {
             "workflow_id": workflow_id,
@@ -132,102 +136,107 @@ class MockVideoService:
             "completed_steps": 0,
             "step_results": {},
             "videos_generated": 0,
-            "created_at": datetime.utcnow().isoformat()
+            "created_at": datetime.utcnow().isoformat(),
         }
-        
+
         MOCK_DATA["workflows"][workflow_id] = workflow_data
-        
+
         # å•Ÿå‹•æ¨¡æ“¬å·¥ä½œæµç¨‹
         asyncio.create_task(self._simulate_workflow(workflow_id, video_count))
-        
-        return web.json_response({
-            "workflow_id": workflow_id,
-            "status": "started",
-            "estimated_duration": 60,
-            "message": "å½±ç‰‡å·¥ä½œæµç¨‹å·²å•Ÿå‹•"
-        })
-    
+
+        return web.json_response(
+            {
+                "workflow_id": workflow_id,
+                "status": "started",
+                "estimated_duration": 60,
+                "message": "å½±ç‰‡å·¥ä½œæµç¨‹å·²å•Ÿå‹•",
+            }
+        )
+
     async def handle_workflow_status(self, request):
-        workflow_id = request.match_info['workflow_id']
-        
+        workflow_id = request.match_info["workflow_id"]
+
         if workflow_id not in MOCK_DATA["workflows"]:
             return web.json_response(
-                {"error": "Workflow not found"}, 
-                status=404
+                {"error": "Workflow not found"}, status=404
             )
-        
+
         workflow = MOCK_DATA["workflows"][workflow_id]
         return web.json_response(workflow)
-    
+
     async def _simulate_workflow(self, workflow_id: str, video_count: int):
         """æ¨¡æ“¬å·¥ä½œæµç¨‹åŸ·è¡Œ"""
         workflow = MOCK_DATA["workflows"][workflow_id]
         steps = [
             "trend_analysis",
-            "keyword_extraction", 
+            "keyword_extraction",
             "script_generation",
             "image_generation",
             "voice_synthesis",
             "video_composition",
-            "quality_check"
+            "quality_check",
         ]
-        
+
         try:
             for i, step in enumerate(steps, 1):
                 # æ¨¡æ“¬æ¯æ­¥é©Ÿè™•ç†æ™‚é–“
                 await asyncio.sleep(2)
-                
+
                 workflow["current_step"] = step
                 workflow["completed_steps"] = i
                 workflow["step_results"][step] = {
                     "success": True,
                     "duration": 2.0,
-                    "details": f"Mock {step} completed successfully"
+                    "details": f"Mock {step} completed successfully",
                 }
                 workflow["updated_at"] = datetime.utcnow().isoformat()
-                
+
                 logger.info(f"å·¥ä½œæµç¨‹ {workflow_id} å®Œæˆæ­¥é©Ÿ: {step}")
-            
+
             # å·¥ä½œæµç¨‹å®Œæˆ
             workflow["status"] = "completed"
             workflow["videos_generated"] = video_count
             workflow["generated_videos"] = [
                 {
                     "video_id": f"video_{uuid.uuid4()}",
-                    "title": f"AI è¶¨å‹¢åˆ†æå½±ç‰‡ {i+1}",
+                    "title": f"AI è¶¨å‹¢åˆ†æå½±ç‰‡ {i + 1}",
                     "description": "åŸºæ–¼æœ€æ–°è¶¨å‹¢æ•¸æ“šç”Ÿæˆçš„å½±ç‰‡",
                     "duration": 30,
                     "quality": "1080p",
-                    "tags": ["AI", "technology", "trends"]
+                    "tags": ["AI", "technology", "trends"],
                 }
                 for i in range(video_count)
             ]
-            
-            logger.info(f"å·¥ä½œæµç¨‹ {workflow_id} å®Œæˆï¼Œç”Ÿæˆ {video_count} å€‹å½±ç‰‡")
-            
+
+            logger.info(
+                f"å·¥ä½œæµç¨‹ {workflow_id} å®Œæˆï¼Œç”Ÿæˆ {video_count} å€‹å½±ç‰‡"
+            )
+
         except Exception as e:
             workflow["status"] = "failed"
             workflow["error"] = str(e)
             logger.error(f"å·¥ä½œæµç¨‹ {workflow_id} å¤±æ•—: {e}")
 
+
 class MockSocialService:
     """ç¤¾ç¾¤æœå‹™æ¨¡æ“¬"""
-    
+
     async def handle_health(self, request):
-        return web.json_response({"status": "healthy", "service": "social-service"})
-    
+        return web.json_response(
+            {"status": "healthy", "service": "social-service"}
+        )
+
     async def handle_publish(self, request):
         data = await request.json()
         user_id = data.get("user_id")
         video_id = data.get("video_id")
         platforms = data.get("platforms", [])
-        
+
         if not all([user_id, video_id, platforms]):
             return web.json_response(
-                {"error": "Missing required parameters"}, 
-                status=400
+                {"error": "Missing required parameters"}, status=400
             )
-        
+
         publish_id = str(uuid.uuid4())
         publish_data = {
             "publish_id": publish_id,
@@ -237,176 +246,214 @@ class MockSocialService:
             "status": "in_progress",
             "platform_results": {},
             "all_completed": False,
-            "created_at": datetime.utcnow().isoformat()
+            "created_at": datetime.utcnow().isoformat(),
         }
-        
+
         MOCK_DATA["publish_records"][publish_id] = publish_data
-        
+
         # å•Ÿå‹•æ¨¡æ“¬ç™¼å¸ƒæµç¨‹
         asyncio.create_task(self._simulate_publishing(publish_id, platforms))
-        
-        return web.json_response({
-            "publish_id": publish_id,
-            "status": "started",
-            "message": "ç™¼å¸ƒæµç¨‹å·²å•Ÿå‹•"
-        })
-    
+
+        return web.json_response(
+            {
+                "publish_id": publish_id,
+                "status": "started",
+                "message": "ç™¼å¸ƒæµç¨‹å·²å•Ÿå‹•",
+            }
+        )
+
     async def handle_publish_status(self, request):
-        publish_id = request.match_info['publish_id']
-        
+        publish_id = request.match_info["publish_id"]
+
         if publish_id not in MOCK_DATA["publish_records"]:
             return web.json_response(
-                {"error": "Publish record not found"}, 
-                status=404
+                {"error": "Publish record not found"}, status=404
             )
-        
+
         publish_record = MOCK_DATA["publish_records"][publish_id]
         return web.json_response(publish_record)
-    
-    async def _simulate_publishing(self, publish_id: str, platforms: List[str]):
+
+    async def _simulate_publishing(
+        self, publish_id: str, platforms: List[str]
+    ):
         """æ¨¡æ“¬ç™¼å¸ƒæµç¨‹"""
         publish_record = MOCK_DATA["publish_records"][publish_id]
-        
+
         try:
             for platform in platforms:
                 # æ¨¡æ“¬ç™¼å¸ƒå»¶é²
                 await asyncio.sleep(3)
-                
+
                 # æ¨¡æ“¬æˆåŠŸç™¼å¸ƒ
                 publish_record["platform_results"][platform] = {
                     "status": "success",
                     "platform_post_id": f"{platform}_{uuid.uuid4()}",
                     "published_at": datetime.utcnow().isoformat(),
-                    "url": f"https://{platform}.com/post/{uuid.uuid4()}"
+                    "url": f"https://{platform}.com/post/{uuid.uuid4()}",
                 }
-                
+
                 logger.info(f"ç™¼å¸ƒ {publish_id} åˆ° {platform} æˆåŠŸ")
-            
+
             publish_record["status"] = "completed"
             publish_record["all_completed"] = True
-            
+
         except Exception as e:
             publish_record["status"] = "failed"
             publish_record["error"] = str(e)
             logger.error(f"ç™¼å¸ƒ {publish_id} å¤±æ•—: {e}")
 
+
 class MockSchedulerService:
     """æ’ç¨‹æœå‹™æ¨¡æ“¬"""
-    
+
     def __init__(self):
         self.is_running = False
         self.config = {}
-    
+
     async def handle_health(self, request):
-        return web.json_response({
-            "status": "healthy", 
-            "service": "scheduler-service",
-            "is_running": self.is_running
-        })
-    
+        return web.json_response(
+            {
+                "status": "healthy",
+                "service": "scheduler-service",
+                "is_running": self.is_running,
+            }
+        )
+
     async def handle_configure(self, request):
         self.config = await request.json()
-        return web.json_response({
-            "message": "æ’ç¨‹å™¨é…ç½®å·²æ›´æ–°",
-            "config": self.config,
-            "status": "configured"
-        })
-    
+        return web.json_response(
+            {
+                "message": "æ’ç¨‹å™¨é…ç½®å·²æ›´æ–°",
+                "config": self.config,
+                "status": "configured",
+            }
+        )
+
     async def handle_start(self, request):
         self.is_running = True
-        return web.json_response({
-            "message": "æ’ç¨‹å™¨å·²å•Ÿå‹•",
-            "status": "running",
-            "started_at": datetime.utcnow().isoformat()
-        })
-    
+        return web.json_response(
+            {
+                "message": "æ’ç¨‹å™¨å·²å•Ÿå‹•",
+                "status": "running",
+                "started_at": datetime.utcnow().isoformat(),
+            }
+        )
+
     async def handle_stop(self, request):
         self.is_running = False
-        return web.json_response({
-            "message": "æ’ç¨‹å™¨å·²åœæ­¢",
-            "status": "stopped",
-            "stopped_at": datetime.utcnow().isoformat()
-        })
-    
+        return web.json_response(
+            {
+                "message": "æ’ç¨‹å™¨å·²åœæ­¢",
+                "status": "stopped",
+                "stopped_at": datetime.utcnow().isoformat(),
+            }
+        )
+
     async def handle_status(self, request):
-        return web.json_response({
-            "is_running": self.is_running,
-            "daily_stats": {
-                "videos_generated": 0,
-                "budget_used": 0.0,
-                "tasks_completed": 0,
-                "tasks_failed": 0
-            },
-            "active_tasks_count": 0,
-            "scheduled_tasks_count": len(MOCK_DATA["scheduled_tasks"]),
-            "config": self.config
-        })
+        return web.json_response(
+            {
+                "is_running": self.is_running,
+                "daily_stats": {
+                    "videos_generated": 0,
+                    "budget_used": 0.0,
+                    "tasks_completed": 0,
+                    "tasks_failed": 0,
+                },
+                "active_tasks_count": 0,
+                "scheduled_tasks_count": len(MOCK_DATA["scheduled_tasks"]),
+                "config": self.config,
+            }
+        )
+
 
 async def create_mock_services():
     """å‰µå»ºæ‰€æœ‰æ¨¡æ“¬æœå‹™"""
-    
+
     # è¶¨å‹¢æœå‹™ (ç«¯å£ 8001)
     trend_service = MockTrendService()
     trend_app = web.Application()
-    trend_app.router.add_get('/health', trend_service.handle_health)
-    trend_app.router.add_post('/api/v1/entrepreneur/fetch-trends', trend_service.handle_fetch_trends)
-    
+    trend_app.router.add_get("/health", trend_service.handle_health)
+    trend_app.router.add_post(
+        "/api/v1/entrepreneur/fetch-trends", trend_service.handle_fetch_trends
+    )
+
     # å½±ç‰‡æœå‹™ (ç«¯å£ 8003)
     video_service = MockVideoService()
     video_app = web.Application()
-    video_app.router.add_get('/health', video_service.handle_health)
-    video_app.router.add_post('/api/v1/entrepreneur/create', video_service.handle_create_workflow)
-    video_app.router.add_get('/api/v1/entrepreneur/status/{workflow_id}', video_service.handle_workflow_status)
-    
+    video_app.router.add_get("/health", video_service.handle_health)
+    video_app.router.add_post(
+        "/api/v1/entrepreneur/create", video_service.handle_create_workflow
+    )
+    video_app.router.add_get(
+        "/api/v1/entrepreneur/status/{workflow_id}",
+        video_service.handle_workflow_status,
+    )
+
     # ç¤¾ç¾¤æœå‹™ (ç«¯å£ 8004)
     social_service = MockSocialService()
     social_app = web.Application()
-    social_app.router.add_get('/health', social_service.handle_health)
-    social_app.router.add_post('/api/v1/entrepreneur/publish', social_service.handle_publish)
-    social_app.router.add_get('/api/v1/entrepreneur/publish-status/{publish_id}', social_service.handle_publish_status)
-    
+    social_app.router.add_get("/health", social_service.handle_health)
+    social_app.router.add_post(
+        "/api/v1/entrepreneur/publish", social_service.handle_publish
+    )
+    social_app.router.add_get(
+        "/api/v1/entrepreneur/publish-status/{publish_id}",
+        social_service.handle_publish_status,
+    )
+
     # æ’ç¨‹æœå‹™ (ç«¯å£ 8008)
     scheduler_service = MockSchedulerService()
     scheduler_app = web.Application()
-    scheduler_app.router.add_get('/health', scheduler_service.handle_health)
-    scheduler_app.router.add_post('/api/v1/entrepreneur-scheduler/configure', scheduler_service.handle_configure)
-    scheduler_app.router.add_post('/api/v1/entrepreneur-scheduler/start', scheduler_service.handle_start)
-    scheduler_app.router.add_post('/api/v1/entrepreneur-scheduler/stop', scheduler_service.handle_stop)
-    scheduler_app.router.add_get('/api/v1/entrepreneur-scheduler/status', scheduler_service.handle_status)
-    
+    scheduler_app.router.add_get("/health", scheduler_service.handle_health)
+    scheduler_app.router.add_post(
+        "/api/v1/entrepreneur-scheduler/configure",
+        scheduler_service.handle_configure,
+    )
+    scheduler_app.router.add_post(
+        "/api/v1/entrepreneur-scheduler/start", scheduler_service.handle_start
+    )
+    scheduler_app.router.add_post(
+        "/api/v1/entrepreneur-scheduler/stop", scheduler_service.handle_stop
+    )
+    scheduler_app.router.add_get(
+        "/api/v1/entrepreneur-scheduler/status",
+        scheduler_service.handle_status,
+    )
+
     # å•Ÿå‹•æ‰€æœ‰æœå‹™
     runners = []
-    
+
     services = [
         (trend_app, 8001, "Trend Service"),
         (video_app, 8003, "Video Service"),
         (social_app, 8004, "Social Service"),
-        (scheduler_app, 8008, "Scheduler Service")
+        (scheduler_app, 8008, "Scheduler Service"),
     ]
-    
+
     for app, port, name in services:
         runner = web.AppRunner(app)
         await runner.setup()
-        site = web.TCPSite(runner, 'localhost', port)
+        site = web.TCPSite(runner, "localhost", port)
         await site.start()
         runners.append(runner)
         logger.info(f"ğŸš€ {name} å•Ÿå‹•åœ¨ç«¯å£ {port}")
-    
+
     return runners
 
+
 async def main():
     """å•Ÿå‹•æ‰€æœ‰æ¨¡æ“¬æœå‹™"""
     logger.info("ğŸ­ å•Ÿå‹• E2E æ¸¬è©¦æ¨¡æ“¬æœå‹™...")
-    
+
     runners = await create_mock_services()
-    
+
     logger.info("âœ… æ‰€æœ‰æ¨¡æ“¬æœå‹™å·²å•Ÿå‹•")
     logger.info("ğŸ“‹ æœå‹™åˆ—è¡¨:")
     logger.info("  - Trend Service: http://localhost:8001")
     logger.info("  - Video Service: http://localhost:8003")
     logger.info("  - Social Service: http://localhost:8004")
     logger.info("  - Scheduler Service: http://localhost:8008")
-    
+
     try:
         # ä¿æŒæœå‹™é‹è¡Œ
         while True:
@@ -417,23 +464,25 @@ async def main():
         for runner in runners:
             await runner.cleanup()
 
+
 def run_mock_services():
     """åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­é‹è¡Œæ¨¡æ“¬æœå‹™"""
     asyncio.run(main())
 
+
 if __name__ == "__main__":
     # åœ¨èƒŒæ™¯å•Ÿå‹•æ¨¡æ“¬æœå‹™
     service_thread = threading.Thread(target=run_mock_services, daemon=True)
     service_thread.start()
-    
+
     # çµ¦æœå‹™æ™‚é–“å•Ÿå‹•
     time.sleep(3)
-    
+
     logger.info("ğŸ§ª æ¨¡æ“¬æœå‹™å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é‹è¡Œ E2E æ¸¬è©¦")
-    
+
     # ä¿æŒä¸»ç¨‹åºé‹è¡Œ
     try:
         while True:
             time.sleep(1)
     except KeyboardInterrupt:
-        logger.info("ğŸ é€€å‡ºæ¨¡æ“¬æœå‹™")
\ No newline at end of file
+        logger.info("ğŸ é€€å‡ºæ¨¡æ“¬æœå‹™")
diff --git a/auto_generate_video_fold6/scripts/performance_benchmark.py b/auto_generate_video_fold6/scripts/performance_benchmark.py
index 6c36367..5037053 100644
--- a/auto_generate_video_fold6/scripts/performance_benchmark.py
+++ b/auto_generate_video_fold6/scripts/performance_benchmark.py
@@ -86,7 +86,12 @@ class PerformanceBenchmark:
                 "acceptable": 500,
                 "poor": 1000,
             },
-            "throughput_rps": {"excellent": 1000, "good": 500, "acceptable": 200, "poor": 100},
+            "throughput_rps": {
+                "excellent": 1000,
+                "good": 500,
+                "acceptable": 200,
+                "poor": 100,
+            },
             "error_rate": {
                 "excellent": 0.001,  # 0.1%
                 "good": 0.01,  # 1%
@@ -99,7 +104,12 @@ class PerformanceBenchmark:
                 "acceptable": 0.7,  # 70%
                 "poor": 0.9,  # 90%
             },
-            "memory_usage_mb": {"excellent": 512, "good": 1024, "acceptable": 2048, "poor": 4096},
+            "memory_usage_mb": {
+                "excellent": 512,
+                "good": 1024,
+                "acceptable": 2048,
+                "poor": 4096,
+            },
         }
 
     def _load_config(self, config_file: str) -> Dict[str, Any]:
@@ -157,7 +167,9 @@ class PerformanceBenchmark:
                 "total_duration_seconds": time.time() - start_time,
                 "overall_grade": overall_grade,
                 "recommendations": recommendations,
-                "industry_comparison": self._generate_industry_comparison(benchmark_results),
+                "industry_comparison": self._generate_industry_comparison(
+                    benchmark_results
+                ),
             }
         )
 
@@ -177,14 +189,22 @@ class PerformanceBenchmark:
             results = {}
 
             for endpoint in endpoints:
-                endpoint_results = await self._test_endpoint_performance(endpoint)
+                endpoint_results = await self._test_endpoint_performance(
+                    endpoint
+                )
                 results[endpoint] = endpoint_results
 
             # è¨ˆç®—å¹³å‡æ€§èƒ½æŒ‡æ¨™
-            avg_response_time = np.mean([r["avg_response_time_ms"] for r in results.values()])
-            avg_throughput = np.mean([r["throughput_rps"] for r in results.values()])
+            avg_response_time = np.mean(
+                [r["avg_response_time_ms"] for r in results.values()]
+            )
+            avg_throughput = np.mean(
+                [r["throughput_rps"] for r in results.values()]
+            )
 
-            grade = self._grade_metric("api_response_time_ms", avg_response_time)
+            grade = self._grade_metric(
+                "api_response_time_ms", avg_response_time
+            )
 
             return {
                 "duration_seconds": time.time() - start_time,
@@ -192,11 +212,17 @@ class PerformanceBenchmark:
                 "avg_response_time_ms": avg_response_time,
                 "avg_throughput_rps": avg_throughput,
                 "grade": grade,
-                "status": "PASS" if grade in ["excellent", "good"] else "NEEDS_IMPROVEMENT",
+                "status": "PASS"
+                if grade in ["excellent", "good"]
+                else "NEEDS_IMPROVEMENT",
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     async def _benchmark_load_testing(self) -> Dict[str, Any]:
         """è² è¼‰æ¸¬è©¦åŸºæº–"""
@@ -223,22 +249,34 @@ class PerformanceBenchmark:
             best_throughput = max([r.throughput_rps for r in results.values()])
             avg_error_rate = np.mean([r.error_rate for r in results.values()])
 
-            throughput_grade = self._grade_metric("throughput_rps", best_throughput)
+            throughput_grade = self._grade_metric(
+                "throughput_rps", best_throughput
+            )
             error_rate_grade = self._grade_metric("error_rate", avg_error_rate)
 
-            overall_grade = self._combine_grades([throughput_grade, error_rate_grade])
+            overall_grade = self._combine_grades(
+                [throughput_grade, error_rate_grade]
+            )
 
             return {
                 "duration_seconds": time.time() - start_time,
-                "load_test_results": {k: asdict(v) for k, v in results.items()},
+                "load_test_results": {
+                    k: asdict(v) for k, v in results.items()
+                },
                 "best_throughput_rps": best_throughput,
                 "avg_error_rate": avg_error_rate,
                 "grade": overall_grade,
-                "status": "PASS" if overall_grade in ["excellent", "good"] else "NEEDS_IMPROVEMENT",
+                "status": "PASS"
+                if overall_grade in ["excellent", "good"]
+                else "NEEDS_IMPROVEMENT",
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     async def _benchmark_database_performance(self) -> Dict[str, Any]:
         """è³‡æ–™åº«æ€§èƒ½åŸºæº–æ¸¬è©¦"""
@@ -254,10 +292,12 @@ class PerformanceBenchmark:
 
             # çµ„åˆè©•ä¼°
             db_grade = self._grade_metric(
-                "api_response_time_ms", pg_results.get("avg_query_time_ms", 999)
+                "api_response_time_ms",
+                pg_results.get("avg_query_time_ms", 999),
             )
             cache_grade = self._grade_metric(
-                "api_response_time_ms", redis_results.get("avg_access_time_ms", 999)
+                "api_response_time_ms",
+                redis_results.get("avg_access_time_ms", 999),
             )
 
             overall_grade = self._combine_grades([db_grade, cache_grade])
@@ -267,11 +307,17 @@ class PerformanceBenchmark:
                 "postgresql": pg_results,
                 "redis": redis_results,
                 "grade": overall_grade,
-                "status": "PASS" if overall_grade in ["excellent", "good"] else "NEEDS_IMPROVEMENT",
+                "status": "PASS"
+                if overall_grade in ["excellent", "good"]
+                else "NEEDS_IMPROVEMENT",
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     async def _benchmark_cache_performance(self) -> Dict[str, Any]:
         """å¿«å–æ€§èƒ½åŸºæº–æ¸¬è©¦"""
@@ -290,10 +336,12 @@ class PerformanceBenchmark:
 
             # ç¶œåˆè©•ä¼°
             read_grade = self._grade_metric(
-                "api_response_time_ms", cache_results.get("avg_read_time_ms", 999)
+                "api_response_time_ms",
+                cache_results.get("avg_read_time_ms", 999),
             )
             write_grade = self._grade_metric(
-                "api_response_time_ms", cache_results.get("avg_write_time_ms", 999)
+                "api_response_time_ms",
+                cache_results.get("avg_write_time_ms", 999),
             )
 
             overall_grade = self._combine_grades([read_grade, write_grade])
@@ -304,11 +352,17 @@ class PerformanceBenchmark:
                 "hit_ratio": hit_ratio_results,
                 "consistency": consistency_results,
                 "grade": overall_grade,
-                "status": "PASS" if overall_grade in ["excellent", "good"] else "NEEDS_IMPROVEMENT",
+                "status": "PASS"
+                if overall_grade in ["excellent", "good"]
+                else "NEEDS_IMPROVEMENT",
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     async def _benchmark_resource_utilization(self) -> Dict[str, Any]:
         """è³‡æºåˆ©ç”¨ç‡åŸºæº–æ¸¬è©¦"""
@@ -336,9 +390,13 @@ class PerformanceBenchmark:
                 memory_samples.append(memory_info.used / (1024**3))  # GB
 
                 if disk_io:
-                    disk_io_samples.append(disk_io.read_bytes + disk_io.write_bytes)
+                    disk_io_samples.append(
+                        disk_io.read_bytes + disk_io.write_bytes
+                    )
                 if network_io:
-                    network_io_samples.append(network_io.bytes_sent + network_io.bytes_recv)
+                    network_io_samples.append(
+                        network_io.bytes_sent + network_io.bytes_recv
+                    )
 
                 await asyncio.sleep(sample_interval)
 
@@ -350,7 +408,9 @@ class PerformanceBenchmark:
 
             # è©•ç´š
             cpu_grade = self._grade_metric("cpu_usage", avg_cpu / 100)
-            memory_grade = self._grade_metric("memory_usage_mb", avg_memory_gb * 1024)
+            memory_grade = self._grade_metric(
+                "memory_usage_mb", avg_memory_gb * 1024
+            )
 
             overall_grade = self._combine_grades([cpu_grade, memory_grade])
 
@@ -367,11 +427,17 @@ class PerformanceBenchmark:
                     "samples": len(memory_samples),
                 },
                 "grade": overall_grade,
-                "status": "PASS" if overall_grade in ["excellent", "good"] else "NEEDS_IMPROVEMENT",
+                "status": "PASS"
+                if overall_grade in ["excellent", "good"]
+                else "NEEDS_IMPROVEMENT",
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     async def _benchmark_concurrent_processing(self) -> Dict[str, Any]:
         """ä½µç™¼è™•ç†æ€§èƒ½åŸºæº–æ¸¬è©¦"""
@@ -388,10 +454,16 @@ class PerformanceBenchmark:
                 results[f"concurrency_{level}"] = level_results
 
             # æ‰¾å‡ºæœ€ä½³æ€§èƒ½é»
-            best_throughput = max([r["throughput_rps"] for r in results.values()])
-            optimal_concurrency = max(results.keys(), key=lambda k: results[k]["throughput_rps"])
+            best_throughput = max(
+                [r["throughput_rps"] for r in results.values()]
+            )
+            optimal_concurrency = max(
+                results.keys(), key=lambda k: results[k]["throughput_rps"]
+            )
 
-            throughput_grade = self._grade_metric("throughput_rps", best_throughput)
+            throughput_grade = self._grade_metric(
+                "throughput_rps", best_throughput
+            )
 
             return {
                 "duration_seconds": time.time() - start_time,
@@ -400,12 +472,18 @@ class PerformanceBenchmark:
                 "optimal_concurrency": optimal_concurrency,
                 "grade": throughput_grade,
                 "status": (
-                    "PASS" if throughput_grade in ["excellent", "good"] else "NEEDS_IMPROVEMENT"
+                    "PASS"
+                    if throughput_grade in ["excellent", "good"]
+                    else "NEEDS_IMPROVEMENT"
                 ),
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     async def _benchmark_ai_services(self) -> Dict[str, Any]:
         """AI æœå‹™æ€§èƒ½åŸºæº–æ¸¬è©¦"""
@@ -423,11 +501,15 @@ class PerformanceBenchmark:
 
             results = {}
             for service in ai_services:
-                service_results = await self._test_ai_service_performance(service)
+                service_results = await self._test_ai_service_performance(
+                    service
+                )
                 results[service] = service_results
 
             # è¨ˆç®—å¹³å‡è™•ç†æ™‚é–“
-            avg_processing_time = np.mean([r["avg_processing_time_ms"] for r in results.values()])
+            avg_processing_time = np.mean(
+                [r["avg_processing_time_ms"] for r in results.values()]
+            )
 
             # AI æœå‹™æœ‰ç‰¹æ®Šçš„æ€§èƒ½æ¨™æº–
             ai_grade = self._grade_ai_service_performance(avg_processing_time)
@@ -437,11 +519,17 @@ class PerformanceBenchmark:
                 "ai_service_results": results,
                 "avg_processing_time_ms": avg_processing_time,
                 "grade": ai_grade,
-                "status": "PASS" if ai_grade in ["excellent", "good"] else "NEEDS_IMPROVEMENT",
+                "status": "PASS"
+                if ai_grade in ["excellent", "good"]
+                else "NEEDS_IMPROVEMENT",
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     async def _benchmark_file_io(self) -> Dict[str, Any]:
         """æª”æ¡ˆ I/O æ€§èƒ½åŸºæº–æ¸¬è©¦"""
@@ -458,11 +546,17 @@ class PerformanceBenchmark:
                 results[f"{size_mb}MB"] = size_results
 
             # è¨ˆç®—å¹³å‡ååé‡
-            avg_read_throughput = np.mean([r["read_throughput_mbps"] for r in results.values()])
-            avg_write_throughput = np.mean([r["write_throughput_mbps"] for r in results.values()])
+            avg_read_throughput = np.mean(
+                [r["read_throughput_mbps"] for r in results.values()]
+            )
+            avg_write_throughput = np.mean(
+                [r["write_throughput_mbps"] for r in results.values()]
+            )
 
             # æª”æ¡ˆ I/O è©•ç´š
-            io_grade = self._grade_file_io_performance(avg_read_throughput, avg_write_throughput)
+            io_grade = self._grade_file_io_performance(
+                avg_read_throughput, avg_write_throughput
+            )
 
             return {
                 "duration_seconds": time.time() - start_time,
@@ -470,11 +564,17 @@ class PerformanceBenchmark:
                 "avg_read_throughput_mbps": avg_read_throughput,
                 "avg_write_throughput_mbps": avg_write_throughput,
                 "grade": io_grade,
-                "status": "PASS" if io_grade in ["excellent", "good"] else "NEEDS_IMPROVEMENT",
+                "status": "PASS"
+                if io_grade in ["excellent", "good"]
+                else "NEEDS_IMPROVEMENT",
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     async def _benchmark_network_performance(self) -> Dict[str, Any]:
         """ç¶²è·¯æ€§èƒ½åŸºæº–æ¸¬è©¦"""
@@ -490,20 +590,30 @@ class PerformanceBenchmark:
             latency_grade = self._grade_metric(
                 "api_response_time_ms", latency_results["avg_latency_ms"]
             )
-            bandwidth_grade = self._grade_network_bandwidth(bandwidth_results["throughput_mbps"])
+            bandwidth_grade = self._grade_network_bandwidth(
+                bandwidth_results["throughput_mbps"]
+            )
 
-            overall_grade = self._combine_grades([latency_grade, bandwidth_grade])
+            overall_grade = self._combine_grades(
+                [latency_grade, bandwidth_grade]
+            )
 
             return {
                 "duration_seconds": time.time() - start_time,
                 "latency": latency_results,
                 "bandwidth": bandwidth_results,
                 "grade": overall_grade,
-                "status": "PASS" if overall_grade in ["excellent", "good"] else "NEEDS_IMPROVEMENT",
+                "status": "PASS"
+                if overall_grade in ["excellent", "good"]
+                else "NEEDS_IMPROVEMENT",
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     async def _benchmark_scalability(self) -> Dict[str, Any]:
         """å¯æ“´å±•æ€§åŸºæº–æ¸¬è©¦"""
@@ -528,7 +638,9 @@ class PerformanceBenchmark:
                 )
 
             # åˆ†æå¯æ“´å±•æ€§ç‰¹å¾µ
-            scalability_analysis = self._analyze_scalability_curve(scalability_data)
+            scalability_analysis = self._analyze_scalability_curve(
+                scalability_data
+            )
 
             return {
                 "duration_seconds": time.time() - start_time,
@@ -543,10 +655,16 @@ class PerformanceBenchmark:
             }
 
         except Exception as e:
-            return {"duration_seconds": time.time() - start_time, "error": str(e), "status": "FAIL"}
+            return {
+                "duration_seconds": time.time() - start_time,
+                "error": str(e),
+                "status": "FAIL",
+            }
 
     # è¼”åŠ©æ¸¬è©¦æ–¹æ³•ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
-    async def _test_endpoint_performance(self, endpoint: str) -> Dict[str, Any]:
+    async def _test_endpoint_performance(
+        self, endpoint: str
+    ) -> Dict[str, Any]:
         """æ¸¬è©¦å–®å€‹ç«¯é»æ€§èƒ½"""
         # æ¨¡æ“¬å¯¦ç¾
         return {
@@ -633,7 +751,9 @@ class PerformanceBenchmark:
             "error_rate": 0.005 * (level / 100),  # éš¨ä½µç™¼å¢åŠ éŒ¯èª¤ç‡ç•¥å¢
         }
 
-    async def _test_ai_service_performance(self, service: str) -> Dict[str, Any]:
+    async def _test_ai_service_performance(
+        self, service: str
+    ) -> Dict[str, Any]:
         """æ¸¬è©¦ AI æœå‹™æ€§èƒ½"""
         # ä¸åŒ AI æœå‹™æœ‰ä¸åŒçš„åŸºæº–è™•ç†æ™‚é–“
         base_times = {
@@ -646,7 +766,9 @@ class PerformanceBenchmark:
         base_time = base_times.get(service, 5000)
 
         return {
-            "avg_processing_time_ms": np.random.normal(base_time, base_time * 0.2),
+            "avg_processing_time_ms": np.random.normal(
+                base_time, base_time * 0.2
+            ),
             "success_rate": 0.95,
             "queue_length": np.random.randint(0, 10),
             "cost_per_request": np.random.uniform(0.01, 0.1),
@@ -659,7 +781,9 @@ class PerformanceBenchmark:
 
         return {
             "read_throughput_mbps": np.random.normal(base_throughput, 20),
-            "write_throughput_mbps": np.random.normal(base_throughput * 0.8, 15),
+            "write_throughput_mbps": np.random.normal(
+                base_throughput * 0.8, 15
+            ),
             "read_latency_ms": np.random.normal(10, 3),
             "write_latency_ms": np.random.normal(15, 5),
         }
@@ -684,7 +808,9 @@ class PerformanceBenchmark:
     async def _test_scalability_point(self, load: int) -> Dict[str, Any]:
         """æ¸¬è©¦ç‰¹å®šè² è¼‰é»çš„æ€§èƒ½"""
         # æ¨¡æ“¬ç³»çµ±åœ¨ä¸åŒè² è¼‰ä¸‹çš„æ€§èƒ½
-        throughput_degradation = max(0, 1 - (load / 1000))  # è² è¼‰å¢åŠ æ™‚ååé‡è¡°æ¸›
+        throughput_degradation = max(
+            0, 1 - (load / 1000)
+        )  # è² è¼‰å¢åŠ æ™‚ååé‡è¡°æ¸›
         response_time_increase = 1 + (load / 200)  # è² è¼‰å¢åŠ æ™‚éŸ¿æ‡‰æ™‚é–“å¢åŠ 
 
         return {
@@ -787,7 +913,9 @@ class PerformanceBenchmark:
         else:
             return "poor"
 
-    def _grade_file_io_performance(self, read_mbps: float, write_mbps: float) -> str:
+    def _grade_file_io_performance(
+        self, read_mbps: float, write_mbps: float
+    ) -> str:
         """æª”æ¡ˆ I/O æ€§èƒ½è©•ç´š"""
         avg_throughput = (read_mbps + write_mbps) / 2
 
@@ -813,7 +941,13 @@ class PerformanceBenchmark:
 
     def _combine_grades(self, grades: List[str]) -> str:
         """çµ„åˆå¤šå€‹è©•ç´š"""
-        grade_scores = {"excellent": 4, "good": 3, "acceptable": 2, "poor": 1, "unknown": 0}
+        grade_scores = {
+            "excellent": 4,
+            "good": 3,
+            "acceptable": 2,
+            "poor": 1,
+            "unknown": 0,
+        }
 
         scores = [grade_scores.get(grade, 0) for grade in grades]
         avg_score = sum(scores) / len(scores) if scores else 0
@@ -842,7 +976,10 @@ class PerformanceBenchmark:
 
         # åŸºæ–¼çµæœç”Ÿæˆå»ºè­°
         for category, result in results.items():
-            if isinstance(result, dict) and result.get("grade") in ["acceptable", "poor"]:
+            if isinstance(result, dict) and result.get("grade") in [
+                "acceptable",
+                "poor",
+            ]:
                 if category == "api_performance":
                     recommendations.append("è€ƒæ…®å¯¦æ–½ API å¿«å–å’ŒéŸ¿æ‡‰å£“ç¸®")
                 elif category == "load_testing":
@@ -859,7 +996,9 @@ class PerformanceBenchmark:
 
         return recommendations
 
-    def _generate_industry_comparison(self, results: Dict[str, Any]) -> Dict[str, str]:
+    def _generate_industry_comparison(
+        self, results: Dict[str, Any]
+    ) -> Dict[str, str]:
         """ç”Ÿæˆæ¥­ç•Œæ¯”è¼ƒ"""
         overall_grade = results.get("overall_grade", "unknown")
 
@@ -873,9 +1012,12 @@ class PerformanceBenchmark:
         return {
             "grade": overall_grade,
             "comparison": comparisons.get(overall_grade, "ç„¡æ³•è©•ä¼°"),
-            "percentile": {"excellent": 95, "good": 80, "acceptable": 60, "poor": 30}.get(
-                overall_grade, 0
-            ),
+            "percentile": {
+                "excellent": 95,
+                "good": 80,
+                "acceptable": 60,
+                "poor": 30,
+            }.get(overall_grade, 0),
         }
 
     async def _generate_performance_report(self, results: Dict[str, Any]):
@@ -886,7 +1028,8 @@ class PerformanceBenchmark:
 
         # ç”Ÿæˆ JSON å ±å‘Š
         json_report_path = (
-            report_dir / f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
+            report_dir
+            / f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
         )
         with open(json_report_path, "w", encoding="utf-8") as f:
             json.dump(results, f, indent=2, ensure_ascii=False, default=str)
@@ -900,8 +1043,12 @@ async def main():
     import argparse
 
     parser = argparse.ArgumentParser(description="æ€§èƒ½åŸºæº–æ¸¬è©¦")
-    parser.add_argument("--config", default="config/benchmark-config.json", help="é…ç½®æª”æ¡ˆè·¯å¾‘")
-    parser.add_argument("--output", default="benchmark-results.json", help="çµæœè¼¸å‡ºæª”æ¡ˆ")
+    parser.add_argument(
+        "--config", default="config/benchmark-config.json", help="é…ç½®æª”æ¡ˆè·¯å¾‘"
+    )
+    parser.add_argument(
+        "--output", default="benchmark-results.json", help="çµæœè¼¸å‡ºæª”æ¡ˆ"
+    )
     parser.add_argument("--verbose", action="store_true", help="è©³ç´°è¼¸å‡º")
 
     args = parser.parse_args()
@@ -909,7 +1056,8 @@ async def main():
     # è¨­ç½®æ—¥èªŒ
     log_level = logging.DEBUG if args.verbose else logging.INFO
     logging.basicConfig(
-        level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+        level=log_level,
+        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
     )
 
     # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
@@ -921,9 +1069,9 @@ async def main():
         json.dump(results, f, indent=2, ensure_ascii=False, default=str)
 
     # è¼¸å‡ºæ‘˜è¦
-    print(f"\n{'='*60}")
+    print(f"\n{'=' * 60}")
     print("ğŸ“Š æ€§èƒ½åŸºæº–æ¸¬è©¦çµæœæ‘˜è¦")
-    print(f"{'='*60}")
+    print(f"{'=' * 60}")
     print(f"ç¸½é«”è©•ç´š: {results.get('overall_grade', 'Unknown')}")
     print(f"æ¸¬è©¦æŒçºŒæ™‚é–“: {results.get('total_duration_seconds', 0):.2f} ç§’")
 
@@ -935,7 +1083,7 @@ async def main():
     for i, recommendation in enumerate(results.get("recommendations", []), 1):
         print(f"{i}. {recommendation}")
 
-    print(f"{'='*60}")
+    print(f"{'=' * 60}")
     print(f"è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {args.output}")
 
     # æ ¹æ“šçµæœè¨­ç½®é€€å‡ºä»£ç¢¼
diff --git a/auto_generate_video_fold6/scripts/remove_branch_protection.py b/auto_generate_video_fold6/scripts/remove_branch_protection.py
index cc21428..842adf0 100755
--- a/auto_generate_video_fold6/scripts/remove_branch_protection.py
+++ b/auto_generate_video_fold6/scripts/remove_branch_protection.py
@@ -84,7 +84,10 @@ def show_current_protection():
         return
 
     url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/branches/{branch}/protection"
-    headers = {"Authorization": f"token {github_token}", "Accept": "application/vnd.github.v3+json"}
+    headers = {
+        "Authorization": f"token {github_token}",
+        "Accept": "application/vnd.github.v3+json",
+    }
 
     try:
         response = requests.get(url, headers=headers)
@@ -120,7 +123,9 @@ def main():
         print("2. Token æ˜¯å¦æœ‰è¶³å¤ æ¬Šé™ (éœ€è¦ repo æ¬Šé™)")
         print("3. å€‰åº«åç¨±æ˜¯å¦æ­£ç¢º")
         print("\nğŸ’¡ æˆ–è€…ç›´æ¥åœ¨ GitHub ç¶²é ç§»é™¤:")
-        print("https://github.com/ilyi1116/auto-video-generation-fold6/settings/branches")
+        print(
+            "https://github.com/ilyi1116/auto-video-generation-fold6/settings/branches"
+        )
 
 
 if __name__ == "__main__":
diff --git a/auto_generate_video_fold6/scripts/run_e2e_test.py b/auto_generate_video_fold6/scripts/run_e2e_test.py
index 48901d8..12b1c44 100644
--- a/auto_generate_video_fold6/scripts/run_e2e_test.py
+++ b/auto_generate_video_fold6/scripts/run_e2e_test.py
@@ -12,21 +12,24 @@ import sys
 import os
 from concurrent.futures import ThreadPoolExecutor
 
+
 class E2ETestRunner:
     def __init__(self):
         self.mock_process = None
-        
+
     def start_mock_services(self):
         """å•Ÿå‹•æ¨¡æ“¬æœå‹™"""
         print("ğŸ­ å•Ÿå‹•æ¨¡æ“¬æœå‹™...")
-        self.mock_process = subprocess.Popen([
-            sys.executable, "mock_services_e2e.py"
-        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
-        
+        self.mock_process = subprocess.Popen(
+            [sys.executable, "mock_services_e2e.py"],
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+        )
+
         # ç­‰å¾…æœå‹™å•Ÿå‹•
         time.sleep(5)
         return self.mock_process.poll() is None
-    
+
     def stop_mock_services(self):
         """åœæ­¢æ¨¡æ“¬æœå‹™"""
         if self.mock_process:
@@ -37,20 +40,22 @@ class E2ETestRunner:
             except subprocess.TimeoutExpired:
                 self.mock_process.kill()
                 self.mock_process.wait()
-    
+
     def run_tests(self):
         """é‹è¡Œ E2E æ¸¬è©¦"""
         print("ğŸ§ª é‹è¡Œ E2E æ¸¬è©¦...")
-        test_process = subprocess.run([
-            sys.executable, "test_e2e_simple.py"
-        ], capture_output=True, text=True)
-        
+        test_process = subprocess.run(
+            [sys.executable, "test_e2e_simple.py"],
+            capture_output=True,
+            text=True,
+        )
+
         print(test_process.stdout)
         if test_process.stderr:
             print("STDERR:", test_process.stderr)
-            
+
         return test_process.returncode == 0
-    
+
     def run(self):
         """åŸ·è¡Œå®Œæ•´çš„ E2E æ¸¬è©¦æµç¨‹"""
         try:
@@ -58,14 +63,14 @@ class E2ETestRunner:
             if not self.start_mock_services():
                 print("âŒ æ¨¡æ“¬æœå‹™å•Ÿå‹•å¤±æ•—")
                 return False
-            
+
             print("âœ… æ¨¡æ“¬æœå‹™å·²å•Ÿå‹•")
-            
+
             # é‹è¡Œæ¸¬è©¦
             success = self.run_tests()
-            
+
             return success
-            
+
         except KeyboardInterrupt:
             print("\nâš ï¸ ç”¨æˆ¶ä¸­æ–·æ¸¬è©¦")
             return False
@@ -76,25 +81,26 @@ class E2ETestRunner:
             # æ¸…ç†è³‡æº
             self.stop_mock_services()
 
+
 def main():
     """ä¸»å‡½æ•¸"""
     print("ğŸš€ å‰µæ¥­è€…æ¨¡å¼ E2E æ¸¬è©¦é‹è¡Œå™¨")
     print("=" * 40)
-    
+
     runner = E2ETestRunner()
-    
+
     # è¨­å®šä¿¡è™Ÿè™•ç†
     def signal_handler(signum, frame):
         print("\nğŸ›‘ æ”¶åˆ°çµ‚æ­¢ä¿¡è™Ÿï¼Œæ¸…ç†è³‡æº...")
         runner.stop_mock_services()
         sys.exit(0)
-    
+
     signal.signal(signal.SIGINT, signal_handler)
     signal.signal(signal.SIGTERM, signal_handler)
-    
+
     # åŸ·è¡Œæ¸¬è©¦
     success = runner.run()
-    
+
     if success:
         print("\nğŸ‰ E2E æ¸¬è©¦å®Œæˆä¸¦é€šéï¼")
         print("ğŸ¯ TDD Green éšæ®µå®Œæˆï¼Œæº–å‚™é€²å…¥ Refactor éšæ®µ")
@@ -104,6 +110,7 @@ def main():
         print("ğŸ”´ éœ€è¦ä¿®å¾©å•é¡Œå¾Œé‡æ–°é‹è¡Œæ¸¬è©¦")
         return 1
 
+
 if __name__ == "__main__":
     exit_code = main()
-    sys.exit(exit_code)
\ No newline at end of file
+    sys.exit(exit_code)
diff --git a/auto_generate_video_fold6/scripts/run_tests.py b/auto_generate_video_fold6/scripts/run_tests.py
index 102db8b..1b60e78 100755
--- a/auto_generate_video_fold6/scripts/run_tests.py
+++ b/auto_generate_video_fold6/scripts/run_tests.py
@@ -15,7 +15,9 @@ from typing import Dict, List, Any, Optional
 import logging
 
 # è¨­ç½®æ—¥èªŒ
-logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
+)
 logger = logging.getLogger(__name__)
 
 
@@ -27,7 +29,10 @@ class TestRunner:
         self.results = {}
 
     def run_python_tests(
-        self, test_type: str = "all", coverage: bool = True, verbose: bool = True
+        self,
+        test_type: str = "all",
+        coverage: bool = True,
+        verbose: bool = True,
     ) -> Dict[str, Any]:
         """åŸ·è¡Œ Python æ¸¬è©¦"""
         logger.info(f"åŸ·è¡Œ Python æ¸¬è©¦: {test_type}")
@@ -46,7 +51,14 @@ class TestRunner:
 
         # è¦†è“‹ç‡é¸é …
         if coverage:
-            cmd.extend(["--cov=.", "--cov-report=html", "--cov-report=xml", "--cov-report=term"])
+            cmd.extend(
+                [
+                    "--cov=.",
+                    "--cov-report=html",
+                    "--cov-report=xml",
+                    "--cov-report=term",
+                ]
+            )
 
         # è©³ç´°è¼¸å‡º
         if verbose:
@@ -62,7 +74,11 @@ class TestRunner:
         start_time = time.time()
         try:
             result = subprocess.run(
-                cmd, cwd=self.project_root, capture_output=True, text=True, timeout=300  # 5åˆ†é˜è¶…æ™‚
+                cmd,
+                cwd=self.project_root,
+                capture_output=True,
+                text=True,
+                timeout=300,  # 5åˆ†é˜è¶…æ™‚
             )
 
             duration = time.time() - start_time
@@ -123,7 +139,10 @@ class TestRunner:
         if not node_modules.exists():
             logger.info("å®‰è£å‰ç«¯ä¾è³´...")
             npm_install = subprocess.run(
-                ["npm", "install"], cwd=frontend_dir, capture_output=True, text=True
+                ["npm", "install"],
+                cwd=frontend_dir,
+                capture_output=True,
+                text=True,
             )
             if npm_install.returncode != 0:
                 return {
@@ -223,7 +242,10 @@ class TestRunner:
 
             # Ruff æª¢æŸ¥
             ruff_result = subprocess.run(
-                ["ruff", "check", "."], cwd=self.project_root, capture_output=True, text=True
+                ["ruff", "check", "."],
+                cwd=self.project_root,
+                capture_output=True,
+                text=True,
             )
             python_results["ruff"] = {
                 "success": ruff_result.returncode == 0,
@@ -238,7 +260,10 @@ class TestRunner:
             frontend_dir = self.project_root / "frontend"
             if frontend_dir.exists():
                 eslint_result = subprocess.run(
-                    ["npm", "run", "lint"], cwd=frontend_dir, capture_output=True, text=True
+                    ["npm", "run", "lint"],
+                    cwd=frontend_dir,
+                    capture_output=True,
+                    text=True,
                 )
                 results["frontend"] = {
                     "eslint": {
@@ -267,7 +292,15 @@ class TestRunner:
 
         # Bandit å®‰å…¨æƒæ
         bandit_result = subprocess.run(
-            ["bandit", "-r", ".", "-f", "json", "-o", "test-results/bandit-report.json"],
+            [
+                "bandit",
+                "-r",
+                ".",
+                "-f",
+                "json",
+                "-o",
+                "test-results/bandit-report.json",
+            ],
             cwd=self.project_root,
             capture_output=True,
             text=True,
@@ -280,7 +313,10 @@ class TestRunner:
 
         # Safety ä¾è³´æª¢æŸ¥
         safety_result = subprocess.run(
-            ["safety", "check", "--json"], cwd=self.project_root, capture_output=True, text=True
+            ["safety", "check", "--json"],
+            cwd=self.project_root,
+            capture_output=True,
+            text=True,
         )
         results["safety"] = {
             "success": safety_result.returncode == 0,
@@ -290,7 +326,9 @@ class TestRunner:
 
         return {
             "type": "security",
-            "success": all(result.get("success", False) for result in results.values()),
+            "success": all(
+                result.get("success", False) for result in results.values()
+            ),
             "results": results,
         }
 
@@ -311,9 +349,17 @@ class TestRunner:
             if health_result.returncode == 0:
                 try:
                     health_data = json.loads(health_result.stdout)
-                    return {"type": "health_check", "success": True, "data": health_data}
+                    return {
+                        "type": "health_check",
+                        "success": True,
+                        "data": health_data,
+                    }
                 except json.JSONDecodeError:
-                    return {"type": "health_check", "success": True, "output": health_result.stdout}
+                    return {
+                        "type": "health_check",
+                        "success": True,
+                        "output": health_result.stdout,
+                    }
             else:
                 return {
                     "type": "health_check",
@@ -325,7 +371,9 @@ class TestRunner:
         except Exception as e:
             return {"type": "health_check", "success": False, "error": str(e)}
 
-    def generate_report(self, results: List[Dict[str, Any]], format: str = "json") -> str:
+    def generate_report(
+        self, results: List[Dict[str, Any]], format: str = "json"
+    ) -> str:
         """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
         logger.info(f"ç”Ÿæˆæ¸¬è©¦å ±å‘Š: {format}")
 
@@ -390,24 +438,24 @@ class TestRunner:
     <div class="header">
         <h1>ğŸ§ª æ¸¬è©¦å ±å‘Š</h1>
         <p>Auto Video Generation System - æ¸¬è©¦åŸ·è¡Œçµæœ</p>
-        <p>ç”Ÿæˆæ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
+        <p>ç”Ÿæˆæ™‚é–“: {time.strftime("%Y-%m-%d %H:%M:%S")}</p>
     </div>
     
     <div class="summary">
         <div class="stat">
-            <div class="stat-value">{summary['total_tests']}</div>
+            <div class="stat-value">{summary["total_tests"]}</div>
             <div>ç¸½æ¸¬è©¦æ•¸</div>
         </div>
         <div class="stat">
-            <div class="stat-value success">{summary['passed']}</div>
+            <div class="stat-value success">{summary["passed"]}</div>
             <div>é€šé</div>
         </div>
         <div class="stat">
-            <div class="stat-value failure">{summary['failed']}</div>
+            <div class="stat-value failure">{summary["failed"]}</div>
             <div>å¤±æ•—</div>
         </div>
         <div class="stat">
-            <div class="stat-value">{summary['duration']:.1f}s</div>
+            <div class="stat-value">{summary["duration"]:.1f}s</div>
             <div>ç¸½è€—æ™‚</div>
         </div>
     </div>
@@ -416,16 +464,22 @@ class TestRunner:
 """
 
         for result in results:
-            status_class = "success" if result.get("success", False) else "failure"
-            badge_class = "status-success" if result.get("success", False) else "status-failure"
+            status_class = (
+                "success" if result.get("success", False) else "failure"
+            )
+            badge_class = (
+                "status-success"
+                if result.get("success", False)
+                else "status-failure"
+            )
             status_text = "é€šé" if result.get("success", False) else "å¤±æ•—"
 
             html += f"""
     <div class="test-result">
         <div class="test-header">
             <span class="status-badge {badge_class}">{status_text}</span>
-            <strong>{result.get('type', 'Unknown')} - {result.get('test_type', 'General')}</strong>
-            <span style="float: right;">è€—æ™‚: {result.get('duration', 0):.1f}s</span>
+            <strong>{result.get("type", "Unknown")} - {result.get("test_type", "General")}</strong>
+            <span style="float: right;">è€—æ™‚: {result.get("duration", 0):.1f}s</span>
         </div>
         <div class="test-details">
 """
@@ -472,11 +526,18 @@ def main():
         default="all",
         help="å‰ç«¯æ¸¬è©¦é¡å‹",
     )
-    parser.add_argument("--no-coverage", action="store_true", help="è·³éè¦†è“‹ç‡æ”¶é›†")
     parser.add_argument(
-        "--report-format", choices=["json", "html"], default="json", help="å ±å‘Šæ ¼å¼"
+        "--no-coverage", action="store_true", help="è·³éè¦†è“‹ç‡æ”¶é›†"
+    )
+    parser.add_argument(
+        "--report-format",
+        choices=["json", "html"],
+        default="json",
+        help="å ±å‘Šæ ¼å¼",
+    )
+    parser.add_argument(
+        "--verbose", "-v", action="store_true", help="è©³ç´°è¼¸å‡º"
     )
-    parser.add_argument("--verbose", "-v", action="store_true", help="è©³ç´°è¼¸å‡º")
 
     args = parser.parse_args()
 
@@ -487,13 +548,17 @@ def main():
         if args.type in ["all", "python"]:
             logger.info("åŸ·è¡Œ Python æ¸¬è©¦...")
             python_result = runner.run_python_tests(
-                test_type=args.python_test_type, coverage=not args.no_coverage, verbose=args.verbose
+                test_type=args.python_test_type,
+                coverage=not args.no_coverage,
+                verbose=args.verbose,
             )
             results.append(python_result)
 
         if args.type in ["all", "frontend"]:
             logger.info("åŸ·è¡Œå‰ç«¯æ¸¬è©¦...")
-            frontend_result = runner.run_frontend_tests(test_type=args.frontend_test_type)
+            frontend_result = runner.run_frontend_tests(
+                test_type=args.frontend_test_type
+            )
             results.append(frontend_result)
 
         if args.type in ["all", "lint"]:
@@ -512,7 +577,9 @@ def main():
             results.append(health_result)
 
         # ç”Ÿæˆå ±å‘Š
-        report_file = runner.generate_report(results, format=args.report_format)
+        report_file = runner.generate_report(
+            results, format=args.report_format
+        )
         logger.info(f"æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
 
         # è¼¸å‡ºæ‘˜è¦
@@ -525,7 +592,7 @@ def main():
         print(f"   é€šé: {passed_tests}")
         print(f"   å¤±æ•—: {failed_tests}")
         print(
-            f"   æˆåŠŸç‡: {(passed_tests/total_tests)*100:.1f}%"
+            f"   æˆåŠŸç‡: {(passed_tests / total_tests) * 100:.1f}%"
             if total_tests > 0
             else "   æˆåŠŸç‡: N/A"
         )
diff --git a/auto_generate_video_fold6/scripts/scheduler.py b/auto_generate_video_fold6/scripts/scheduler.py
index 2095a3d..42f7c57 100644
--- a/auto_generate_video_fold6/scripts/scheduler.py
+++ b/auto_generate_video_fold6/scripts/scheduler.py
@@ -37,7 +37,10 @@ except ImportError:
 logging.basicConfig(
     level=logging.INFO,
     format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
-    handlers=[logging.FileHandler("/app/logs/scheduler.log"), logging.StreamHandler(sys.stdout)],
+    handlers=[
+        logging.FileHandler("/app/logs/scheduler.log"),
+        logging.StreamHandler(sys.stdout),
+    ],
 )
 logger = logging.getLogger(__name__)
 
@@ -77,9 +80,15 @@ class VideoScheduler:
                     "auto_generation_interval": scheduling.get(
                         "auto_generation_interval", 6
                     ),  # å°æ™‚
-                    "work_hours_only": scheduling.get("work_hours_only", False),
-                    "work_hours": scheduling.get("work_hours", {"start": "09:00", "end": "18:00"}),
-                    "max_daily_videos": get_config("generation.daily_video_limit", 10),
+                    "work_hours_only": scheduling.get(
+                        "work_hours_only", False
+                    ),
+                    "work_hours": scheduling.get(
+                        "work_hours", {"start": "09:00", "end": "18:00"}
+                    ),
+                    "max_daily_videos": get_config(
+                        "generation.daily_video_limit", 10
+                    ),
                     "batch_size": get_config("generation.batch_size", 3),
                 }
             else:
@@ -119,7 +128,10 @@ class VideoScheduler:
             # ä¼°ç®—æ‰¹æ¬¡æˆæœ¬
             estimated_cost = self._estimate_batch_cost()
 
-            can_proceed, message = await self.budget_controller.pre_operation_check(
+            (
+                can_proceed,
+                message,
+            ) = await self.budget_controller.pre_operation_check(
                 "scheduled_generation", estimated_cost
             )
 
@@ -174,7 +186,9 @@ class VideoScheduler:
         # æª¢æŸ¥é–“éš”æ™‚é–“
         last_run_key = "auto_generation"
         if last_run_key in self.last_run:
-            interval_hours = self.schedule_config.get("auto_generation_interval", 6)
+            interval_hours = self.schedule_config.get(
+                "auto_generation_interval", 6
+            )
             time_since_last = datetime.now() - self.last_run[last_run_key]
 
             if time_since_last < timedelta(hours=interval_hours):
diff --git a/auto_generate_video_fold6/scripts/security_scanner.py b/auto_generate_video_fold6/scripts/security_scanner.py
index db812bd..5cd1048 100644
--- a/auto_generate_video_fold6/scripts/security_scanner.py
+++ b/auto_generate_video_fold6/scripts/security_scanner.py
@@ -147,7 +147,11 @@ class SecurityScanner:
                     "compliance_check",
                 ],
                 "target_directories": [".", "services", "scripts"],
-                "exclude_patterns": ["node_modules", "*.min.js", "__pycache__"],
+                "exclude_patterns": [
+                    "node_modules",
+                    "*.min.js",
+                    "__pycache__",
+                ],
                 "severity_threshold": "medium",
                 "compliance_frameworks": ["OWASP", "NIST", "CIS"],
                 "network_targets": ["localhost"],
@@ -167,13 +171,19 @@ class SecurityScanner:
             scan_results = {}
 
             if "static_analysis" in self.config.get("scan_types", []):
-                scan_results["static_analysis"] = await self._run_static_analysis()
+                scan_results[
+                    "static_analysis"
+                ] = await self._run_static_analysis()
 
             if "dependency_scan" in self.config.get("scan_types", []):
-                scan_results["dependency_scan"] = await self._run_dependency_scan()
+                scan_results[
+                    "dependency_scan"
+                ] = await self._run_dependency_scan()
 
             if "container_scan" in self.config.get("scan_types", []):
-                scan_results["container_scan"] = await self._run_container_scan()
+                scan_results[
+                    "container_scan"
+                ] = await self._run_container_scan()
 
             if "network_scan" in self.config.get("scan_types", []):
                 scan_results["network_scan"] = await self._run_network_scan()
@@ -182,16 +192,22 @@ class SecurityScanner:
                 scan_results["web_app_scan"] = await self._run_web_app_scan()
 
             if "compliance_check" in self.config.get("scan_types", []):
-                scan_results["compliance_check"] = await self._run_compliance_check()
+                scan_results[
+                    "compliance_check"
+                ] = await self._run_compliance_check()
 
             # API å®‰å…¨æ¸¬è©¦
             scan_results["api_security"] = await self._run_api_security_scan()
 
             # é…ç½®å®‰å…¨æª¢æŸ¥
-            scan_results["config_security"] = await self._run_config_security_scan()
+            scan_results[
+                "config_security"
+            ] = await self._run_config_security_scan()
 
             # å¯†ç¢¼å­¸å®‰å…¨æª¢æŸ¥
-            scan_results["crypto_security"] = await self._run_crypto_security_scan()
+            scan_results[
+                "crypto_security"
+            ] = await self._run_crypto_security_scan()
 
             end_time = datetime.utcnow()
             duration = (end_time - start_time).total_seconds()
@@ -217,7 +233,9 @@ class SecurityScanner:
             # ä¿å­˜å ±å‘Š
             await self._save_security_report(report)
 
-            logger.info(f"âœ… å®‰å…¨æƒæå®Œæˆï¼Œç™¼ç¾ {len(self.findings)} å€‹å®‰å…¨å•é¡Œ")
+            logger.info(
+                f"âœ… å®‰å…¨æƒæå®Œæˆï¼Œç™¼ç¾ {len(self.findings)} å€‹å®‰å…¨å•é¡Œ"
+            )
             return report
 
         except Exception as e:
@@ -272,8 +290,12 @@ class SecurityScanner:
                             description=issue.text,
                             severity=self._map_bandit_severity(issue.severity),
                             category=self._map_bandit_category(issue.test_id),
-                            cwe_id=self._get_cwe_for_bandit_issue(issue.test_id),
-                            owasp_category=self._get_owasp_for_bandit_issue(issue.test_id),
+                            cwe_id=self._get_cwe_for_bandit_issue(
+                                issue.test_id
+                            ),
+                            owasp_category=self._get_owasp_for_bandit_issue(
+                                issue.test_id
+                            ),
                             file_path=py_file,
                             line_number=issue.lineno,
                             code_snippet=issue.get_code(),
@@ -281,7 +303,8 @@ class SecurityScanner:
                             references=[
                                 f"https://bandit.readthedocs.io/en/latest/plugins/{issue.test_id.lower()}.html"
                             ],
-                            confidence=issue.confidence.value / 3.0,  # è½‰æ›ç‚º 0-1 ç¯„åœ
+                            confidence=issue.confidence.value
+                            / 3.0,  # è½‰æ›ç‚º 0-1 ç¯„åœ
                             scanner="bandit",
                             timestamp=datetime.utcnow(),
                         )
@@ -323,9 +346,18 @@ class SecurityScanner:
             for ruleset in rulesets:
                 try:
                     # åŸ·è¡Œ Semgrep æƒæ
-                    cmd = ["semgrep", "--config", ruleset, "--json", "--quiet", "."]
-
-                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
+                    cmd = [
+                        "semgrep",
+                        "--config",
+                        ruleset,
+                        "--json",
+                        "--quiet",
+                        ".",
+                    ]
+
+                    result = subprocess.run(
+                        cmd, capture_output=True, text=True, timeout=300
+                    )
 
                     if result.returncode == 0 and result.stdout:
                         semgrep_results = json.loads(result.stdout)
@@ -333,12 +365,12 @@ class SecurityScanner:
                         for finding_data in semgrep_results.get("results", []):
                             # Create unique ID from path, line, and check_id
                             path = finding_data.get("path", "")
-                            line = finding_data.get("start", {}).get("line", "")
+                            line = finding_data.get("start", {}).get(
+                                "line", ""
+                            )
                             check_id = finding_data.get("check_id", "")
                             id_string = f"{path}_{line}_{check_id}"
-                            finding_id = (
-                                f"semgrep_{hashlib.md5(id_string.encode()).hexdigest()[:8]}"
-                            )
+                            finding_id = f"semgrep_{hashlib.md5(id_string.encode()).hexdigest()[:8]}"
 
                             finding = SecurityFinding(
                                 finding_id=finding_id,
@@ -350,11 +382,19 @@ class SecurityScanner:
                                 category=self._map_semgrep_category(
                                     finding_data.get("check_id", "")
                                 ),
-                                cwe_id=self._extract_cwe_from_semgrep(finding_data),
-                                owasp_category=self._extract_owasp_from_semgrep(finding_data),
+                                cwe_id=self._extract_cwe_from_semgrep(
+                                    finding_data
+                                ),
+                                owasp_category=self._extract_owasp_from_semgrep(
+                                    finding_data
+                                ),
                                 file_path=finding_data.get("path"),
-                                line_number=finding_data.get("start", {}).get("line"),
-                                code_snippet=finding_data.get("extra", {}).get("lines", ""),
+                                line_number=finding_data.get("start", {}).get(
+                                    "line"
+                                ),
+                                code_snippet=finding_data.get("extra", {}).get(
+                                    "lines", ""
+                                ),
                                 remediation=f"ä¿®å¾© {finding_data.get('check_id')} æ¼æ´",
                                 references=[
                                     f"https://semgrep.dev/r/{finding_data.get('check_id', '')}"
@@ -375,9 +415,15 @@ class SecurityScanner:
                 "rulesets_used": rulesets,
                 "findings_count": len(findings),
                 "critical_count": len(
-                    [f for f in findings if f.severity == SeverityLevel.CRITICAL]
+                    [
+                        f
+                        for f in findings
+                        if f.severity == SeverityLevel.CRITICAL
+                    ]
+                ),
+                "high_count": len(
+                    [f for f in findings if f.severity == SeverityLevel.HIGH]
                 ),
-                "high_count": len([f for f in findings if f.severity == SeverityLevel.HIGH]),
                 "findings": [asdict(f) for f in findings],
             }
 
@@ -510,7 +556,8 @@ class SecurityScanner:
 
         for config_file in config_files:
             if any(
-                exclude in str(config_file) for exclude in self.config.get("exclude_patterns", [])
+                exclude in str(config_file)
+                for exclude in self.config.get("exclude_patterns", [])
             ):
                 continue
 
@@ -522,7 +569,9 @@ class SecurityScanner:
         return {
             "config_files_scanned": len(config_files),
             "findings_count": len(findings),
-            "insecure_settings": len([f for f in findings if "insecure" in f.title.lower()]),
+            "insecure_settings": len(
+                [f for f in findings if "insecure" in f.title.lower()]
+            ),
         }
 
     async def _run_crypto_security_scan(self) -> Dict[str, Any]:
@@ -548,7 +597,9 @@ class SecurityScanner:
         return {
             "crypto_checks": 3,
             "findings_count": len(findings),
-            "weak_crypto_count": len([f for f in findings if "weak" in f.title.lower()]),
+            "weak_crypto_count": len(
+                [f for f in findings if "weak" in f.title.lower()]
+            ),
             "deprecated_crypto_count": len(
                 [f for f in findings if "deprecated" in f.title.lower()]
             ),
@@ -563,9 +614,13 @@ class SecurityScanner:
 
         for framework in frameworks:
             if framework == "OWASP":
-                compliance_results["OWASP"] = await self._check_owasp_compliance()
+                compliance_results[
+                    "OWASP"
+                ] = await self._check_owasp_compliance()
             elif framework == "NIST":
-                compliance_results["NIST"] = await self._check_nist_compliance()
+                compliance_results[
+                    "NIST"
+                ] = await self._check_nist_compliance()
             elif framework == "CIS":
                 compliance_results["CIS"] = await self._check_cis_compliance()
 
@@ -580,10 +635,19 @@ class SecurityScanner:
         secret_patterns = [
             (r'password\s*=\s*["\'][^"\']{8,}["\']', "hardcoded_password"),
             (r'api[_-]?key\s*=\s*["\'][^"\']{16,}["\']', "hardcoded_api_key"),
-            (r'secret[_-]?key\s*=\s*["\'][^"\']{16,}["\']', "hardcoded_secret_key"),
+            (
+                r'secret[_-]?key\s*=\s*["\'][^"\']{16,}["\']',
+                "hardcoded_secret_key",
+            ),
             (r'token\s*=\s*["\'][^"\']{20,}["\']', "hardcoded_token"),
-            (r'aws[_-]?access[_-]?key[_-]?id\s*=\s*["\'][^"\']+["\']', "aws_access_key"),
-            (r'aws[_-]?secret[_-]?access[_-]?key\s*=\s*["\'][^"\']+["\']', "aws_secret_key"),
+            (
+                r'aws[_-]?access[_-]?key[_-]?id\s*=\s*["\'][^"\']+["\']',
+                "aws_access_key",
+            ),
+            (
+                r'aws[_-]?secret[_-]?access[_-]?key\s*=\s*["\'][^"\']+["\']',
+                "aws_secret_key",
+            ),
         ]
 
         # æƒæç¨‹å¼ç¢¼æª”æ¡ˆ
@@ -593,12 +657,15 @@ class SecurityScanner:
 
         for code_file in code_files:
             if any(
-                exclude in str(code_file) for exclude in self.config.get("exclude_patterns", [])
+                exclude in str(code_file)
+                for exclude in self.config.get("exclude_patterns", [])
             ):
                 continue
 
             try:
-                with open(code_file, "r", encoding="utf-8", errors="ignore") as f:
+                with open(
+                    code_file, "r", encoding="utf-8", errors="ignore"
+                ) as f:
                     content = f.read()
                     lines = content.split("\n")
 
@@ -652,12 +719,15 @@ class SecurityScanner:
 
         for config_file in config_files:
             if any(
-                exclude in str(config_file) for exclude in self.config.get("exclude_patterns", [])
+                exclude in str(config_file)
+                for exclude in self.config.get("exclude_patterns", [])
             ):
                 continue
 
             try:
-                with open(config_file, "r", encoding="utf-8", errors="ignore") as f:
+                with open(
+                    config_file, "r", encoding="utf-8", errors="ignore"
+                ) as f:
                     content = f.read()
                     lines = content.split("\n")
 
@@ -699,7 +769,11 @@ class SecurityScanner:
             "python": [
                 (r"eval\s*\(", "eval_usage", "ä½¿ç”¨ eval() å¯èƒ½å°è‡´ç¨‹å¼ç¢¼æ³¨å…¥"),
                 (r"exec\s*\(", "exec_usage", "ä½¿ç”¨ exec() å¯èƒ½å°è‡´ç¨‹å¼ç¢¼æ³¨å…¥"),
-                (r"os\.system\s*\(", "os_system_usage", "ä½¿ç”¨ os.system() å¯èƒ½å°è‡´å‘½ä»¤æ³¨å…¥"),
+                (
+                    r"os\.system\s*\(",
+                    "os_system_usage",
+                    "ä½¿ç”¨ os.system() å¯èƒ½å°è‡´å‘½ä»¤æ³¨å…¥",
+                ),
                 (
                     r"subprocess\.call\s*\(.*shell=True",
                     "subprocess_shell",
@@ -713,31 +787,48 @@ class SecurityScanner:
                     "function_constructor",
                     "ä½¿ç”¨ Function æ§‹é€ å™¨å¯èƒ½å°è‡´ç¨‹å¼ç¢¼æ³¨å…¥",
                 ),
-                (r"innerHTML\s*=", "innerHTML_usage", "ä½¿ç”¨ innerHTML å¯èƒ½å°è‡´ XSS"),
-                (r"document\.write\s*\(", "document_write", "ä½¿ç”¨ document.write å¯èƒ½å°è‡´ XSS"),
+                (
+                    r"innerHTML\s*=",
+                    "innerHTML_usage",
+                    "ä½¿ç”¨ innerHTML å¯èƒ½å°è‡´ XSS",
+                ),
+                (
+                    r"document\.write\s*\(",
+                    "document_write",
+                    "ä½¿ç”¨ document.write å¯èƒ½å°è‡´ XSS",
+                ),
             ],
         }
 
-        file_extensions = {".py": "python", ".js": "javascript", ".ts": "javascript"}
+        file_extensions = {
+            ".py": "python",
+            ".js": "javascript",
+            ".ts": "javascript",
+        }
 
         for ext, lang in file_extensions.items():
             code_files = list(Path(".").rglob(f"*{ext}"))
 
             for code_file in code_files:
                 if any(
-                    exclude in str(code_file) for exclude in self.config.get("exclude_patterns", [])
+                    exclude in str(code_file)
+                    for exclude in self.config.get("exclude_patterns", [])
                 ):
                     continue
 
                 try:
-                    with open(code_file, "r", encoding="utf-8", errors="ignore") as f:
+                    with open(
+                        code_file, "r", encoding="utf-8", errors="ignore"
+                    ) as f:
                         content = f.read()
                         lines = content.split("\n")
 
                         for line_num, line in enumerate(lines, 1):
-                            for pattern, func_name, description in dangerous_functions.get(
-                                lang, []
-                            ):
+                            for (
+                                pattern,
+                                func_name,
+                                description,
+                            ) in dangerous_functions.get(lang, []):
                                 if re.search(pattern, line):
                                     finding = SecurityFinding(
                                         finding_id=f"dangerous_func_{hashlib.md5(f'{code_file}_{line_num}_{func_name}'.encode()).hexdigest()[:8]}",
@@ -784,12 +875,15 @@ class SecurityScanner:
 
         for code_file in code_files:
             if any(
-                exclude in str(code_file) for exclude in self.config.get("exclude_patterns", [])
+                exclude in str(code_file)
+                for exclude in self.config.get("exclude_patterns", [])
             ):
                 continue
 
             try:
-                with open(code_file, "r", encoding="utf-8", errors="ignore") as f:
+                with open(
+                    code_file, "r", encoding="utf-8", errors="ignore"
+                ) as f:
                     content = f.read()
                     lines = content.split("\n")
 
@@ -905,7 +999,9 @@ class SecurityScanner:
         """æª¢æŸ¥ CSRF ä¿è­·"""
         return {"csrf_vulnerabilities": 0, "forms_checked": 0}
 
-    async def _analyze_config_file(self, config_file: Path) -> List[SecurityFinding]:
+    async def _analyze_config_file(
+        self, config_file: Path
+    ) -> List[SecurityFinding]:
         """åˆ†æé…ç½®æª”æ¡ˆ"""
         # ç°¡åŒ–å¯¦ç¾
         return []
@@ -998,27 +1094,51 @@ class SecurityScanner:
         else:
             return "A05"  # Security Misconfiguration
 
-    def _extract_cwe_from_semgrep(self, finding_data: Dict[str, Any]) -> Optional[str]:
+    def _extract_cwe_from_semgrep(
+        self, finding_data: Dict[str, Any]
+    ) -> Optional[str]:
         """å¾ Semgrep çµæœä¸­æå– CWE"""
         # æª¢æŸ¥ metadata ä¸­æ˜¯å¦æœ‰ CWE è³‡è¨Š
         metadata = finding_data.get("extra", {}).get("metadata", {})
         return metadata.get("cwe")
 
-    def _extract_owasp_from_semgrep(self, finding_data: Dict[str, Any]) -> Optional[str]:
+    def _extract_owasp_from_semgrep(
+        self, finding_data: Dict[str, Any]
+    ) -> Optional[str]:
         """å¾ Semgrep çµæœä¸­æå– OWASP é¡åˆ¥"""
         metadata = finding_data.get("extra", {}).get("metadata", {})
         return metadata.get("owasp")
 
-    def _generate_security_summary(self, scan_results: Dict[str, Any]) -> Dict[str, Any]:
+    def _generate_security_summary(
+        self, scan_results: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """ç”Ÿæˆå®‰å…¨æ‘˜è¦"""
         total_findings = len(self.findings)
 
         severity_counts = {
-            "critical": len([f for f in self.findings if f.severity == SeverityLevel.CRITICAL]),
-            "high": len([f for f in self.findings if f.severity == SeverityLevel.HIGH]),
-            "medium": len([f for f in self.findings if f.severity == SeverityLevel.MEDIUM]),
-            "low": len([f for f in self.findings if f.severity == SeverityLevel.LOW]),
-            "info": len([f for f in self.findings if f.severity == SeverityLevel.INFO]),
+            "critical": len(
+                [
+                    f
+                    for f in self.findings
+                    if f.severity == SeverityLevel.CRITICAL
+                ]
+            ),
+            "high": len(
+                [f for f in self.findings if f.severity == SeverityLevel.HIGH]
+            ),
+            "medium": len(
+                [
+                    f
+                    for f in self.findings
+                    if f.severity == SeverityLevel.MEDIUM
+                ]
+            ),
+            "low": len(
+                [f for f in self.findings if f.severity == SeverityLevel.LOW]
+            ),
+            "info": len(
+                [f for f in self.findings if f.severity == SeverityLevel.INFO]
+            ),
         }
 
         category_counts = {}
@@ -1043,8 +1163,12 @@ class SecurityScanner:
     def _check_compliance_status(self) -> Dict[str, bool]:
         """æª¢æŸ¥åˆè¦ç‹€æ…‹"""
         # åŸºæ–¼ç™¼ç¾çš„å•é¡Œè©•ä¼°åˆè¦æ€§
-        critical_issues = len([f for f in self.findings if f.severity == SeverityLevel.CRITICAL])
-        high_issues = len([f for f in self.findings if f.severity == SeverityLevel.HIGH])
+        critical_issues = len(
+            [f for f in self.findings if f.severity == SeverityLevel.CRITICAL]
+        )
+        high_issues = len(
+            [f for f in self.findings if f.severity == SeverityLevel.HIGH]
+        )
 
         return {
             "OWASP_TOP_10": critical_issues == 0 and high_issues < 5,
@@ -1067,7 +1191,9 @@ class SecurityScanner:
             SeverityLevel.INFO: 0.5,
         }
 
-        total_score = sum(weights.get(finding.severity, 0) for finding in self.findings)
+        total_score = sum(
+            weights.get(finding.severity, 0) for finding in self.findings
+        )
 
         # æ­£è¦åŒ–åˆ° 0-100 ç¯„åœ
         # å‡è¨­ 100 å€‹é«˜åš´é‡ç¨‹åº¦å•é¡Œç‚ºæœ€å¤§åˆ†æ•¸
@@ -1091,7 +1217,9 @@ class SecurityScanner:
 
         # ä¿å­˜ JSON å ±å‘Š
         with open(report_file, "w", encoding="utf-8") as f:
-            json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
+            json.dump(
+                report_data, f, indent=2, ensure_ascii=False, default=str
+            )
 
         logger.info(f"å®‰å…¨å ±å‘Šå·²ä¿å­˜: {report_file}")
 
@@ -1102,8 +1230,12 @@ async def main():
     import argparse
 
     parser = argparse.ArgumentParser(description="å®‰å…¨æ¼æ´æƒæ")
-    parser.add_argument("--config", default="config/security-config.json", help="é…ç½®æª”æ¡ˆè·¯å¾‘")
-    parser.add_argument("--output", default="security-report.json", help="çµæœè¼¸å‡ºæª”æ¡ˆ")
+    parser.add_argument(
+        "--config", default="config/security-config.json", help="é…ç½®æª”æ¡ˆè·¯å¾‘"
+    )
+    parser.add_argument(
+        "--output", default="security-report.json", help="çµæœè¼¸å‡ºæª”æ¡ˆ"
+    )
     parser.add_argument(
         "--severity",
         choices=["critical", "high", "medium", "low"],
@@ -1117,7 +1249,8 @@ async def main():
     # è¨­ç½®æ—¥èªŒ
     log_level = logging.DEBUG if args.verbose else logging.INFO
     logging.basicConfig(
-        level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+        level=log_level,
+        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
     )
 
     # åŸ·è¡Œå®‰å…¨æƒæ
@@ -1129,9 +1262,9 @@ async def main():
         json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)
 
     # è¼¸å‡ºæ‘˜è¦
-    print(f"\n{'='*60}")
+    print(f"\n{'=' * 60}")
     print("ğŸ”’ å®‰å…¨æƒæçµæœæ‘˜è¦")
-    print(f"{'='*60}")
+    print(f"{'=' * 60}")
     print(f"æƒæç›®æ¨™: {report.target}")
     print(f"æƒææŒçºŒæ™‚é–“: {report.duration_seconds:.2f} ç§’")
     print(f"ç¸½ç™¼ç¾å•é¡Œ: {len(report.findings)}")
@@ -1148,7 +1281,7 @@ async def main():
         status = "âœ… åˆè¦" if compliant else "âŒ ä¸åˆè¦"
         print(f"  {framework}: {status}")
 
-    print(f"\n{'='*60}")
+    print(f"\n{'=' * 60}")
     print(f"è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {args.output}")
 
     # æ ¹æ“šåš´é‡ç¨‹åº¦è¨­ç½®é€€å‡ºä»£ç¢¼
diff --git a/auto_generate_video_fold6/scripts/service_manager.py b/auto_generate_video_fold6/scripts/service_manager.py
index ee622ce..fd71ad0 100755
--- a/auto_generate_video_fold6/scripts/service_manager.py
+++ b/auto_generate_video_fold6/scripts/service_manager.py
@@ -21,7 +21,8 @@ from enum import Enum
 
 # è¨­ç½®æ—¥èªŒ
 logging.basicConfig(
-    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+    level=logging.INFO,
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
 )
 logger = logging.getLogger(__name__)
 
@@ -71,7 +72,9 @@ class ServiceStatus:
         return {
             **asdict(self),
             "state": self.state.value,
-            "start_time": self.start_time.isoformat() if self.start_time else None,
+            "start_time": self.start_time.isoformat()
+            if self.start_time
+            else None,
             "metadata": self.metadata or {},
         }
 
@@ -187,7 +190,9 @@ class ServiceManager:
 
         for service in default_services:
             self.services[service.name] = service
-            self.status[service.name] = ServiceStatus(name=service.name, state=ServiceState.STOPPED)
+            self.status[service.name] = ServiceStatus(
+                name=service.name, state=ServiceState.STOPPED
+            )
 
         # ä¿å­˜é è¨­é…ç½®
         self._save_config()
@@ -195,7 +200,11 @@ class ServiceManager:
     def _save_config(self):
         """ä¿å­˜æœå‹™é…ç½®"""
         try:
-            config_data = {"services": [asdict(service) for service in self.services.values()]}
+            config_data = {
+                "services": [
+                    asdict(service) for service in self.services.values()
+                ]
+            }
 
             config_path = Path(self.config_file)
             config_path.parent.mkdir(parents=True, exist_ok=True)
@@ -272,10 +281,16 @@ class ServiceManager:
                 if service.health_check_url.startswith("http"):
                     # HTTP å¥åº·æª¢æŸ¥
                     timeout = aiohttp.ClientTimeout(total=5)
-                    async with aiohttp.ClientSession(timeout=timeout) as session:
-                        async with session.get(service.health_check_url) as response:
+                    async with aiohttp.ClientSession(
+                        timeout=timeout
+                    ) as session:
+                        async with session.get(
+                            service.health_check_url
+                        ) as response:
                             if response.status == 200:
-                                logger.info(f"æœå‹™ {service_name} å¥åº·æª¢æŸ¥é€šé")
+                                logger.info(
+                                    f"æœå‹™ {service_name} å¥åº·æª¢æŸ¥é€šé"
+                                )
                                 return True
 
                 elif service.health_check_url.startswith("redis://"):
@@ -283,7 +298,9 @@ class ServiceManager:
                     try:
                         import redis
 
-                        r = redis.Redis(host="localhost", port=6379, decode_responses=True)
+                        r = redis.Redis(
+                            host="localhost", port=6379, decode_responses=True
+                        )
                         r.ping()
                         logger.info(f"æœå‹™ {service_name} å¥åº·æª¢æŸ¥é€šé")
                         return True
@@ -351,7 +368,9 @@ class ServiceManager:
             if await self._wait_for_health_check(service_name):
                 status.state = ServiceState.RUNNING
                 status.health_status = "healthy"
-                logger.info(f"æœå‹™ {service_name} å•Ÿå‹•æˆåŠŸ (PID: {process.pid})")
+                logger.info(
+                    f"æœå‹™ {service_name} å•Ÿå‹•æˆåŠŸ (PID: {process.pid})"
+                )
                 return True
             else:
                 # å¥åº·æª¢æŸ¥å¤±æ•—ï¼Œåœæ­¢é€²ç¨‹
@@ -397,7 +416,9 @@ class ServiceManager:
                     try:
                         process.wait(timeout=service.shutdown_timeout)
                     except psutil.TimeoutExpired:
-                        logger.warning(f"æœå‹™ {service_name} å„ªé›…åœæ­¢è¶…æ™‚ï¼Œå¼·åˆ¶çµ‚æ­¢")
+                        logger.warning(
+                            f"æœå‹™ {service_name} å„ªé›…åœæ­¢è¶…æ™‚ï¼Œå¼·åˆ¶çµ‚æ­¢"
+                        )
                         process.kill()
                         process.wait(timeout=10)
 
@@ -443,7 +464,9 @@ class ServiceManager:
             ready = []
             for service_name in remaining:
                 service = self.services[service_name]
-                if not service.dependencies or all(dep in ordered for dep in service.dependencies):
+                if not service.dependencies or all(
+                    dep in ordered for dep in service.dependencies
+                ):
                     ready.append(service_name)
 
             if not ready:
@@ -508,12 +531,24 @@ class ServiceManager:
 
         return {
             "total_services": len(self.services),
-            "running": sum(1 for state in states if state == ServiceState.RUNNING),
-            "stopped": sum(1 for state in states if state == ServiceState.STOPPED),
-            "failed": sum(1 for state in states if state == ServiceState.FAILED),
-            "starting": sum(1 for state in states if state == ServiceState.STARTING),
-            "stopping": sum(1 for state in states if state == ServiceState.STOPPING),
-            "services": {name: status.to_dict() for name, status in self.status.items()},
+            "running": sum(
+                1 for state in states if state == ServiceState.RUNNING
+            ),
+            "stopped": sum(
+                1 for state in states if state == ServiceState.STOPPED
+            ),
+            "failed": sum(
+                1 for state in states if state == ServiceState.FAILED
+            ),
+            "starting": sum(
+                1 for state in states if state == ServiceState.STARTING
+            ),
+            "stopping": sum(
+                1 for state in states if state == ServiceState.STOPPING
+            ),
+            "services": {
+                name: status.to_dict() for name, status in self.status.items()
+            },
             "timestamp": datetime.now().isoformat(),
         }
 
@@ -533,8 +568,10 @@ class ServiceManager:
 
                             # è‡ªå‹•é‡å•Ÿ (å¦‚æœå•Ÿç”¨)
                             service = self.services[service_name]
-                            if service.auto_restart and status.restart_count < service.max_restarts:
-
+                            if (
+                                service.auto_restart
+                                and status.restart_count < service.max_restarts
+                            ):
                                 logger.info(
                                     f"è‡ªå‹•é‡å•Ÿæœå‹™ {service_name} (ç¬¬ {status.restart_count + 1} æ¬¡)"
                                 )
@@ -566,7 +603,9 @@ async def main():
         if command == "start":
             if service_name:
                 success = await manager.start_service(service_name)
-                print(f"æœå‹™ {service_name} å•Ÿå‹• {'æˆåŠŸ' if success else 'å¤±æ•—'}")
+                print(
+                    f"æœå‹™ {service_name} å•Ÿå‹• {'æˆåŠŸ' if success else 'å¤±æ•—'}"
+                )
             else:
                 success = await manager.start_all_services()
                 print(f"æ‰€æœ‰æœå‹™å•Ÿå‹• {'æˆåŠŸ' if success else 'å¤±æ•—'}")
@@ -574,7 +613,9 @@ async def main():
         elif command == "stop":
             if service_name:
                 success = await manager.stop_service(service_name)
-                print(f"æœå‹™ {service_name} åœæ­¢ {'æˆåŠŸ' if success else 'å¤±æ•—'}")
+                print(
+                    f"æœå‹™ {service_name} åœæ­¢ {'æˆåŠŸ' if success else 'å¤±æ•—'}"
+                )
             else:
                 success = await manager.stop_all_services()
                 print(f"æ‰€æœ‰æœå‹™åœæ­¢ {'æˆåŠŸ' if success else 'å¤±æ•—'}")
@@ -582,7 +623,9 @@ async def main():
         elif command == "restart":
             if service_name:
                 success = await manager.restart_service(service_name)
-                print(f"æœå‹™ {service_name} é‡å•Ÿ {'æˆåŠŸ' if success else 'å¤±æ•—'}")
+                print(
+                    f"æœå‹™ {service_name} é‡å•Ÿ {'æˆåŠŸ' if success else 'å¤±æ•—'}"
+                )
             else:
                 await manager.stop_all_services()
                 await asyncio.sleep(3)
@@ -593,7 +636,11 @@ async def main():
             if service_name:
                 status = manager.get_service_status(service_name)
                 if status:
-                    print(json.dumps(status.to_dict(), indent=2, ensure_ascii=False))
+                    print(
+                        json.dumps(
+                            status.to_dict(), indent=2, ensure_ascii=False
+                        )
+                    )
                 else:
                     print(f"æœå‹™ {service_name} ä¸å­˜åœ¨")
             else:
diff --git a/auto_generate_video_fold6/scripts/test-coverage-audit.py b/auto_generate_video_fold6/scripts/test-coverage-audit.py
index 3510904..8a4525a 100644
--- a/auto_generate_video_fold6/scripts/test-coverage-audit.py
+++ b/auto_generate_video_fold6/scripts/test-coverage-audit.py
@@ -14,18 +14,23 @@ from datetime import datetime
 
 class TestCoverageAuditor:
     """æ¸¬è©¦è¦†è“‹ç‡å¯©æŸ¥å™¨"""
-    
+
     def __init__(self, project_root: str):
         self.project_root = Path(project_root)
         self.services_dir = self.project_root / "services"
         self.existing_tests = set()
         self.source_files = []
         self.coverage_report = {}
-    
+
     def scan_existing_tests(self):
         """æƒæç¾æœ‰æ¸¬è©¦æ–‡ä»¶"""
-        test_patterns = ["test_*.py", "*_test.py", "tests/*.py", "**/*test*.py"]
-        
+        test_patterns = [
+            "test_*.py",
+            "*_test.py",
+            "tests/*.py",
+            "**/*test*.py",
+        ]
+
         for pattern in test_patterns:
             for test_file in self.project_root.rglob(pattern):
                 if test_file.is_file():
@@ -37,90 +42,109 @@ class TestCoverageAuditor:
                         module_name = test_name[:-5]
                     else:
                         module_name = test_name
-                    
+
                     self.existing_tests.add(module_name)
-        
+
         print(f"ç™¼ç¾ {len(self.existing_tests)} å€‹ç¾æœ‰æ¸¬è©¦æ–‡ä»¶")
-    
+
     def scan_source_files(self):
         """æƒææ‰€æœ‰æºç¢¼æ–‡ä»¶"""
         python_files = list(self.project_root.rglob("*.py"))
-        
+
         for py_file in python_files:
             # è·³éæ¸¬è©¦æ–‡ä»¶ã€__pycache__ã€migrations ç­‰
-            if any(skip in str(py_file) for skip in [
-                "test", "__pycache__", "migrations", "venv", ".git", 
-                "node_modules", "dist", "build"
-            ]):
+            if any(
+                skip in str(py_file)
+                for skip in [
+                    "test",
+                    "__pycache__",
+                    "migrations",
+                    "venv",
+                    ".git",
+                    "node_modules",
+                    "dist",
+                    "build",
+                ]
+            ):
                 continue
-                
+
             # åˆ†ææ–‡ä»¶å…§å®¹
             try:
-                with open(py_file, 'r', encoding='utf-8') as f:
+                with open(py_file, "r", encoding="utf-8") as f:
                     content = f.read()
                     tree = ast.parse(content)
-                    
+
                 file_info = self.analyze_python_file(py_file, tree)
                 if file_info:
                     self.source_files.append(file_info)
-                    
+
             except Exception as e:
                 print(f"åˆ†ææ–‡ä»¶ {py_file} æ™‚å‡ºéŒ¯: {e}")
-    
+
     def analyze_python_file(self, file_path: Path, tree: ast.AST) -> Dict:
         """åˆ†æ Python æ–‡ä»¶çµæ§‹"""
         classes = []
         functions = []
-        
+
         for node in ast.walk(tree):
             if isinstance(node, ast.ClassDef):
                 methods = []
                 for item in node.body:
                     if isinstance(item, ast.FunctionDef):
-                        methods.append({
-                            "name": item.name,
-                            "line": item.lineno,
-                            "is_private": item.name.startswith('_'),
-                            "is_async": isinstance(item, ast.AsyncFunctionDef),
-                            "args": len(item.args.args)
-                        })
-                
-                classes.append({
-                    "name": node.name,
-                    "line": node.lineno,
-                    "methods": methods
-                })
-            
+                        methods.append(
+                            {
+                                "name": item.name,
+                                "line": item.lineno,
+                                "is_private": item.name.startswith("_"),
+                                "is_async": isinstance(
+                                    item, ast.AsyncFunctionDef
+                                ),
+                                "args": len(item.args.args),
+                            }
+                        )
+
+                classes.append(
+                    {
+                        "name": node.name,
+                        "line": node.lineno,
+                        "methods": methods,
+                    }
+                )
+
             elif isinstance(node, ast.FunctionDef) and node.col_offset == 0:
-                functions.append({
-                    "name": node.name,
-                    "line": node.lineno,
-                    "is_async": isinstance(node, ast.AsyncFunctionDef),
-                    "args": len(node.args.args)
-                })
-        
+                functions.append(
+                    {
+                        "name": node.name,
+                        "line": node.lineno,
+                        "is_async": isinstance(node, ast.AsyncFunctionDef),
+                        "args": len(node.args.args),
+                    }
+                )
+
         relative_path = file_path.relative_to(self.project_root)
-        module_name = str(relative_path).replace('/', '.').replace('.py', '')
-        
+        module_name = str(relative_path).replace("/", ".").replace(".py", "")
+
         return {
             "file_path": str(file_path),
             "relative_path": str(relative_path),
             "module_name": module_name,
             "classes": classes,
             "functions": functions,
-            "has_test": any(test in str(file_path) for test in self.existing_tests),
-            "complexity": len(classes) + len(functions)
+            "has_test": any(
+                test in str(file_path) for test in self.existing_tests
+            ),
+            "complexity": len(classes) + len(functions),
         }
-    
+
     def generate_coverage_report(self):
         """ç”Ÿæˆè¦†è“‹ç‡å ±å‘Š"""
         total_files = len(self.source_files)
         tested_files = sum(1 for f in self.source_files if f["has_test"])
-        
+
         # æŒ‰æœå‹™åˆ†çµ„
         services = {}
         unorganized = []
-        
+
         for file_info in self.source_files:
             if file_info["relative_path"].startswith("services/"):
                 service_name = file_info["relative_path"].split("/")[1]
@@ -129,85 +153,103 @@ class TestCoverageAuditor:
                 services[service_name].append(file_info)
             else:
                 unorganized.append(file_info)
-        
+
         self.coverage_report = {
             "timestamp": datetime.now().isoformat(),
             "summary": {
                 "total_files": total_files,
                 "tested_files": tested_files,
-                "coverage_percentage": round((tested_files / total_files) * 100, 2) if total_files > 0 else 0,
-                "untested_files": total_files - tested_files
+                "coverage_percentage": round(
+                    (tested_files / total_files) * 100, 2
+                )
+                if total_files > 0
+                else 0,
+                "untested_files": total_files - tested_files,
             },
             "services": {},
             "unorganized": unorganized,
-            "priority_files": []
+            "priority_files": [],
         }
-        
+
         # åˆ†æå„æœå‹™è¦†è“‹ç‡
         for service_name, files in services.items():
             service_tested = sum(1 for f in files if f["has_test"])
             service_total = len(files)
-            
+
             self.coverage_report["services"][service_name] = {
                 "total_files": service_total,
                 "tested_files": service_tested,
-                "coverage_percentage": round((service_tested / service_total) * 100, 2) if service_total > 0 else 0,
-                "files": files
+                "coverage_percentage": round(
+                    (service_tested / service_total) * 100, 2
+                )
+                if service_total > 0
+                else 0,
+                "files": files,
             }
-        
+
         # è­˜åˆ¥é«˜å„ªå…ˆç´šéœ€è¦æ¸¬è©¦çš„æ–‡ä»¶ (è¤‡é›œåº¦é«˜ä¸”ç„¡æ¸¬è©¦)
         priority_files = []
         for file_info in self.source_files:
             if not file_info["has_test"] and file_info["complexity"] >= 3:
-                priority_files.append({
-                    "file": file_info["relative_path"],
-                    "complexity": file_info["complexity"],
-                    "classes_count": len(file_info["classes"]),
-                    "functions_count": len(file_info["functions"])
-                })
-        
+                priority_files.append(
+                    {
+                        "file": file_info["relative_path"],
+                        "complexity": file_info["complexity"],
+                        "classes_count": len(file_info["classes"]),
+                        "functions_count": len(file_info["functions"]),
+                    }
+                )
+
         # æŒ‰è¤‡é›œåº¦æ’åº
         priority_files.sort(key=lambda x: x["complexity"], reverse=True)
         self.coverage_report["priority_files"] = priority_files[:20]  # å‰20å€‹
-    
+
     def generate_test_templates(self, output_dir: str = None):
         """ç‚ºå„ªå…ˆç´šæ–‡ä»¶ç”Ÿæˆæ¸¬è©¦æ¨¡æ¿"""
         if output_dir is None:
             output_dir = self.project_root / "test_templates"
         else:
             output_dir = Path(output_dir)
-        
+
         output_dir.mkdir(exist_ok=True)
-        
+
         templates_created = []
-        
-        for priority_file in self.coverage_report["priority_files"][:10]:  # å‰10å€‹
+
+        for priority_file in self.coverage_report["priority_files"][
+            :10
+        ]:  # å‰10å€‹
             file_path = self.project_root / priority_file["file"]
-            
+
             # æ‰¾åˆ°å°æ‡‰çš„æ–‡ä»¶ä¿¡æ¯
             file_info = next(
-                (f for f in self.source_files if f["relative_path"] == priority_file["file"]), 
-                None
+                (
+                    f
+                    for f in self.source_files
+                    if f["relative_path"] == priority_file["file"]
+                ),
+                None,
             )
-            
+
             if file_info:
                 template_content = self.create_test_template(file_info)
-                
+
                 # ç¢ºå®šæ¸¬è©¦æ–‡ä»¶è·¯å¾„
-                test_filename = f"test_{Path(file_info['relative_path']).stem}.py"
+                test_filename = (
+                    f"test_{Path(file_info['relative_path']).stem}.py"
+                )
                 test_file_path = output_dir / test_filename
-                
-                with open(test_file_path, 'w', encoding='utf-8') as f:
+
+                with open(test_file_path, "w", encoding="utf-8") as f:
                     f.write(template_content)
-                
+
                 templates_created.append(str(test_file_path))
-        
+
         return templates_created
-    
+
     def create_test_template(self, file_info: Dict) -> str:
         """å‰µå»ºæ¸¬è©¦æ¨¡æ¿"""
         module_path = file_info["module_name"]
-        
+
         template = f'''#!/usr/bin/env python3
 """
 {file_info["relative_path"]} çš„æ¸¬è©¦æ–‡ä»¶
@@ -231,7 +273,7 @@ class TestModule:
         pass
 
 '''
-        
+
         # ç‚ºæ¯å€‹é¡ç”Ÿæˆæ¸¬è©¦
         for class_info in file_info["classes"]:
             template += f'''
@@ -243,7 +285,7 @@ class Test{class_info["name"]}:
         pass
     
 '''
-            
+
             # ç‚ºæ¯å€‹æ–¹æ³•ç”Ÿæˆæ¸¬è©¦
             for method in class_info["methods"]:
                 if not method["is_private"]:  # åªç‚ºå…¬å…±æ–¹æ³•ç”Ÿæˆæ¸¬è©¦
@@ -253,10 +295,10 @@ class Test{class_info["name"]}:
         assert True
     
 '''
-        
+
         # ç‚ºå‡½æ•¸ç”Ÿæˆæ¸¬è©¦
         for func_info in file_info["functions"]:
-            if not func_info["name"].startswith('_'):  # åªç‚ºå…¬å…±å‡½æ•¸ç”Ÿæˆæ¸¬è©¦
+            if not func_info["name"].startswith("_"):  # åªç‚ºå…¬å…±å‡½æ•¸ç”Ÿæˆæ¸¬è©¦
                 template += f'''def test_{func_info["name"]}():
     """æ¸¬è©¦ {func_info["name"]} å‡½æ•¸"""
     # TODO: å¯¦ç¾æ¸¬è©¦é‚è¼¯
@@ -264,75 +306,87 @@ class Test{class_info["name"]}:
 
 
 '''
-        
+
         return template
-    
+
     def save_report(self, output_file: str = None):
         """ä¿å­˜è¦†è“‹ç‡å ±å‘Š"""
         if output_file is None:
             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-            output_file = self.project_root / f"test_coverage_report_{timestamp}.json"
-        
-        with open(output_file, 'w', encoding='utf-8') as f:
+            output_file = (
+                self.project_root / f"test_coverage_report_{timestamp}.json"
+            )
+
+        with open(output_file, "w", encoding="utf-8") as f:
             json.dump(self.coverage_report, f, indent=2, ensure_ascii=False)
-        
+
         return output_file
-    
+
     def print_summary(self):
         """æ‰“å°æ‘˜è¦å ±å‘Š"""
         summary = self.coverage_report["summary"]
-        
-        print("\n" + "="*60)
+
+        print("\n" + "=" * 60)
         print("ğŸ§ª æ¸¬è©¦è¦†è“‹ç‡å¯©æŸ¥å ±å‘Š")
-        print("="*60)
+        print("=" * 60)
         print(f"ğŸ“Š ç¸½æ–‡ä»¶æ•¸: {summary['total_files']}")
         print(f"âœ… å·²æ¸¬è©¦æ–‡ä»¶: {summary['tested_files']}")
         print(f"âŒ æœªæ¸¬è©¦æ–‡ä»¶: {summary['untested_files']}")
         print(f"ğŸ“ˆ è¦†è“‹ç‡: {summary['coverage_percentage']}%")
-        
+
         print(f"\nğŸ¯ å„æœå‹™è¦†è“‹ç‡:")
-        for service_name, service_info in self.coverage_report["services"].items():
+        for service_name, service_info in self.coverage_report[
+            "services"
+        ].items():
             coverage = service_info["coverage_percentage"]
-            status = "ğŸŸ¢" if coverage >= 80 else "ğŸŸ¡" if coverage >= 60 else "ğŸ”´"
-            print(f"  {status} {service_name}: {coverage}% ({service_info['tested_files']}/{service_info['total_files']})")
-        
+            status = (
+                "ğŸŸ¢" if coverage >= 80 else "ğŸŸ¡" if coverage >= 60 else "ğŸ”´"
+            )
+            print(
+                f"  {status} {service_name}: {coverage}% ({service_info['tested_files']}/{service_info['total_files']})"
+            )
+
         print(f"\nğŸš¨ é«˜å„ªå…ˆç´šéœ€è¦æ¸¬è©¦çš„æ–‡ä»¶ (è¤‡é›œåº¦ â‰¥ 3):")
-        for i, priority_file in enumerate(self.coverage_report["priority_files"][:10], 1):
-            print(f"  {i:2d}. {priority_file['file']} (è¤‡é›œåº¦: {priority_file['complexity']})")
-    
+        for i, priority_file in enumerate(
+            self.coverage_report["priority_files"][:10], 1
+        ):
+            print(
+                f"  {i:2d}. {priority_file['file']} (è¤‡é›œåº¦: {priority_file['complexity']})"
+            )
+
     def run_audit(self):
         """åŸ·è¡Œå®Œæ•´å¯©æŸ¥"""
         print("ğŸ” æƒæç¾æœ‰æ¸¬è©¦æ–‡ä»¶...")
         self.scan_existing_tests()
-        
+
         print("ğŸ“ æƒææºç¢¼æ–‡ä»¶...")
         self.scan_source_files()
-        
+
         print("ğŸ“Š ç”Ÿæˆè¦†è“‹ç‡å ±å‘Š...")
         self.generate_coverage_report()
-        
+
         print("ğŸ“ ç”Ÿæˆæ¸¬è©¦æ¨¡æ¿...")
         templates = self.generate_test_templates()
-        
+
         print("ğŸ’¾ ä¿å­˜å ±å‘Š...")
         report_file = self.save_report()
-        
+
         self.print_summary()
-        
+
         print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_file}")
         print(f"ğŸ§ª æ¸¬è©¦æ¨¡æ¿å·²ç”Ÿæˆ {len(templates)} å€‹æ–‡ä»¶")
-        
+
         return report_file, templates
 
 
 if __name__ == "__main__":
     import sys
-    
+
     project_root = sys.argv[1] if len(sys.argv) > 1 else "."
-    
+
     auditor = TestCoverageAuditor(project_root)
     report_file, templates = auditor.run_audit()
-    
+
     print(f"\nğŸ‰ å¯©æŸ¥å®Œæˆï¼")
     print(f"ğŸ“Š å ±å‘Š: {report_file}")
-    print(f"ğŸ§ª æ¨¡æ¿: {len(templates)} å€‹æ–‡ä»¶å·²ç”Ÿæˆ")
\ No newline at end of file
+    print(f"ğŸ§ª æ¨¡æ¿: {len(templates)} å€‹æ–‡ä»¶å·²ç”Ÿæˆ")
diff --git a/auto_generate_video_fold6/scripts/test_backup_recovery.py b/auto_generate_video_fold6/scripts/test_backup_recovery.py
index 711fb70..6a7941f 100644
--- a/auto_generate_video_fold6/scripts/test_backup_recovery.py
+++ b/auto_generate_video_fold6/scripts/test_backup_recovery.py
@@ -22,7 +22,7 @@ from dataclasses import dataclass
 # è¨­ç½®æ—¥èªŒ
 logging.basicConfig(
     level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
 )
 logger = logging.getLogger(__name__)
 
@@ -30,6 +30,7 @@ logger = logging.getLogger(__name__)
 @dataclass
 class TestResult:
     """æ¸¬è©¦çµæœ"""
+
     test_name: str
     success: bool
     duration: float
@@ -39,86 +40,90 @@ class TestResult:
 
 class BackupRecoveryTester:
     """å‚™ä»½æ¢å¾©æ¸¬è©¦å™¨"""
-    
+
     def __init__(self):
         self.test_results: List[TestResult] = []
         self.temp_dir = Path(tempfile.mkdtemp(prefix="backup_test_"))
         self.config = self._load_config()
-        
+
     def _load_config(self) -> Dict:
         """è¼‰å…¥æ¸¬è©¦é…ç½®"""
         return {
-            'postgres': {
-                'host': os.getenv('POSTGRES_HOST', 'localhost'),
-                'port': os.getenv('POSTGRES_PORT', '5432'),
-                'user': os.getenv('POSTGRES_USER', 'auto_video_user'),
-                'password': os.getenv('POSTGRES_PASSWORD', 'password'),
-                'database': os.getenv('POSTGRES_DB', 'auto_video_db')
+            "postgres": {
+                "host": os.getenv("POSTGRES_HOST", "localhost"),
+                "port": os.getenv("POSTGRES_PORT", "5432"),
+                "user": os.getenv("POSTGRES_USER", "auto_video_user"),
+                "password": os.getenv("POSTGRES_PASSWORD", "password"),
+                "database": os.getenv("POSTGRES_DB", "auto_video_db"),
             },
-            'redis': {
-                'host': os.getenv('REDIS_HOST', 'localhost'),
-                'port': int(os.getenv('REDIS_PORT', '6379')),
-                'password': os.getenv('REDIS_PASSWORD', '')
+            "redis": {
+                "host": os.getenv("REDIS_HOST", "localhost"),
+                "port": int(os.getenv("REDIS_PORT", "6379")),
+                "password": os.getenv("REDIS_PASSWORD", ""),
+            },
+            "s3": {
+                "endpoint_url": os.getenv(
+                    "S3_ENDPOINT_URL", "http://localhost:9000"
+                ),
+                "access_key": os.getenv("S3_ACCESS_KEY_ID", "minioadmin"),
+                "secret_key": os.getenv("S3_SECRET_ACCESS_KEY", "minioadmin"),
+                "bucket": os.getenv("S3_BUCKET_NAME", "auto-video-backups"),
             },
-            's3': {
-                'endpoint_url': os.getenv('S3_ENDPOINT_URL', 'http://localhost:9000'),
-                'access_key': os.getenv('S3_ACCESS_KEY_ID', 'minioadmin'),
-                'secret_key': os.getenv('S3_SECRET_ACCESS_KEY', 'minioadmin'),
-                'bucket': os.getenv('S3_BUCKET_NAME', 'auto-video-backups')
-            }
         }
 
     async def test_database_backup_restore(self) -> TestResult:
         """æ¸¬è©¦è³‡æ–™åº«å‚™ä»½å’Œæ¢å¾©"""
         start_time = datetime.now()
         test_name = "è³‡æ–™åº«å‚™ä»½æ¢å¾©æ¸¬è©¦"
-        
+
         try:
             # 1. å‰µå»ºæ¸¬è©¦è³‡æ–™
             test_table = f"backup_test_{int(datetime.now().timestamp())}"
             test_data = [
-                ('test_user_1', 'test@example.com', '2024-01-01'),
-                ('test_user_2', 'user2@example.com', '2024-01-02'),
-                ('test_user_3', 'user3@example.com', '2024-01-03')
+                ("test_user_1", "test@example.com", "2024-01-01"),
+                ("test_user_2", "user2@example.com", "2024-01-02"),
+                ("test_user_3", "user3@example.com", "2024-01-03"),
             ]
-            
+
             logger.info(f"å‰µå»ºæ¸¬è©¦è¡¨: {test_table}")
             await self._create_test_data(test_table, test_data)
-            
+
             # 2. åŸ·è¡Œè³‡æ–™åº«å‚™ä»½
             backup_file = self.temp_dir / f"{test_table}_backup.sql.gz"
             logger.info(f"åŸ·è¡Œè³‡æ–™åº«å‚™ä»½åˆ°: {backup_file}")
             backup_success = await self._backup_database(backup_file)
-            
+
             if not backup_success:
                 raise Exception("è³‡æ–™åº«å‚™ä»½å¤±æ•—")
-            
+
             # 3. åˆªé™¤æ¸¬è©¦è³‡æ–™
             logger.info("åˆªé™¤åŸå§‹æ¸¬è©¦è³‡æ–™")
             await self._delete_test_data(test_table)
-            
+
             # 4. é©—è­‰è³‡æ–™å·²åˆªé™¤
             data_count = await self._count_test_data(test_table)
             if data_count > 0:
                 raise Exception("æ¸¬è©¦è³‡æ–™åˆªé™¤å¤±æ•—")
-            
+
             # 5. åŸ·è¡Œè³‡æ–™æ¢å¾©
             logger.info("åŸ·è¡Œè³‡æ–™åº«æ¢å¾©")
             restore_success = await self._restore_database(backup_file)
-            
+
             if not restore_success:
                 raise Exception("è³‡æ–™åº«æ¢å¾©å¤±æ•—")
-            
+
             # 6. é©—è­‰æ¢å¾©çš„è³‡æ–™
             logger.info("é©—è­‰æ¢å¾©çš„è³‡æ–™")
             restored_data = await self._get_test_data(test_table)
-            
+
             if len(restored_data) != len(test_data):
-                raise Exception(f"æ¢å¾©çš„è³‡æ–™æ•¸é‡ä¸æ­£ç¢º: æœŸæœ› {len(test_data)}, å¯¦éš› {len(restored_data)}")
-            
+                raise Exception(
+                    f"æ¢å¾©çš„è³‡æ–™æ•¸é‡ä¸æ­£ç¢º: æœŸæœ› {len(test_data)}, å¯¦éš› {len(restored_data)}"
+                )
+
             # 7. æ¸…ç†æ¸¬è©¦è³‡æ–™
             await self._delete_test_data(test_table)
-            
+
             duration = (datetime.now() - start_time).total_seconds()
             return TestResult(
                 test_name=test_name,
@@ -126,11 +131,13 @@ class BackupRecoveryTester:
                 duration=duration,
                 message=f"æˆåŠŸå‚™ä»½å’Œæ¢å¾© {len(test_data)} ç­†è³‡æ–™",
                 details={
-                    'backup_file_size': backup_file.stat().st_size if backup_file.exists() else 0,
-                    'restored_records': len(restored_data)
-                }
+                    "backup_file_size": backup_file.stat().st_size
+                    if backup_file.exists()
+                    else 0,
+                    "restored_records": len(restored_data),
+                },
             )
-            
+
         except Exception as e:
             duration = (datetime.now() - start_time).total_seconds()
             logger.error(f"è³‡æ–™åº«å‚™ä»½æ¢å¾©æ¸¬è©¦å¤±æ•—: {e}")
@@ -138,67 +145,68 @@ class BackupRecoveryTester:
                 test_name=test_name,
                 success=False,
                 duration=duration,
-                message=str(e)
+                message=str(e),
             )
 
     async def test_redis_backup_restore(self) -> TestResult:
         """æ¸¬è©¦ Redis å‚™ä»½å’Œæ¢å¾©"""
         start_time = datetime.now()
         test_name = "Redis å‚™ä»½æ¢å¾©æ¸¬è©¦"
-        
+
         try:
             # 1. å‰µå»ºæ¸¬è©¦è³‡æ–™
             test_keys = {
-                f"test:backup:{i}": f"test_value_{i}" 
-                for i in range(100)
+                f"test:backup:{i}": f"test_value_{i}" for i in range(100)
             }
-            
+
             logger.info(f"åœ¨ Redis ä¸­å‰µå»º {len(test_keys)} å€‹æ¸¬è©¦éµ")
             redis_client = redis.Redis(
-                host=self.config['redis']['host'],
-                port=self.config['redis']['port'],
-                password=self.config['redis']['password'],
-                decode_responses=True
+                host=self.config["redis"]["host"],
+                port=self.config["redis"]["port"],
+                password=self.config["redis"]["password"],
+                decode_responses=True,
             )
-            
+
             for key, value in test_keys.items():
                 redis_client.set(key, value)
-            
+
             # 2. åŸ·è¡Œ Redis å‚™ä»½ (RDB dump)
             backup_file = self.temp_dir / "redis_backup.rdb"
             logger.info(f"åŸ·è¡Œ Redis å‚™ä»½åˆ°: {backup_file}")
-            
+
             # è§¸ç™¼ Redis ä¿å­˜
             redis_client.bgsave()
-            
+
             # ç­‰å¾…å‚™ä»½å®Œæˆ
             await asyncio.sleep(1)
-            
+
             # è¤‡è£½ RDB æ–‡ä»¶
             redis_dump_path = "/var/lib/redis/dump.rdb"  # é»˜èªè·¯å¾‘
             if not Path(redis_dump_path).exists():
                 redis_dump_path = "/data/dump.rdb"  # Docker å®¹å™¨è·¯å¾‘
-            
+
             if Path(redis_dump_path).exists():
                 shutil.copy2(redis_dump_path, backup_file)
             else:
                 # å¦‚æœæ‰¾ä¸åˆ° RDB æ–‡ä»¶ï¼Œä½¿ç”¨æ‰‹å‹•æ–¹å¼
                 await self._manual_redis_backup(redis_client, backup_file)
-            
+
             # 3. æ¸…é™¤æ¸¬è©¦è³‡æ–™
             logger.info("æ¸…é™¤ Redis æ¸¬è©¦è³‡æ–™")
             for key in test_keys.keys():
                 redis_client.delete(key)
-            
+
             # 4. é©—è­‰è³‡æ–™å·²æ¸…é™¤
             remaining_keys = redis_client.keys("test:backup:*")
             if remaining_keys:
-                raise Exception(f"æ¸¬è©¦è³‡æ–™æ¸…é™¤å¤±æ•—ï¼Œä»æœ‰ {len(remaining_keys)} å€‹éµ")
-            
+                raise Exception(
+                    f"æ¸¬è©¦è³‡æ–™æ¸…é™¤å¤±æ•—ï¼Œä»æœ‰ {len(remaining_keys)} å€‹éµ"
+                )
+
             # 5. æ¨¡æ“¬æ¢å¾© (é‡æ–°æ’å…¥è³‡æ–™)
             logger.info("æ¨¡æ“¬ Redis è³‡æ–™æ¢å¾©")
             await self._restore_redis_data(redis_client, test_keys)
-            
+
             # 6. é©—è­‰æ¢å¾©çš„è³‡æ–™
             logger.info("é©—è­‰æ¢å¾©çš„è³‡æ–™")
             restored_count = 0
@@ -206,14 +214,16 @@ class BackupRecoveryTester:
                 actual_value = redis_client.get(key)
                 if actual_value == expected_value:
                     restored_count += 1
-            
+
             if restored_count != len(test_keys):
-                raise Exception(f"æ¢å¾©çš„è³‡æ–™ä¸å®Œæ•´: æœŸæœ› {len(test_keys)}, å¯¦éš› {restored_count}")
-            
+                raise Exception(
+                    f"æ¢å¾©çš„è³‡æ–™ä¸å®Œæ•´: æœŸæœ› {len(test_keys)}, å¯¦éš› {restored_count}"
+                )
+
             # 7. æ¸…ç†æ¸¬è©¦è³‡æ–™
             for key in test_keys.keys():
                 redis_client.delete(key)
-            
+
             duration = (datetime.now() - start_time).total_seconds()
             return TestResult(
                 test_name=test_name,
@@ -221,11 +231,13 @@ class BackupRecoveryTester:
                 duration=duration,
                 message=f"æˆåŠŸå‚™ä»½å’Œæ¢å¾© {len(test_keys)} å€‹ Redis éµ",
                 details={
-                    'backup_file_size': backup_file.stat().st_size if backup_file.exists() else 0,
-                    'restored_keys': restored_count
-                }
+                    "backup_file_size": backup_file.stat().st_size
+                    if backup_file.exists()
+                    else 0,
+                    "restored_keys": restored_count,
+                },
             )
-            
+
         except Exception as e:
             duration = (datetime.now() - start_time).total_seconds()
             logger.error(f"Redis å‚™ä»½æ¢å¾©æ¸¬è©¦å¤±æ•—: {e}")
@@ -233,67 +245,74 @@ class BackupRecoveryTester:
                 test_name=test_name,
                 success=False,
                 duration=duration,
-                message=str(e)
+                message=str(e),
             )
 
     async def test_file_backup_restore(self) -> TestResult:
         """æ¸¬è©¦æª”æ¡ˆå‚™ä»½å’Œæ¢å¾©"""
         start_time = datetime.now()
         test_name = "æª”æ¡ˆå‚™ä»½æ¢å¾©æ¸¬è©¦"
-        
+
         try:
             # 1. å‰µå»ºæ¸¬è©¦æª”æ¡ˆ
             test_files_dir = self.temp_dir / "test_files"
             test_files_dir.mkdir(exist_ok=True)
-            
+
             test_files = {}
             for i in range(10):
                 file_path = test_files_dir / f"test_file_{i}.txt"
-                content = f"Test file {i} content\nCreated at {datetime.now()}\n" * 100
-                file_path.write_text(content, encoding='utf-8')
+                content = (
+                    f"Test file {i} content\nCreated at {datetime.now()}\n"
+                    * 100
+                )
+                file_path.write_text(content, encoding="utf-8")
                 test_files[file_path.name] = len(content)
-            
+
             logger.info(f"å‰µå»ºäº† {len(test_files)} å€‹æ¸¬è©¦æª”æ¡ˆ")
-            
+
             # 2. åŸ·è¡Œæª”æ¡ˆå‚™ä»½
             backup_archive = self.temp_dir / "files_backup.tar.gz"
             logger.info(f"åŸ·è¡Œæª”æ¡ˆå‚™ä»½åˆ°: {backup_archive}")
-            
+
             with tarfile.open(backup_archive, "w:gz") as tar:
                 tar.add(test_files_dir, arcname="test_files")
-            
+
             # 3. åˆªé™¤åŸå§‹æª”æ¡ˆ
             logger.info("åˆªé™¤åŸå§‹æ¸¬è©¦æª”æ¡ˆ")
             shutil.rmtree(test_files_dir)
-            
+
             # 4. é©—è­‰æª”æ¡ˆå·²åˆªé™¤
             if test_files_dir.exists():
                 raise Exception("æ¸¬è©¦æª”æ¡ˆåˆªé™¤å¤±æ•—")
-            
+
             # 5. åŸ·è¡Œæª”æ¡ˆæ¢å¾©
             logger.info("åŸ·è¡Œæª”æ¡ˆæ¢å¾©")
             with tarfile.open(backup_archive, "r:gz") as tar:
                 tar.extractall(self.temp_dir)
-            
+
             # 6. é©—è­‰æ¢å¾©çš„æª”æ¡ˆ
             restored_files_dir = self.temp_dir / "test_files"
             if not restored_files_dir.exists():
                 raise Exception("æª”æ¡ˆæ¢å¾©å¤±æ•—ï¼Œç›®éŒ„ä¸å­˜åœ¨")
-            
+
             restored_files = list(restored_files_dir.glob("*.txt"))
             if len(restored_files) != len(test_files):
-                raise Exception(f"æ¢å¾©çš„æª”æ¡ˆæ•¸é‡ä¸æ­£ç¢º: æœŸæœ› {len(test_files)}, å¯¦éš› {len(restored_files)}")
-            
+                raise Exception(
+                    f"æ¢å¾©çš„æª”æ¡ˆæ•¸é‡ä¸æ­£ç¢º: æœŸæœ› {len(test_files)}, å¯¦éš› {len(restored_files)}"
+                )
+
             # é©—è­‰æª”æ¡ˆå…§å®¹
             for file_path in restored_files:
                 if file_path.name not in test_files:
                     raise Exception(f"ç™¼ç¾æ„å¤–çš„æª”æ¡ˆ: {file_path.name}")
-                
-                actual_size = len(file_path.read_text(encoding='utf-8'))
+
+                actual_size = len(file_path.read_text(encoding="utf-8"))
                 expected_size = test_files[file_path.name]
                 if actual_size != expected_size:
-                    raise Exception(f"æª”æ¡ˆ {file_path.name} å¤§å°ä¸æ­£ç¢º: æœŸæœ› {expected_size}, å¯¦éš› {actual_size}")
-            
+                    raise Exception(
+                        f"æª”æ¡ˆ {file_path.name} å¤§å°ä¸æ­£ç¢º: æœŸæœ› {expected_size}, å¯¦éš› {actual_size}"
+                    )
+
             duration = (datetime.now() - start_time).total_seconds()
             return TestResult(
                 test_name=test_name,
@@ -301,11 +320,13 @@ class BackupRecoveryTester:
                 duration=duration,
                 message=f"æˆåŠŸå‚™ä»½å’Œæ¢å¾© {len(test_files)} å€‹æª”æ¡ˆ",
                 details={
-                    'backup_archive_size': backup_archive.stat().st_size if backup_archive.exists() else 0,
-                    'restored_files': len(restored_files)
-                }
+                    "backup_archive_size": backup_archive.stat().st_size
+                    if backup_archive.exists()
+                    else 0,
+                    "restored_files": len(restored_files),
+                },
             )
-            
+
         except Exception as e:
             duration = (datetime.now() - start_time).total_seconds()
             logger.error(f"æª”æ¡ˆå‚™ä»½æ¢å¾©æ¸¬è©¦å¤±æ•—: {e}")
@@ -313,68 +334,74 @@ class BackupRecoveryTester:
                 test_name=test_name,
                 success=False,
                 duration=duration,
-                message=str(e)
+                message=str(e),
             )
 
     async def test_s3_backup_restore(self) -> TestResult:
         """æ¸¬è©¦ S3 å‚™ä»½å’Œæ¢å¾©"""
         start_time = datetime.now()
         test_name = "S3 å‚™ä»½æ¢å¾©æ¸¬è©¦"
-        
+
         try:
             # 1. è¨­ç½® S3 å®¢æˆ¶ç«¯
             s3_client = boto3.client(
-                's3',
-                endpoint_url=self.config['s3']['endpoint_url'],
-                aws_access_key_id=self.config['s3']['access_key'],
-                aws_secret_access_key=self.config['s3']['secret_key']
+                "s3",
+                endpoint_url=self.config["s3"]["endpoint_url"],
+                aws_access_key_id=self.config["s3"]["access_key"],
+                aws_secret_access_key=self.config["s3"]["secret_key"],
             )
-            
-            bucket_name = self.config['s3']['bucket']
-            
+
+            bucket_name = self.config["s3"]["bucket"]
+
             # 2. å‰µå»ºæ¸¬è©¦æª”æ¡ˆ
             test_file = self.temp_dir / "s3_test_file.json"
             test_data = {
-                'test_id': int(datetime.now().timestamp()),
-                'data': [f"item_{i}" for i in range(1000)],
-                'timestamp': datetime.now().isoformat()
+                "test_id": int(datetime.now().timestamp()),
+                "data": [f"item_{i}" for i in range(1000)],
+                "timestamp": datetime.now().isoformat(),
             }
-            
-            test_file.write_text(json.dumps(test_data, indent=2), encoding='utf-8')
+
+            test_file.write_text(
+                json.dumps(test_data, indent=2), encoding="utf-8"
+            )
             logger.info(f"å‰µå»ºæ¸¬è©¦æª”æ¡ˆ: {test_file}")
-            
+
             # 3. ä¸Šå‚³åˆ° S3
             s3_key = f"backup-test/{test_file.name}"
             logger.info(f"ä¸Šå‚³æª”æ¡ˆåˆ° S3: {s3_key}")
-            
+
             s3_client.upload_file(str(test_file), bucket_name, s3_key)
-            
+
             # 4. åˆªé™¤æœ¬åœ°æª”æ¡ˆ
             test_file.unlink()
             logger.info("åˆªé™¤æœ¬åœ°æ¸¬è©¦æª”æ¡ˆ")
-            
+
             # 5. å¾ S3 ä¸‹è¼‰æª”æ¡ˆ
             restored_file = self.temp_dir / "restored_s3_test_file.json"
             logger.info(f"å¾ S3 ä¸‹è¼‰æª”æ¡ˆ: {restored_file}")
-            
+
             s3_client.download_file(bucket_name, s3_key, str(restored_file))
-            
+
             # 6. é©—è­‰æ¢å¾©çš„æª”æ¡ˆ
             if not restored_file.exists():
                 raise Exception("æª”æ¡ˆå¾ S3 æ¢å¾©å¤±æ•—")
-            
-            restored_data = json.loads(restored_file.read_text(encoding='utf-8'))
-            
-            if restored_data['test_id'] != test_data['test_id']:
+
+            restored_data = json.loads(
+                restored_file.read_text(encoding="utf-8")
+            )
+
+            if restored_data["test_id"] != test_data["test_id"]:
                 raise Exception("æ¢å¾©çš„æª”æ¡ˆå…§å®¹ä¸æ­£ç¢º")
-            
-            if len(restored_data['data']) != len(test_data['data']):
-                raise Exception(f"æ¢å¾©çš„è³‡æ–™é …ç›®æ•¸é‡ä¸æ­£ç¢º: æœŸæœ› {len(test_data['data'])}, å¯¦éš› {len(restored_data['data'])}")
-            
+
+            if len(restored_data["data"]) != len(test_data["data"]):
+                raise Exception(
+                    f"æ¢å¾©çš„è³‡æ–™é …ç›®æ•¸é‡ä¸æ­£ç¢º: æœŸæœ› {len(test_data['data'])}, å¯¦éš› {len(restored_data['data'])}"
+                )
+
             # 7. æ¸…ç† S3 ä¸­çš„æ¸¬è©¦æª”æ¡ˆ
             s3_client.delete_object(Bucket=bucket_name, Key=s3_key)
             logger.info("æ¸…ç† S3 ä¸­çš„æ¸¬è©¦æª”æ¡ˆ")
-            
+
             duration = (datetime.now() - start_time).total_seconds()
             return TestResult(
                 test_name=test_name,
@@ -382,11 +409,11 @@ class BackupRecoveryTester:
                 duration=duration,
                 message=f"æˆåŠŸé€šé S3 å‚™ä»½å’Œæ¢å¾©æª”æ¡ˆ",
                 details={
-                    'file_size': restored_file.stat().st_size,
-                    'data_items': len(restored_data['data'])
-                }
+                    "file_size": restored_file.stat().st_size,
+                    "data_items": len(restored_data["data"]),
+                },
             )
-            
+
         except Exception as e:
             duration = (datetime.now() - start_time).total_seconds()
             logger.error(f"S3 å‚™ä»½æ¢å¾©æ¸¬è©¦å¤±æ•—: {e}")
@@ -394,12 +421,14 @@ class BackupRecoveryTester:
                 test_name=test_name,
                 success=False,
                 duration=duration,
-                message=str(e)
+                message=str(e),
             )
 
-    async def _create_test_data(self, table_name: str, data: List[Tuple]) -> None:
+    async def _create_test_data(
+        self, table_name: str, data: List[Tuple]
+    ) -> None:
         """å‰µå»ºæ¸¬è©¦è³‡æ–™"""
-        conn = psycopg2.connect(**self.config['postgres'])
+        conn = psycopg2.connect(**self.config["postgres"])
         try:
             with conn.cursor() as cur:
                 # å‰µå»ºæ¸¬è©¦è¡¨
@@ -411,21 +440,21 @@ class BackupRecoveryTester:
                         created_date DATE
                     )
                 """)
-                
+
                 # æ’å…¥æ¸¬è©¦è³‡æ–™
                 for row in data:
                     cur.execute(
                         f"INSERT INTO {table_name} (name, email, created_date) VALUES (%s, %s, %s)",
-                        row
+                        row,
                     )
-                
+
                 conn.commit()
         finally:
             conn.close()
 
     async def _delete_test_data(self, table_name: str) -> None:
         """åˆªé™¤æ¸¬è©¦è³‡æ–™"""
-        conn = psycopg2.connect(**self.config['postgres'])
+        conn = psycopg2.connect(**self.config["postgres"])
         try:
             with conn.cursor() as cur:
                 cur.execute(f"DROP TABLE IF EXISTS {table_name}")
@@ -435,7 +464,7 @@ class BackupRecoveryTester:
 
     async def _count_test_data(self, table_name: str) -> int:
         """è¨ˆç®—æ¸¬è©¦è³‡æ–™æ•¸é‡"""
-        conn = psycopg2.connect(**self.config['postgres'])
+        conn = psycopg2.connect(**self.config["postgres"])
         try:
             with conn.cursor() as cur:
                 cur.execute(f"SELECT COUNT(*) FROM {table_name}")
@@ -447,10 +476,12 @@ class BackupRecoveryTester:
 
     async def _get_test_data(self, table_name: str) -> List[Tuple]:
         """ç²å–æ¸¬è©¦è³‡æ–™"""
-        conn = psycopg2.connect(**self.config['postgres'])
+        conn = psycopg2.connect(**self.config["postgres"])
         try:
             with conn.cursor() as cur:
-                cur.execute(f"SELECT name, email, created_date FROM {table_name}")
+                cur.execute(
+                    f"SELECT name, email, created_date FROM {table_name}"
+                )
                 return cur.fetchall()
         except psycopg2.Error:
             return []
@@ -467,12 +498,12 @@ class BackupRecoveryTester:
                 f"--username={self.config['postgres']['user']}",
                 f"--dbname={self.config['postgres']['database']}",
                 "--no-password",
-                "--format=custom"
+                "--format=custom",
             ]
-            
+
             env = os.environ.copy()
-            env["PGPASSWORD"] = self.config['postgres']['password']
-            
+            env["PGPASSWORD"] = self.config["postgres"]["password"]
+
             with gzip.open(backup_file, "wt") as f:
                 result = subprocess.run(
                     cmd,
@@ -480,11 +511,11 @@ class BackupRecoveryTester:
                     stderr=subprocess.PIPE,
                     env=env,
                     check=True,
-                    text=True
+                    text=True,
                 )
-            
+
             return backup_file.exists() and backup_file.stat().st_size > 0
-            
+
         except subprocess.CalledProcessError as e:
             logger.error(f"è³‡æ–™åº«å‚™ä»½å¤±æ•—: {e.stderr}")
             return False
@@ -499,36 +530,40 @@ class BackupRecoveryTester:
                     f"--port={self.config['postgres']['port']}",
                     f"--username={self.config['postgres']['user']}",
                     f"--dbname={self.config['postgres']['database']}",
-                    "--no-password"
+                    "--no-password",
                 ]
-                
+
                 env = os.environ.copy()
-                env["PGPASSWORD"] = self.config['postgres']['password']
-                
+                env["PGPASSWORD"] = self.config["postgres"]["password"]
+
                 result = subprocess.run(
                     cmd,
                     stdin=f,
                     stderr=subprocess.PIPE,
                     env=env,
                     check=True,
-                    text=True
+                    text=True,
                 )
-            
+
             return True
-            
+
         except subprocess.CalledProcessError as e:
             logger.error(f"è³‡æ–™åº«æ¢å¾©å¤±æ•—: {e.stderr}")
             return False
 
-    async def _manual_redis_backup(self, redis_client, backup_file: Path) -> None:
+    async def _manual_redis_backup(
+        self, redis_client, backup_file: Path
+    ) -> None:
         """æ‰‹å‹• Redis å‚™ä»½"""
         backup_data = {}
         for key in redis_client.scan_iter("test:backup:*"):
             backup_data[key] = redis_client.get(key)
-        
-        backup_file.write_text(json.dumps(backup_data), encoding='utf-8')
 
-    async def _restore_redis_data(self, redis_client, test_keys: Dict[str, str]) -> None:
+        backup_file.write_text(json.dumps(backup_data), encoding="utf-8")
+
+    async def _restore_redis_data(
+        self, redis_client, test_keys: Dict[str, str]
+    ) -> None:
         """æ¢å¾© Redis è³‡æ–™"""
         for key, value in test_keys.items():
             redis_client.set(key, value)
@@ -537,58 +572,62 @@ class BackupRecoveryTester:
         """åŸ·è¡Œæ‰€æœ‰å‚™ä»½æ¢å¾©æ¸¬è©¦"""
         logger.info("é–‹å§‹åŸ·è¡Œå‚™ä»½æ¢å¾©æ¸¬è©¦å¥—ä»¶")
         start_time = datetime.now()
-        
+
         try:
             # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
             tests = [
                 self.test_database_backup_restore(),
                 self.test_redis_backup_restore(),
                 self.test_file_backup_restore(),
-                self.test_s3_backup_restore()
+                self.test_s3_backup_restore(),
             ]
-            
-            self.test_results = await asyncio.gather(*tests, return_exceptions=True)
-            
+
+            self.test_results = await asyncio.gather(
+                *tests, return_exceptions=True
+            )
+
             # è™•ç†ç•°å¸¸çµæœ
             for i, result in enumerate(self.test_results):
                 if isinstance(result, Exception):
                     self.test_results[i] = TestResult(
-                        test_name=f"æ¸¬è©¦ {i+1}",
+                        test_name=f"æ¸¬è©¦ {i + 1}",
                         success=False,
                         duration=0,
-                        message=str(result)
+                        message=str(result),
                     )
-            
+
             # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
             total_tests = len(self.test_results)
             passed_tests = sum(1 for r in self.test_results if r.success)
             failed_tests = total_tests - passed_tests
             total_duration = (datetime.now() - start_time).total_seconds()
-            
+
             report = {
-                'summary': {
-                    'total_tests': total_tests,
-                    'passed': passed_tests,
-                    'failed': failed_tests,
-                    'success_rate': (passed_tests / total_tests) * 100 if total_tests > 0 else 0,
-                    'total_duration': total_duration,
-                    'timestamp': datetime.now().isoformat()
+                "summary": {
+                    "total_tests": total_tests,
+                    "passed": passed_tests,
+                    "failed": failed_tests,
+                    "success_rate": (passed_tests / total_tests) * 100
+                    if total_tests > 0
+                    else 0,
+                    "total_duration": total_duration,
+                    "timestamp": datetime.now().isoformat(),
                 },
-                'results': [
+                "results": [
                     {
-                        'test_name': r.test_name,
-                        'success': r.success,
-                        'duration': r.duration,
-                        'message': r.message,
-                        'details': r.details
+                        "test_name": r.test_name,
+                        "success": r.success,
+                        "duration": r.duration,
+                        "message": r.message,
+                        "details": r.details,
                     }
                     for r in self.test_results
-                ]
+                ],
             }
-            
+
             logger.info(f"æ¸¬è©¦å®Œæˆ: {passed_tests}/{total_tests} é€šé")
             return report
-            
+
         finally:
             # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
             if self.temp_dir.exists():
@@ -597,34 +636,34 @@ class BackupRecoveryTester:
 
     def generate_report(self, report: Dict[str, Any]) -> str:
         """ç”Ÿæˆäººé¡å¯è®€çš„æ¸¬è©¦å ±å‘Š"""
-        summary = report['summary']
-        
+        summary = report["summary"]
+
         report_text = f"""
 =================================================
 å‚™ä»½æ¢å¾©æ¸¬è©¦å ±å‘Š
 =================================================
-æ¸¬è©¦æ™‚é–“: {summary['timestamp']}
-ç¸½æ¸¬è©¦æ•¸: {summary['total_tests']}
-é€šéæ¸¬è©¦: {summary['passed']}
-å¤±æ•—æ¸¬è©¦: {summary['failed']}
-æˆåŠŸç‡: {summary['success_rate']:.1f}%
-ç¸½è€—æ™‚: {summary['total_duration']:.2f} ç§’
+æ¸¬è©¦æ™‚é–“: {summary["timestamp"]}
+ç¸½æ¸¬è©¦æ•¸: {summary["total_tests"]}
+é€šéæ¸¬è©¦: {summary["passed"]}
+å¤±æ•—æ¸¬è©¦: {summary["failed"]}
+æˆåŠŸç‡: {summary["success_rate"]:.1f}%
+ç¸½è€—æ™‚: {summary["total_duration"]:.2f} ç§’
 
 è©³ç´°çµæœ:
 -------------------------------------------------
 """
-        
-        for result in report['results']:
-            status = "âœ… é€šé" if result['success'] else "âŒ å¤±æ•—"
+
+        for result in report["results"]:
+            status = "âœ… é€šé" if result["success"] else "âŒ å¤±æ•—"
             report_text += f"""
-{status} {result['test_name']}
-   è€—æ™‚: {result['duration']:.2f} ç§’
-   è¨Šæ¯: {result['message']}
+{status} {result["test_name"]}
+   è€—æ™‚: {result["duration"]:.2f} ç§’
+   è¨Šæ¯: {result["message"]}
 """
-            if result['details']:
-                for key, value in result['details'].items():
+            if result["details"]:
+                for key, value in result["details"].items():
                     report_text += f"   {key}: {value}\n"
-        
+
         report_text += "\n================================================="
         return report_text
 
@@ -632,30 +671,32 @@ class BackupRecoveryTester:
 async def main():
     """ä¸»å‡½æ•¸"""
     tester = BackupRecoveryTester()
-    
+
     try:
         logger.info("é–‹å§‹å‚™ä»½æ¢å¾©æ¸¬è©¦")
         report = await tester.run_all_tests()
-        
+
         # è¼¸å‡ºå ±å‘Š
         print(tester.generate_report(report))
-        
+
         # ä¿å­˜ JSON å ±å‘Š
         report_file = Path("backup_recovery_test_report.json")
-        report_file.write_text(json.dumps(report, indent=2, ensure_ascii=False))
+        report_file.write_text(
+            json.dumps(report, indent=2, ensure_ascii=False)
+        )
         logger.info(f"è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")
-        
+
         # è¿”å›é©ç•¶çš„é€€å‡ºç¢¼
-        if report['summary']['failed'] > 0:
+        if report["summary"]["failed"] > 0:
             exit(1)
         else:
             logger.info("æ‰€æœ‰å‚™ä»½æ¢å¾©æ¸¬è©¦é€šé!")
             exit(0)
-            
+
     except Exception as e:
         logger.error(f"æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
         exit(1)
 
 
 if __name__ == "__main__":
-    asyncio.run(main())
\ No newline at end of file
+    asyncio.run(main())
diff --git a/auto_generate_video_fold6/scripts/validate_backup_config.py b/auto_generate_video_fold6/scripts/validate_backup_config.py
index 73e1af2..b93b5a2 100644
--- a/auto_generate_video_fold6/scripts/validate_backup_config.py
+++ b/auto_generate_video_fold6/scripts/validate_backup_config.py
@@ -14,8 +14,7 @@ from dataclasses import dataclass
 
 # è¨­ç½®æ—¥èªŒ
 logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(levelname)s - %(message)s'
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger(__name__)
 
@@ -23,6 +22,7 @@ logger = logging.getLogger(__name__)
 @dataclass
 class ValidationResult:
     """é©—è­‰çµæœ"""
+
     component: str
     success: bool
     message: str
@@ -31,11 +31,11 @@ class ValidationResult:
 
 class BackupConfigValidator:
     """å‚™ä»½é…ç½®é©—è­‰å™¨"""
-    
+
     def __init__(self):
         self.results: List[ValidationResult] = []
         self.project_root = Path(__file__).parent.parent
-    
+
     def validate_backup_scripts(self) -> ValidationResult:
         """é©—è­‰å‚™ä»½è…³æœ¬å­˜åœ¨æ€§å’Œå¯åŸ·è¡Œæ€§"""
         try:
@@ -43,47 +43,47 @@ class BackupConfigValidator:
                 "scripts/backup_manager.py",
                 "scripts/backup-system.sh",
                 "scripts/restore-system.sh",
-                "scripts/disaster_recovery.py"
+                "scripts/disaster_recovery.py",
             ]
-            
+
             missing_scripts = []
             non_executable = []
-            
+
             for script_path in backup_scripts:
                 full_path = self.project_root / script_path
                 if not full_path.exists():
                     missing_scripts.append(script_path)
                 elif not os.access(full_path, os.X_OK):
                     non_executable.append(script_path)
-            
+
             if missing_scripts:
                 return ValidationResult(
                     component="å‚™ä»½è…³æœ¬",
                     success=False,
-                    message=f"ç¼ºå°‘å‚™ä»½è…³æœ¬: {', '.join(missing_scripts)}"
+                    message=f"ç¼ºå°‘å‚™ä»½è…³æœ¬: {', '.join(missing_scripts)}",
                 )
-            
+
             if non_executable:
                 return ValidationResult(
                     component="å‚™ä»½è…³æœ¬",
                     success=False,
-                    message=f"è…³æœ¬ä¸å¯åŸ·è¡Œ: {', '.join(non_executable)}"
+                    message=f"è…³æœ¬ä¸å¯åŸ·è¡Œ: {', '.join(non_executable)}",
                 )
-            
+
             return ValidationResult(
                 component="å‚™ä»½è…³æœ¬",
                 success=True,
                 message="æ‰€æœ‰å‚™ä»½è…³æœ¬éƒ½å­˜åœ¨ä¸”å¯åŸ·è¡Œ",
-                details={"scripts": backup_scripts}
+                details={"scripts": backup_scripts},
             )
-            
+
         except Exception as e:
             return ValidationResult(
                 component="å‚™ä»½è…³æœ¬",
                 success=False,
-                message=f"é©—è­‰å¤±æ•—: {str(e)}"
+                message=f"é©—è­‰å¤±æ•—: {str(e)}",
             )
-    
+
     def validate_backup_dependencies(self) -> ValidationResult:
         """é©—è­‰å‚™ä»½æ‰€éœ€çš„ç³»çµ±ä¾è³´"""
         try:
@@ -92,141 +92,165 @@ class BackupConfigValidator:
                 ("psql", "PostgreSQL å®¢æˆ¶ç«¯"),
                 ("tar", "æª”æ¡ˆæ­¸æª”å·¥å…·"),
                 ("gzip", "å£“ç¸®å·¥å…·"),
-                ("docker", "å®¹å™¨ç®¡ç†å·¥å…·")
+                ("docker", "å®¹å™¨ç®¡ç†å·¥å…·"),
             ]
-            
+
             missing_deps = []
             available_deps = []
-            
+
             for cmd, description in required_commands:
                 try:
                     result = subprocess.run(
                         [cmd, "--version"],
                         capture_output=True,
                         check=True,
-                        timeout=10
+                        timeout=10,
                     )
                     available_deps.append((cmd, description))
-                except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
+                except (
+                    subprocess.CalledProcessError,
+                    FileNotFoundError,
+                    subprocess.TimeoutExpired,
+                ):
                     missing_deps.append((cmd, description))
-            
+
             if missing_deps:
                 return ValidationResult(
                     component="ç³»çµ±ä¾è³´",
                     success=False,
                     message=f"ç¼ºå°‘å¿…è¦å·¥å…·: {', '.join([cmd for cmd, _ in missing_deps])}",
-                    details={"missing": [{"command": cmd, "description": desc} for cmd, desc in missing_deps]}
+                    details={
+                        "missing": [
+                            {"command": cmd, "description": desc}
+                            for cmd, desc in missing_deps
+                        ]
+                    },
                 )
-            
+
             return ValidationResult(
                 component="ç³»çµ±ä¾è³´",
                 success=True,
                 message="æ‰€æœ‰å¿…è¦çš„ç³»çµ±å·¥å…·éƒ½å¯ç”¨",
-                details={"available": [{"command": cmd, "description": desc} for cmd, desc in available_deps]}
+                details={
+                    "available": [
+                        {"command": cmd, "description": desc}
+                        for cmd, desc in available_deps
+                    ]
+                },
             )
-            
+
         except Exception as e:
             return ValidationResult(
                 component="ç³»çµ±ä¾è³´",
                 success=False,
-                message=f"é©—è­‰å¤±æ•—: {str(e)}"
+                message=f"é©—è­‰å¤±æ•—: {str(e)}",
             )
-    
+
     def validate_backup_configuration(self) -> ValidationResult:
         """é©—è­‰å‚™ä»½é…ç½®æ–‡ä»¶"""
         try:
             config_files = [
                 "config/backup-config.json",
                 "config/disaster-recovery-config.json",
-                "scripts/backup/config.yaml"
+                "scripts/backup/config.yaml",
             ]
-            
+
             validation_issues = []
             valid_configs = []
-            
+
             for config_path in config_files:
                 full_path = self.project_root / config_path
                 if not full_path.exists():
                     validation_issues.append(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                     continue
-                
+
                 try:
-                    if config_path.endswith('.json'):
-                        with open(full_path, 'r', encoding='utf-8') as f:
+                    if config_path.endswith(".json"):
+                        with open(full_path, "r", encoding="utf-8") as f:
                             config_data = json.load(f)
-                    elif config_path.endswith('.yaml') or config_path.endswith('.yml'):
+                    elif config_path.endswith(".yaml") or config_path.endswith(
+                        ".yml"
+                    ):
                         # ç°¡å–®é©—è­‰ YAML æ–‡ä»¶æ ¼å¼
-                        with open(full_path, 'r', encoding='utf-8') as f:
+                        with open(full_path, "r", encoding="utf-8") as f:
                             content = f.read()
                             if not content.strip():
-                                validation_issues.append(f"é…ç½®æ–‡ä»¶ç‚ºç©º: {config_path}")
+                                validation_issues.append(
+                                    f"é…ç½®æ–‡ä»¶ç‚ºç©º: {config_path}"
+                                )
                                 continue
-                    
+
                     valid_configs.append(config_path)
-                    
+
                 except json.JSONDecodeError as e:
-                    validation_issues.append(f"JSON æ ¼å¼éŒ¯èª¤ {config_path}: {str(e)}")
+                    validation_issues.append(
+                        f"JSON æ ¼å¼éŒ¯èª¤ {config_path}: {str(e)}"
+                    )
                 except Exception as e:
-                    validation_issues.append(f"é…ç½®æ–‡ä»¶è®€å–å¤±æ•— {config_path}: {str(e)}")
-            
+                    validation_issues.append(
+                        f"é…ç½®æ–‡ä»¶è®€å–å¤±æ•— {config_path}: {str(e)}"
+                    )
+
             if validation_issues:
                 return ValidationResult(
                     component="å‚™ä»½é…ç½®",
                     success=False,
                     message=f"é…ç½®é©—è­‰å¤±æ•—: {'; '.join(validation_issues)}",
-                    details={"issues": validation_issues, "valid_configs": valid_configs}
+                    details={
+                        "issues": validation_issues,
+                        "valid_configs": valid_configs,
+                    },
                 )
-            
+
             return ValidationResult(
                 component="å‚™ä»½é…ç½®",
                 success=True,
                 message="æ‰€æœ‰å‚™ä»½é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¢º",
-                details={"valid_configs": valid_configs}
+                details={"valid_configs": valid_configs},
             )
-            
+
         except Exception as e:
             return ValidationResult(
                 component="å‚™ä»½é…ç½®",
                 success=False,
-                message=f"é©—è­‰å¤±æ•—: {str(e)}"
+                message=f"é©—è­‰å¤±æ•—: {str(e)}",
             )
-    
+
     def validate_backup_directories(self) -> ValidationResult:
         """é©—è­‰å‚™ä»½ç›®éŒ„çµæ§‹"""
         try:
-            required_dirs = [
-                "scripts/backup",
-                "monitoring/backup",
-                "config"
-            ]
-            
+            required_dirs = ["scripts/backup", "monitoring/backup", "config"]
+
             missing_dirs = []
             existing_dirs = []
-            
+
             for dir_path in required_dirs:
                 full_path = self.project_root / dir_path
                 if not full_path.exists() or not full_path.is_dir():
                     missing_dirs.append(dir_path)
                 else:
                     existing_dirs.append(dir_path)
-            
+
             # æª¢æŸ¥è‡¨æ™‚å‚™ä»½ç›®éŒ„æ¬Šé™
             temp_backup_dirs = ["/tmp/backups", "/var/backups", "/opt/backups"]
             writable_temp_dirs = []
-            
+
             for temp_dir in temp_backup_dirs:
                 temp_path = Path(temp_dir)
                 if temp_path.exists() and os.access(temp_path, os.W_OK):
                     writable_temp_dirs.append(temp_dir)
-            
+
             if missing_dirs and not writable_temp_dirs:
                 return ValidationResult(
                     component="å‚™ä»½ç›®éŒ„",
                     success=False,
                     message=f"ç¼ºå°‘å¿…è¦ç›®éŒ„: {', '.join(missing_dirs)}ï¼Œä¸”æ²’æœ‰å¯å¯«çš„è‡¨æ™‚å‚™ä»½ç›®éŒ„",
-                    details={"missing_dirs": missing_dirs, "temp_dirs_checked": temp_backup_dirs}
+                    details={
+                        "missing_dirs": missing_dirs,
+                        "temp_dirs_checked": temp_backup_dirs,
+                    },
                 )
-            
+
             return ValidationResult(
                 component="å‚™ä»½ç›®éŒ„",
                 success=True,
@@ -234,35 +258,35 @@ class BackupConfigValidator:
                 details={
                     "existing_dirs": existing_dirs,
                     "missing_dirs": missing_dirs,
-                    "writable_temp_dirs": writable_temp_dirs
-                }
+                    "writable_temp_dirs": writable_temp_dirs,
+                },
             )
-            
+
         except Exception as e:
             return ValidationResult(
                 component="å‚™ä»½ç›®éŒ„",
                 success=False,
-                message=f"é©—è­‰å¤±æ•—: {str(e)}"
+                message=f"é©—è­‰å¤±æ•—: {str(e)}",
             )
-    
+
     def validate_docker_compose_backup_config(self) -> ValidationResult:
         """é©—è­‰ Docker Compose ä¸­çš„å‚™ä»½ç›¸é—œé…ç½®"""
         try:
             docker_compose_files = [
                 "docker-compose.yml",
-                "docker/docker-compose.prod.yml"
+                "docker/docker-compose.prod.yml",
             ]
-            
+
             backup_volume_configs = []
             missing_configs = []
-            
+
             for compose_file in docker_compose_files:
                 full_path = self.project_root / compose_file
                 if not full_path.exists():
                     continue
-                
-                content = full_path.read_text(encoding='utf-8')
-                
+
+                content = full_path.read_text(encoding="utf-8")
+
                 # æª¢æŸ¥æ˜¯å¦æœ‰å‚™ä»½ç›¸é—œçš„å·é…ç½®
                 backup_indicators = [
                     "postgres_data:",
@@ -270,43 +294,42 @@ class BackupConfigValidator:
                     "minio_data:",
                     "backup:",
                     "/var/backups",
-                    "/opt/backups"
+                    "/opt/backups",
                 ]
-                
+
                 found_configs = []
                 for indicator in backup_indicators:
                     if indicator in content:
                         found_configs.append(indicator)
-                
+
                 if found_configs:
-                    backup_volume_configs.append({
-                        "file": compose_file,
-                        "configs": found_configs
-                    })
+                    backup_volume_configs.append(
+                        {"file": compose_file, "configs": found_configs}
+                    )
                 else:
                     missing_configs.append(compose_file)
-            
+
             if not backup_volume_configs:
                 return ValidationResult(
                     component="Docker å‚™ä»½é…ç½®",
                     success=False,
-                    message="Docker Compose æ–‡ä»¶ä¸­ç¼ºå°‘å‚™ä»½ç›¸é—œé…ç½®"
+                    message="Docker Compose æ–‡ä»¶ä¸­ç¼ºå°‘å‚™ä»½ç›¸é—œé…ç½®",
                 )
-            
+
             return ValidationResult(
                 component="Docker å‚™ä»½é…ç½®",
                 success=True,
                 message="Docker Compose å‚™ä»½é…ç½®æ­£ç¢º",
-                details={"backup_configs": backup_volume_configs}
+                details={"backup_configs": backup_volume_configs},
             )
-            
+
         except Exception as e:
             return ValidationResult(
                 component="Docker å‚™ä»½é…ç½®",
                 success=False,
-                message=f"é©—è­‰å¤±æ•—: {str(e)}"
+                message=f"é©—è­‰å¤±æ•—: {str(e)}",
             )
-    
+
     def validate_environment_variables(self) -> ValidationResult:
         """é©—è­‰å‚™ä»½ç›¸é—œçš„ç’°å¢ƒè®Šæ•¸"""
         try:
@@ -318,37 +341,45 @@ class BackupConfigValidator:
                 "DATABASE_URL",
                 "REDIS_URL",
                 "S3_ACCESS_KEY_ID",
-                "S3_SECRET_ACCESS_KEY"
+                "S3_SECRET_ACCESS_KEY",
             ]
-            
+
             missing_vars = []
             configured_vars = []
-            
+
             for var_name in backup_env_vars:
                 if os.getenv(var_name):
                     configured_vars.append(var_name)
                 else:
                     missing_vars.append(var_name)
-            
+
             # æª¢æŸ¥ .env.example æ–‡ä»¶ä¸­æ˜¯å¦åŒ…å«é€™äº›è®Šæ•¸
             env_example_files = [
                 ".env.example",
-                "auto_generate_video_fold6/.env.example"
+                "auto_generate_video_fold6/.env.example",
             ]
-            
+
             documented_vars = []
             for env_file in env_example_files:
-                env_path = self.project_root / env_file if not env_file.startswith('auto_generate_video_fold6') else self.project_root.parent / env_file
+                env_path = (
+                    self.project_root / env_file
+                    if not env_file.startswith("auto_generate_video_fold6")
+                    else self.project_root.parent / env_file
+                )
                 if env_path.exists():
-                    content = env_path.read_text(encoding='utf-8')
+                    content = env_path.read_text(encoding="utf-8")
                     for var_name in backup_env_vars:
                         if var_name in content:
                             documented_vars.append(var_name)
-            
-            undocumented_vars = [var for var in backup_env_vars if var not in documented_vars]
-            
-            success = len(missing_vars) <= len(backup_env_vars) * 0.5  # å…è¨±æœ€å¤š 50% çš„è®Šæ•¸æœªè¨­ç½®
-            
+
+            undocumented_vars = [
+                var for var in backup_env_vars if var not in documented_vars
+            ]
+
+            success = (
+                len(missing_vars) <= len(backup_env_vars) * 0.5
+            )  # å…è¨±æœ€å¤š 50% çš„è®Šæ•¸æœªè¨­ç½®
+
             return ValidationResult(
                 component="ç’°å¢ƒè®Šæ•¸",
                 success=success,
@@ -357,104 +388,110 @@ class BackupConfigValidator:
                     "configured": configured_vars,
                     "missing": missing_vars,
                     "documented": documented_vars,
-                    "undocumented": undocumented_vars
-                }
+                    "undocumented": undocumented_vars,
+                },
             )
-            
+
         except Exception as e:
             return ValidationResult(
                 component="ç’°å¢ƒè®Šæ•¸",
                 success=False,
-                message=f"é©—è­‰å¤±æ•—: {str(e)}"
+                message=f"é©—è­‰å¤±æ•—: {str(e)}",
             )
-    
+
     def run_validation(self) -> Dict:
         """åŸ·è¡Œæ‰€æœ‰é©—è­‰"""
         logger.info("é–‹å§‹å‚™ä»½é…ç½®é©—è­‰")
-        
+
         validators = [
             self.validate_backup_scripts,
             self.validate_backup_dependencies,
             self.validate_backup_configuration,
             self.validate_backup_directories,
             self.validate_docker_compose_backup_config,
-            self.validate_environment_variables
+            self.validate_environment_variables,
         ]
-        
+
         self.results = []
         for validator in validators:
             try:
                 result = validator()
                 self.results.append(result)
-                
+
                 status = "âœ…" if result.success else "âŒ"
                 logger.info(f"{status} {result.component}: {result.message}")
-                
+
             except Exception as e:
                 error_result = ValidationResult(
                     component=validator.__name__,
                     success=False,
-                    message=f"é©—è­‰å™¨åŸ·è¡Œå¤±æ•—: {str(e)}"
+                    message=f"é©—è­‰å™¨åŸ·è¡Œå¤±æ•—: {str(e)}",
                 )
                 self.results.append(error_result)
-                logger.error(f"âŒ {error_result.component}: {error_result.message}")
-        
+                logger.error(
+                    f"âŒ {error_result.component}: {error_result.message}"
+                )
+
         # ç”Ÿæˆç¸½çµå ±å‘Š
         total_tests = len(self.results)
         passed_tests = sum(1 for r in self.results if r.success)
         failed_tests = total_tests - passed_tests
-        
+
         report = {
             "summary": {
                 "total_validations": total_tests,
                 "passed": passed_tests,
                 "failed": failed_tests,
-                "success_rate": (passed_tests / total_tests) * 100 if total_tests > 0 else 0
+                "success_rate": (passed_tests / total_tests) * 100
+                if total_tests > 0
+                else 0,
             },
             "results": [
                 {
                     "component": r.component,
                     "success": r.success,
                     "message": r.message,
-                    "details": r.details
+                    "details": r.details,
                 }
                 for r in self.results
-            ]
+            ],
         }
-        
-        logger.info(f"é©—è­‰å®Œæˆ: {passed_tests}/{total_tests} é€šé ({report['summary']['success_rate']:.1f}%)")
+
+        logger.info(
+            f"é©—è­‰å®Œæˆ: {passed_tests}/{total_tests} é€šé ({report['summary']['success_rate']:.1f}%)"
+        )
         return report
-    
+
     def generate_report(self, report: Dict) -> str:
         """ç”Ÿæˆäººé¡å¯è®€çš„é©—è­‰å ±å‘Š"""
-        summary = report['summary']
-        
+        summary = report["summary"]
+
         report_text = f"""
 =================================================
 å‚™ä»½é…ç½®é©—è­‰å ±å‘Š
 =================================================
-ç¸½é©—è­‰é …: {summary['total_validations']}
-é€šéé …ç›®: {summary['passed']}
-å¤±æ•—é …ç›®: {summary['failed']}
-æˆåŠŸç‡: {summary['success_rate']:.1f}%
+ç¸½é©—è­‰é …: {summary["total_validations"]}
+é€šéé …ç›®: {summary["passed"]}
+å¤±æ•—é …ç›®: {summary["failed"]}
+æˆåŠŸç‡: {summary["success_rate"]:.1f}%
 
 è©³ç´°çµæœ:
 -------------------------------------------------
 """
-        
-        for result in report['results']:
-            status = "âœ… é€šé" if result['success'] else "âŒ å¤±æ•—"
+
+        for result in report["results"]:
+            status = "âœ… é€šé" if result["success"] else "âŒ å¤±æ•—"
             report_text += f"""
-{status} {result['component']}
-   è¨Šæ¯: {result['message']}
+{status} {result["component"]}
+   è¨Šæ¯: {result["message"]}
 """
-            if result['details']:
-                for key, value in result['details'].items():
+            if result["details"]:
+                for key, value in result["details"].items():
                     if isinstance(value, list):
                         report_text += f"   {key}: {len(value)} é …\n"
                     else:
                         report_text += f"   {key}: {value}\n"
-        
+
         report_text += "\n================================================="
         return report_text
 
@@ -462,30 +499,32 @@ class BackupConfigValidator:
 def main():
     """ä¸»å‡½æ•¸"""
     validator = BackupConfigValidator()
-    
+
     try:
         report = validator.run_validation()
-        
+
         # è¼¸å‡ºå ±å‘Š
         print(validator.generate_report(report))
-        
+
         # ä¿å­˜ JSON å ±å‘Š
         report_file = Path("backup_config_validation_report.json")
-        report_file.write_text(json.dumps(report, indent=2, ensure_ascii=False))
+        report_file.write_text(
+            json.dumps(report, indent=2, ensure_ascii=False)
+        )
         logger.info(f"è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")
-        
+
         # è¿”å›é©ç•¶çš„é€€å‡ºç¢¼
-        if report['summary']['failed'] > 0:
+        if report["summary"]["failed"] > 0:
             logger.warning("æŸäº›å‚™ä»½é…ç½®é©—è­‰å¤±æ•—ï¼Œå»ºè­°æª¢æŸ¥ä¸¦ä¿®å¾©")
             exit(1)
         else:
             logger.info("æ‰€æœ‰å‚™ä»½é…ç½®é©—è­‰é€šé!")
             exit(0)
-            
+
     except Exception as e:
         logger.error(f"é©—è­‰åŸ·è¡Œå¤±æ•—: {e}")
         exit(1)
 
 
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
diff --git a/auto_generate_video_fold6/security/generate_security_report.py b/auto_generate_video_fold6/security/generate_security_report.py
index c893f1b..b475afe 100644
--- a/auto_generate_video_fold6/security/generate_security_report.py
+++ b/auto_generate_video_fold6/security/generate_security_report.py
@@ -22,7 +22,13 @@ class SecurityReportGenerator:
 
     def __init__(self):
         self.findings = []
-        self.summary = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0, "UNKNOWN": 0}
+        self.summary = {
+            "CRITICAL": 0,
+            "HIGH": 0,
+            "MEDIUM": 0,
+            "LOW": 0,
+            "UNKNOWN": 0,
+        }
         self.scanner_results = {}
 
     def process_bandit_report(self, file_path: str):
@@ -35,7 +41,9 @@ class SecurityReportGenerator:
                 finding = {
                     "id": f"bandit_{result.get('test_id')}_{result.get('line_number')}",
                     "scanner": "bandit",
-                    "severity": result.get("issue_severity", "UNKNOWN").upper(),
+                    "severity": result.get(
+                        "issue_severity", "UNKNOWN"
+                    ).upper(),
                     "confidence": result.get("issue_confidence", "UNKNOWN"),
                     "title": result.get("issue_text", ""),
                     "description": result.get("issue_text", ""),
@@ -98,7 +106,11 @@ class SecurityReportGenerator:
                 data = json.load(f)
 
             # Safety å ±å‘Šå¯èƒ½æ˜¯åˆ—è¡¨æ ¼å¼
-            vulnerabilities = data if isinstance(data, list) else data.get("vulnerabilities", [])
+            vulnerabilities = (
+                data
+                if isinstance(data, list)
+                else data.get("vulnerabilities", [])
+            )
 
             for vuln in vulnerabilities:
                 finding = {
@@ -148,7 +160,9 @@ class SecurityReportGenerator:
                     "line_number": 0,
                     "rule_id": vuln_id,
                     "remediation": f"Update to version {vuln.get('fixAvailable', {}).get('version', 'latest')}",
-                    "cve_id": vuln.get("cves", [None])[0] if vuln.get("cves") else None,
+                    "cve_id": vuln.get("cves", [None])[0]
+                    if vuln.get("cves")
+                    else None,
                     "created_at": datetime.now().isoformat(),
                 }
                 self.findings.append(finding)
@@ -160,7 +174,9 @@ class SecurityReportGenerator:
             }
 
         except Exception as e:
-            logger.error(f"Failed to process npm audit report {file_path}: {e}")
+            logger.error(
+                f"Failed to process npm audit report {file_path}: {e}"
+            )
 
     def process_trivy_report(self, file_path: str):
         """è™•ç† Trivy å ±å‘Š"""
@@ -214,12 +230,16 @@ class SecurityReportGenerator:
                     finding = {
                         "id": f"trivy_config_{misconfig.get('ID')}_{misconfig.get('CauseMetadata', {}).get('StartLine', 0)}",
                         "scanner": "trivy_config",
-                        "severity": misconfig.get("Severity", "UNKNOWN").upper(),
+                        "severity": misconfig.get(
+                            "Severity", "UNKNOWN"
+                        ).upper(),
                         "confidence": "HIGH",
                         "title": f"Configuration issue: {misconfig.get('Title', '')}",
                         "description": misconfig.get("Description", ""),
                         "file_path": result.get("Target", ""),
-                        "line_number": misconfig.get("CauseMetadata", {}).get("StartLine", 0),
+                        "line_number": misconfig.get("CauseMetadata", {}).get(
+                            "StartLine", 0
+                        ),
                         "rule_id": misconfig.get("ID", ""),
                         "remediation": misconfig.get("Resolution", ""),
                         "created_at": datetime.now().isoformat(),
@@ -261,8 +281,12 @@ class SecurityReportGenerator:
                     line_number = 0
 
                     if locations:
-                        physical_location = locations[0].get("physicalLocation", {})
-                        artifact_location = physical_location.get("artifactLocation", {})
+                        physical_location = locations[0].get(
+                            "physicalLocation", {}
+                        )
+                        artifact_location = physical_location.get(
+                            "artifactLocation", {}
+                        )
                         file_path = artifact_location.get("uri", "")
 
                         region = physical_location.get("region", {})
@@ -289,7 +313,9 @@ class SecurityReportGenerator:
                     self._update_summary(finding["severity"])
 
             self.scanner_results[scanner_name] = {
-                "total_findings": sum(len(run.get("results", [])) for run in runs),
+                "total_findings": sum(
+                    len(run.get("results", [])) for run in runs
+                ),
                 "status": "completed",
             }
 
@@ -314,7 +340,9 @@ class SecurityReportGenerator:
                         logger.info(f"Processing Bandit report: {file_path}")
                         self.process_bandit_report(str(file_path))
 
-                    elif "semgrep" in file_name and file_name.endswith(".json"):
+                    elif "semgrep" in file_name and file_name.endswith(
+                        ".json"
+                    ):
                         logger.info(f"Processing Semgrep report: {file_path}")
                         self.process_semgrep_report(str(file_path))
 
@@ -322,8 +350,12 @@ class SecurityReportGenerator:
                         logger.info(f"Processing Safety report: {file_path}")
                         self.process_safety_report(str(file_path))
 
-                    elif "npm-audit" in file_name and file_name.endswith(".json"):
-                        logger.info(f"Processing npm audit report: {file_path}")
+                    elif "npm-audit" in file_name and file_name.endswith(
+                        ".json"
+                    ):
+                        logger.info(
+                            f"Processing npm audit report: {file_path}"
+                        )
                         self.process_npm_audit_report(str(file_path))
 
                     elif "trivy" in file_name and file_name.endswith(".json"):
@@ -331,7 +363,9 @@ class SecurityReportGenerator:
                         self.process_trivy_report(str(file_path))
 
                     elif file_name.endswith(".sarif"):
-                        scanner_name = self._extract_scanner_name_from_sarif(file_name)
+                        scanner_name = self._extract_scanner_name_from_sarif(
+                            file_name
+                        )
                         logger.info(
                             f"Processing SARIF report: {file_path} (scanner: {scanner_name})"
                         )
@@ -357,7 +391,12 @@ class SecurityReportGenerator:
 
         return report
 
-    def save_report(self, report: Dict[str, Any], output_path: str, format_type: str = "json"):
+    def save_report(
+        self,
+        report: Dict[str, Any],
+        output_path: str,
+        format_type: str = "json",
+    ):
         """å„²å­˜å ±å‘Šåˆ°æª”æ¡ˆ"""
         output_file = Path(output_path)
         output_file.parent.mkdir(parents=True, exist_ok=True)
@@ -392,7 +431,12 @@ class SecurityReportGenerator:
 
     def _map_sarif_severity(self, level: str) -> str:
         """æ˜ å°„ SARIF åš´é‡ç¨‹åº¦"""
-        mapping = {"error": "HIGH", "warning": "MEDIUM", "note": "LOW", "info": "LOW"}
+        mapping = {
+            "error": "HIGH",
+            "warning": "MEDIUM",
+            "note": "LOW",
+            "info": "LOW",
+        }
         return mapping.get(level.lower(), "UNKNOWN")
 
     def _extract_scanner_name_from_sarif(self, file_name: str) -> str:
@@ -416,7 +460,9 @@ class SecurityReportGenerator:
         high_count = self.summary.get("HIGH", 0)
 
         if critical_count > 0:
-            recommendations.append(f"ğŸš¨ ç«‹å³ä¿®å¾© {critical_count} å€‹é—œéµå®‰å…¨å•é¡Œ")
+            recommendations.append(
+                f"ğŸš¨ ç«‹å³ä¿®å¾© {critical_count} å€‹é—œéµå®‰å…¨å•é¡Œ"
+            )
 
         if high_count > 0:
             recommendations.append(f"âš ï¸ å„ªå…ˆä¿®å¾© {high_count} å€‹é«˜é¢¨éšªå®‰å…¨å•é¡Œ")
@@ -427,7 +473,10 @@ class SecurityReportGenerator:
             scanner = finding["scanner"]
             scanner_counts[scanner] = scanner_counts.get(scanner, 0) + 1
 
-        if scanner_counts.get("trivy_secrets", 0) > 0 or scanner_counts.get("gitleaks", 0) > 0:
+        if (
+            scanner_counts.get("trivy_secrets", 0) > 0
+            or scanner_counts.get("gitleaks", 0) > 0
+        ):
             recommendations.append("ğŸ”‘ æª¢æŸ¥ä¸¦ç§»é™¤ç¨‹å¼ç¢¼ä¸­çš„ç¡¬ç·¨ç¢¼å¯†é‘°")
 
         if scanner_counts.get("bandit", 0) > 0:
@@ -477,7 +526,11 @@ class SecurityReportGenerator:
             "critical_findings": critical_count,
             "high_findings": high_count,
             "remediation_priority": (
-                "immediate" if critical_count > 0 else "high" if high_count > 0 else "normal"
+                "immediate"
+                if critical_count > 0
+                else "high"
+                if high_count > 0
+                else "normal"
             ),
         }
 
@@ -485,24 +538,24 @@ class SecurityReportGenerator:
         """ç”Ÿæˆ Markdown æ ¼å¼å ±å‘Š"""
         markdown = f"""# Security Scan Report
 
-**Generated:** {report['metadata']['generated_at']}  
-**Total Scanners:** {report['metadata']['total_scanners']}  
-**Total Findings:** {report['metadata']['total_findings']}
+**Generated:** {report["metadata"]["generated_at"]}  
+**Total Scanners:** {report["metadata"]["total_scanners"]}  
+**Total Findings:** {report["metadata"]["total_findings"]}
 
 ## Summary
 
 | Severity | Count |
 |----------|-------|
-| Critical | {report['summary']['CRITICAL']} |
-| High     | {report['summary']['HIGH']} |
-| Medium   | {report['summary']['MEDIUM']} |
-| Low      | {report['summary']['LOW']} |
+| Critical | {report["summary"]["CRITICAL"]} |
+| High     | {report["summary"]["HIGH"]} |
+| Medium   | {report["summary"]["MEDIUM"]} |
+| Low      | {report["summary"]["LOW"]} |
 
 ## Risk Assessment
 
-**Risk Level:** {report['risk_assessment']['risk_level']}  
-**Risk Score:** {report['risk_assessment']['risk_score']}/10  
-**Remediation Priority:** {report['risk_assessment']['remediation_priority']}
+**Risk Level:** {report["risk_assessment"]["risk_level"]}  
+**Risk Score:** {report["risk_assessment"]["risk_score"]}/10  
+**Remediation Priority:** {report["risk_assessment"]["remediation_priority"]}
 
 ## Recommendations
 
@@ -514,24 +567,28 @@ class SecurityReportGenerator:
         markdown += "\n## Scanner Results\n\n"
 
         for scanner, result in report["scanner_results"].items():
-            markdown += (
-                f"- **{scanner}**: {result['total_findings']} findings ({result['status']})\n"
-            )
+            markdown += f"- **{scanner}**: {result['total_findings']} findings ({result['status']})\n"
 
         if report["summary"]["CRITICAL"] > 0 or report["summary"]["HIGH"] > 0:
             markdown += "\n## Critical and High Severity Findings\n\n"
 
             for finding in report["findings"]:
                 if finding["severity"] in ["CRITICAL", "HIGH"]:
-                    markdown += f"### {finding['severity']}: {finding['title']}\n\n"
+                    markdown += (
+                        f"### {finding['severity']}: {finding['title']}\n\n"
+                    )
                     markdown += f"**File:** `{finding['file_path']}`  \n"
                     markdown += f"**Line:** {finding['line_number']}  \n"
                     markdown += f"**Scanner:** {finding['scanner']}  \n"
                     markdown += f"**Rule:** {finding['rule_id']}  \n"
-                    markdown += f"**Description:** {finding['description']}  \n"
+                    markdown += (
+                        f"**Description:** {finding['description']}  \n"
+                    )
 
                     if finding["remediation"]:
-                        markdown += f"**Remediation:** {finding['remediation']}  \n"
+                        markdown += (
+                            f"**Remediation:** {finding['remediation']}  \n"
+                        )
 
                     markdown += "\n---\n\n"
 
@@ -539,13 +596,20 @@ class SecurityReportGenerator:
 
 
 def main():
-    parser = argparse.ArgumentParser(description="Generate comprehensive security report")
+    parser = argparse.ArgumentParser(
+        description="Generate comprehensive security report"
+    )
     parser.add_argument(
-        "--input-dir", required=True, help="Directory containing security scan reports"
+        "--input-dir",
+        required=True,
+        help="Directory containing security scan reports",
     )
     parser.add_argument("--output", required=True, help="Output file path")
     parser.add_argument(
-        "--format", choices=["json", "yaml", "markdown"], default="json", help="Output format"
+        "--format",
+        choices=["json", "yaml", "markdown"],
+        default="json",
+        help="Output format",
     )
 
     args = parser.parse_args()
diff --git a/auto_generate_video_fold6/security/security-scanner.py b/auto_generate_video_fold6/security/security-scanner.py
index 5f5ba89..3f4b859 100644
--- a/auto_generate_video_fold6/security/security-scanner.py
+++ b/auto_generate_video_fold6/security/security-scanner.py
@@ -81,15 +81,24 @@ class SecurityScanner:
                 "config": self.config_dir / "bandit-config.yaml",
             },
             "safety": {"image": "pyupio/safety:latest", "config": None},
-            "semgrep": {"image": "returntocorp/semgrep:latest", "config": None},
-            "gitleaks": {"image": "zricethezav/gitleaks:latest", "config": None},
+            "semgrep": {
+                "image": "returntocorp/semgrep:latest",
+                "config": None,
+            },
+            "gitleaks": {
+                "image": "zricethezav/gitleaks:latest",
+                "config": None,
+            },
         }
 
         # æƒææ­·å²
         self.scan_history = []
 
     async def run_comprehensive_scan(
-        self, target_path: str, scan_types: List[str] = None, docker_images: List[str] = None
+        self,
+        target_path: str,
+        scan_types: List[str] = None,
+        docker_images: List[str] = None,
     ) -> Dict[str, ScanResult]:
         """åŸ·è¡Œç¶œåˆå®‰å…¨æƒæ"""
         if scan_types is None:
@@ -97,7 +106,9 @@ class SecurityScanner:
 
         results = {}
 
-        self.logger.info(f"Starting comprehensive security scan for {target_path}")
+        self.logger.info(
+            f"Starting comprehensive security scan for {target_path}"
+        )
 
         # ä¸¦è¡ŒåŸ·è¡Œä¸åŒé¡å‹çš„æƒæ
         tasks = []
@@ -133,7 +144,9 @@ class SecurityScanner:
                         results[scan_result.scanner] = scan_result
 
         # ç”Ÿæˆç¶œåˆå ±å‘Š
-        comprehensive_report = await self._generate_comprehensive_report(results)
+        comprehensive_report = await self._generate_comprehensive_report(
+            results
+        )
 
         # ç™¼é€å‘Šè­¦
         await self._send_security_alerts(results)
@@ -170,11 +183,15 @@ class SecurityScanner:
             end_time=end_time,
             status=status,
             findings=findings,
-            metadata={"scan_duration": (end_time - start_time).total_seconds()},
+            metadata={
+                "scan_duration": (end_time - start_time).total_seconds()
+            },
             summary=self._summarize_findings(findings),
         )
 
-    async def _run_bandit_scan(self, target_path: str) -> List[SecurityFinding]:
+    async def _run_bandit_scan(
+        self, target_path: str
+    ) -> List[SecurityFinding]:
         """åŸ·è¡Œ Bandit æƒæ"""
         findings = []
 
@@ -198,7 +215,9 @@ class SecurityScanner:
             ]
 
             # åŸ·è¡Œæƒæ
-            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
+            result = subprocess.run(
+                cmd, capture_output=True, text=True, timeout=300
+            )
 
             if result.returncode in [0, 1]:  # 0=ç„¡å•é¡Œ, 1=ç™¼ç¾å•é¡Œ
                 if result.stdout:
@@ -207,14 +226,18 @@ class SecurityScanner:
                     for result_item in bandit_data.get("results", []):
                         finding = SecurityFinding(
                             id=f"bandit_{result_item.get('test_id')}_{int(time.time())}",
-                            severity=result_item.get("issue_severity", "UNKNOWN").upper(),
+                            severity=result_item.get(
+                                "issue_severity", "UNKNOWN"
+                            ).upper(),
                             title=result_item.get("issue_text", ""),
                             description=result_item.get("issue_text", ""),
                             file_path=result_item.get("filename", ""),
                             line_number=result_item.get("line_number", 0),
                             scanner="bandit",
                             rule_id=result_item.get("test_id", ""),
-                            confidence=result_item.get("issue_confidence", "UNKNOWN"),
+                            confidence=result_item.get(
+                                "issue_confidence", "UNKNOWN"
+                            ),
                             remediation=result_item.get("more_info", ""),
                             created_at=datetime.now().isoformat(),
                         )
@@ -225,7 +248,9 @@ class SecurityScanner:
 
         return findings
 
-    async def _run_semgrep_scan(self, target_path: str) -> List[SecurityFinding]:
+    async def _run_semgrep_scan(
+        self, target_path: str
+    ) -> List[SecurityFinding]:
         """åŸ·è¡Œ Semgrep æƒæ"""
         findings = []
 
@@ -244,7 +269,9 @@ class SecurityScanner:
             ]
 
             # åŸ·è¡Œæƒæ
-            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
+            result = subprocess.run(
+                cmd, capture_output=True, text=True, timeout=600
+            )
 
             if result.returncode in [0, 1]:
                 if result.stdout:
@@ -252,20 +279,30 @@ class SecurityScanner:
 
                     for result_item in semgrep_data.get("results", []):
                         severity = self._map_semgrep_severity(
-                            result_item.get("extra", {}).get("severity", "INFO")
+                            result_item.get("extra", {}).get(
+                                "severity", "INFO"
+                            )
                         )
 
                         finding = SecurityFinding(
                             id=f"semgrep_{result_item.get('check_id')}_{int(time.time())}",
                             severity=severity,
-                            title=result_item.get("extra", {}).get("message", ""),
-                            description=result_item.get("extra", {}).get("message", ""),
+                            title=result_item.get("extra", {}).get(
+                                "message", ""
+                            ),
+                            description=result_item.get("extra", {}).get(
+                                "message", ""
+                            ),
                             file_path=result_item.get("path", ""),
-                            line_number=result_item.get("start", {}).get("line", 0),
+                            line_number=result_item.get("start", {}).get(
+                                "line", 0
+                            ),
                             scanner="semgrep",
                             rule_id=result_item.get("check_id", ""),
                             confidence="HIGH",
-                            remediation=result_item.get("extra", {}).get("fix", ""),
+                            remediation=result_item.get("extra", {}).get(
+                                "fix", ""
+                            ),
                             created_at=datetime.now().isoformat(),
                         )
                         findings.append(finding)
@@ -305,11 +342,15 @@ class SecurityScanner:
             end_time=end_time,
             status=status,
             findings=findings,
-            metadata={"scan_duration": (end_time - start_time).total_seconds()},
+            metadata={
+                "scan_duration": (end_time - start_time).total_seconds()
+            },
             summary=self._summarize_findings(findings),
         )
 
-    async def _run_safety_scan(self, target_path: str) -> List[SecurityFinding]:
+    async def _run_safety_scan(
+        self, target_path: str
+    ) -> List[SecurityFinding]:
         """åŸ·è¡Œ Safety æƒæ"""
         findings = []
 
@@ -331,7 +372,9 @@ class SecurityScanner:
                     f"/code/{req_file.name}",
                 ]
 
-                result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
+                result = subprocess.run(
+                    cmd, capture_output=True, text=True, timeout=120
+                )
 
                 if result.stdout:
                     safety_data = json.loads(result.stdout)
@@ -382,12 +425,16 @@ class SecurityScanner:
                     "--json",
                 ]
 
-                result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
+                result = subprocess.run(
+                    cmd, capture_output=True, text=True, timeout=180
+                )
 
                 if result.stdout:
                     audit_data = json.loads(result.stdout)
 
-                    for vuln_id, vuln in audit_data.get("vulnerabilities", {}).items():
+                    for vuln_id, vuln in audit_data.get(
+                        "vulnerabilities", {}
+                    ).items():
                         severity = vuln.get("severity", "unknown").upper()
 
                         finding = SecurityFinding(
@@ -401,7 +448,9 @@ class SecurityScanner:
                             rule_id=vuln_id,
                             confidence="HIGH",
                             remediation=f"Update to version {vuln.get('fixAvailable', {}).get('version', 'latest')}",
-                            cve_id=vuln.get("cves", [None])[0] if vuln.get("cves") else None,
+                            cve_id=vuln.get("cves", [None])[0]
+                            if vuln.get("cves")
+                            else None,
                             created_at=datetime.now().isoformat(),
                         )
                         findings.append(finding)
@@ -441,11 +490,15 @@ class SecurityScanner:
             end_time=end_time,
             status=status,
             findings=findings,
-            metadata={"scan_duration": (end_time - start_time).total_seconds()},
+            metadata={
+                "scan_duration": (end_time - start_time).total_seconds()
+            },
             summary=self._summarize_findings(findings),
         )
 
-    async def _run_trivy_secret_scan(self, target_path: str) -> List[SecurityFinding]:
+    async def _run_trivy_secret_scan(
+        self, target_path: str
+    ) -> List[SecurityFinding]:
         """åŸ·è¡Œ Trivy å¯†é‘°æƒæ"""
         findings = []
 
@@ -469,7 +522,9 @@ class SecurityScanner:
                 "/code",
             ]
 
-            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
+            result = subprocess.run(
+                cmd, capture_output=True, text=True, timeout=300
+            )
 
             if result.returncode == 0 and result.stdout:
                 trivy_data = json.loads(result.stdout)
@@ -496,7 +551,9 @@ class SecurityScanner:
 
         return findings
 
-    async def _run_gitleaks_scan(self, target_path: str) -> List[SecurityFinding]:
+    async def _run_gitleaks_scan(
+        self, target_path: str
+    ) -> List[SecurityFinding]:
         """åŸ·è¡Œ Gitleaks æƒæ"""
         findings = []
 
@@ -514,7 +571,9 @@ class SecurityScanner:
                 "--report-path=/tmp/gitleaks.json",
             ]
 
-            result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
+            result = subprocess.run(
+                cmd, capture_output=True, text=True, timeout=180
+            )
 
             # Gitleaks åœ¨æ‰¾åˆ°å¯†é‘°æ™‚è¿”å›éé›¶ç‹€æ…‹ç¢¼
             if result.stdout:
@@ -568,7 +627,9 @@ class SecurityScanner:
                 image_name,
             ]
 
-            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
+            result = subprocess.run(
+                cmd, capture_output=True, text=True, timeout=600
+            )
 
             if result.returncode == 0 and result.stdout:
                 trivy_data = json.loads(result.stdout)
@@ -608,7 +669,9 @@ class SecurityScanner:
             end_time=end_time,
             status=status,
             findings=findings,
-            metadata={"scan_duration": (end_time - start_time).total_seconds()},
+            metadata={
+                "scan_duration": (end_time - start_time).total_seconds()
+            },
             summary=self._summarize_findings(findings),
         )
 
@@ -627,7 +690,9 @@ class SecurityScanner:
             findings.extend(k8s_findings)
 
             # æƒæ Docker Compose é…ç½®
-            compose_findings = await self._scan_docker_compose_configs(target_path)
+            compose_findings = await self._scan_docker_compose_configs(
+                target_path
+            )
             findings.extend(compose_findings)
 
             status = "completed"
@@ -646,11 +711,15 @@ class SecurityScanner:
             end_time=end_time,
             status=status,
             findings=findings,
-            metadata={"scan_duration": (end_time - start_time).total_seconds()},
+            metadata={
+                "scan_duration": (end_time - start_time).total_seconds()
+            },
             summary=self._summarize_findings(findings),
         )
 
-    async def _scan_dockerfiles(self, target_path: str) -> List[SecurityFinding]:
+    async def _scan_dockerfiles(
+        self, target_path: str
+    ) -> List[SecurityFinding]:
         """æƒæ Dockerfile å®‰å…¨å•é¡Œ"""
         findings = []
 
@@ -671,20 +740,28 @@ class SecurityScanner:
                     f"/code/{dockerfile.name}",
                 ]
 
-                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
+                result = subprocess.run(
+                    cmd, capture_output=True, text=True, timeout=60
+                )
 
                 if result.returncode == 0 and result.stdout:
                     trivy_data = json.loads(result.stdout)
 
                     for result_item in trivy_data.get("Results", []):
-                        for misconfig in result_item.get("Misconfigurations", []):
+                        for misconfig in result_item.get(
+                            "Misconfigurations", []
+                        ):
                             finding = SecurityFinding(
                                 id=f"dockerfile_{misconfig.get('ID')}_{int(time.time())}",
-                                severity=misconfig.get("Severity", "UNKNOWN").upper(),
+                                severity=misconfig.get(
+                                    "Severity", "UNKNOWN"
+                                ).upper(),
                                 title=f"Dockerfile issue: {misconfig.get('Title', '')}",
                                 description=misconfig.get("Description", ""),
                                 file_path=str(dockerfile),
-                                line_number=misconfig.get("CauseMetadata", {}).get("StartLine", 0),
+                                line_number=misconfig.get(
+                                    "CauseMetadata", {}
+                                ).get("StartLine", 0),
                                 scanner="trivy_dockerfile",
                                 rule_id=misconfig.get("ID", ""),
                                 confidence="HIGH",
@@ -694,11 +771,15 @@ class SecurityScanner:
                             findings.append(finding)
 
             except Exception as e:
-                self.logger.error(f"Dockerfile scan failed for {dockerfile}: {e}")
+                self.logger.error(
+                    f"Dockerfile scan failed for {dockerfile}: {e}"
+                )
 
         return findings
 
-    async def _scan_kubernetes_configs(self, target_path: str) -> List[SecurityFinding]:
+    async def _scan_kubernetes_configs(
+        self, target_path: str
+    ) -> List[SecurityFinding]:
         """æƒæ Kubernetes é…ç½®å®‰å…¨å•é¡Œ"""
         findings = []
 
@@ -727,41 +808,57 @@ class SecurityScanner:
                             f"/code/{k8s_file.name}",
                         ]
 
-                        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
+                        result = subprocess.run(
+                            cmd, capture_output=True, text=True, timeout=60
+                        )
 
                         if result.returncode == 0 and result.stdout:
                             trivy_data = json.loads(result.stdout)
 
                             for result_item in trivy_data.get("Results", []):
-                                for misconfig in result_item.get("Misconfigurations", []):
+                                for misconfig in result_item.get(
+                                    "Misconfigurations", []
+                                ):
                                     finding = SecurityFinding(
                                         id=f"k8s_{misconfig.get('ID')}_{int(time.time())}",
-                                        severity=misconfig.get("Severity", "UNKNOWN").upper(),
+                                        severity=misconfig.get(
+                                            "Severity", "UNKNOWN"
+                                        ).upper(),
                                         title=f"K8s config issue: {misconfig.get('Title', '')}",
-                                        description=misconfig.get("Description", ""),
-                                        file_path=str(k8s_file),
-                                        line_number=misconfig.get("CauseMetadata", {}).get(
-                                            "StartLine", 0
+                                        description=misconfig.get(
+                                            "Description", ""
                                         ),
+                                        file_path=str(k8s_file),
+                                        line_number=misconfig.get(
+                                            "CauseMetadata", {}
+                                        ).get("StartLine", 0),
                                         scanner="trivy_k8s",
                                         rule_id=misconfig.get("ID", ""),
                                         confidence="HIGH",
-                                        remediation=misconfig.get("Resolution", ""),
+                                        remediation=misconfig.get(
+                                            "Resolution", ""
+                                        ),
                                         created_at=datetime.now().isoformat(),
                                     )
                                     findings.append(finding)
 
             except Exception as e:
-                self.logger.debug(f"K8s config scan failed for {k8s_file}: {e}")
+                self.logger.debug(
+                    f"K8s config scan failed for {k8s_file}: {e}"
+                )
 
         return findings
 
-    async def _scan_docker_compose_configs(self, target_path: str) -> List[SecurityFinding]:
+    async def _scan_docker_compose_configs(
+        self, target_path: str
+    ) -> List[SecurityFinding]:
         """æƒæ Docker Compose é…ç½®å®‰å…¨å•é¡Œ"""
         findings = []
 
         compose_files = list(Path(target_path).rglob("docker-compose*.yml"))
-        compose_files.extend(list(Path(target_path).rglob("docker-compose*.yaml")))
+        compose_files.extend(
+            list(Path(target_path).rglob("docker-compose*.yaml"))
+        )
 
         for compose_file in compose_files:
             try:
@@ -778,20 +875,28 @@ class SecurityScanner:
                     f"/code/{compose_file.name}",
                 ]
 
-                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
+                result = subprocess.run(
+                    cmd, capture_output=True, text=True, timeout=60
+                )
 
                 if result.returncode == 0 and result.stdout:
                     trivy_data = json.loads(result.stdout)
 
                     for result_item in trivy_data.get("Results", []):
-                        for misconfig in result_item.get("Misconfigurations", []):
+                        for misconfig in result_item.get(
+                            "Misconfigurations", []
+                        ):
                             finding = SecurityFinding(
                                 id=f"compose_{misconfig.get('ID')}_{int(time.time())}",
-                                severity=misconfig.get("Severity", "UNKNOWN").upper(),
+                                severity=misconfig.get(
+                                    "Severity", "UNKNOWN"
+                                ).upper(),
                                 title=f"Docker Compose issue: {misconfig.get('Title', '')}",
                                 description=misconfig.get("Description", ""),
                                 file_path=str(compose_file),
-                                line_number=misconfig.get("CauseMetadata", {}).get("StartLine", 0),
+                                line_number=misconfig.get(
+                                    "CauseMetadata", {}
+                                ).get("StartLine", 0),
                                 scanner="trivy_compose",
                                 rule_id=misconfig.get("ID", ""),
                                 confidence="HIGH",
@@ -801,13 +906,23 @@ class SecurityScanner:
                             findings.append(finding)
 
             except Exception as e:
-                self.logger.error(f"Docker Compose scan failed for {compose_file}: {e}")
+                self.logger.error(
+                    f"Docker Compose scan failed for {compose_file}: {e}"
+                )
 
         return findings
 
-    def _summarize_findings(self, findings: List[SecurityFinding]) -> Dict[str, int]:
+    def _summarize_findings(
+        self, findings: List[SecurityFinding]
+    ) -> Dict[str, int]:
         """çµ±è¨ˆç™¼ç¾æ‘˜è¦"""
-        summary = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0, "UNKNOWN": 0}
+        summary = {
+            "CRITICAL": 0,
+            "HIGH": 0,
+            "MEDIUM": 0,
+            "LOW": 0,
+            "UNKNOWN": 0,
+        }
 
         for finding in findings:
             severity = finding.severity.upper()
@@ -830,7 +945,13 @@ class SecurityScanner:
         report = {
             "scan_timestamp": datetime.now().isoformat(),
             "total_scanners": len(results),
-            "summary": {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0, "UNKNOWN": 0},
+            "summary": {
+                "CRITICAL": 0,
+                "HIGH": 0,
+                "MEDIUM": 0,
+                "LOW": 0,
+                "UNKNOWN": 0,
+            },
             "scanner_results": {},
             "recommendations": [],
         }
@@ -852,16 +973,23 @@ class SecurityScanner:
             all_findings.extend(scan_result.findings)
 
         # ç”Ÿæˆå»ºè­°
-        report["recommendations"] = self._generate_recommendations(all_findings)
+        report["recommendations"] = self._generate_recommendations(
+            all_findings
+        )
 
         # å„²å­˜å ±å‘Š
-        report_file = self.reports_dir / f"comprehensive_security_report_{int(time.time())}.json"
+        report_file = (
+            self.reports_dir
+            / f"comprehensive_security_report_{int(time.time())}.json"
+        )
         with open(report_file, "w") as f:
             json.dump(report, f, indent=2, default=str)
 
         return report
 
-    def _generate_recommendations(self, findings: List[SecurityFinding]) -> List[str]:
+    def _generate_recommendations(
+        self, findings: List[SecurityFinding]
+    ) -> List[str]:
         """ç”Ÿæˆå®‰å…¨å»ºè­°"""
         recommendations = []
 
@@ -878,7 +1006,9 @@ class SecurityScanner:
         # æª¢æŸ¥å¸¸è¦‹å•é¡Œé¡å‹
         scanner_counts = {}
         for finding in findings:
-            scanner_counts[finding.scanner] = scanner_counts.get(finding.scanner, 0) + 1
+            scanner_counts[finding.scanner] = (
+                scanner_counts.get(finding.scanner, 0) + 1
+            )
 
         if scanner_counts.get("trivy_secrets", 0) > 0:
             recommendations.append("æª¢æŸ¥ä¸¦ç§»é™¤ç¨‹å¼ç¢¼ä¸­çš„ç¡¬ç·¨ç¢¼å¯†é‘°")
@@ -896,7 +1026,9 @@ class SecurityScanner:
 
     async def _send_security_alerts(self, results: Dict[str, ScanResult]):
         """ç™¼é€å®‰å…¨å‘Šè­¦"""
-        total_critical = sum(r.summary.get("CRITICAL", 0) for r in results.values())
+        total_critical = sum(
+            r.summary.get("CRITICAL", 0) for r in results.values()
+        )
         total_high = sum(r.summary.get("HIGH", 0) for r in results.values())
 
         if total_critical > 0 or total_high > 5:
@@ -925,7 +1057,10 @@ async def main():
     results = await scanner.run_comprehensive_scan(
         target_path="./",
         scan_types=["code", "dependencies", "secrets"],
-        docker_images=["auto-video/api-gateway:latest", "auto-video/auth-service:latest"],
+        docker_images=[
+            "auto-video/api-gateway:latest",
+            "auto-video/auth-service:latest",
+        ],
     )
 
     # è¼¸å‡ºçµæœæ‘˜è¦
diff --git a/auto_generate_video_fold6/security/zero-trust-security.py b/auto_generate_video_fold6/security/zero-trust-security.py
index c32a29a..38adac3 100644
--- a/auto_generate_video_fold6/security/zero-trust-security.py
+++ b/auto_generate_video_fold6/security/zero-trust-security.py
@@ -109,7 +109,9 @@ class DeviceFingerprinter:
 
         return device_id
 
-    async def _store_device_info(self, device_id: str, fingerprint_data: Dict[str, Any]):
+    async def _store_device_info(
+        self, device_id: str, fingerprint_data: Dict[str, Any]
+    ):
         """å­˜å„²è¨­å‚™è³‡è¨Š"""
         device_info = {
             "fingerprint": fingerprint_data,
@@ -123,9 +125,13 @@ class DeviceFingerprinter:
         if existing_info:
             existing_data = json.loads(existing_info)
             device_info["first_seen"] = existing_data["first_seen"]
-            device_info["trust_score"] = min(existing_data["trust_score"] + 0.1, 1.0)
+            device_info["trust_score"] = min(
+                existing_data["trust_score"] + 0.1, 1.0
+            )
 
-        self.redis_client.setex(f"device:{device_id}", timedelta(days=90), json.dumps(device_info))
+        self.redis_client.setex(
+            f"device:{device_id}", timedelta(days=90), json.dumps(device_info)
+        )
 
     async def get_device_trust_score(self, device_id: str) -> float:
         """ç²å–è¨­å‚™ä¿¡ä»»åˆ†æ•¸"""
@@ -143,27 +149,40 @@ class BehaviorAnalyzer:
         self.redis_client = redis.Redis(host="localhost", port=6379, db=4)
         self.behavior_models = {}
 
-    async def analyze_behavior(self, user_id: str, action: str, context: SecurityContext) -> float:
+    async def analyze_behavior(
+        self, user_id: str, action: str, context: SecurityContext
+    ) -> float:
         """åˆ†æç”¨æˆ¶è¡Œç‚ºä¸¦è¿”å›é¢¨éšªåˆ†æ•¸"""
 
         # ç²å–ç”¨æˆ¶æ­·å²è¡Œç‚º
         behavior_history = await self._get_user_behavior_history(user_id)
 
         # æ™‚é–“åˆ†æ
-        time_risk = await self._analyze_time_patterns(user_id, context.timestamp)
+        time_risk = await self._analyze_time_patterns(
+            user_id, context.timestamp
+        )
 
         # åœ°ç†ä½ç½®åˆ†æ
-        location_risk = await self._analyze_location_patterns(user_id, context.location)
+        location_risk = await self._analyze_location_patterns(
+            user_id, context.location
+        )
 
         # é »ç‡åˆ†æ
-        frequency_risk = await self._analyze_frequency_patterns(user_id, action)
+        frequency_risk = await self._analyze_frequency_patterns(
+            user_id, action
+        )
 
         # è¨­å‚™åˆ†æ
-        device_risk = await self._analyze_device_patterns(user_id, context.device_id)
+        device_risk = await self._analyze_device_patterns(
+            user_id, context.device_id
+        )
 
         # ç¶œåˆé¢¨éšªè©•ä¼°
         total_risk = (
-            time_risk * 0.2 + location_risk * 0.3 + frequency_risk * 0.3 + device_risk * 0.2
+            time_risk * 0.2
+            + location_risk * 0.3
+            + frequency_risk * 0.3
+            + device_risk * 0.2
         )
 
         # è¨˜éŒ„ç•¶å‰è¡Œç‚º
@@ -172,14 +191,20 @@ class BehaviorAnalyzer:
         logger.info(f"ç”¨æˆ¶ {user_id} è¡Œç‚ºé¢¨éšªåˆ†æ: {total_risk:.3f}")
         return min(total_risk, 1.0)
 
-    async def _get_user_behavior_history(self, user_id: str) -> List[Dict[str, Any]]:
+    async def _get_user_behavior_history(
+        self, user_id: str
+    ) -> List[Dict[str, Any]]:
         """ç²å–ç”¨æˆ¶è¡Œç‚ºæ­·å²"""
         history_key = f"behavior_history:{user_id}"
-        history_data = self.redis_client.lrange(history_key, 0, 100)  # æœ€è¿‘100æ¢
+        history_data = self.redis_client.lrange(
+            history_key, 0, 100
+        )  # æœ€è¿‘100æ¢
 
         return [json.loads(item) for item in history_data]
 
-    async def _analyze_time_patterns(self, user_id: str, current_time: datetime) -> float:
+    async def _analyze_time_patterns(
+        self, user_id: str, current_time: datetime
+    ) -> float:
         """åˆ†ææ™‚é–“æ¨¡å¼ç•°å¸¸"""
         # ç²å–ç”¨æˆ¶å¸¸ç”¨æ™‚é–“æ®µ
         history = await self._get_user_behavior_history(user_id)
@@ -224,7 +249,9 @@ class BehaviorAnalyzer:
         else:
             return 0.7  # æ–°åœ°é»ï¼Œé«˜é¢¨éšª
 
-    async def _analyze_frequency_patterns(self, user_id: str, action: str) -> float:
+    async def _analyze_frequency_patterns(
+        self, user_id: str, action: str
+    ) -> float:
         """åˆ†ææ“ä½œé »ç‡ç•°å¸¸"""
         # æª¢æŸ¥æœ€è¿‘1å°æ™‚å…§çš„åŒé¡æ“ä½œæ¬¡æ•¸
         recent_actions = []
@@ -250,11 +277,15 @@ class BehaviorAnalyzer:
 
         return min(frequency_risk, 1.0)
 
-    async def _analyze_device_patterns(self, user_id: str, device_id: str) -> float:
+    async def _analyze_device_patterns(
+        self, user_id: str, device_id: str
+    ) -> float:
         """åˆ†æè¨­å‚™ä½¿ç”¨æ¨¡å¼"""
         # æª¢æŸ¥ç”¨æˆ¶æ˜¯å¦ç¶“å¸¸ä½¿ç”¨æ­¤è¨­å‚™
         history = await self._get_user_behavior_history(user_id)
-        device_usage = sum(1 for record in history if record.get("device_id") == device_id)
+        device_usage = sum(
+            1 for record in history if record.get("device_id") == device_id
+        )
 
         if device_usage > 10:
             return 0.1  # å¸¸ç”¨è¨­å‚™ï¼Œä½é¢¨éšª
@@ -264,7 +295,11 @@ class BehaviorAnalyzer:
             return 0.6  # æ–°è¨­å‚™ï¼Œé«˜é¢¨éšª
 
     async def _record_behavior(
-        self, user_id: str, action: str, context: SecurityContext, risk_score: float
+        self,
+        user_id: str,
+        action: str,
+        context: SecurityContext,
+        risk_score: float,
     ):
         """è¨˜éŒ„ç”¨æˆ¶è¡Œç‚º"""
         behavior_record = {
@@ -290,25 +325,36 @@ class RiskEngine:
         self.device_fingerprinter = DeviceFingerprinter()
         self.threat_intelligence = ThreatIntelligence()
 
-    async def assess_risk(self, user_id: str, action: str, context: SecurityContext) -> float:
+    async def assess_risk(
+        self, user_id: str, action: str, context: SecurityContext
+    ) -> float:
         """ç¶œåˆé¢¨éšªè©•ä¼°"""
 
         # è¡Œç‚ºé¢¨éšª
-        behavior_risk = await self.behavior_analyzer.analyze_behavior(user_id, action, context)
+        behavior_risk = await self.behavior_analyzer.analyze_behavior(
+            user_id, action, context
+        )
 
         # è¨­å‚™é¢¨éšª
-        device_trust = await self.device_fingerprinter.get_device_trust_score(context.device_id)
+        device_trust = await self.device_fingerprinter.get_device_trust_score(
+            context.device_id
+        )
         device_risk = 1.0 - device_trust
 
         # å¨è„…æƒ…å ±é¢¨éšª
-        threat_risk = await self.threat_intelligence.check_ip_reputation(context.ip_address)
+        threat_risk = await self.threat_intelligence.check_ip_reputation(
+            context.ip_address
+        )
 
         # æœƒè©±é¢¨éšª
         session_risk = await self._assess_session_risk(context.session_id)
 
         # ç¶œåˆé¢¨éšªè¨ˆç®—
         total_risk = (
-            behavior_risk * 0.4 + device_risk * 0.2 + threat_risk * 0.3 + session_risk * 0.1
+            behavior_risk * 0.4
+            + device_risk * 0.2
+            + threat_risk * 0.3
+            + session_risk * 0.1
         )
 
         logger.info(
@@ -366,7 +412,9 @@ class ThreatIntelligence:
         reputation_score = await self._query_threat_intelligence(ip_address)
 
         # ç·©å­˜çµæœ
-        self.redis_client.setex(reputation_key, timedelta(hours=1), str(reputation_score))
+        self.redis_client.setex(
+            reputation_key, timedelta(hours=1), str(reputation_score)
+        )
 
         return reputation_score
 
@@ -388,7 +436,9 @@ class ThreatIntelligence:
                         # æ¨¡æ“¬ API èª¿ç”¨ï¼ˆå¯¦éš›éœ€è¦ API å¯†é‘°ï¼‰
                         # response = await client.get(source.format(ip=ip_address))
                         # æ¨¡æ“¬éŸ¿æ‡‰
-                        simulated_score = 0.1 if ip_address.startswith("192.168.") else 0.0
+                        simulated_score = (
+                            0.1 if ip_address.startswith("192.168.") else 0.0
+                        )
                         risk_scores.append(simulated_score)
                     except Exception as e:
                         logger.warning(f"å¨è„…æƒ…å ±æŸ¥è©¢å¤±æ•—: {e}")
@@ -439,12 +489,19 @@ class PolicyEngine:
         )
 
     async def evaluate_access(
-        self, user_id: str, resource: str, action: str, context: SecurityContext
+        self,
+        user_id: str,
+        resource: str,
+        action: str,
+        context: SecurityContext,
     ) -> Dict[str, Any]:
         """è©•ä¼°è¨ªå•æ¬Šé™"""
 
         if resource not in self.policies:
-            return {"allowed": False, "reason": f"æœªå®šç¾©è³‡æº {resource} çš„è¨ªå•ç­–ç•¥"}
+            return {
+                "allowed": False,
+                "reason": f"æœªå®šç¾©è³‡æº {resource} çš„è¨ªå•ç­–ç•¥",
+            }
 
         policy = self.policies[resource]
 
@@ -470,9 +527,14 @@ class PolicyEngine:
             }
 
         # æª¢æŸ¥é¡å¤–æ¢ä»¶
-        condition_check = await self._check_policy_conditions(user_id, policy, context)
+        condition_check = await self._check_policy_conditions(
+            user_id, policy, context
+        )
         if not condition_check["passed"]:
-            return {"allowed": False, "reason": f"ç­–ç•¥æ¢ä»¶æª¢æŸ¥å¤±æ•—: {condition_check['reason']}"}
+            return {
+                "allowed": False,
+                "reason": f"ç­–ç•¥æ¢ä»¶æª¢æŸ¥å¤±æ•—: {condition_check['reason']}",
+            }
 
         return {
             "allowed": True,
@@ -501,8 +563,13 @@ class PolicyEngine:
                     return {"passed": False, "reason": "éœ€è¦å¤šå› ç´ èªè­‰"}
 
             elif condition == "ip_whitelist":
-                if value and not await self._check_ip_whitelist(user_id, context.ip_address):
-                    return {"passed": False, "reason": f"IP {context.ip_address} ä¸åœ¨ç™½åå–®ä¸­"}
+                if value and not await self._check_ip_whitelist(
+                    user_id, context.ip_address
+                ):
+                    return {
+                        "passed": False,
+                        "reason": f"IP {context.ip_address} ä¸åœ¨ç™½åå–®ä¸­",
+                    }
 
         return {"passed": True, "reason": "æ‰€æœ‰æ¢ä»¶æª¢æŸ¥é€šé"}
 
@@ -536,7 +603,11 @@ class ZeroTrustGateway:
         self.audit_logger = AuditLogger()
 
     async def authenticate_and_authorize(
-        self, token: str, resource: str, action: str, request_data: Dict[str, Any]
+        self,
+        token: str,
+        resource: str,
+        action: str,
+        request_data: Dict[str, Any],
     ) -> Dict[str, Any]:
         """é›¶ä¿¡ä»»èªè­‰å’Œæˆæ¬Š"""
 
@@ -549,7 +620,9 @@ class ZeroTrustGateway:
             user_id = token_data["user_id"]
 
             # 2. ç”Ÿæˆè¨­å‚™æŒ‡ç´‹
-            device_id = await self.device_fingerprinter.generate_fingerprint(request_data)
+            device_id = await self.device_fingerprinter.generate_fingerprint(
+                request_data
+            )
 
             # 3. æ§‹å»ºå®‰å…¨ä¸Šä¸‹æ–‡
             context = SecurityContext(
@@ -558,14 +631,18 @@ class ZeroTrustGateway:
                 ip_address=request_data.get("ip_address", ""),
                 user_agent=request_data.get("user_agent", ""),
                 location=request_data.get("location"),
-                trust_level=await self._calculate_trust_level(user_id, device_id),
+                trust_level=await self._calculate_trust_level(
+                    user_id, device_id
+                ),
                 risk_score=0.0,  # å°‡åœ¨é¢¨éšªè©•ä¼°ä¸­è¨ˆç®—
                 session_id=token_data["session_id"],
                 timestamp=datetime.utcnow(),
             )
 
             # 4. é¢¨éšªè©•ä¼°
-            context.risk_score = await self.risk_engine.assess_risk(user_id, action, context)
+            context.risk_score = await self.risk_engine.assess_risk(
+                user_id, action, context
+            )
 
             # 5. ç­–ç•¥è©•ä¼°
             access_decision = await self.policy_engine.evaluate_access(
@@ -593,7 +670,9 @@ class ZeroTrustGateway:
         """é©—è­‰ JWT ä»¤ç‰Œ"""
         try:
             # å¯¦éš›å¯¦ç¾ä¸­æ‡‰è©²ä½¿ç”¨é©ç•¶çš„å¯†é‘°å’Œç®—æ³•
-            decoded = jwt.decode(token, "your-secret-key", algorithms=["HS256"])
+            decoded = jwt.decode(
+                token, "your-secret-key", algorithms=["HS256"]
+            )
 
             # æª¢æŸ¥ä»¤ç‰Œæ˜¯å¦è¢«æ’¤éŠ·
             if await self._is_token_revoked(token):
@@ -616,14 +695,18 @@ class ZeroTrustGateway:
         revoked_key = f"revoked_token:{token_hash}"
         return self.redis_client.exists(revoked_key)
 
-    async def _calculate_trust_level(self, user_id: str, device_id: str) -> TrustLevel:
+    async def _calculate_trust_level(
+        self, user_id: str, device_id: str
+    ) -> TrustLevel:
         """è¨ˆç®—ç”¨æˆ¶ä¿¡ä»»ç­‰ç´š"""
 
         # ç²å–ç”¨æˆ¶åŸºç¤ä¿¡ä»»åˆ†æ•¸
         user_trust = await self._get_user_trust_score(user_id)
 
         # ç²å–è¨­å‚™ä¿¡ä»»åˆ†æ•¸
-        device_trust = await self.device_fingerprinter.get_device_trust_score(device_id)
+        device_trust = await self.device_fingerprinter.get_device_trust_score(
+            device_id
+        )
 
         # ç¶œåˆä¿¡ä»»åˆ†æ•¸
         combined_trust = (user_trust + device_trust) / 2
@@ -654,7 +737,12 @@ class AuditLogger:
         self.redis_client = redis.Redis(host="localhost", port=6379, db=6)
 
     async def log_access_attempt(
-        self, user_id: str, resource: str, action: str, context: SecurityContext, success: bool
+        self,
+        user_id: str,
+        resource: str,
+        action: str,
+        context: SecurityContext,
+        success: bool,
     ):
         """è¨˜éŒ„è¨ªå•å˜—è©¦"""
 
@@ -667,7 +755,10 @@ class AuditLogger:
             risk_level=self._determine_risk_level(context.risk_score),
             context=context,
             timestamp=datetime.utcnow(),
-            metadata={"success": success, "trust_level": context.trust_level.name},
+            metadata={
+                "success": success,
+                "trust_level": context.trust_level.name,
+            },
         )
 
         # è¨˜éŒ„åˆ°å¯©è¨ˆæ—¥èªŒ
@@ -746,7 +837,10 @@ async def main():
 
     # é›¶ä¿¡ä»»èªè­‰
     result = await gateway.authenticate_and_authorize(
-        token=token, resource="video_generation", action="create", request_data=request_data
+        token=token,
+        resource="video_generation",
+        action="create",
+        request_data=request_data,
     )
 
     if result["authorized"]:
diff --git a/auto_generate_video_fold6/services/ai-service/app/services/text_generator.py b/auto_generate_video_fold6/services/ai-service/app/services/text_generator.py
index 74e1a3f..17d1912 100644
--- a/auto_generate_video_fold6/services/ai-service/app/services/text_generator.py
+++ b/auto_generate_video_fold6/services/ai-service/app/services/text_generator.py
@@ -305,7 +305,7 @@ REQUIREMENTS:
     approximately {duration_seconds * 2.5} words)
 - Target Audience: {target_audience}
 - Tone: {tone}
-{f'- Include Keywords: {keyword_text}' if keyword_text else ''}
+{f"- Include Keywords: {keyword_text}" if keyword_text else ""}
 
 SCRIPT GUIDELINES:
 1. Start with a strong hook in the first 3 seconds
@@ -343,7 +343,7 @@ SCRIPT CONTENT:
 REQUIREMENTS:
 - Style: {style}
 - Maximum length: {max_length} characters
-{f'- Include keywords: {keyword_text}' if keyword_text else ''}
+{f"- Include keywords: {keyword_text}" if keyword_text else ""}
 
 STYLE CHARACTERISTICS:
 - Catchy: Use power words, numbers, emotional triggers
@@ -381,9 +381,13 @@ OPTIMIZATION TASK:
 OPTIMIZATION GUIDELINES:
 - Maintain the core message and hook
 - Preserve the most engaging elements
-- {'Remove redundant points, combine ideas, use more concise \
-    language' if action == 'shorten' else 'Add supporting details, \
-        examples, or elaboration on key points'}
+- {
+            "Remove redundant points, combine ideas, use more concise \
+    language"
+            if action == "shorten"
+            else "Add supporting details, \
+        examples, or elaboration on key points"
+        }
 - Keep the natural flow and transitions
 - Maintain call-to-action
 
diff --git a/auto_generate_video_fold6/services/ai-service/gemini_client.py b/auto_generate_video_fold6/services/ai-service/gemini_client.py
index beabe70..0cf810b 100644
--- a/auto_generate_video_fold6/services/ai-service/gemini_client.py
+++ b/auto_generate_video_fold6/services/ai-service/gemini_client.py
@@ -153,7 +153,6 @@ class GeminiClient:
                 json=request_data,
                 headers={"Content-Type": "application/json"},
             ) as response:
-
                 duration = time.time() - start_time
 
                 if response.status != 200:
@@ -312,7 +311,6 @@ class GeminiClient:
                 json=request_data,
                 headers={"Content-Type": "application/json"},
             ) as response:
-
                 if response.status == 200:
                     result_data = await response.json()
                     duration = time.time() - start_time
diff --git a/auto_generate_video_fold6/services/ai-service/mlops_pipeline.py b/auto_generate_video_fold6/services/ai-service/mlops_pipeline.py
index 62de7c4..300f083 100644
--- a/auto_generate_video_fold6/services/ai-service/mlops_pipeline.py
+++ b/auto_generate_video_fold6/services/ai-service/mlops_pipeline.py
@@ -348,9 +348,9 @@ class ExperimentTracker:
         duration_hours: int = 24,
     ) -> str:
         """å•Ÿå‹• A/B æ¸¬è©¦"""
-        test_id = f"ab_test_{model_a.model_id}_{model_b
-                                                .model_id}_{int \
-                                                    (datetime.utcnow().timestamp())}"
+        test_id = f"ab_test_{model_a.model_id}_{model_b.model_id}_{
+            int(datetime.utcnow().timestamp())
+        }"
 
         ab_test = {
             "id": test_id,
@@ -515,8 +515,9 @@ class MLOpsPipeline:
     ):
         """è·¯ç”±æŒ‡å®šç™¾åˆ†æ¯”çš„æµé‡åˆ°æ–°æ¨¡å‹"""
         logger.info(
-            f"è·¯ç”± {percentage}% æµé‡åˆ° {model_version
-                                    .model_id}:{model_version.version}"
+            f"è·¯ç”± {percentage}% æµé‡åˆ° {model_version.model_id}:{
+                model_version.version
+            }"
         )
         # å¯¦éš›å¯¦ç¾æœƒæ›´æ–° Istio VirtualService é…ç½®
 
diff --git a/auto_generate_video_fold6/services/ai-service/tests/test_ai_services.py b/auto_generate_video_fold6/services/ai-service/tests/test_ai_services.py
index 592d0cc..1bae30b 100644
--- a/auto_generate_video_fold6/services/ai-service/tests/test_ai_services.py
+++ b/auto_generate_video_fold6/services/ai-service/tests/test_ai_services.py
@@ -70,9 +70,9 @@ class TestAIServices:
         # Setup mock
         mock_response = Mock()
         mock_response.choices = [Mock()]
-        mock_response.choices[0].message.content = (
-            "Generated test script content"
-        )
+        mock_response.choices[
+            0
+        ].message.content = "Generated test script content"
 
         mock_client = AsyncMock()
         mock_client.chat.completions.create.return_value = mock_response
@@ -106,7 +106,6 @@ class TestAIServices:
             patch.object(settings, "stability_api_key", ""),
             patch.object(settings, "openai_api_key", ""),
         ):
-
             with pytest.raises(
                 Exception, match="No image generation service available"
             ):
diff --git a/auto_generate_video_fold6/services/auth-service/app/config.py b/auto_generate_video_fold6/services/auth-service/app/config.py
index a8abaa8..769a399 100644
--- a/auto_generate_video_fold6/services/auth-service/app/config.py
+++ b/auto_generate_video_fold6/services/auth-service/app/config.py
@@ -11,7 +11,9 @@ class Settings(BaseSettings):
     database_pool_size: int = int(os.getenv("DATABASE_POOL_SIZE", "10"))
     database_max_overflow: int = int(os.getenv("DATABASE_MAX_OVERFLOW", "20"))
     database_pool_timeout: int = int(os.getenv("DATABASE_POOL_TIMEOUT", "30"))
-    database_pool_recycle: int = int(os.getenv("DATABASE_POOL_RECYCLE", "3600"))
+    database_pool_recycle: int = int(
+        os.getenv("DATABASE_POOL_RECYCLE", "3600")
+    )
 
     # JWT Configuration
     jwt_secret_key: str = os.getenv("JWT_SECRET_KEY")
diff --git a/auto_generate_video_fold6/services/auth-service/tests/test_routers.py b/auto_generate_video_fold6/services/auth-service/tests/test_routers.py
index 5f0ee05..0c2fd12 100644
--- a/auto_generate_video_fold6/services/auth-service/tests/test_routers.py
+++ b/auto_generate_video_fold6/services/auth-service/tests/test_routers.py
@@ -36,7 +36,6 @@ class TestAuthRouters:
             patch("app.crud.get_user_by_email") as mock_get_email,
             patch("app.crud.get_user_by_username") as mock_get_username,
         ):
-
             mock_get_email.return_value = None
             mock_get_username.return_value = None
             mock_create.return_value = {
@@ -91,7 +90,6 @@ class TestAuthRouters:
             patch("app.security.create_access_token") as mock_token,
             patch("app.security.create_refresh_token") as mock_refresh,
         ):
-
             mock_get_user.return_value = {
                 "id": 1,
                 "email": "testuser@example.com",
@@ -123,7 +121,6 @@ class TestAuthRouters:
             patch("app.crud.get_user_by_email") as mock_get_user,
             patch("app.security.verify_password") as mock_verify,
         ):
-
             mock_get_user.return_value = {
                 "id": 1,
                 "email": "testuser@example.com",
@@ -149,7 +146,6 @@ class TestAuthRouters:
             patch("app.crud.get_user_by_email") as mock_get_user,
             patch("app.security.verify_password") as mock_verify,
         ):
-
             mock_get_user.return_value = {
                 "id": 1,
                 "email": "inactive@example.com",
@@ -172,7 +168,6 @@ class TestAuthRouters:
             patch("app.crud.get_user") as mock_get_user,
             patch("app.security.create_access_token") as mock_token,
         ):
-
             mock_verify.return_value = {"user_id": 1}
             mock_get_user.return_value = {
                 "id": 1,
@@ -239,7 +234,6 @@ class TestAuthRouters:
             patch("app.dependencies.get_current_user") as mock_get_current,
             patch("app.crud.invalidate_user_tokens") as mock_invalidate,
         ):
-
             mock_get_current.return_value = {"id": 1}
             mock_invalidate.return_value = True
 
@@ -262,7 +256,6 @@ class TestAuthRouters:
             patch("app.security.verify_password") as mock_verify,
             patch("app.crud.update_user_password") as mock_update,
         ):
-
             mock_get_current.return_value = {
                 "id": 1,
                 "hashed_password": "hashed_old_password",
@@ -292,7 +285,6 @@ class TestAuthRouters:
             patch("app.dependencies.get_current_user") as mock_get_current,
             patch("app.security.verify_password") as mock_verify,
         ):
-
             mock_get_current.return_value = {
                 "id": 1,
                 "hashed_password": "hashed_old_password",
diff --git a/auto_generate_video_fold6/services/compliance-service/compliance_framework.py b/auto_generate_video_fold6/services/compliance-service/compliance_framework.py
index 5848c0c..5024a79 100644
--- a/auto_generate_video_fold6/services/compliance-service/compliance_framework.py
+++ b/auto_generate_video_fold6/services/compliance-service/compliance_framework.py
@@ -168,9 +168,9 @@ class GDPRCompliance:
 
         if not is_valid:
             logger.warning(
-                f"åŒæ„é©—è­‰å¤±æ•—: {consent_record.consent_id}, å¤±æ•—é …ç›®: {[k for
-                                                               k, v \
-                                                                   in validations.items() if not v]}"
+                f"åŒæ„é©—è­‰å¤±æ•—: {consent_record.consent_id}, å¤±æ•—é …ç›®: {
+                    [k for k, v in validations.items() if not v]
+                }"
             )
 
         return is_valid
@@ -517,7 +517,6 @@ class AuditLogger:
         try:
             alert_config = self.config.get("alerts", {})
             if alert_config.get("enabled", False):
-
                 alert_message = f"""
                 é«˜é¢¨éšªåˆè¦äº‹ä»¶è­¦å ±
 
@@ -528,7 +527,7 @@ class AuditLogger:
                 æ“ä½œ: {event.action}
                 é¢¨éšªç­‰ç´š: {event.risk_level}
                 æ™‚é–“: {event.timestamp}
-                åˆè¦æ¨™æº–: {', '.join(event.compliance_tags)}
+                åˆè¦æ¨™æº–: {", ".join(event.compliance_tags)}
 
                 è«‹ç«‹å³æª¢æŸ¥ä¸¦æ¡å–é©ç•¶æªæ–½ã€‚
                 """
diff --git a/auto_generate_video_fold6/services/graphql-gateway/app/middleware.py b/auto_generate_video_fold6/services/graphql-gateway/app/middleware.py
index e61541e..6b0cdf2 100644
--- a/auto_generate_video_fold6/services/graphql-gateway/app/middleware.py
+++ b/auto_generate_video_fold6/services/graphql-gateway/app/middleware.py
@@ -94,7 +94,9 @@ class CacheMiddleware(BaseHTTPMiddleware):
                     response_body += chunk
 
                 await self.cache_client.setex(
-                    cache_key, 300, response_body.decode()  # 5 åˆ†é˜ TTL
+                    cache_key,
+                    300,
+                    response_body.decode(),  # 5 åˆ†é˜ TTL
                 )
 
                 # é‡å»ºå›æ‡‰
diff --git a/auto_generate_video_fold6/services/music-service/suno_client.py b/auto_generate_video_fold6/services/music-service/suno_client.py
index 772794c..7e68566 100644
--- a/auto_generate_video_fold6/services/music-service/suno_client.py
+++ b/auto_generate_video_fold6/services/music-service/suno_client.py
@@ -109,7 +109,6 @@ class SunoClient:
             async with self.session.post(
                 f"{self.base_url}/api/generate", json=data
             ) as response:
-
                 if response.status != 200:
                     error_text = await response.text()
                     logger.error(
diff --git a/auto_generate_video_fold6/services/scheduler-service/app/entrepreneur_scheduler.py b/auto_generate_video_fold6/services/scheduler-service/app/entrepreneur_scheduler.py
index 91a8d90..13b4bfc 100644
--- a/auto_generate_video_fold6/services/scheduler-service/app/entrepreneur_scheduler.py
+++ b/auto_generate_video_fold6/services/scheduler-service/app/entrepreneur_scheduler.py
@@ -214,8 +214,9 @@ class EntrepreneurScheduler:
             if task.retry_count < self.config.retry_attempts:
                 task.retry_count += 1
                 logger.warning(
-                    f"ä»»å‹™åŸ·è¡Œå¤±æ•—ï¼Œæº–å‚™é‡è©¦ ({task.retry_count}/{self.config
-                                                       .retry_attempts}): {e}"
+                    f"ä»»å‹™åŸ·è¡Œå¤±æ•—ï¼Œæº–å‚™é‡è©¦ ({task.retry_count}/{
+                        self.config.retry_attempts
+                    }): {e}"
                 )
                 # å»¶é²å¾Œé‡è©¦
                 await asyncio.sleep(self.config.retry_delay_minutes * 60)
@@ -315,9 +316,9 @@ class EntrepreneurScheduler:
             "config": {
                 "daily_video_limit": self.config.daily_video_limit,
                 "daily_budget_limit": self.config.daily_budget_limit,
-                "work_hours": f"{self.config.work_hours_start}-{self
-                                                                 \
-                                                                    .config.work_hours_end}",
+                "work_hours": f"{self.config.work_hours_start}-{
+                    self.config.work_hours_end
+                }",
             },
         }
 
diff --git a/auto_generate_video_fold6/services/scheduler-service/app/entrepreneur_scheduler_refactored.py b/auto_generate_video_fold6/services/scheduler-service/app/entrepreneur_scheduler_refactored.py
index bed39c5..c2aabe2 100644
--- a/auto_generate_video_fold6/services/scheduler-service/app/entrepreneur_scheduler_refactored.py
+++ b/auto_generate_video_fold6/services/scheduler-service/app/entrepreneur_scheduler_refactored.py
@@ -293,9 +293,9 @@ class StatisticsManager:
     def update_stats(self, task: ScheduledTask):
         """æ›´æ–°çµ±è¨ˆæ•¸æ“š"""
         if task.status == TaskStatus.COMPLETED:
-            self.daily_stats[
-                "videos_generated"
-            ] += task.metrics.videos_generated
+            self.daily_stats["videos_generated"] += (
+                task.metrics.videos_generated
+            )
             self.daily_stats["budget_used"] += task.metrics.cost_incurred
             self.daily_stats["tasks_completed"] += 1
             self.daily_stats["total_execution_time"] += (
diff --git a/auto_generate_video_fold6/services/scheduler-service/app/routers/entrepreneur_scheduler_routes.py b/auto_generate_video_fold6/services/scheduler-service/app/routers/entrepreneur_scheduler_routes.py
index 800a21e..8594dcd 100644
--- a/auto_generate_video_fold6/services/scheduler-service/app/routers/entrepreneur_scheduler_routes.py
+++ b/auto_generate_video_fold6/services/scheduler-service/app/routers/entrepreneur_scheduler_routes.py
@@ -316,7 +316,7 @@ async def list_scheduled_tasks(
         # æ’åºå’Œåˆ†é 
         user_tasks.sort(key=lambda x: x["created_at"], reverse=True)
         total = len(user_tasks)
-        tasks = user_tasks[offset: offset + limit]
+        tasks = user_tasks[offset : offset + limit]
 
         return {
             "tasks": tasks,
diff --git a/auto_generate_video_fold6/services/scheduler-service/app/routers/scheduler.py b/auto_generate_video_fold6/services/scheduler-service/app/routers/scheduler.py
index caf871c..108b00a 100644
--- a/auto_generate_video_fold6/services/scheduler-service/app/routers/scheduler.py
+++ b/auto_generate_video_fold6/services/scheduler-service/app/routers/scheduler.py
@@ -44,7 +44,7 @@ async def schedule_post(
                 PlatformAccount.id == request.platform_account_id,
                 PlatformAccount.user_id == current_user["user_id"],
                 PlatformAccount.platform == request.platform,
-                PlatformAccount.is_active  is True,
+                PlatformAccount.is_active is True,
             )
             .first()
         )
@@ -306,7 +306,7 @@ async def get_platform_accounts(
         db.query(PlatformAccount)
         .filter(
             PlatformAccount.user_id == current_user["user_id"],
-            PlatformAccount.is_active  is True,
+            PlatformAccount.is_active is True,
         )
         .all()
     )
diff --git a/auto_generate_video_fold6/services/scheduler-service/app/tasks.py b/auto_generate_video_fold6/services/scheduler-service/app/tasks.py
index 92b7f19..9d0b2f9 100644
--- a/auto_generate_video_fold6/services/scheduler-service/app/tasks.py
+++ b/auto_generate_video_fold6/services/scheduler-service/app/tasks.py
@@ -66,7 +66,7 @@ def publish_post(post_id: int):
             .filter(
                 PlatformAccount.user_id == post.user_id,
                 PlatformAccount.platform == post.platform,
-                PlatformAccount.is_active  is True,
+                PlatformAccount.is_active is True,
             )
             .first()
         )
diff --git a/auto_generate_video_fold6/services/scheduler-service/test_refactored_scheduler.py b/auto_generate_video_fold6/services/scheduler-service/test_refactored_scheduler.py
index 5a55e15..ea6116c 100644
--- a/auto_generate_video_fold6/services/scheduler-service/test_refactored_scheduler.py
+++ b/auto_generate_video_fold6/services/scheduler-service/test_refactored_scheduler.py
@@ -160,7 +160,8 @@ async def test_failure_and_retry():
     # å‰µå»ºæœƒå¤±æ•—çš„ Mock å®¢æˆ¶ç«¯
     mock_client = MockVideoServiceClient(should_succeed=False, delay=0.05)
     config = SchedulerConfig(
-        retry_attempts=2, retry_delay_minutes=1  # æœ€å°æœ‰æ•ˆå€¼
+        retry_attempts=2,
+        retry_delay_minutes=1,  # æœ€å°æœ‰æ•ˆå€¼
     )
 
     scheduler = EntrepreneurScheduler(config, mock_client)
@@ -196,7 +197,8 @@ async def test_scheduler_lifecycle():
 
     mock_client = MockVideoServiceClient(should_succeed=True, delay=0.1)
     config = SchedulerConfig(
-        check_interval_minutes=1, health_check_interval_minutes=1  # æœ€å°æœ‰æ•ˆå€¼
+        check_interval_minutes=1,
+        health_check_interval_minutes=1,  # æœ€å°æœ‰æ•ˆå€¼
     )
 
     scheduler = EntrepreneurScheduler(config, mock_client)
diff --git a/auto_generate_video_fold6/services/scheduler-service/test_scheduler_simple.py b/auto_generate_video_fold6/services/scheduler-service/test_scheduler_simple.py
index 72d82a2..c35eb65 100644
--- a/auto_generate_video_fold6/services/scheduler-service/test_scheduler_simple.py
+++ b/auto_generate_video_fold6/services/scheduler-service/test_scheduler_simple.py
@@ -199,7 +199,8 @@ async def test_config_validation():
     # æ¸¬è©¦ç„¡æ•ˆé™åˆ¶
     try:
         invalid_config = SchedulerConfig(
-            daily_video_limit=0, daily_budget_limit=-5.0  # ç„¡æ•ˆé™åˆ¶
+            daily_video_limit=0,
+            daily_budget_limit=-5.0,  # ç„¡æ•ˆé™åˆ¶
         )
         invalid_config.validate()
         assert False, "æ‡‰è©²æ‹‹å‡ºé™åˆ¶éŒ¯èª¤"
diff --git a/auto_generate_video_fold6/services/scheduler-service/tests/test_scheduler.py b/auto_generate_video_fold6/services/scheduler-service/tests/test_scheduler.py
index b34c139..2d8a211 100644
--- a/auto_generate_video_fold6/services/scheduler-service/tests/test_scheduler.py
+++ b/auto_generate_video_fold6/services/scheduler-service/tests/test_scheduler.py
@@ -141,7 +141,7 @@ def test_connect_platform_account(mock_verify_token, client, auth_headers):
     data = response.json()
     assert data["platform"] == account_data["platform"]
     assert data["platform_user_id"] == account_data["platform_user_id"]
-    assert data["is_active"]  is True
+    assert data["is_active"] is True
 
 
 @patch("app.auth.verify_token")
diff --git a/auto_generate_video_fold6/services/social-service/app/entrepreneur_publisher.py b/auto_generate_video_fold6/services/social-service/app/entrepreneur_publisher.py
index 0c65972..e463043 100644
--- a/auto_generate_video_fold6/services/social-service/app/entrepreneur_publisher.py
+++ b/auto_generate_video_fold6/services/social-service/app/entrepreneur_publisher.py
@@ -164,8 +164,9 @@ class EntrepreneurPublisher:
         """å¤šå¹³å°æ‰¹æ¬¡ç™¼å¸ƒ"""
 
         try:
-            request_id = f"batch_{datetime.utcnow().strftime
-                                  ('%Y%m%d_%H%M%S')}_{request.user_id}"
+            request_id = f"batch_{
+                datetime.utcnow().strftime('%Y%m%d_%H%M%S')
+            }_{request.user_id}"
 
             batch_result = BatchPublishResult(
                 request_id=request_id,
@@ -445,11 +446,9 @@ class EntrepreneurPublisher:
                 return await client.publish_reel(
                     video_url=content["video_url"],
                     access_token=access_token,
-                    caption=f"{
-                        content['title']}\n\n{
-                        content['description']}\n\n{
-                        ' '.join(
-                            content['hashtags'])}",
+                    caption=f"{content['title']}\n\n{
+                        content['description']
+                    }\n\n{' '.join(content['hashtags'])}",
                     allow_comments=content.get("allow_comments", True),
                     allow_sharing=content.get("allow_sharing", True),
                 )
@@ -526,8 +525,9 @@ class EntrepreneurPublisher:
         """æ’ç¨‹æ‰¹æ¬¡ç™¼å¸ƒ"""
 
         try:
-            request_id = f"scheduled_{datetime.utcnow().strftime
-                                      ('%Y%m%d_%H%M%S')}_{request.user_id}"
+            request_id = f"scheduled_{
+                datetime.utcnow().strftime('%Y%m%d_%H%M%S')
+            }_{request.user_id}"
 
             # ç‚ºæ¯å€‹å¹³å°å‰µå»ºæ’ç¨‹ä»»å‹™
             for platform, scheduled_time in publish_times.items():
@@ -601,7 +601,7 @@ class EntrepreneurPublisher:
         # æŒ‰æ™‚é–“æ’åº
         user_history.sort(key=lambda x: x.started_at, reverse=True)
 
-        return user_history[offset: offset + limit]
+        return user_history[offset : offset + limit]
 
     def get_active_publications(
         self, user_id: str
diff --git a/auto_generate_video_fold6/services/social-service/app/platforms/youtube.py b/auto_generate_video_fold6/services/social-service/app/platforms/youtube.py
index 8741972..c28d799 100644
--- a/auto_generate_video_fold6/services/social-service/app/platforms/youtube.py
+++ b/auto_generate_video_fold6/services/social-service/app/platforms/youtube.py
@@ -92,7 +92,8 @@ async def publish_video(
 
         # ä½¿ç”¨ YouTube Data API v3 ä¸Šå‚³å½±ç‰‡
         upload_url = f"{
-            settings.YOUTUBE_API_BASE}/videos?uploadType=resumable&part=snippet,status"
+            settings.YOUTUBE_API_BASE
+        }/videos?uploadType=resumable&part=snippet,status"
 
         async with aiohttp.ClientSession() as session:
             # åˆå§‹åŒ–ä¸Šå‚³
diff --git a/auto_generate_video_fold6/services/social-service/tests/test_platforms.py b/auto_generate_video_fold6/services/social-service/tests/test_platforms.py
index 4917538..61110cd 100644
--- a/auto_generate_video_fold6/services/social-service/tests/test_platforms.py
+++ b/auto_generate_video_fold6/services/social-service/tests/test_platforms.py
@@ -126,7 +126,7 @@ async def test_publish_to_tiktok_success(mock_publish):
 
     assert response.status_code == 200
     data = response.json()
-    assert data["success"]  is True
+    assert data["success"] is True
     assert data["post_id"] == "tiktok_post_123"
 
 
@@ -148,7 +148,7 @@ async def test_publish_to_tiktok_failure(mock_publish):
 
     assert response.status_code == 200
     data = response.json()
-    assert data["success"]  is False
+    assert data["success"] is False
     assert "Failed to publish to TikTok" in data["error"]
 
 
@@ -176,7 +176,7 @@ async def test_publish_to_youtube_success(mock_publish):
 
     assert response.status_code == 200
     data = response.json()
-    assert data["success"]  is True
+    assert data["success"] is True
     assert data["post_id"] == "youtube_video_456"
 
 
@@ -204,7 +204,7 @@ async def test_publish_to_instagram_success(mock_publish):
 
     assert response.status_code == 200
     data = response.json()
-    assert data["success"]  is True
+    assert data["success"] is True
     assert data["post_id"] == "instagram_post_789"
 
 
diff --git a/auto_generate_video_fold6/services/storage-service/app/models.py b/auto_generate_video_fold6/services/storage-service/app/models.py
index 8ec4d93..be8d51e 100644
--- a/auto_generate_video_fold6/services/storage-service/app/models.py
+++ b/auto_generate_video_fold6/services/storage-service/app/models.py
@@ -142,9 +142,9 @@ class FileProcessingJob(Base):
     completed_at = Column(DateTime(timezone=True))
 
     def __repr__(self):
-        return f"<FileProcessingJob(id={self.id}, job_type={self
-                                                             \
-                                                                .job_type}, status={self.status})>"
+        return f"<FileProcessingJob(id={self.id}, job_type={
+            self.job_type
+        }, status={self.status})>"
 
 
 class FileDownload(Base):
diff --git a/auto_generate_video_fold6/services/storage-service/app/routers/download.py b/auto_generate_video_fold6/services/storage-service/app/routers/download.py
index 0135da8..b0efb45 100644
--- a/auto_generate_video_fold6/services/storage-service/app/routers/download.py
+++ b/auto_generate_video_fold6/services/storage-service/app/routers/download.py
@@ -199,9 +199,9 @@ async def download_file(
                 yield file_data
 
             headers = {
-                "Content-Disposition": f'attachment; filename="{quote
-                                                                (file \
-                                                                    .original_filename)}"',
+                "Content-Disposition": f'attachment; filename="{
+                    quote(file.original_filename)
+                }"',
                 "Content-Type": file.mime_type,
                 "Content-Length": str(file.file_size),
             }
diff --git a/auto_generate_video_fold6/services/storage-service/app/routers/upload.py b/auto_generate_video_fold6/services/storage-service/app/routers/upload.py
index c865d0d..6a38699 100644
--- a/auto_generate_video_fold6/services/storage-service/app/routers/upload.py
+++ b/auto_generate_video_fold6/services/storage-service/app/routers/upload.py
@@ -199,8 +199,9 @@ async def upload_multiple_files(
         if len(files) > settings.max_upload_files:
             raise HTTPException(
                 status_code=400,
-                detail=f"Too many files. Maximum allowed: {settings
-                                                           .max_upload_files}",
+                detail=f"Too many files. Maximum allowed: {
+                    settings.max_upload_files
+                }",
             )
 
         uploaded_files = []
diff --git a/auto_generate_video_fold6/services/storage-service/app/storage.py b/auto_generate_video_fold6/services/storage-service/app/storage.py
index 3bbe1a6..c98ee45 100644
--- a/auto_generate_video_fold6/services/storage-service/app/storage.py
+++ b/auto_generate_video_fold6/services/storage-service/app/storage.py
@@ -334,9 +334,9 @@ class StorageManager:
 
         now = datetime.utcnow()
 
-        object_key = f"{user_id}/{file_type}/{now.year:04d}/{now
-                                                              \
-                                                                 .month:02d}/{safe_filename}"
+        object_key = f"{user_id}/{file_type}/{now.year:04d}/{now.month:02d}/{
+            safe_filename
+        }"
         return object_key
 
     def calculate_file_hash(self, file_data: bytes) -> str:
diff --git a/auto_generate_video_fold6/services/training-worker/main.py b/auto_generate_video_fold6/services/training-worker/main.py
index ee94101..548bcd3 100644
--- a/auto_generate_video_fold6/services/training-worker/main.py
+++ b/auto_generate_video_fold6/services/training-worker/main.py
@@ -205,8 +205,9 @@ def train_voice_model(self, task_data: Dict[str, Any]):
 
     except Exception as e:
         logger.error(
-            f"Training failed for task {task_data.get('task_id',
-                                                      'unknown')}: {str(e)}"
+            f"Training failed for task {task_data.get('task_id', 'unknown')}: {
+                str(e)
+            }"
         )
 
         task.status = TrainingStatus.FAILED
diff --git a/auto_generate_video_fold6/services/training-worker/tests/test_worker.py b/auto_generate_video_fold6/services/training-worker/tests/test_worker.py
index 9fe9338..a9ff864 100644
--- a/auto_generate_video_fold6/services/training-worker/tests/test_worker.py
+++ b/auto_generate_video_fold6/services/training-worker/tests/test_worker.py
@@ -43,10 +43,10 @@ def test_data_path_validation(mock_exists):
     def validate_data_path(path):
         return os.path.exists(path)
 
-    assert validate_data_path("/valid/path")  is True
+    assert validate_data_path("/valid/path") is True
 
     mock_exists.return_value = False
-    assert validate_data_path("/invalid/path")  is False
+    assert validate_data_path("/invalid/path") is False
 
 
 def test_training_config_validation():
@@ -63,7 +63,7 @@ def test_training_config_validation():
         required_keys = ["model_type", "epochs", "batch_size", "learning_rate"]
         return all(key in config for key in required_keys)
 
-    assert validate_config(valid_config)  is True
+    assert validate_config(valid_config) is True
 
     invalid_config = {"model_type": "voice_cloning"}
-    assert validate_config(invalid_config)  is False
+    assert validate_config(invalid_config) is False
diff --git a/auto_generate_video_fold6/services/video-service/ai/gemini_client.py b/auto_generate_video_fold6/services/video-service/ai/gemini_client.py
index 806ecb3..390cf91 100644
--- a/auto_generate_video_fold6/services/video-service/ai/gemini_client.py
+++ b/auto_generate_video_fold6/services/video-service/ai/gemini_client.py
@@ -302,7 +302,9 @@ Make sure the total duration of all scenes equals {duration} seconds.
             scene_type = (
                 "intro"
                 if i == 0
-                else "outro" if i == scene_count - 1 else "main"
+                else "outro"
+                if i == scene_count - 1
+                else "main"
             )
 
             scenes.append(
@@ -361,7 +363,7 @@ Please provide only the JSON array of caption segments.
                 # Fallback: split narration into chunks
                 words = narration.split()
                 return [
-                    " ".join(words[i: i + 5]) for i in range(0, len(words), 5)
+                    " ".join(words[i : i + 5]) for i in range(0, len(words), 5)
                 ]
 
         except Exception as e:
@@ -369,7 +371,7 @@ Please provide only the JSON array of caption segments.
             # Fallback: simple word chunking
             words = narration.split()
             return [
-                " ".join(words[i: i + 5]) for i in range(0, len(words), 5)
+                " ".join(words[i : i + 5]) for i in range(0, len(words), 5)
             ]
 
     async def close(self):
diff --git a/auto_generate_video_fold6/services/video-service/ai/stable_diffusion_client.py b/auto_generate_video_fold6/services/video-service/ai/stable_diffusion_client.py
index 2d0591a..1b5e402 100644
--- a/auto_generate_video_fold6/services/video-service/ai/stable_diffusion_client.py
+++ b/auto_generate_video_fold6/services/video-service/ai/stable_diffusion_client.py
@@ -173,12 +173,12 @@ class StableDiffusionClient:
                 f"{self.base_url}/generation/stable-diffusion-xl-1024-v1-0/text-to-image",
                 json=payload,
             ) as response:
-
                 if response.status != 200:
                     error_text = await response.text()
                     raise Exception(
-                        f"Stable Diffusion API error: {response
-                                                       .status} - {error_text}"
+                        f"Stable Diffusion API error: {response.status} - {
+                            error_text
+                        }"
                     )
 
                 result = await response.json()
@@ -229,7 +229,7 @@ class StableDiffusionClient:
         # Execute all tasks concurrently with some delay to avoid rate limits
         results = []
         for i in range(0, len(tasks), 3):  # Process 3 at a time
-            batch = tasks[i: i + 3]
+            batch = tasks[i : i + 3]
             batch_results = await asyncio.gather(
                 *batch, return_exceptions=True
             )
@@ -350,7 +350,6 @@ class StableDiffusionClient:
                 f"{self.base_url}/generation/esrgan-v1-x2plus/image-to-image/upscale",
                 json=payload,
             ) as response:
-
                 if response.status != 200:
                     error_text = await response.text()
                     raise Exception(
diff --git a/auto_generate_video_fold6/services/video-service/ai/suno_client.py b/auto_generate_video_fold6/services/video-service/ai/suno_client.py
index 90d0a6a..8bf6ec1 100644
--- a/auto_generate_video_fold6/services/video-service/ai/suno_client.py
+++ b/auto_generate_video_fold6/services/video-service/ai/suno_client.py
@@ -172,9 +172,9 @@ class SunoAIClient:
                         )
                     elif status == "failed":
                         raise Exception(
-                            f"Voice generation failed: {result.get
-                                                        ('error', \
-                                                            'Unknown error')}"
+                            f"Voice generation failed: {
+                                result.get('error', 'Unknown error')
+                            }"
                         )
                     elif status in ["queued", "processing"]:
                         # Check timeout
diff --git a/auto_generate_video_fold6/services/video-service/auth.py b/auto_generate_video_fold6/services/video-service/auth.py
index f5ec850..0937185 100644
--- a/auto_generate_video_fold6/services/video-service/auth.py
+++ b/auto_generate_video_fold6/services/video-service/auth.py
@@ -103,7 +103,6 @@ async def verify_token_remote(
                 headers=headers,
                 timeout=aiohttp.ClientTimeout(total=10),
             ) as response:
-
                 if response.status == 200:
                     result = await response.json()
                     user_id = result.get("user_id")
@@ -158,7 +157,6 @@ async def get_user_info(token: str) -> Optional[Dict[str, Any]]:
                 headers=headers,
                 timeout=aiohttp.ClientTimeout(total=10),
             ) as response:
-
                 if response.status == 200:
                     user_info = await response.json()
                     logger.debug(
@@ -249,7 +247,6 @@ async def check_user_permissions(
                 json=payload,
                 timeout=aiohttp.ClientTimeout(total=10),
             ) as response:
-
                 if response.status == 200:
                     result = await response.json()
                     has_permission = result.get("allowed", False)
diff --git a/auto_generate_video_fold6/services/video-service/routers/entrepreneur_workflows.py b/auto_generate_video_fold6/services/video-service/routers/entrepreneur_workflows.py
index 7ce7743..475f60c 100644
--- a/auto_generate_video_fold6/services/video-service/routers/entrepreneur_workflows.py
+++ b/auto_generate_video_fold6/services/video-service/routers/entrepreneur_workflows.py
@@ -398,7 +398,7 @@ async def list_user_workflows(
 
         # åˆ†é 
         total = len(user_workflows)
-        workflows = user_workflows[offset: offset + limit]
+        workflows = user_workflows[offset : offset + limit]
 
         return {
             "workflows": workflows,
diff --git a/auto_generate_video_fold6/services/video-service/routers/social_media.py b/auto_generate_video_fold6/services/video-service/routers/social_media.py
index 2c4f09b..64466be 100644
--- a/auto_generate_video_fold6/services/video-service/routers/social_media.py
+++ b/auto_generate_video_fold6/services/video-service/routers/social_media.py
@@ -160,8 +160,7 @@ async def publish_to_social_media(
 
             return SocialPublishResponse(
                 success=successful > 0,
-                message=f"Published to {successful}/{len(request
-                                                         .platforms)} \
+                message=f"Published to {successful}/{len(request.platforms)} \
                                                              platforms successfully",
                 results=formatted_results,
                 total_platforms=len(request.platforms),
diff --git a/auto_generate_video_fold6/services/video-service/social/platforms.py b/auto_generate_video_fold6/services/video-service/social/platforms.py
index 782e00a..f4b0db7 100644
--- a/auto_generate_video_fold6/services/video-service/social/platforms.py
+++ b/auto_generate_video_fold6/services/video-service/social/platforms.py
@@ -155,7 +155,6 @@ class TikTokClient(SocialPlatform):
         async with session.post(
             f"{self.upload_url}/video/upload", data=data, headers=headers
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
                 raise Exception(
@@ -201,7 +200,6 @@ class TikTokClient(SocialPlatform):
         async with session.post(
             f"{self.base_url}/video/publish", json=payload, headers=headers
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
                 raise Exception(
@@ -237,7 +235,6 @@ class TikTokClient(SocialPlatform):
                 params={"video_id": platform_id},
                 headers=headers,
             ) as response:
-
                 if response.status != 200:
                     return {"error": f"API error: {response.status}"}
 
@@ -276,7 +273,6 @@ class TikTokClient(SocialPlatform):
                 params={"video_id": platform_id},
                 headers=headers,
             ) as response:
-
                 return response.status == 200
 
         except Exception as e:
@@ -383,7 +379,6 @@ class YouTubeClient(SocialPlatform):
             data=body,
             headers=headers,
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
                 raise Exception(
@@ -454,7 +449,6 @@ class YouTubeClient(SocialPlatform):
                 params={"id": platform_id},
                 headers=headers,
             ) as response:
-
                 return response.status == 204
 
         except Exception as e:
@@ -524,12 +518,12 @@ class InstagramClient(SocialPlatform):
         async with session.post(
             f"{self.base_url}/{self.business_account_id}/media", params=params
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
                 raise Exception(
-                    f"Instagram media creation error: {response
-                                                       .status} - {error_text}"
+                    f"Instagram media creation error: {response.status} - {
+                        error_text
+                    }"
                 )
 
             result = await response.json()
@@ -552,7 +546,6 @@ class InstagramClient(SocialPlatform):
             f"{self.base_url}/{self.business_account_id}/media_publish",
             params=params,
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
                 raise Exception(
@@ -584,7 +577,6 @@ class InstagramClient(SocialPlatform):
             async with session.get(
                 f"{self.base_url}/{media_id}", params=params
             ) as response:
-
                 if response.status == 200:
                     result = await response.json()
                     return result.get("permalink")
@@ -610,7 +602,6 @@ class InstagramClient(SocialPlatform):
             async with session.get(
                 f"{self.base_url}/{platform_id}", params=params
             ) as response:
-
                 if response.status != 200:
                     return {"error": f"API error: {response.status}"}
 
@@ -648,7 +639,6 @@ class InstagramClient(SocialPlatform):
             async with session.delete(
                 f"{self.base_url}/{platform_id}", params=params
             ) as response:
-
                 return response.status == 200
 
         except Exception as e:
diff --git a/auto_generate_video_fold6/services/video-service/test_docker_validation.py b/auto_generate_video_fold6/services/video-service/test_docker_validation.py
index 994a209..d8863e4 100644
--- a/auto_generate_video_fold6/services/video-service/test_docker_validation.py
+++ b/auto_generate_video_fold6/services/video-service/test_docker_validation.py
@@ -122,9 +122,9 @@ def test_dockerfile_exists():
 
         # åŸºæœ¬ Dockerfile é©—è­‰
         assert "FROM" in content, "Dockerfile should contain FROM instruction"
-        assert (
-            "WORKDIR" in content or "RUN" in content
-        ), "Dockerfile should contain basic instructions"
+        assert "WORKDIR" in content or "RUN" in content, (
+            "Dockerfile should contain basic instructions"
+        )
 
         print(f"âœ… Dockerfile å­˜åœ¨æ–¼: {dockerfile_path}")
     else:
diff --git a/auto_generate_video_fold6/services/video-service/test_tdd_refactor.py b/auto_generate_video_fold6/services/video-service/test_tdd_refactor.py
index dba8be6..ccd37e3 100644
--- a/auto_generate_video_fold6/services/video-service/test_tdd_refactor.py
+++ b/auto_generate_video_fold6/services/video-service/test_tdd_refactor.py
@@ -55,9 +55,9 @@ class RefactorTest:
             pass  # æ­£ç¢ºæ‹‹å‡ºç•°å¸¸
         except Exception as e:
             raise AssertionError(
-                f"Expected {exception_type.__name__}, but got {type(e
-                                                                    ) \
-                                                                        .__name__}: {e}"
+                f"Expected {exception_type.__name__}, but got {
+                    type(e).__name__
+                }: {e}"
             )
 
     def run_test(self, test_func, test_name):
diff --git a/auto_generate_video_fold6/services/video-service/tests/test_video_service.py b/auto_generate_video_fold6/services/video-service/tests/test_video_service.py
index 69e335a..4fd4082 100644
--- a/auto_generate_video_fold6/services/video-service/tests/test_video_service.py
+++ b/auto_generate_video_fold6/services/video-service/tests/test_video_service.py
@@ -195,9 +195,7 @@ class TestAIIntegration:
         with patch.object(client, "_get_session") as mock_session:
             mock_response = AsyncMock()
             mock_response.status = 200
-            mock_session.return_value.get.return_value.__aenter__.return_value = (
-                mock_response
-            )
+            mock_session.return_value.get.return_value.__aenter__.return_value = mock_response
 
             health = await client.health_check()
             assert health["status"] == "healthy"
@@ -255,9 +253,7 @@ class TestAIIntegration:
                     }
                 ]
             }
-            mock_session.return_value.post.return_value.__aenter__.return_value = (
-                mock_response
-            )
+            mock_session.return_value.post.return_value.__aenter__.return_value = mock_response
 
             with patch.object(
                 client,
@@ -306,7 +302,6 @@ class TestVideoComposition:
                     "_upload_media",
                     return_value="http://test.com/preview.mp4",
                 ):
-
                     result = await composer.create_video(
                         script_scenes=sample_script_response.scenes,
                         voice_url="http://test.com/voice.mp3",
@@ -361,7 +356,7 @@ class TestSocialMediaIntegration:
 
             result = await manager.publish_to_platform("tiktok", request)
 
-            assert result.success  is True
+            assert result.success is True
             assert result.platform == "tiktok"
             assert result.platform_id == "tiktok123"
 
diff --git a/auto_generate_video_fold6/services/video-service/video/composer.py b/auto_generate_video_fold6/services/video-service/video/composer.py
index 02df01e..91523f1 100644
--- a/auto_generate_video_fold6/services/video-service/video/composer.py
+++ b/auto_generate_video_fold6/services/video-service/video/composer.py
@@ -564,9 +564,9 @@ class VideoComposer:
         return {
             "duration": float(format_info.get("duration", 0)),
             "file_size": int(format_info.get("size", 0)),
-            "resolution": f"{video_stream.get('width', 0
-                                              )}x{video_stream.get \
-                                                  ('height', 0)}",
+            "resolution": f"{video_stream.get('width', 0)}x{
+                video_stream.get('height', 0)
+            }",
             "format": format_info.get("format_name", "mp4").split(",")[0],
         }
 
diff --git a/auto_generate_video_fold6/services/voice-enhancement/app/routers/cloning.py b/auto_generate_video_fold6/services/voice-enhancement/app/routers/cloning.py
index 8e7f8ae..f4032ba 100644
--- a/auto_generate_video_fold6/services/voice-enhancement/app/routers/cloning.py
+++ b/auto_generate_video_fold6/services/voice-enhancement/app/routers/cloning.py
@@ -107,7 +107,9 @@ async def create_voice_profile(
             # è®€å–éŸ³è¨Šæ•¸æ“š
             audio_data = await audio_file.read()
             if len(audio_data) == 0:
-                raise HTTPException(status_code=400, detail=f"æ–‡ä»¶ {i + 1} ç‚ºç©º")
+                raise HTTPException(
+                    status_code=400, detail=f"æ–‡ä»¶ {i + 1} ç‚ºç©º"
+                )
 
             audio_samples.append(audio_data)
 
diff --git a/auto_generate_video_fold6/shared/api/standard_responses.py b/auto_generate_video_fold6/shared/api/standard_responses.py
index 4301f45..9256047 100644
--- a/auto_generate_video_fold6/shared/api/standard_responses.py
+++ b/auto_generate_video_fold6/shared/api/standard_responses.py
@@ -13,11 +13,12 @@ from dataclasses import dataclass
 import json
 
 
-T = TypeVar('T')
+T = TypeVar("T")
 
 
 class ResponseStatus(Enum):
     """å›æ‡‰ç‹€æ…‹"""
+
     SUCCESS = "success"
     ERROR = "error"
     WARNING = "warning"
@@ -26,15 +27,16 @@ class ResponseStatus(Enum):
 
 class PaginationInfo(BaseModel):
     """åˆ†é ä¿¡æ¯"""
+
     page: int = Field(ge=1, description="ç•¶å‰é ç¢¼")
     size: int = Field(ge=1, le=100, description="æ¯é å¤§å°")
     total: int = Field(ge=0, description="ç¸½è¨˜éŒ„æ•¸")
     pages: int = Field(ge=0, description="ç¸½é æ•¸")
     has_next: bool = Field(description="æ˜¯å¦æœ‰ä¸‹ä¸€é ")
     has_prev: bool = Field(description="æ˜¯å¦æœ‰ä¸Šä¸€é ")
-    
+
     @classmethod
-    def create(cls, page: int, size: int, total: int) -> 'PaginationInfo':
+    def create(cls, page: int, size: int, total: int) -> "PaginationInfo":
         """å‰µå»ºåˆ†é ä¿¡æ¯"""
         pages = (total + size - 1) // size  # å‘ä¸Šå–æ•´
         return cls(
@@ -43,12 +45,13 @@ class PaginationInfo(BaseModel):
             total=total,
             pages=pages,
             has_next=page < pages,
-            has_prev=page > 1
+            has_prev=page > 1,
         )
 
 
 class APIMetadata(BaseModel):
     """API å…ƒæ•¸æ“š"""
+
     timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())
     request_id: Optional[str] = None
     version: str = "v1"
@@ -61,22 +64,28 @@ class APIMetadata(BaseModel):
 
 class StandardResponse(BaseModel, Generic[T]):
     """æ¨™æº– API å›æ‡‰æ ¼å¼"""
-    
+
     status: ResponseStatus = Field(description="å›æ‡‰ç‹€æ…‹")
     message: str = Field(description="å›æ‡‰æ¶ˆæ¯")
     data: Optional[T] = Field(default=None, description="å›æ‡‰æ•¸æ“š")
-    errors: Optional[List[Dict[str, Any]]] = Field(default=None, description="éŒ¯èª¤è©³æƒ…")
+    errors: Optional[List[Dict[str, Any]]] = Field(
+        default=None, description="éŒ¯èª¤è©³æƒ…"
+    )
     warnings: Optional[List[str]] = Field(default=None, description="è­¦å‘Šä¿¡æ¯")
-    pagination: Optional[PaginationInfo] = Field(default=None, description="åˆ†é ä¿¡æ¯")
-    metadata: APIMetadata = Field(default_factory=APIMetadata, description="å…ƒæ•¸æ“š")
-    
+    pagination: Optional[PaginationInfo] = Field(
+        default=None, description="åˆ†é ä¿¡æ¯"
+    )
+    metadata: APIMetadata = Field(
+        default_factory=APIMetadata, description="å…ƒæ•¸æ“š"
+    )
+
     class Config:
         use_enum_values = True
-        
+
     def to_dict(self) -> Dict[str, Any]:
         """è½‰æ›ç‚ºå­—å…¸"""
         return self.dict(exclude_none=True)
-    
+
     def to_json(self) -> str:
         """è½‰æ›ç‚º JSON"""
         return json.dumps(self.to_dict(), ensure_ascii=False, default=str)
@@ -84,96 +93,97 @@ class StandardResponse(BaseModel, Generic[T]):
 
 class PaginatedResponse(StandardResponse[List[T]]):
     """åˆ†é å›æ‡‰æ ¼å¼"""
+
     pagination: PaginationInfo = Field(description="åˆ†é ä¿¡æ¯")
 
 
 class ResponseBuilder:
     """å›æ‡‰å»ºæ§‹å™¨"""
-    
+
     def __init__(self):
-        self.response_data = {
-            "metadata": APIMetadata()
-        }
-    
-    def status(self, status: ResponseStatus) -> 'ResponseBuilder':
+        self.response_data = {"metadata": APIMetadata()}
+
+    def status(self, status: ResponseStatus) -> "ResponseBuilder":
         """è¨­ç½®ç‹€æ…‹"""
         self.response_data["status"] = status
         return self
-    
-    def message(self, message: str) -> 'ResponseBuilder':
+
+    def message(self, message: str) -> "ResponseBuilder":
         """è¨­ç½®æ¶ˆæ¯"""
         self.response_data["message"] = message
         return self
-    
-    def data(self, data: Any) -> 'ResponseBuilder':
+
+    def data(self, data: Any) -> "ResponseBuilder":
         """è¨­ç½®æ•¸æ“š"""
         self.response_data["data"] = data
         return self
-    
-    def errors(self, errors: List[Dict[str, Any]]) -> 'ResponseBuilder':
+
+    def errors(self, errors: List[Dict[str, Any]]) -> "ResponseBuilder":
         """è¨­ç½®éŒ¯èª¤"""
         self.response_data["errors"] = errors
         return self
-    
-    def add_error(self, code: str, message: str, field: str = None) -> 'ResponseBuilder':
+
+    def add_error(
+        self, code: str, message: str, field: str = None
+    ) -> "ResponseBuilder":
         """æ·»åŠ éŒ¯èª¤"""
         if "errors" not in self.response_data:
             self.response_data["errors"] = []
-        
+
         error = {"code": code, "message": message}
         if field:
             error["field"] = field
-        
+
         self.response_data["errors"].append(error)
         return self
-    
-    def warnings(self, warnings: List[str]) -> 'ResponseBuilder':
+
+    def warnings(self, warnings: List[str]) -> "ResponseBuilder":
         """è¨­ç½®è­¦å‘Š"""
         self.response_data["warnings"] = warnings
         return self
-    
-    def add_warning(self, warning: str) -> 'ResponseBuilder':
+
+    def add_warning(self, warning: str) -> "ResponseBuilder":
         """æ·»åŠ è­¦å‘Š"""
         if "warnings" not in self.response_data:
             self.response_data["warnings"] = []
         self.response_data["warnings"].append(warning)
         return self
-    
-    def pagination(self, pagination: PaginationInfo) -> 'ResponseBuilder':
+
+    def pagination(self, pagination: PaginationInfo) -> "ResponseBuilder":
         """è¨­ç½®åˆ†é """
         self.response_data["pagination"] = pagination
         return self
-    
-    def request_id(self, request_id: str) -> 'ResponseBuilder':
+
+    def request_id(self, request_id: str) -> "ResponseBuilder":
         """è¨­ç½®è«‹æ±‚ ID"""
         self.response_data["metadata"].request_id = request_id
         return self
-    
-    def service(self, service: str) -> 'ResponseBuilder':
+
+    def service(self, service: str) -> "ResponseBuilder":
         """è¨­ç½®æœå‹™åç¨±"""
         self.response_data["metadata"].service = service
         return self
-    
-    def method(self, method: str) -> 'ResponseBuilder':
+
+    def method(self, method: str) -> "ResponseBuilder":
         """è¨­ç½®æ–¹æ³•"""
         self.response_data["metadata"].method = method
         return self
-    
-    def path(self, path: str) -> 'ResponseBuilder':
+
+    def path(self, path: str) -> "ResponseBuilder":
         """è¨­ç½®è·¯å¾‘"""
         self.response_data["metadata"].path = path
         return self
-    
-    def execution_time(self, time_ms: float) -> 'ResponseBuilder':
+
+    def execution_time(self, time_ms: float) -> "ResponseBuilder":
         """è¨­ç½®åŸ·è¡Œæ™‚é–“"""
         self.response_data["metadata"].execution_time_ms = time_ms
         return self
-    
-    def user_id(self, user_id: str) -> 'ResponseBuilder':
+
+    def user_id(self, user_id: str) -> "ResponseBuilder":
         """è¨­ç½®ç”¨æˆ¶ ID"""
         self.response_data["metadata"].user_id = user_id
         return self
-    
+
     def build(self) -> StandardResponse:
         """å»ºæ§‹å›æ‡‰"""
         return StandardResponse(**self.response_data)
@@ -181,19 +191,17 @@ class ResponseBuilder:
 
 # å¿«æ·å›æ‡‰å‡½æ•¸
 def success_response(
-    message: str = "æ“ä½œæˆåŠŸ",
-    data: Any = None,
-    request_id: str = None
+    message: str = "æ“ä½œæˆåŠŸ", data: Any = None, request_id: str = None
 ) -> StandardResponse:
     """æˆåŠŸå›æ‡‰"""
     builder = ResponseBuilder().status(ResponseStatus.SUCCESS).message(message)
-    
+
     if data is not None:
         builder.data(data)
-    
+
     if request_id:
         builder.request_id(request_id)
-    
+
     return builder.build()
 
 
@@ -201,70 +209,69 @@ def error_response(
     message: str,
     errors: List[Dict[str, Any]] = None,
     status_code: int = status.HTTP_400_BAD_REQUEST,
-    request_id: str = None
+    request_id: str = None,
 ) -> StandardResponse:
     """éŒ¯èª¤å›æ‡‰"""
     builder = ResponseBuilder().status(ResponseStatus.ERROR).message(message)
-    
+
     if errors:
         builder.errors(errors)
-    
+
     if request_id:
         builder.request_id(request_id)
-    
+
     return builder.build()
 
 
 def validation_error_response(
     field_errors: Dict[str, str],
     message: str = "è¼¸å…¥é©—è­‰å¤±æ•—",
-    request_id: str = None
+    request_id: str = None,
 ) -> StandardResponse:
     """é©—è­‰éŒ¯èª¤å›æ‡‰"""
     errors = [
         {"code": "VALIDATION_ERROR", "field": field, "message": msg}
         for field, msg in field_errors.items()
     ]
-    
-    return error_response(message, errors, status.HTTP_422_UNPROCESSABLE_ENTITY, request_id)
+
+    return error_response(
+        message, errors, status.HTTP_422_UNPROCESSABLE_ENTITY, request_id
+    )
 
 
 def not_found_response(
-    resource: str = "è³‡æº",
-    request_id: str = None
+    resource: str = "è³‡æº", request_id: str = None
 ) -> StandardResponse:
     """æœªæ‰¾åˆ°å›æ‡‰"""
     return error_response(
         f"{resource}æœªæ‰¾åˆ°",
         [{"code": "NOT_FOUND", "message": f"è«‹æ±‚çš„{resource}ä¸å­˜åœ¨"}],
         status.HTTP_404_NOT_FOUND,
-        request_id
+        request_id,
     )
 
 
 def unauthorized_response(
-    message: str = "æœªæˆæ¬Šè¨ªå•",
-    request_id: str = None
+    message: str = "æœªæˆæ¬Šè¨ªå•", request_id: str = None
 ) -> StandardResponse:
     """æœªæˆæ¬Šå›æ‡‰"""
     return error_response(
         message,
         [{"code": "UNAUTHORIZED", "message": message}],
         status.HTTP_401_UNAUTHORIZED,
-        request_id
+        request_id,
     )
 
 
 def forbidden_response(
-    message: str = "æ¬Šé™ä¸è¶³",
-    request_id: str = None
+    message: str = "æ¬Šé™ä¸è¶³", request_id: str = None
 ) -> StandardResponse:
     """ç¦æ­¢è¨ªå•å›æ‡‰"""
     return error_response(
         message,
         [{"code": "FORBIDDEN", "message": message}],
         status.HTTP_403_FORBIDDEN,
-        request_id
+        request_id,
     )
 
 
@@ -272,58 +279,74 @@ def paginated_response(
     data: List[Any],
     pagination: PaginationInfo,
     message: str = "æŸ¥è©¢æˆåŠŸ",
-    request_id: str = None
+    request_id: str = None,
 ) -> PaginatedResponse:
     """åˆ†é å›æ‡‰"""
-    builder = ResponseBuilder().status(ResponseStatus.SUCCESS).message(message).data(data).pagination(pagination)
-    
+    builder = (
+        ResponseBuilder()
+        .status(ResponseStatus.SUCCESS)
+        .message(message)
+        .data(data)
+        .pagination(pagination)
+    )
+
     if request_id:
         builder.request_id(request_id)
-    
+
     response_data = builder.build().dict()
     return PaginatedResponse(**response_data)
 
 
 def internal_error_response(
-    message: str = "å…§éƒ¨æœå‹™éŒ¯èª¤",
-    request_id: str = None
+    message: str = "å…§éƒ¨æœå‹™éŒ¯èª¤", request_id: str = None
 ) -> StandardResponse:
     """å…§éƒ¨éŒ¯èª¤å›æ‡‰"""
     return error_response(
         message,
         [{"code": "INTERNAL_ERROR", "message": "æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œé‡è©¦"}],
         status.HTTP_500_INTERNAL_SERVER_ERROR,
-        request_id
+        request_id,
     )
 
 
 def rate_limit_response(
     message: str = "è«‹æ±‚é »ç‡è¶…å‡ºé™åˆ¶",
     retry_after: int = 60,
-    request_id: str = None
+    request_id: str = None,
 ) -> StandardResponse:
     """é™æµå›æ‡‰"""
     return error_response(
         message,
-        [{"code": "RATE_LIMIT_EXCEEDED", "message": f"è«‹åœ¨ {retry_after} ç§’å¾Œé‡è©¦"}],
+        [
+            {
+                "code": "RATE_LIMIT_EXCEEDED",
+                "message": f"è«‹åœ¨ {retry_after} ç§’å¾Œé‡è©¦",
+            }
+        ],
         status.HTTP_429_TOO_MANY_REQUESTS,
-        request_id
+        request_id,
     )
 
 
 # è³‡æ–™æ¨¡å‹æ¨™æº–åŒ–
 class StandardListQuery(BaseModel):
     """æ¨™æº–åˆ—è¡¨æŸ¥è©¢åƒæ•¸"""
+
     page: int = Field(1, ge=1, description="é ç¢¼")
     size: int = Field(20, ge=1, le=100, description="æ¯é å¤§å°")
     sort: Optional[str] = Field(None, description="æ’åºå­—æ®µ")
-    order: Optional[str] = Field("asc", regex="^(asc|desc)$", description="æ’åºæ–¹å‘")
-    search: Optional[str] = Field(None, min_length=1, max_length=100, description="æœç´¢é—œéµå­—")
+    order: Optional[str] = Field(
+        "asc", regex="^(asc|desc)$", description="æ’åºæ–¹å‘"
+    )
+    search: Optional[str] = Field(
+        None, min_length=1, max_length=100, description="æœç´¢é—œéµå­—"
+    )
     filters: Optional[Dict[str, Any]] = Field(None, description="ç¯©é¸æ¢ä»¶")
 
 
 class StandardCreateResponse(BaseModel):
     """æ¨™æº–å‰µå»ºå›æ‡‰"""
+
     id: Union[str, int] = Field(description="å‰µå»ºçš„è³‡æº ID")
     created_at: datetime = Field(description="å‰µå»ºæ™‚é–“")
     message: str = Field(default="å‰µå»ºæˆåŠŸ", description="æ“ä½œæ¶ˆæ¯")
@@ -331,6 +354,7 @@ class StandardCreateResponse(BaseModel):
 
 class StandardUpdateResponse(BaseModel):
     """æ¨™æº–æ›´æ–°å›æ‡‰"""
+
     id: Union[str, int] = Field(description="æ›´æ–°çš„è³‡æº ID")
     updated_at: datetime = Field(description="æ›´æ–°æ™‚é–“")
     message: str = Field(default="æ›´æ–°æˆåŠŸ", description="æ“ä½œæ¶ˆæ¯")
@@ -338,6 +362,7 @@ class StandardUpdateResponse(BaseModel):
 
 class StandardDeleteResponse(BaseModel):
     """æ¨™æº–åˆªé™¤å›æ‡‰"""
+
     id: Union[str, int] = Field(description="åˆªé™¤çš„è³‡æº ID")
     deleted_at: datetime = Field(description="åˆªé™¤æ™‚é–“")
     message: str = Field(default="åˆªé™¤æˆåŠŸ", description="æ“ä½œæ¶ˆæ¯")
@@ -347,6 +372,7 @@ class StandardDeleteResponse(BaseModel):
 @dataclass
 class APIEndpointDoc:
     """API ç«¯é»æ–‡æª”"""
+
     path: str
     method: str
     summary: str
@@ -356,7 +382,7 @@ class APIEndpointDoc:
     response_model: Optional[BaseModel] = None
     status_codes: Dict[int, str] = None
     examples: Dict[str, Any] = None
-    
+
     def __post_init__(self):
         if self.status_codes is None:
             self.status_codes = {
@@ -365,22 +391,22 @@ class APIEndpointDoc:
                 401: "æœªæˆæ¬Šè¨ªå•",
                 403: "æ¬Šé™ä¸è¶³",
                 404: "è³‡æºæœªæ‰¾åˆ°",
-                500: "å…§éƒ¨æœå‹™éŒ¯èª¤"
+                500: "å…§éƒ¨æœå‹™éŒ¯èª¤",
             }
 
 
 class APIDocumentationGenerator:
     """API æ–‡æª”ç”Ÿæˆå™¨"""
-    
+
     def __init__(self, service_name: str, version: str = "v1"):
         self.service_name = service_name
         self.version = version
         self.endpoints = []
-    
+
     def add_endpoint(self, endpoint: APIEndpointDoc):
         """æ·»åŠ ç«¯é»"""
         self.endpoints.append(endpoint)
-    
+
     def generate_openapi_spec(self) -> Dict[str, Any]:
         """ç”Ÿæˆ OpenAPI è¦ç¯„"""
         spec = {
@@ -388,11 +414,17 @@ class APIDocumentationGenerator:
             "info": {
                 "title": f"{self.service_name} API",
                 "version": self.version,
-                "description": f"{self.service_name} å¾®æœå‹™ API æ–‡æª”"
+                "description": f"{self.service_name} å¾®æœå‹™ API æ–‡æª”",
             },
             "servers": [
-                {"url": f"http://localhost:8000/api/{self.version}", "description": "é–‹ç™¼ç’°å¢ƒ"},
-                {"url": f"https://api.autovideo.com/{self.version}", "description": "ç”Ÿç”¢ç’°å¢ƒ"}
+                {
+                    "url": f"http://localhost:8000/api/{self.version}",
+                    "description": "é–‹ç™¼ç’°å¢ƒ",
+                },
+                {
+                    "url": f"https://api.autovideo.com/{self.version}",
+                    "description": "ç”Ÿç”¢ç’°å¢ƒ",
+                },
             ],
             "paths": {},
             "components": {
@@ -400,22 +432,33 @@ class APIDocumentationGenerator:
                     "StandardResponse": {
                         "type": "object",
                         "properties": {
-                            "status": {"type": "string", "enum": ["success", "error", "warning", "info"]},
+                            "status": {
+                                "type": "string",
+                                "enum": [
+                                    "success",
+                                    "error",
+                                    "warning",
+                                    "info",
+                                ],
+                            },
                             "message": {"type": "string"},
                             "data": {"type": "object"},
-                            "errors": {"type": "array", "items": {"type": "object"}},
-                            "metadata": {"type": "object"}
-                        }
+                            "errors": {
+                                "type": "array",
+                                "items": {"type": "object"},
+                            },
+                            "metadata": {"type": "object"},
+                        },
                     }
                 }
-            }
+            },
         }
-        
+
         # æ·»åŠ ç«¯é»æ–‡æª”
         for endpoint in self.endpoints:
             if endpoint.path not in spec["paths"]:
                 spec["paths"][endpoint.path] = {}
-            
+
             spec["paths"][endpoint.path][endpoint.method.lower()] = {
                 "summary": endpoint.summary,
                 "description": endpoint.description,
@@ -423,21 +466,21 @@ class APIDocumentationGenerator:
                 "responses": {
                     str(code): {"description": desc}
                     for code, desc in endpoint.status_codes.items()
-                }
+                },
             }
-        
+
         return spec
-    
+
     def save_documentation(self, output_file: str = None) -> str:
         """ä¿å­˜æ–‡æª”"""
         if output_file is None:
             output_file = f"{self.service_name.lower()}_api_docs.json"
-        
+
         spec = self.generate_openapi_spec()
-        
-        with open(output_file, 'w', encoding='utf-8') as f:
+
+        with open(output_file, "w", encoding="utf-8") as f:
             json.dump(spec, f, indent=2, ensure_ascii=False)
-        
+
         return output_file
 
 
@@ -446,52 +489,51 @@ def monitor_response_time(func):
     """éŸ¿æ‡‰æ™‚é–“ç›£æ§è£é£¾å™¨"""
     import time
     from functools import wraps
-    
+
     @wraps(func)
     async def wrapper(*args, **kwargs):
         start_time = time.time()
         try:
             result = await func(*args, **kwargs)
             execution_time = (time.time() - start_time) * 1000
-            
+
             # å¦‚æœçµæœæ˜¯ StandardResponseï¼Œæ›´æ–°åŸ·è¡Œæ™‚é–“
             if isinstance(result, StandardResponse):
                 result.metadata.execution_time_ms = execution_time
-            
+
             return result
         except Exception as e:
             execution_time = (time.time() - start_time) * 1000
             # è¨˜éŒ„éŒ¯èª¤å’ŒåŸ·è¡Œæ™‚é–“
             raise e
-    
+
     return wrapper
 
 
 if __name__ == "__main__":
     # æ¸¬è©¦æ¨™æº–å›æ‡‰æ ¼å¼
     print("=== æ¨™æº– API å›æ‡‰æ ¼å¼æ¸¬è©¦ ===")
-    
+
     # æˆåŠŸå›æ‡‰
     success = success_response("ç”¨æˆ¶æŸ¥è©¢æˆåŠŸ", {"id": 1, "name": "æ¸¬è©¦ç”¨æˆ¶"})
     print("æˆåŠŸå›æ‡‰:")
     print(success.to_json())
-    
+
     # éŒ¯èª¤å›æ‡‰
     error = validation_error_response(
         {"username": "ç”¨æˆ¶åä¸èƒ½ç‚ºç©º", "email": "éƒµç®±æ ¼å¼éŒ¯èª¤"}
     )
     print("\né©—è­‰éŒ¯èª¤å›æ‡‰:")
     print(error.to_json())
-    
+
     # åˆ†é å›æ‡‰
     pagination = PaginationInfo.create(page=1, size=10, total=100)
     paginated = paginated_response(
-        [{"id": i, "name": f"ç”¨æˆ¶{i}"} for i in range(1, 11)],
-        pagination
+        [{"id": i, "name": f"ç”¨æˆ¶{i}"} for i in range(1, 11)], pagination
     )
     print("\nåˆ†é å›æ‡‰:")
     print(paginated.to_json())
-    
+
     # API æ–‡æª”ç”Ÿæˆ
     doc_gen = APIDocumentationGenerator("Auth Service")
     endpoint = APIEndpointDoc(
@@ -499,9 +541,9 @@ if __name__ == "__main__":
         method="GET",
         summary="ç²å–ç”¨æˆ¶åˆ—è¡¨",
         description="ç²å–ç³»çµ±ä¸­çš„ç”¨æˆ¶åˆ—è¡¨ï¼Œæ”¯æŒåˆ†é å’Œæœç´¢",
-        tags=["ç”¨æˆ¶ç®¡ç†"]
+        tags=["ç”¨æˆ¶ç®¡ç†"],
     )
     doc_gen.add_endpoint(endpoint)
-    
+
     docs_file = doc_gen.save_documentation()
-    print(f"\nAPI æ–‡æª”å·²ç”Ÿæˆ: {docs_file}")
\ No newline at end of file
+    print(f"\nAPI æ–‡æª”å·²ç”Ÿæˆ: {docs_file}")
diff --git a/auto_generate_video_fold6/shared/error_handling/unified_errors.py b/auto_generate_video_fold6/shared/error_handling/unified_errors.py
index 1943402..a8e2bd8 100644
--- a/auto_generate_video_fold6/shared/error_handling/unified_errors.py
+++ b/auto_generate_video_fold6/shared/error_handling/unified_errors.py
@@ -16,7 +16,7 @@ from pydantic import BaseModel
 
 class ErrorCode(Enum):
     """æ¨™æº–éŒ¯èª¤ä»£ç¢¼"""
-    
+
     # é€šç”¨éŒ¯èª¤ (1000-1999)
     UNKNOWN_ERROR = "ERR_1000"
     VALIDATION_ERROR = "ERR_1001"
@@ -26,41 +26,41 @@ class ErrorCode(Enum):
     CONFLICT = "ERR_1005"
     RATE_LIMIT_EXCEEDED = "ERR_1006"
     SERVICE_UNAVAILABLE = "ERR_1007"
-    
+
     # ç”¨æˆ¶ç›¸é—œéŒ¯èª¤ (2000-2999)
     USER_NOT_FOUND = "ERR_2001"
     USER_ALREADY_EXISTS = "ERR_2002"
     INVALID_CREDENTIALS = "ERR_2003"
     EMAIL_NOT_VERIFIED = "ERR_2004"
     PASSWORD_TOO_WEAK = "ERR_2005"
-    
+
     # å½±ç‰‡è™•ç†éŒ¯èª¤ (3000-3999)
     VIDEO_PROCESSING_FAILED = "ERR_3001"
     INVALID_VIDEO_FORMAT = "ERR_3002"
     VIDEO_TOO_LARGE = "ERR_3003"
     INSUFFICIENT_STORAGE = "ERR_3004"
     RENDERING_TIMEOUT = "ERR_3005"
-    
+
     # AI æœå‹™éŒ¯èª¤ (4000-4999)
     AI_SERVICE_ERROR = "ERR_4001"
     MODEL_NOT_AVAILABLE = "ERR_4002"
     GENERATION_FAILED = "ERR_4003"
     QUOTA_EXCEEDED = "ERR_4004"
     INVALID_PROMPT = "ERR_4005"
-    
+
     # ç¤¾ç¾¤åª’é«”éŒ¯èª¤ (5000-5999)
     PLATFORM_API_ERROR = "ERR_5001"
     INVALID_API_KEY = "ERR_5002"
     UPLOAD_FAILED = "ERR_5003"
     PLATFORM_RATE_LIMIT = "ERR_5004"
     CONTENT_POLICY_VIOLATION = "ERR_5005"
-    
+
     # è³‡æ–™åº«éŒ¯èª¤ (6000-6999)
     DATABASE_CONNECTION_ERROR = "ERR_6001"
     DATABASE_TRANSACTION_ERROR = "ERR_6002"
     DATA_INTEGRITY_ERROR = "ERR_6003"
     MIGRATION_ERROR = "ERR_6004"
-    
+
     # å¤–éƒ¨æœå‹™éŒ¯èª¤ (7000-7999)
     EXTERNAL_API_ERROR = "ERR_7001"
     NETWORK_TIMEOUT = "ERR_7002"
@@ -70,6 +70,7 @@ class ErrorCode(Enum):
 
 class ErrorSeverity(Enum):
     """éŒ¯èª¤åš´é‡ç¨‹åº¦"""
+
     LOW = "low"
     MEDIUM = "medium"
     HIGH = "high"
@@ -78,6 +79,7 @@ class ErrorSeverity(Enum):
 
 class ErrorCategory(Enum):
     """éŒ¯èª¤åˆ†é¡"""
+
     USER_ERROR = "user_error"
     SYSTEM_ERROR = "system_error"
     EXTERNAL_ERROR = "external_error"
@@ -86,6 +88,7 @@ class ErrorCategory(Enum):
 
 class ErrorDetail(BaseModel):
     """éŒ¯èª¤è©³æƒ…"""
+
     field: Optional[str] = None
     message: str
     code: Optional[str] = None
@@ -93,7 +96,7 @@ class ErrorDetail(BaseModel):
 
 class UnifiedError(BaseModel):
     """çµ±ä¸€éŒ¯èª¤æ ¼å¼"""
-    
+
     code: str
     message: str
     details: Optional[List[ErrorDetail]] = None
@@ -107,11 +110,11 @@ class UnifiedError(BaseModel):
     path: Optional[str] = None
     stack_trace: Optional[str] = None
     context: Optional[Dict[str, Any]] = None
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
         return self.dict(exclude_none=True)
-    
+
     def to_json(self) -> str:
         """è½‰æ›ç‚º JSON æ ¼å¼"""
         return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)
@@ -119,93 +122,95 @@ class UnifiedError(BaseModel):
 
 class ErrorBuilder:
     """éŒ¯èª¤å»ºæ§‹å™¨"""
-    
+
     def __init__(self):
         self.error_data = {
             "timestamp": datetime.now().isoformat(),
             "severity": ErrorSeverity.MEDIUM.value,
-            "category": ErrorCategory.SYSTEM_ERROR.value
+            "category": ErrorCategory.SYSTEM_ERROR.value,
         }
-    
-    def code(self, error_code: Union[ErrorCode, str]) -> 'ErrorBuilder':
+
+    def code(self, error_code: Union[ErrorCode, str]) -> "ErrorBuilder":
         """è¨­ç½®éŒ¯èª¤ä»£ç¢¼"""
         if isinstance(error_code, ErrorCode):
             self.error_data["code"] = error_code.value
         else:
             self.error_data["code"] = error_code
         return self
-    
-    def message(self, message: str) -> 'ErrorBuilder':
+
+    def message(self, message: str) -> "ErrorBuilder":
         """è¨­ç½®éŒ¯èª¤æ¶ˆæ¯"""
         self.error_data["message"] = message
         return self
-    
-    def details(self, details: List[ErrorDetail]) -> 'ErrorBuilder':
+
+    def details(self, details: List[ErrorDetail]) -> "ErrorBuilder":
         """è¨­ç½®éŒ¯èª¤è©³æƒ…"""
         self.error_data["details"] = details
         return self
-    
-    def add_detail(self, field: str = None, message: str = "", code: str = None) -> 'ErrorBuilder':
+
+    def add_detail(
+        self, field: str = None, message: str = "", code: str = None
+    ) -> "ErrorBuilder":
         """æ·»åŠ éŒ¯èª¤è©³æƒ…"""
         if "details" not in self.error_data:
             self.error_data["details"] = []
-        
+
         detail = ErrorDetail(field=field, message=message, code=code)
         self.error_data["details"].append(detail)
         return self
-    
-    def severity(self, severity: ErrorSeverity) -> 'ErrorBuilder':
+
+    def severity(self, severity: ErrorSeverity) -> "ErrorBuilder":
         """è¨­ç½®åš´é‡ç¨‹åº¦"""
         self.error_data["severity"] = severity.value
         return self
-    
-    def category(self, category: ErrorCategory) -> 'ErrorBuilder':
+
+    def category(self, category: ErrorCategory) -> "ErrorBuilder":
         """è¨­ç½®éŒ¯èª¤åˆ†é¡"""
         self.error_data["category"] = category.value
         return self
-    
-    def request_id(self, request_id: str) -> 'ErrorBuilder':
+
+    def request_id(self, request_id: str) -> "ErrorBuilder":
         """è¨­ç½®è«‹æ±‚ ID"""
         self.error_data["request_id"] = request_id
         return self
-    
-    def user_id(self, user_id: str) -> 'ErrorBuilder':
+
+    def user_id(self, user_id: str) -> "ErrorBuilder":
         """è¨­ç½®ç”¨æˆ¶ ID"""
         self.error_data["user_id"] = user_id
         return self
-    
-    def service(self, service: str) -> 'ErrorBuilder':
+
+    def service(self, service: str) -> "ErrorBuilder":
         """è¨­ç½®æœå‹™åç¨±"""
         self.error_data["service"] = service
         return self
-    
-    def method(self, method: str) -> 'ErrorBuilder':
+
+    def method(self, method: str) -> "ErrorBuilder":
         """è¨­ç½®æ–¹æ³•åç¨±"""
         self.error_data["method"] = method
         return self
-    
-    def path(self, path: str) -> 'ErrorBuilder':
+
+    def path(self, path: str) -> "ErrorBuilder":
         """è¨­ç½®è«‹æ±‚è·¯å¾‘"""
         self.error_data["path"] = path
         return self
-    
-    def context(self, context: Dict[str, Any]) -> 'ErrorBuilder':
+
+    def context(self, context: Dict[str, Any]) -> "ErrorBuilder":
         """è¨­ç½®ä¸Šä¸‹æ–‡ä¿¡æ¯"""
         self.error_data["context"] = context
         return self
-    
-    def add_context(self, key: str, value: Any) -> 'ErrorBuilder':
+
+    def add_context(self, key: str, value: Any) -> "ErrorBuilder":
         """æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯"""
         if "context" not in self.error_data:
             self.error_data["context"] = {}
         self.error_data["context"][key] = value
         return self
-    
-    def with_stack_trace(self) -> 'ErrorBuilder':
+
+    def with_stack_trace(self) -> "ErrorBuilder":
         """æ·»åŠ å †ç–Šè¿½è¹¤"""
         self.error_data["stack_trace"] = traceback.format_exc()
         return self
-    
+
     def build(self) -> UnifiedError:
         """å»ºæ§‹éŒ¯èª¤å°è±¡"""
         return UnifiedError(**self.error_data)
@@ -213,79 +218,99 @@ class ErrorBuilder:
 
 class ErrorHandler:
     """éŒ¯èª¤è™•ç†å™¨"""
-    
+
     def __init__(self, service_name: str, logger: logging.Logger = None):
         self.service_name = service_name
         self.logger = logger or logging.getLogger(__name__)
-    
-    def handle_error(self, error: Exception, request_id: str = None, 
-                    user_id: str = None, context: Dict[str, Any] = None) -> UnifiedError:
+
+    def handle_error(
+        self,
+        error: Exception,
+        request_id: str = None,
+        user_id: str = None,
+        context: Dict[str, Any] = None,
+    ) -> UnifiedError:
         """è™•ç†ç•°å¸¸ä¸¦è½‰æ›ç‚ºçµ±ä¸€éŒ¯èª¤æ ¼å¼"""
-        
+
         builder = ErrorBuilder().service(self.service_name)
-        
+
         if request_id:
             builder.request_id(request_id)
-        
+
         if user_id:
             builder.user_id(user_id)
-        
+
         if context:
             builder.context(context)
-        
+
         # æ ¹æ“šç•°å¸¸é¡å‹è¨­ç½®éŒ¯èª¤ä¿¡æ¯
         if isinstance(error, ValueError):
-            builder.code(ErrorCode.VALIDATION_ERROR).message(str(error)).category(ErrorCategory.USER_ERROR)
+            builder.code(ErrorCode.VALIDATION_ERROR).message(
+                str(error)
+            ).category(ErrorCategory.USER_ERROR)
         elif isinstance(error, FileNotFoundError):
-            builder.code(ErrorCode.NOT_FOUND).message("è³‡æºæœªæ‰¾åˆ°").category(ErrorCategory.USER_ERROR)
+            builder.code(ErrorCode.NOT_FOUND).message("è³‡æºæœªæ‰¾åˆ°").category(
+                ErrorCategory.USER_ERROR
+            )
         elif isinstance(error, PermissionError):
-            builder.code(ErrorCode.AUTHORIZATION_ERROR).message("æ¬Šé™ä¸è¶³").category(ErrorCategory.USER_ERROR)
+            builder.code(ErrorCode.AUTHORIZATION_ERROR).message(
+                "æ¬Šé™ä¸è¶³"
+            ).category(ErrorCategory.USER_ERROR)
         elif isinstance(error, ConnectionError):
-            builder.code(ErrorCode.EXTERNAL_API_ERROR).message("å¤–éƒ¨æœå‹™é€£æ¥å¤±æ•—").category(ErrorCategory.EXTERNAL_ERROR).severity(ErrorSeverity.HIGH)
+            builder.code(ErrorCode.EXTERNAL_API_ERROR).message(
+                "å¤–éƒ¨æœå‹™é€£æ¥å¤±æ•—"
+            ).category(ErrorCategory.EXTERNAL_ERROR).severity(
+                ErrorSeverity.HIGH
+            )
         elif isinstance(error, TimeoutError):
-            builder.code(ErrorCode.SERVICE_TIMEOUT).message("æœå‹™è¶…æ™‚").category(ErrorCategory.SYSTEM_ERROR).severity(ErrorSeverity.HIGH)
+            builder.code(ErrorCode.SERVICE_TIMEOUT).message(
+                "æœå‹™è¶…æ™‚"
+            ).category(ErrorCategory.SYSTEM_ERROR).severity(ErrorSeverity.HIGH)
         else:
-            builder.code(ErrorCode.UNKNOWN_ERROR).message(f"æœªçŸ¥éŒ¯èª¤: {str(error)}").severity(ErrorSeverity.HIGH)
-        
+            builder.code(ErrorCode.UNKNOWN_ERROR).message(
+                f"æœªçŸ¥éŒ¯èª¤: {str(error)}"
+            ).severity(ErrorSeverity.HIGH)
+
         # åœ¨é–‹ç™¼æ¨¡å¼ä¸‹æ·»åŠ å †ç–Šè¿½è¹¤
         import os
+
         if os.getenv("DEBUG", "false").lower() == "true":
             builder.with_stack_trace()
-        
+
         unified_error = builder.build()
-        
+
         # è¨˜éŒ„éŒ¯èª¤
         self._log_error(unified_error)
-        
+
         return unified_error
-    
+
     def _log_error(self, error: UnifiedError):
         """è¨˜éŒ„éŒ¯èª¤"""
         log_level = self._get_log_level(error.severity)
-        
+
         log_message = f"[{error.code}] {error.message}"
-        
+
         extra_data = {
             "error_code": error.code,
             "severity": error.severity,
             "category": error.category,
             "service": error.service,
             "request_id": error.request_id,
-            "user_id": error.user_id
+            "user_id": error.user_id,
         }
-        
+
         self.logger.log(log_level, log_message, extra=extra_data)
-    
+
     def _get_log_level(self, severity: str) -> int:
         """æ ¹æ“šåš´é‡ç¨‹åº¦ç²å–æ—¥èªŒç´šåˆ¥"""
         mapping = {
             ErrorSeverity.LOW.value: logging.INFO,
             ErrorSeverity.MEDIUM.value: logging.WARNING,
             ErrorSeverity.HIGH.value: logging.ERROR,
-            ErrorSeverity.CRITICAL.value: logging.CRITICAL
+            ErrorSeverity.CRITICAL.value: logging.CRITICAL,
         }
         return mapping.get(severity, logging.ERROR)
-    
+
     def to_http_exception(self, error: UnifiedError) -> HTTPException:
         """è½‰æ›ç‚º FastAPI HTTP ç•°å¸¸"""
         status_code_mapping = {
@@ -297,14 +322,13 @@ class ErrorHandler:
             ErrorCode.RATE_LIMIT_EXCEEDED.value: status.HTTP_429_TOO_MANY_REQUESTS,
             ErrorCode.SERVICE_UNAVAILABLE.value: status.HTTP_503_SERVICE_UNAVAILABLE,
         }
-        
-        status_code = status_code_mapping.get(error.code, status.HTTP_500_INTERNAL_SERVER_ERROR)
-        
-        return HTTPException(
-            status_code=status_code,
-            detail=error.to_dict()
+
+        status_code = status_code_mapping.get(
+            error.code, status.HTTP_500_INTERNAL_SERVER_ERROR
         )
 
+        return HTTPException(status_code=status_code, detail=error.to_dict())
+
 
 # å…¨åŸŸéŒ¯èª¤è™•ç†å™¨å¯¦ä¾‹
 _global_handlers: Dict[str, ErrorHandler] = {}
@@ -328,7 +352,9 @@ def create_error(error_code: ErrorCode, message: str = None) -> ErrorBuilder:
 # å¸¸ç”¨éŒ¯èª¤å¿«æ·å‡½æ•¸
 def validation_error(message: str, field: str = None) -> UnifiedError:
     """å‰µå»ºé©—è­‰éŒ¯èª¤"""
-    builder = create_error(ErrorCode.VALIDATION_ERROR, message).category(ErrorCategory.USER_ERROR)
+    builder = create_error(ErrorCode.VALIDATION_ERROR, message).category(
+        ErrorCategory.USER_ERROR
+    )
     if field:
         builder.add_detail(field=field, message=message)
     return builder.build()
@@ -336,71 +362,79 @@ def validation_error(message: str, field: str = None) -> UnifiedError:
 
 def not_found_error(resource: str) -> UnifiedError:
     """å‰µå»ºè³‡æºæœªæ‰¾åˆ°éŒ¯èª¤"""
-    return create_error(
-        ErrorCode.NOT_FOUND, 
-        f"{resource} æœªæ‰¾åˆ°"
-    ).category(ErrorCategory.USER_ERROR).build()
+    return (
+        create_error(ErrorCode.NOT_FOUND, f"{resource} æœªæ‰¾åˆ°")
+        .category(ErrorCategory.USER_ERROR)
+        .build()
+    )
 
 
 def unauthorized_error(message: str = "æœªæˆæ¬Šè¨ªå•") -> UnifiedError:
     """å‰µå»ºæœªæˆæ¬ŠéŒ¯èª¤"""
-    return create_error(
-        ErrorCode.AUTHORIZATION_ERROR, 
-        message
-    ).category(ErrorCategory.USER_ERROR).build()
+    return (
+        create_error(ErrorCode.AUTHORIZATION_ERROR, message)
+        .category(ErrorCategory.USER_ERROR)
+        .build()
+    )
 
 
 def internal_error(message: str = "å…§éƒ¨æœå‹™éŒ¯èª¤") -> UnifiedError:
     """å‰µå»ºå…§éƒ¨éŒ¯èª¤"""
-    return create_error(
-        ErrorCode.UNKNOWN_ERROR, 
-        message
-    ).severity(ErrorSeverity.HIGH).build()
+    return (
+        create_error(ErrorCode.UNKNOWN_ERROR, message)
+        .severity(ErrorSeverity.HIGH)
+        .build()
+    )
 
 
 def external_service_error(service: str, message: str = None) -> UnifiedError:
     """å‰µå»ºå¤–éƒ¨æœå‹™éŒ¯èª¤"""
     if message is None:
         message = f"{service} æœå‹™æš«æ™‚ä¸å¯ç”¨"
-    
-    return create_error(
-        ErrorCode.EXTERNAL_API_ERROR, 
-        message
-    ).category(ErrorCategory.EXTERNAL_ERROR).severity(ErrorSeverity.HIGH).add_context("external_service", service).build()
+
+    return (
+        create_error(ErrorCode.EXTERNAL_API_ERROR, message)
+        .category(ErrorCategory.EXTERNAL_ERROR)
+        .severity(ErrorSeverity.HIGH)
+        .add_context("external_service", service)
+        .build()
+    )
 
 
 # éŒ¯èª¤çµ±è¨ˆå’Œç›£æ§
 class ErrorMetrics:
     """éŒ¯èª¤æŒ‡æ¨™æ”¶é›†"""
-    
+
     def __init__(self):
         self.error_counts = {}
         self.error_history = []
-    
+
     def record_error(self, error: UnifiedError):
         """è¨˜éŒ„éŒ¯èª¤"""
         # çµ±è¨ˆéŒ¯èª¤è¨ˆæ•¸
         key = f"{error.service}:{error.code}"
         self.error_counts[key] = self.error_counts.get(key, 0) + 1
-        
+
         # ä¿å­˜éŒ¯èª¤æ­·å² (æœ€è¿‘1000æ¢)
-        self.error_history.append({
-            "timestamp": error.timestamp,
-            "service": error.service,
-            "code": error.code,
-            "severity": error.severity,
-            "category": error.category
-        })
-        
+        self.error_history.append(
+            {
+                "timestamp": error.timestamp,
+                "service": error.service,
+                "code": error.code,
+                "severity": error.severity,
+                "category": error.category,
+            }
+        )
+
         if len(self.error_history) > 1000:
             self.error_history = self.error_history[-1000:]
-    
+
     def get_error_stats(self) -> Dict[str, Any]:
         """ç²å–éŒ¯èª¤çµ±è¨ˆ"""
         return {
             "total_errors": len(self.error_history),
             "error_counts": self.error_counts,
-            "recent_errors": self.error_history[-10:]  # æœ€è¿‘10æ¢
+            "recent_errors": self.error_history[-10:],  # æœ€è¿‘10æ¢
         }
 
 
@@ -411,30 +445,36 @@ error_metrics = ErrorMetrics()
 if __name__ == "__main__":
     # æ¸¬è©¦éŒ¯èª¤è™•ç†ç³»çµ±
     print("=== çµ±ä¸€éŒ¯èª¤è™•ç†ç³»çµ±æ¸¬è©¦ ===")
-    
+
     # æ¸¬è©¦éŒ¯èª¤å»ºæ§‹å™¨
-    error = create_error(ErrorCode.VALIDATION_ERROR, "ç”¨æˆ¶åä¸èƒ½ç‚ºç©º").add_detail(
-        field="username", message="ç”¨æˆ¶åæ˜¯å¿…å¡«æ¬„ä½"
-    ).severity(ErrorSeverity.MEDIUM).category(ErrorCategory.USER_ERROR).build()
-    
+    error = (
+        create_error(ErrorCode.VALIDATION_ERROR, "ç”¨æˆ¶åä¸èƒ½ç‚ºç©º")
+        .add_detail(field="username", message="ç”¨æˆ¶åæ˜¯å¿…å¡«æ¬„ä½")
+        .severity(ErrorSeverity.MEDIUM)
+        .category(ErrorCategory.USER_ERROR)
+        .build()
+    )
+
     print("å»ºæ§‹çš„éŒ¯èª¤:")
     print(error.to_json())
-    
+
     # æ¸¬è©¦éŒ¯èª¤è™•ç†å™¨
     handler = ErrorHandler("test-service")
-    
+
     try:
         raise ValueError("æ¸¬è©¦éŒ¯èª¤")
     except Exception as e:
-        handled_error = handler.handle_error(e, request_id="req-123", user_id="user-456")
+        handled_error = handler.handle_error(
+            e, request_id="req-123", user_id="user-456"
+        )
         print("\nè™•ç†å¾Œçš„éŒ¯èª¤:")
         print(handled_error.to_json())
-    
+
     # æ¸¬è©¦å¿«æ·å‡½æ•¸
     validation_err = validation_error("å¯†ç¢¼å¤ªçŸ­", "password")
     print("\né©—è­‰éŒ¯èª¤:")
     print(validation_err.to_json())
-    
+
     not_found_err = not_found_error("ç”¨æˆ¶")
     print("\næœªæ‰¾åˆ°éŒ¯èª¤:")
-    print(not_found_err.to_json())
\ No newline at end of file
+    print(not_found_err.to_json())
diff --git a/auto_generate_video_fold6/test_docker_containerization.py b/auto_generate_video_fold6/test_docker_containerization.py
index 9b62c93..384a825 100644
--- a/auto_generate_video_fold6/test_docker_containerization.py
+++ b/auto_generate_video_fold6/test_docker_containerization.py
@@ -19,17 +19,14 @@ import logging
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 class DockerContainerizationTest:
     """Docker å®¹å™¨åŒ– TDD æ¸¬è©¦å¥—ä»¶"""
-    
+
     def __init__(self):
         self.project_root = Path(__file__).parent
-        self.results = {
-            "tests_passed": 0,
-            "tests_failed": 0,
-            "errors": []
-        }
-    
+        self.results = {"tests_passed": 0, "tests_failed": 0, "errors": []}
+
     def _record_result(self, test_name: str, success: bool, error: str = None):
         """è¨˜éŒ„æ¸¬è©¦çµæœ"""
         if success:
@@ -39,145 +36,205 @@ class DockerContainerizationTest:
             self.results["tests_failed"] += 1
             self.results["errors"].append(f"{test_name}: {error}")
             logger.error(f"âŒ {test_name} å¤±æ•—: {error}")
-    
+
     def test_dockerfile_exists_and_multistage(self):
         """æ¸¬è©¦ Dockerfile å­˜åœ¨ä¸”ç‚ºå¤šéšæ®µæ§‹å»º"""
         try:
             # æª¢æŸ¥å„æœå‹™çš„ Dockerfile
             expected_dockerfiles = [
                 "services/trend-service/Dockerfile",
-                "services/video-service/Dockerfile", 
+                "services/video-service/Dockerfile",
                 "services/social-service/Dockerfile",
                 "services/scheduler-service/Dockerfile",
-                "frontend/Dockerfile"
+                "frontend/Dockerfile",
             ]
-            
+
             for dockerfile_path in expected_dockerfiles:
                 full_path = self.project_root / dockerfile_path
-                assert full_path.exists(), f"Dockerfile ä¸å­˜åœ¨: {dockerfile_path}"
-                
+                assert full_path.exists(), (
+                    f"Dockerfile ä¸å­˜åœ¨: {dockerfile_path}"
+                )
+
                 # è®€å– Dockerfile å…§å®¹
                 content = full_path.read_text()
-                
+
                 # æª¢æŸ¥å¤šéšæ®µæ§‹å»ºæ¨™è¨˜
                 stages = content.count("FROM ")
-                assert stages >= 2, f"Dockerfile {dockerfile_path} ä¸æ˜¯å¤šéšæ®µæ§‹å»º (åªæœ‰ {stages} å€‹ FROM)"
-                
+                assert stages >= 2, (
+                    f"Dockerfile {dockerfile_path} ä¸æ˜¯å¤šéšæ®µæ§‹å»º (åªæœ‰ {stages} å€‹ FROM)"
+                )
+
                 # æª¢æŸ¥å¿…è¦çš„æ§‹å»ºéšæ®µï¼ˆä¸å€åˆ†å¤§å°å¯«ï¼‰
                 content_lower = content.lower()
-                assert "as builder" in content_lower or "as build" in content_lower, f"ç¼ºå°‘ builder éšæ®µ: {dockerfile_path}"
-                assert "as runtime" in content_lower or "as production" in content_lower, f"ç¼ºå°‘ runtime éšæ®µ: {dockerfile_path}"
-            
+                assert (
+                    "as builder" in content_lower
+                    or "as build" in content_lower
+                ), f"ç¼ºå°‘ builder éšæ®µ: {dockerfile_path}"
+                assert (
+                    "as runtime" in content_lower
+                    or "as production" in content_lower
+                ), f"ç¼ºå°‘ runtime éšæ®µ: {dockerfile_path}"
+
             self._record_result("dockerfile_multistage_structure", True)
-            
+
         except Exception as e:
-            self._record_result("dockerfile_multistage_structure", False, str(e))
-    
+            self._record_result(
+                "dockerfile_multistage_structure", False, str(e)
+            )
+
     def test_docker_compose_production_config(self):
         """æ¸¬è©¦ç”Ÿç”¢ç´š Docker Compose é…ç½®"""
         try:
             compose_files = [
                 "docker-compose.yml",
                 "docker-compose.prod.yml",
-                "docker-compose.dev.yml"
+                "docker-compose.dev.yml",
             ]
-            
+
             for compose_file in compose_files:
                 full_path = self.project_root / compose_file
-                assert full_path.exists(), f"Docker Compose æ–‡ä»¶ä¸å­˜åœ¨: {compose_file}"
-                
+                assert full_path.exists(), (
+                    f"Docker Compose æ–‡ä»¶ä¸å­˜åœ¨: {compose_file}"
+                )
+
                 # è§£æ YAML é…ç½®
-                with open(full_path, 'r') as f:
+                with open(full_path, "r") as f:
                     config = yaml.safe_load(f)
-                
+
                 # æª¢æŸ¥æœå‹™å®šç¾©
-                assert "services" in config, f"ç¼ºå°‘ services å®šç¾©: {compose_file}"
-                
+                assert "services" in config, (
+                    f"ç¼ºå°‘ services å®šç¾©: {compose_file}"
+                )
+
                 expected_services = [
-                    "trend-service", "video-service", "social-service", 
-                    "scheduler-service", "frontend", "postgres", "redis"
+                    "trend-service",
+                    "video-service",
+                    "social-service",
+                    "scheduler-service",
+                    "frontend",
+                    "postgres",
+                    "redis",
                 ]
-                
+
                 for service in expected_services:
-                    assert service in config["services"], f"ç¼ºå°‘æœå‹™å®šç¾©: {service} in {compose_file}"
-                
+                    assert service in config["services"], (
+                        f"ç¼ºå°‘æœå‹™å®šç¾©: {service} in {compose_file}"
+                    )
+
                 # æª¢æŸ¥ç”Ÿç”¢é…ç½®ç‰¹å®šè¦æ±‚
                 if "prod" in compose_file:
                     # æª¢æŸ¥è³‡æºé™åˆ¶
-                    for service_name, service_config in config["services"].items():
-                        if service_name in ["trend-service", "video-service", "social-service", "scheduler-service"]:
-                            assert "deploy" in service_config, f"ç”Ÿç”¢æœå‹™ç¼ºå°‘ deploy é…ç½®: {service_name}"
-                            assert "resources" in service_config["deploy"], f"ç¼ºå°‘è³‡æºé™åˆ¶: {service_name}"
-            
+                    for service_name, service_config in config[
+                        "services"
+                    ].items():
+                        if service_name in [
+                            "trend-service",
+                            "video-service",
+                            "social-service",
+                            "scheduler-service",
+                        ]:
+                            assert "deploy" in service_config, (
+                                f"ç”Ÿç”¢æœå‹™ç¼ºå°‘ deploy é…ç½®: {service_name}"
+                            )
+                            assert "resources" in service_config["deploy"], (
+                                f"ç¼ºå°‘è³‡æºé™åˆ¶: {service_name}"
+                            )
+
             self._record_result("docker_compose_production_config", True)
-            
+
         except Exception as e:
-            self._record_result("docker_compose_production_config", False, str(e))
-    
+            self._record_result(
+                "docker_compose_production_config", False, str(e)
+            )
+
     def test_dockerfile_optimization_practices(self):
         """æ¸¬è©¦ Dockerfile å„ªåŒ–å¯¦è¸"""
         try:
             dockerfile_paths = [
                 "services/trend-service/Dockerfile",
-                "services/video-service/Dockerfile"
+                "services/video-service/Dockerfile",
             ]
-            
+
             for dockerfile_path in dockerfile_paths:
                 full_path = self.project_root / dockerfile_path
                 if not full_path.exists():
                     continue
-                    
+
                 content = full_path.read_text()
-                
+
                 # æª¢æŸ¥å„ªåŒ–å¯¦è¸
                 optimizations = {
                     "ä½¿ç”¨ .dockerignore": True,  # å°‡åœ¨å¾ŒçºŒæª¢æŸ¥
-                    "å¤šéšæ®µæ§‹å»ºæ¸›å°‘æ˜ åƒå¤§å°": "FROM python:3.11-slim" in content,
-                    "åˆä½µ RUN æŒ‡ä»¤": content.count("RUN") <= 5,  # é™åˆ¶ RUN æŒ‡ä»¤æ•¸é‡
-                    "æ¸…ç†å¿«å–": "rm -rf" in content or "apt-get clean" in content,
+                    "å¤šéšæ®µæ§‹å»ºæ¸›å°‘æ˜ åƒå¤§å°": "FROM python:3.11-slim"
+                    in content,
+                    "åˆä½µ RUN æŒ‡ä»¤": content.count("RUN")
+                    <= 5,  # é™åˆ¶ RUN æŒ‡ä»¤æ•¸é‡
+                    "æ¸…ç†å¿«å–": "rm -rf" in content
+                    or "apt-get clean" in content,
                     "é root ç”¨æˆ¶": "USER " in content,
-                    "å¥åº·æª¢æŸ¥": "HEALTHCHECK" in content
+                    "å¥åº·æª¢æŸ¥": "HEALTHCHECK" in content,
                 }
-                
+
                 for optimization, passed in optimizations.items():
                     if optimization == "ä½¿ç”¨ .dockerignore":
                         dockerignore_path = full_path.parent / ".dockerignore"
                         passed = dockerignore_path.exists()
-                    
+
                     if not passed:
-                        logger.warning(f"å„ªåŒ–å»ºè­°: {dockerfile_path} - {optimization}")
-            
+                        logger.warning(
+                            f"å„ªåŒ–å»ºè­°: {dockerfile_path} - {optimization}"
+                        )
+
             self._record_result("dockerfile_optimization_practices", True)
-            
+
         except Exception as e:
-            self._record_result("dockerfile_optimization_practices", False, str(e))
-    
+            self._record_result(
+                "dockerfile_optimization_practices", False, str(e)
+            )
+
     def test_container_security_practices(self):
         """æ¸¬è©¦å®¹å™¨å®‰å…¨å¯¦è¸"""
         try:
             security_checks = []
-            
+
             # æª¢æŸ¥ .dockerignore æ–‡ä»¶
-            for service_dir in ["services/trend-service", "services/video-service", "frontend"]:
-                dockerignore_path = self.project_root / service_dir / ".dockerignore"
+            for service_dir in [
+                "services/trend-service",
+                "services/video-service",
+                "frontend",
+            ]:
+                dockerignore_path = (
+                    self.project_root / service_dir / ".dockerignore"
+                )
                 if dockerignore_path.exists():
                     content = dockerignore_path.read_text()
-                    
+
                     # æª¢æŸ¥æ•æ„Ÿæ–‡ä»¶æ’é™¤
-                    sensitive_patterns = [".env", "*.key", "*.pem", ".git", "node_modules", "__pycache__"]
+                    sensitive_patterns = [
+                        ".env",
+                        "*.key",
+                        "*.pem",
+                        ".git",
+                        "node_modules",
+                        "__pycache__",
+                    ]
                     for pattern in sensitive_patterns:
                         if pattern not in content:
-                            security_checks.append(f"ç¼ºå°‘æ’é™¤æ¨¡å¼: {pattern} in {dockerignore_path}")
-            
+                            security_checks.append(
+                                f"ç¼ºå°‘æ’é™¤æ¨¡å¼: {pattern} in {dockerignore_path}"
+                            )
+
             # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸å®‰å…¨
             compose_path = self.project_root / "docker-compose.prod.yml"
             if compose_path.exists():
-                with open(compose_path, 'r') as f:
+                with open(compose_path, "r") as f:
                     config = yaml.safe_load(f)
-                
-                for service_name, service_config in config.get("services", {}).items():
+
+                for service_name, service_config in config.get(
+                    "services", {}
+                ).items():
                     env_vars = service_config.get("environment", {})
-                    
+
                     # è™•ç†ç’°å¢ƒè®Šæ•¸æ ¼å¼ï¼ˆå¯èƒ½æ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼‰
                     if isinstance(env_vars, list):
                         # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œè·³éè©³ç´°æª¢æŸ¥
@@ -187,229 +244,305 @@ class DockerContainerizationTest:
                         sensitive_vars = ["PASSWORD", "SECRET", "KEY", "TOKEN"]
                         for var_name, var_value in env_vars.items():
                             for sensitive in sensitive_vars:
-                                if sensitive in var_name.upper() and not str(var_value).startswith("${"):
-                                    security_checks.append(f"ç¡¬ç·¨ç¢¼æ•æ„Ÿè®Šæ•¸: {var_name} in {service_name}")
-            
+                                if sensitive in var_name.upper() and not str(
+                                    var_value
+                                ).startswith("${"):
+                                    security_checks.append(
+                                        f"ç¡¬ç·¨ç¢¼æ•æ„Ÿè®Šæ•¸: {var_name} in {service_name}"
+                                    )
+
             # å¦‚æœæœ‰å®‰å…¨å•é¡Œï¼Œè¨˜éŒ„ä½†ä¸å¤±æ•—ï¼ˆå› ç‚ºé€™æ˜¯åˆå§‹æ¸¬è©¦ï¼‰
             if security_checks:
                 logger.warning(f"å®‰å…¨å»ºè­°: {security_checks}")
-            
+
             self._record_result("container_security_practices", True)
-            
+
         except Exception as e:
             self._record_result("container_security_practices", False, str(e))
-    
+
     def test_build_and_run_simulation(self):
         """æ¸¬è©¦æ§‹å»ºå’Œé‹è¡Œæ¨¡æ“¬ï¼ˆä¸å¯¦éš›æ§‹å»ºï¼‰"""
         try:
             # æ¨¡æ“¬æ§‹å»ºéç¨‹æª¢æŸ¥
             build_requirements = {
                 "Python å¾Œç«¯æœå‹™": {
-                    "requirements_files": ["requirements.txt", "requirements-prod.txt"],
+                    "requirements_files": [
+                        "requirements.txt",
+                        "requirements-prod.txt",
+                    ],
                     "app_files": ["main.py", "app.py"],
-                    "config_files": ["config/", "settings.py"]
+                    "config_files": ["config/", "settings.py"],
                 },
                 "å‰ç«¯æœå‹™": {
                     "package_files": ["package.json", "package-lock.json"],
                     "build_files": ["vite.config.js", "tailwind.config.js"],
-                    "source_files": ["src/", "public/"]
-                }
+                    "source_files": ["src/", "public/"],
+                },
             }
-            
+
             # æª¢æŸ¥æ§‹å»ºæ‰€éœ€æ–‡ä»¶
             missing_files = []
-            
+
             # æª¢æŸ¥å¾Œç«¯æœå‹™æ–‡ä»¶
-            for service_dir in ["services/trend-service", "services/video-service"]:
+            for service_dir in [
+                "services/trend-service",
+                "services/video-service",
+            ]:
                 service_path = self.project_root / service_dir
                 if service_path.exists():
-                    for req_file in build_requirements["Python å¾Œç«¯æœå‹™"]["requirements_files"]:
+                    for req_file in build_requirements["Python å¾Œç«¯æœå‹™"][
+                        "requirements_files"
+                    ]:
                         if not (service_path / req_file).exists():
                             missing_files.append(f"{service_dir}/{req_file}")
-            
+
             # æª¢æŸ¥å‰ç«¯æ–‡ä»¶
             frontend_path = self.project_root / "frontend"
             if frontend_path.exists():
-                for req_file in build_requirements["å‰ç«¯æœå‹™"]["package_files"]:
+                for req_file in build_requirements["å‰ç«¯æœå‹™"][
+                    "package_files"
+                ]:
                     if not (frontend_path / req_file).exists():
                         missing_files.append(f"frontend/{req_file}")
-            
+
             # è¨˜éŒ„ç¼ºå°‘çš„æ–‡ä»¶ï¼ˆä¸æœƒå°è‡´æ¸¬è©¦å¤±æ•—ï¼Œå› ç‚ºé€™æ˜¯ Red éšæ®µï¼‰
             if missing_files:
                 logger.info(f"å¾…å‰µå»ºçš„æ§‹å»ºæ–‡ä»¶: {missing_files}")
-            
+
             self._record_result("build_requirements_check", True)
-            
+
         except Exception as e:
             self._record_result("build_requirements_check", False, str(e))
-    
+
     def test_service_discovery_and_networking(self):
         """æ¸¬è©¦æœå‹™ç™¼ç¾å’Œç¶²è·¯é…ç½®"""
         try:
             compose_path = self.project_root / "docker-compose.yml"
-            
+
             # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¨˜éŒ„ç‚ºé æœŸå¤±æ•—
             if not compose_path.exists():
-                self._record_result("service_networking_config", False, "docker-compose.yml ä¸å­˜åœ¨ï¼ˆé æœŸå¤±æ•—ï¼‰")
+                self._record_result(
+                    "service_networking_config",
+                    False,
+                    "docker-compose.yml ä¸å­˜åœ¨ï¼ˆé æœŸå¤±æ•—ï¼‰",
+                )
                 return
-            
-            with open(compose_path, 'r') as f:
+
+            with open(compose_path, "r") as f:
                 config = yaml.safe_load(f)
-            
+
             # æª¢æŸ¥ç¶²è·¯é…ç½®
             networks = config.get("networks", {})
             assert "app-network" in networks, "ç¼ºå°‘æ‡‰ç”¨ç¶²è·¯å®šç¾©"
-            
+
             # æª¢æŸ¥æœå‹™ç¶²è·¯é€£æ¥
-            for service_name, service_config in config.get("services", {}).items():
-                if service_name not in ["postgres", "redis"]:  # æ’é™¤åŸºç¤è¨­æ–½æœå‹™
+            for service_name, service_config in config.get(
+                "services", {}
+            ).items():
+                if service_name not in [
+                    "postgres",
+                    "redis",
+                ]:  # æ’é™¤åŸºç¤è¨­æ–½æœå‹™
                     networks_config = service_config.get("networks", [])
-                    assert "app-network" in networks_config, f"æœå‹™ {service_name} æœªåŠ å…¥æ‡‰ç”¨ç¶²è·¯"
-            
+                    assert "app-network" in networks_config, (
+                        f"æœå‹™ {service_name} æœªåŠ å…¥æ‡‰ç”¨ç¶²è·¯"
+                    )
+
             # æª¢æŸ¥ç«¯å£æš´éœ²é…ç½®
             exposed_services = ["frontend", "trend-service", "video-service"]
             for service in exposed_services:
                 if service in config["services"]:
                     ports = config["services"][service].get("ports", [])
                     assert len(ports) > 0, f"æœå‹™ {service} æœªæš´éœ²ç«¯å£"
-            
+
             self._record_result("service_networking_config", True)
-            
+
         except Exception as e:
             self._record_result("service_networking_config", False, str(e))
-    
+
     def test_health_checks_and_monitoring(self):
         """æ¸¬è©¦å¥åº·æª¢æŸ¥å’Œç›£æ§é…ç½®"""
         try:
             compose_path = self.project_root / "docker-compose.yml"
-            
+
             if not compose_path.exists():
-                self._record_result("health_monitoring_config", False, "docker-compose.yml ä¸å­˜åœ¨ï¼ˆé æœŸå¤±æ•—ï¼‰")
+                self._record_result(
+                    "health_monitoring_config",
+                    False,
+                    "docker-compose.yml ä¸å­˜åœ¨ï¼ˆé æœŸå¤±æ•—ï¼‰",
+                )
                 return
-            
-            with open(compose_path, 'r') as f:
+
+            with open(compose_path, "r") as f:
                 config = yaml.safe_load(f)
-            
+
             # æª¢æŸ¥å¥åº·æª¢æŸ¥é…ç½®
-            app_services = ["trend-service", "video-service", "social-service", "scheduler-service"]
-            
+            app_services = [
+                "trend-service",
+                "video-service",
+                "social-service",
+                "scheduler-service",
+            ]
+
             for service in app_services:
                 if service in config["services"]:
                     service_config = config["services"][service]
                     healthcheck = service_config.get("healthcheck", {})
-                    
+
                     # æª¢æŸ¥å¥åº·æª¢æŸ¥é…ç½®
                     if not healthcheck:
                         logger.warning(f"æœå‹™ {service} ç¼ºå°‘å¥åº·æª¢æŸ¥é…ç½®")
                         continue
-                    
+
                     # æª¢æŸ¥å¥åº·æª¢æŸ¥åƒæ•¸
-                    required_params = ["test", "interval", "timeout", "retries"]
+                    required_params = [
+                        "test",
+                        "interval",
+                        "timeout",
+                        "retries",
+                    ]
                     for param in required_params:
                         if param not in healthcheck:
-                            logger.warning(f"æœå‹™ {service} å¥åº·æª¢æŸ¥ç¼ºå°‘åƒæ•¸: {param}")
-            
+                            logger.warning(
+                                f"æœå‹™ {service} å¥åº·æª¢æŸ¥ç¼ºå°‘åƒæ•¸: {param}"
+                            )
+
             # æª¢æŸ¥ç›£æ§æœå‹™é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
             monitoring_services = ["prometheus", "grafana", "jaeger"]
             for monitor_service in monitoring_services:
                 if monitor_service in config.get("services", {}):
                     logger.info(f"ç™¼ç¾ç›£æ§æœå‹™: {monitor_service}")
-            
+
             self._record_result("health_monitoring_config", True)
-            
+
         except Exception as e:
             self._record_result("health_monitoring_config", False, str(e))
-    
+
     def test_environment_configuration(self):
         """æ¸¬è©¦ç’°å¢ƒé…ç½®ç®¡ç†"""
         try:
             # æª¢æŸ¥ç’°å¢ƒé…ç½®æ–‡ä»¶
-            env_files = [".env.example", ".env.prod.example", ".env.dev.example"]
+            env_files = [
+                ".env.example",
+                ".env.prod.example",
+                ".env.dev.example",
+            ]
             existing_env_files = []
-            
+
             for env_file in env_files:
                 env_path = self.project_root / env_file
                 if env_path.exists():
                     existing_env_files.append(env_file)
-                    
+
                     # è®€å–ä¸¦æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
                     content = env_path.read_text()
-                    
+
                     # æª¢æŸ¥å¿…è¦çš„ç’°å¢ƒè®Šæ•¸
                     required_vars = [
-                        "DATABASE_URL", "REDIS_URL", "JWT_SECRET",
-                        "API_BASE_URL", "OPENAI_API_KEY"
+                        "DATABASE_URL",
+                        "REDIS_URL",
+                        "JWT_SECRET",
+                        "API_BASE_URL",
+                        "OPENAI_API_KEY",
                     ]
-                    
+
                     for var in required_vars:
                         if var not in content:
-                            logger.warning(f"ç’°å¢ƒæ–‡ä»¶ {env_file} ç¼ºå°‘è®Šæ•¸: {var}")
-            
+                            logger.warning(
+                                f"ç’°å¢ƒæ–‡ä»¶ {env_file} ç¼ºå°‘è®Šæ•¸: {var}"
+                            )
+
             # æª¢æŸ¥ Docker Compose ç’°å¢ƒè®Šæ•¸ä½¿ç”¨
             compose_files = ["docker-compose.yml", "docker-compose.prod.yml"]
             for compose_file in compose_files:
                 compose_path = self.project_root / compose_file
                 if compose_path.exists():
-                    with open(compose_path, 'r') as f:
+                    with open(compose_path, "r") as f:
                         config = yaml.safe_load(f)
-                    
+
                     # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸å¼•ç”¨æ ¼å¼
-                    for service_name, service_config in config.get("services", {}).items():
+                    for service_name, service_config in config.get(
+                        "services", {}
+                    ).items():
                         env_vars = service_config.get("environment", {})
-                        
+
                         # è™•ç†ä¸åŒçš„ç’°å¢ƒè®Šæ•¸æ ¼å¼
                         if isinstance(env_vars, dict):
                             for var_name, var_value in env_vars.items():
-                                if isinstance(var_value, str) and var_value.startswith("$"):
-                                    logger.info(f"ç™¼ç¾ç’°å¢ƒè®Šæ•¸å¼•ç”¨: {var_name}={var_value} in {service_name}")
+                                if isinstance(
+                                    var_value, str
+                                ) and var_value.startswith("$"):
+                                    logger.info(
+                                        f"ç™¼ç¾ç’°å¢ƒè®Šæ•¸å¼•ç”¨: {var_name}={var_value} in {service_name}"
+                                    )
                         elif isinstance(env_vars, list):
                             for env_item in env_vars:
-                                if isinstance(env_item, str) and "=" in env_item:
-                                    var_name, var_value = env_item.split("=", 1)
+                                if (
+                                    isinstance(env_item, str)
+                                    and "=" in env_item
+                                ):
+                                    var_name, var_value = env_item.split(
+                                        "=", 1
+                                    )
                                     if var_value.startswith("$"):
-                                        logger.info(f"ç™¼ç¾ç’°å¢ƒè®Šæ•¸å¼•ç”¨: {var_name}={var_value} in {service_name}")
-            
+                                        logger.info(
+                                            f"ç™¼ç¾ç’°å¢ƒè®Šæ•¸å¼•ç”¨: {var_name}={var_value} in {service_name}"
+                                        )
+
             # è¨˜éŒ„çµæœï¼ˆå…è¨±éƒ¨åˆ†ç¼ºå¤±ï¼Œå› ç‚ºé€™æ˜¯ Red éšæ®µï¼‰
             self._record_result("environment_configuration", True)
-            
+
         except Exception as e:
             self._record_result("environment_configuration", False, str(e))
-    
+
     def print_results(self):
         """æ‰“å°æ¸¬è©¦çµæœ"""
-        total_tests = self.results["tests_passed"] + self.results["tests_failed"]
-        success_rate = (self.results["tests_passed"] / total_tests * 100) if total_tests > 0 else 0
-        
+        total_tests = (
+            self.results["tests_passed"] + self.results["tests_failed"]
+        )
+        success_rate = (
+            (self.results["tests_passed"] / total_tests * 100)
+            if total_tests > 0
+            else 0
+        )
+
         logger.info("=" * 60)
         logger.info("ğŸ”´ TDD Red éšæ®µ: Docker å®¹å™¨åŒ–æ¸¬è©¦çµæœ")
         logger.info("=" * 60)
         logger.info(f"âœ… é€šéæ¸¬è©¦: {self.results['tests_passed']}")
         logger.info(f"âŒ å¤±æ•—æ¸¬è©¦: {self.results['tests_failed']}")
         logger.info(f"ğŸ“ˆ ç•¶å‰å®Œæˆç‡: {success_rate:.1f}%")
-        
+
         if self.results["errors"]:
             logger.info("\nğŸ¯ éœ€è¦å¯¦ä½œçš„åŠŸèƒ½:")
             for error in self.results["errors"]:
                 logger.info(f"  - {error}")
-        
+
         # Red éšæ®µè©•ä¼°
         if success_rate < 30:
-            logger.info("\nğŸ”´ TDD Red éšæ®µç‹€æ…‹: å®Œç¾ - å¤§éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œå®šç¾©äº†æ¸…æ™°çš„ç›®æ¨™")
+            logger.info(
+                "\nğŸ”´ TDD Red éšæ®µç‹€æ…‹: å®Œç¾ - å¤§éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œå®šç¾©äº†æ¸…æ™°çš„ç›®æ¨™"
+            )
         elif success_rate < 60:
-            logger.info("\nğŸŸ¡ TDD Red éšæ®µç‹€æ…‹: è‰¯å¥½ - æœ‰äº›åŸºç¤å·²å­˜åœ¨ï¼Œéœ€è¦æ›´å¤šå¯¦ä½œ")
+            logger.info(
+                "\nğŸŸ¡ TDD Red éšæ®µç‹€æ…‹: è‰¯å¥½ - æœ‰äº›åŸºç¤å·²å­˜åœ¨ï¼Œéœ€è¦æ›´å¤šå¯¦ä½œ"
+            )
         else:
-            logger.info("\nğŸŸ¢ TDD Red éšæ®µç‹€æ…‹: æ„å¤– - å¾ˆå¤šåŠŸèƒ½å·²å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦èª¿æ•´æ¸¬è©¦")
-        
+            logger.info(
+                "\nğŸŸ¢ TDD Red éšæ®µç‹€æ…‹: æ„å¤– - å¾ˆå¤šåŠŸèƒ½å·²å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦èª¿æ•´æ¸¬è©¦"
+            )
+
         return success_rate < 50  # Red éšæ®µæœŸæœ›ä½æˆåŠŸç‡
 
+
 def main():
     """åŸ·è¡Œ Docker å®¹å™¨åŒ– TDD Red éšæ®µæ¸¬è©¦"""
     logger.info("ğŸ”´ é–‹å§‹ TDD Red éšæ®µ: Docker å®¹å™¨åŒ–æ¸¬è©¦")
     logger.info("ç›®æ¨™: å®šç¾©ç”Ÿç”¢ç´š Docker å¤šéšæ®µæ§‹å»ºçš„æœŸæœ›è¡Œç‚º")
     logger.info("=" * 60)
-    
+
     test_suite = DockerContainerizationTest()
-    
+
     try:
         # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
         test_suite.test_dockerfile_exists_and_multistage()
@@ -420,10 +553,10 @@ def main():
         test_suite.test_service_discovery_and_networking()
         test_suite.test_health_checks_and_monitoring()
         test_suite.test_environment_configuration()
-        
+
         # æ‰“å°çµæœ
         is_proper_red = test_suite.print_results()
-        
+
         if is_proper_red:
             logger.info("\nğŸ‰ TDD Red éšæ®µæˆåŠŸï¼")
             logger.info("âœ¨ å·²å®šç¾©å®Œæ•´çš„ Docker å®¹å™¨åŒ–éœ€æ±‚")
@@ -431,19 +564,20 @@ def main():
         else:
             logger.info("\nğŸ¤” TDD Red éšæ®µæ„å¤–é€šéè¼ƒå¤šæ¸¬è©¦")
             logger.info("ğŸ”§ å¯èƒ½éœ€è¦èª¿æ•´æ¸¬è©¦æˆ–æª¢æŸ¥ç¾æœ‰å¯¦ä½œ")
-        
+
         return is_proper_red
-        
+
     except Exception as e:
         logger.error(f"âŒ Red éšæ®µæ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}")
         return False
 
+
 if __name__ == "__main__":
     success = main()
-    
+
     if success:
         logger.info("ğŸ TDD Red éšæ®µå®Œæˆ - Docker å®¹å™¨åŒ–éœ€æ±‚å·²å®šç¾©")
         exit(0)
     else:
         logger.error("ğŸ›‘ TDD Red éšæ®µéœ€è¦èª¿æ•´")
-        exit(1)
\ No newline at end of file
+        exit(1)
diff --git a/auto_generate_video_fold6/test_e2e_entrepreneur_workflow.py b/auto_generate_video_fold6/test_e2e_entrepreneur_workflow.py
index 75b58b2..3ebf8ea 100644
--- a/auto_generate_video_fold6/test_e2e_entrepreneur_workflow.py
+++ b/auto_generate_video_fold6/test_e2e_entrepreneur_workflow.py
@@ -30,9 +30,9 @@ logger = logging.getLogger(__name__)
 # æœå‹™ç«¯é»é…ç½®
 SERVICES = {
     "trend_service": "http://localhost:8001",
-    "video_service": "http://localhost:8003", 
+    "video_service": "http://localhost:8003",
     "social_service": "http://localhost:8004",
-    "scheduler_service": "http://localhost:8008"
+    "scheduler_service": "http://localhost:8008",
 }
 
 # æ¸¬è©¦é…ç½®
@@ -48,22 +48,23 @@ TEST_CONFIG = {
         "check_interval_minutes": 1,
         "categories": ["technology", "ai"],
         "platforms": ["tiktok", "youtube-shorts"],
-        "auto_publish": True
+        "auto_publish": True,
     },
     "expected_workflow_steps": [
         "trend_analysis",
-        "keyword_extraction", 
+        "keyword_extraction",
         "script_generation",
         "image_generation",
         "voice_synthesis",
         "video_composition",
-        "platform_publishing"
-    ]
+        "platform_publishing",
+    ],
 }
 
+
 class E2ETestResult:
     """E2E æ¸¬è©¦çµæœè¿½è¹¤"""
-    
+
     def __init__(self):
         self.start_time = datetime.utcnow()
         self.service_health_checks: Dict[str, bool] = {}
@@ -71,257 +72,315 @@ class E2ETestResult:
         self.errors: List[str] = []
         self.success_rate = 0.0
         self.total_execution_time = 0.0
-        
+
     def add_error(self, error: str):
         self.errors.append(f"[{datetime.utcnow().isoformat()}] {error}")
         logger.error(error)
-        
-    def add_step_result(self, step: str, success: bool, duration: float, details: Dict = None):
+
+    def add_step_result(
+        self, step: str, success: bool, duration: float, details: Dict = None
+    ):
         self.workflow_steps[step] = {
             "success": success,
             "duration": duration,
             "timestamp": datetime.utcnow().isoformat(),
-            "details": details or {}
+            "details": details or {},
         }
-        
+
     def finalize(self):
-        self.total_execution_time = (datetime.utcnow() - self.start_time).total_seconds()
-        successful_steps = sum(1 for step in self.workflow_steps.values() if step["success"])
-        self.success_rate = (successful_steps / len(self.workflow_steps)) * 100 if self.workflow_steps else 0
+        self.total_execution_time = (
+            datetime.utcnow() - self.start_time
+        ).total_seconds()
+        successful_steps = sum(
+            1 for step in self.workflow_steps.values() if step["success"]
+        )
+        self.success_rate = (
+            (successful_steps / len(self.workflow_steps)) * 100
+            if self.workflow_steps
+            else 0
+        )
 
 
 class E2ETestRunner:
     """ç«¯å°ç«¯æ¸¬è©¦åŸ·è¡Œå™¨"""
-    
+
     def __init__(self):
         self.session: Optional[aiohttp.ClientSession] = None
         self.result = E2ETestResult()
-        
+
     async def __aenter__(self):
         self.session = aiohttp.ClientSession(
             timeout=aiohttp.ClientTimeout(total=300)  # 5åˆ†é˜è¶…æ™‚
         )
         return self
-        
+
     async def __aexit__(self, exc_type, exc_val, exc_tb):
         if self.session:
             await self.session.close()
-            
+
     async def health_check_services(self) -> bool:
         """æª¢æŸ¥æ‰€æœ‰æœå‹™å¥åº·ç‹€æ…‹"""
         logger.info("ğŸ¥ é–‹å§‹æœå‹™å¥åº·æª¢æŸ¥...")
-        
+
         all_healthy = True
         for service_name, base_url in SERVICES.items():
             try:
                 async with self.session.get(f"{base_url}/health") as response:
                     is_healthy = response.status == 200
-                    self.result.service_health_checks[service_name] = is_healthy
-                    
+                    self.result.service_health_checks[service_name] = (
+                        is_healthy
+                    )
+
                     if is_healthy:
                         logger.info(f"âœ… {service_name} å¥åº·ç‹€æ…‹æ­£å¸¸")
                     else:
-                        logger.error(f"âŒ {service_name} å¥åº·æª¢æŸ¥å¤±æ•— (HTTP {response.status})")
+                        logger.error(
+                            f"âŒ {service_name} å¥åº·æª¢æŸ¥å¤±æ•— (HTTP {response.status})"
+                        )
                         all_healthy = False
-                        
+
             except Exception as e:
                 self.result.service_health_checks[service_name] = False
                 self.result.add_error(f"{service_name} é€£æ¥å¤±æ•—: {str(e)}")
                 all_healthy = False
-                
+
         return all_healthy
-        
+
     async def test_trend_analysis_flow(self) -> Dict[str, Any]:
         """æ¸¬è©¦è¶¨å‹¢åˆ†ææµç¨‹"""
         logger.info("ğŸ“ˆ æ¸¬è©¦è¶¨å‹¢åˆ†ææµç¨‹...")
         start_time = time.time()
-        
+
         try:
             # 1. è§¸ç™¼è¶¨å‹¢æŠ“å–
-            trend_url = f"{SERVICES['trend_service']}/api/v1/entrepreneur/fetch-trends"
+            trend_url = (
+                f"{SERVICES['trend_service']}/api/v1/entrepreneur/fetch-trends"
+            )
             trend_payload = {
                 "categories": TEST_CONFIG["entrepreneur_config"]["categories"],
                 "platforms": ["tiktok", "youtube"],
                 "hours_back": 24,
-                "min_engagement": 1000
+                "min_engagement": 1000,
             }
-            
-            async with self.session.post(trend_url, json=trend_payload) as response:
+
+            async with self.session.post(
+                trend_url, json=trend_payload
+            ) as response:
                 if response.status != 200:
                     raise Exception(f"è¶¨å‹¢æŠ“å–å¤±æ•—: HTTP {response.status}")
-                    
+
                 trend_data = await response.json()
-                
+
             # 2. é©—è­‰è¶¨å‹¢æ•¸æ“šæ ¼å¼
             required_fields = ["trends", "total_count", "categories_analyzed"]
             for field in required_fields:
                 if field not in trend_data:
                     raise Exception(f"è¶¨å‹¢æ•¸æ“šç¼ºå°‘å¿…è¦æ¬„ä½: {field}")
-                    
+
             # 3. æª¢æŸ¥è¶¨å‹¢å“è³ª
             trends = trend_data.get("trends", [])
             if len(trends) < 3:
                 raise Exception(f"è¶¨å‹¢æ•¸é‡ä¸è¶³ï¼Œåƒ…ç²å¾— {len(trends)} å€‹è¶¨å‹¢")
-                
+
             high_quality_trends = [
-                t for t in trends 
-                if t.get("engagement_score", 0) >= 0.7
+                t for t in trends if t.get("engagement_score", 0) >= 0.7
             ]
-            
+
             duration = time.time() - start_time
-            self.result.add_step_result("trend_analysis", True, duration, {
-                "trends_found": len(trends),
-                "high_quality_trends": len(high_quality_trends),
-                "categories": trend_data.get("categories_analyzed", [])
-            })
-            
+            self.result.add_step_result(
+                "trend_analysis",
+                True,
+                duration,
+                {
+                    "trends_found": len(trends),
+                    "high_quality_trends": len(high_quality_trends),
+                    "categories": trend_data.get("categories_analyzed", []),
+                },
+            )
+
             return trend_data
-            
+
         except Exception as e:
             duration = time.time() - start_time
             self.result.add_step_result("trend_analysis", False, duration)
             self.result.add_error(f"è¶¨å‹¢åˆ†æå¤±æ•—: {str(e)}")
             raise
-            
+
     async def test_scheduler_configuration(self) -> str:
         """æ¸¬è©¦æ’ç¨‹å™¨é…ç½®"""
         logger.info("âš™ï¸ æ¸¬è©¦æ’ç¨‹å™¨é…ç½®...")
         start_time = time.time()
-        
+
         try:
             # é…ç½®æ’ç¨‹å™¨
             config_url = f"{SERVICES['scheduler_service']}/api/v1/entrepreneur-scheduler/configure"
             config_payload = TEST_CONFIG["entrepreneur_config"]
-            
+
             # æ·»åŠ æ¸¬è©¦ç”¨æˆ¶èªè­‰é ­ (åœ¨å¯¦éš›ç’°å¢ƒä¸­æœƒå¾èªè­‰æœå‹™ç²å–)
             headers = {
                 "Authorization": "Bearer test_token_e2e",
-                "Content-Type": "application/json"
+                "Content-Type": "application/json",
             }
-            
-            async with self.session.post(config_url, json=config_payload, headers=headers) as response:
+
+            async with self.session.post(
+                config_url, json=config_payload, headers=headers
+            ) as response:
                 if response.status != 200:
                     raise Exception(f"æ’ç¨‹å™¨é…ç½®å¤±æ•—: HTTP {response.status}")
-                    
+
                 config_result = await response.json()
-                
+
             # å•Ÿå‹•æ’ç¨‹å™¨
             start_url = f"{SERVICES['scheduler_service']}/api/v1/entrepreneur-scheduler/start"
-            async with self.session.post(start_url, headers=headers) as response:
+            async with self.session.post(
+                start_url, headers=headers
+            ) as response:
                 if response.status != 200:
                     raise Exception(f"æ’ç¨‹å™¨å•Ÿå‹•å¤±æ•—: HTTP {response.status}")
-                    
+
                 start_result = await response.json()
-                
+
             duration = time.time() - start_time
-            self.result.add_step_result("scheduler_config", True, duration, {
-                "config_status": config_result.get("status"),
-                "scheduler_status": start_result.get("status")
-            })
-            
+            self.result.add_step_result(
+                "scheduler_config",
+                True,
+                duration,
+                {
+                    "config_status": config_result.get("status"),
+                    "scheduler_status": start_result.get("status"),
+                },
+            )
+
             return start_result.get("message", "æ’ç¨‹å™¨å·²å•Ÿå‹•")
-            
+
         except Exception as e:
             duration = time.time() - start_time
             self.result.add_step_result("scheduler_config", False, duration)
             self.result.add_error(f"æ’ç¨‹å™¨é…ç½®å¤±æ•—: {str(e)}")
             raise
-            
-    async def test_video_workflow_execution(self, trend_data: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def test_video_workflow_execution(
+        self, trend_data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """æ¸¬è©¦å½±ç‰‡å·¥ä½œæµç¨‹åŸ·è¡Œ"""
         logger.info("ğŸ¬ æ¸¬è©¦å½±ç‰‡å·¥ä½œæµç¨‹åŸ·è¡Œ...")
         start_time = time.time()
-        
+
         try:
             # é¸æ“‡æœ€é«˜åˆ†è¶¨å‹¢
             trends = trend_data.get("trends", [])
             if not trends:
                 raise Exception("æ²’æœ‰å¯ç”¨çš„è¶¨å‹¢æ•¸æ“š")
-                
-            best_trend = max(trends, key=lambda t: t.get("engagement_score", 0))
-            
+
+            best_trend = max(
+                trends, key=lambda t: t.get("engagement_score", 0)
+            )
+
             # å‰µå»ºå½±ç‰‡å·¥ä½œæµç¨‹
-            workflow_url = f"{SERVICES['video_service']}/api/v1/entrepreneur/create"
+            workflow_url = (
+                f"{SERVICES['video_service']}/api/v1/entrepreneur/create"
+            )
             workflow_payload = {
                 "user_id": TEST_CONFIG["user_id"],
                 "trend_keywords": best_trend.get("keywords", []),
                 "video_count": 1,
                 "categories": TEST_CONFIG["entrepreneur_config"]["categories"],
                 "platforms": TEST_CONFIG["entrepreneur_config"]["platforms"],
-                "quality_threshold": 0.8
+                "quality_threshold": 0.8,
             }
-            
+
             headers = {
                 "Authorization": "Bearer test_token_e2e",
-                "Content-Type": "application/json"
+                "Content-Type": "application/json",
             }
-            
-            async with self.session.post(workflow_url, json=workflow_payload, headers=headers) as response:
+
+            async with self.session.post(
+                workflow_url, json=workflow_payload, headers=headers
+            ) as response:
                 if response.status != 200:
                     error_text = await response.text()
-                    raise Exception(f"å½±ç‰‡å·¥ä½œæµç¨‹å‰µå»ºå¤±æ•—: HTTP {response.status}, éŒ¯èª¤: {error_text}")
-                    
+                    raise Exception(
+                        f"å½±ç‰‡å·¥ä½œæµç¨‹å‰µå»ºå¤±æ•—: HTTP {response.status}, éŒ¯èª¤: {error_text}"
+                    )
+
                 workflow_result = await response.json()
-                
+
             workflow_id = workflow_result.get("workflow_id")
             if not workflow_id:
                 raise Exception("å·¥ä½œæµç¨‹IDæœªè¿”å›")
-                
+
             # ç›£æ§å·¥ä½œæµç¨‹é€²åº¦
             progress_url = f"{SERVICES['video_service']}/api/v1/entrepreneur/status/{workflow_id}"
             max_wait_time = 300  # 5åˆ†é˜æœ€å¤§ç­‰å¾…æ™‚é–“
             start_wait = time.time()
-            
+
             final_status = None
             while time.time() - start_wait < max_wait_time:
-                async with self.session.get(progress_url, headers=headers) as response:
+                async with self.session.get(
+                    progress_url, headers=headers
+                ) as response:
                     if response.status == 200:
                         status_data = await response.json()
                         current_status = status_data.get("status")
-                        
-                        logger.info(f"å·¥ä½œæµç¨‹ {workflow_id} ç‹€æ…‹: {current_status}")
-                        
+
+                        logger.info(
+                            f"å·¥ä½œæµç¨‹ {workflow_id} ç‹€æ…‹: {current_status}"
+                        )
+
                         if current_status in ["completed", "failed"]:
                             final_status = status_data
                             break
-                            
+
                 await asyncio.sleep(10)  # æ¯10ç§’æª¢æŸ¥ä¸€æ¬¡
-                
+
             if not final_status:
                 raise Exception("å·¥ä½œæµç¨‹åŸ·è¡Œè¶…æ™‚")
-                
+
             if final_status.get("status") != "completed":
-                raise Exception(f"å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {final_status.get('error')}")
-                
+                raise Exception(
+                    f"å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {final_status.get('error')}"
+                )
+
             duration = time.time() - start_time
-            self.result.add_step_result("video_workflow", True, duration, {
-                "workflow_id": workflow_id,
-                "videos_generated": final_status.get("videos_generated", 0),
-                "total_steps": len(final_status.get("step_results", {}))
-            })
-            
+            self.result.add_step_result(
+                "video_workflow",
+                True,
+                duration,
+                {
+                    "workflow_id": workflow_id,
+                    "videos_generated": final_status.get(
+                        "videos_generated", 0
+                    ),
+                    "total_steps": len(final_status.get("step_results", {})),
+                },
+            )
+
             return final_status
-            
+
         except Exception as e:
             duration = time.time() - start_time
             self.result.add_step_result("video_workflow", False, duration)
             self.result.add_error(f"å½±ç‰‡å·¥ä½œæµç¨‹å¤±æ•—: {str(e)}")
             raise
-            
-    async def test_publishing_workflow(self, video_data: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def test_publishing_workflow(
+        self, video_data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """æ¸¬è©¦ç™¼å¸ƒå·¥ä½œæµç¨‹"""
         logger.info("ğŸ“¢ æ¸¬è©¦ç™¼å¸ƒå·¥ä½œæµç¨‹...")
         start_time = time.time()
-        
+
         try:
             videos = video_data.get("generated_videos", [])
             if not videos:
                 raise Exception("æ²’æœ‰å¯ç™¼å¸ƒçš„å½±ç‰‡")
-                
+
             video = videos[0]  # å–ç¬¬ä¸€å€‹å½±ç‰‡
-            
+
             # ç™¼å¸ƒåˆ°å„å€‹å¹³å°
-            publish_url = f"{SERVICES['social_service']}/api/v1/entrepreneur/publish"
+            publish_url = (
+                f"{SERVICES['social_service']}/api/v1/entrepreneur/publish"
+            )
             publish_payload = {
                 "user_id": TEST_CONFIG["user_id"],
                 "video_id": video.get("video_id"),
@@ -330,115 +389,137 @@ class E2ETestRunner:
                 "metadata": {
                     "title": video.get("title"),
                     "description": video.get("description"),
-                    "tags": video.get("tags", [])
-                }
+                    "tags": video.get("tags", []),
+                },
             }
-            
+
             headers = {
                 "Authorization": "Bearer test_token_e2e",
-                "Content-Type": "application/json"
+                "Content-Type": "application/json",
             }
-            
-            async with self.session.post(publish_url, json=publish_payload, headers=headers) as response:
+
+            async with self.session.post(
+                publish_url, json=publish_payload, headers=headers
+            ) as response:
                 if response.status != 200:
                     raise Exception(f"ç™¼å¸ƒè«‹æ±‚å¤±æ•—: HTTP {response.status}")
-                    
+
                 publish_result = await response.json()
-                
+
             publish_id = publish_result.get("publish_id")
-            
+
             # ç›£æ§ç™¼å¸ƒç‹€æ…‹
             status_url = f"{SERVICES['social_service']}/api/v1/entrepreneur/publish-status/{publish_id}"
             max_wait_time = 120  # 2åˆ†é˜ç­‰å¾…ç™¼å¸ƒå®Œæˆ
             start_wait = time.time()
-            
+
             final_publish_status = None
             while time.time() - start_wait < max_wait_time:
-                async with self.session.get(status_url, headers=headers) as response:
+                async with self.session.get(
+                    status_url, headers=headers
+                ) as response:
                     if response.status == 200:
                         status_data = await response.json()
-                        
+
                         if status_data.get("all_completed", False):
                             final_publish_status = status_data
                             break
-                            
+
                 await asyncio.sleep(5)  # æ¯5ç§’æª¢æŸ¥ä¸€æ¬¡
-                
+
             if not final_publish_status:
                 raise Exception("ç™¼å¸ƒæµç¨‹ç›£æ§è¶…æ™‚")
-                
+
             successful_platforms = [
-                p for p in final_publish_status.get("platform_results", {}).values()
+                p
+                for p in final_publish_status.get(
+                    "platform_results", {}
+                ).values()
                 if p.get("status") == "success"
             ]
-            
+
             if len(successful_platforms) == 0:
                 raise Exception("æ‰€æœ‰å¹³å°ç™¼å¸ƒéƒ½å¤±æ•—")
-                
+
             duration = time.time() - start_time
-            self.result.add_step_result("publishing_workflow", True, duration, {
-                "publish_id": publish_id,
-                "successful_platforms": len(successful_platforms),
-                "total_platforms": len(TEST_CONFIG["entrepreneur_config"]["platforms"])
-            })
-            
+            self.result.add_step_result(
+                "publishing_workflow",
+                True,
+                duration,
+                {
+                    "publish_id": publish_id,
+                    "successful_platforms": len(successful_platforms),
+                    "total_platforms": len(
+                        TEST_CONFIG["entrepreneur_config"]["platforms"]
+                    ),
+                },
+            )
+
             return final_publish_status
-            
+
         except Exception as e:
             duration = time.time() - start_time
             self.result.add_step_result("publishing_workflow", False, duration)
             self.result.add_error(f"ç™¼å¸ƒå·¥ä½œæµç¨‹å¤±æ•—: {str(e)}")
             raise
-            
+
     async def test_error_recovery(self) -> bool:
         """æ¸¬è©¦éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶"""
         logger.info("ğŸ”„ æ¸¬è©¦éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶...")
         start_time = time.time()
-        
+
         try:
             # å‰µå»ºä¸€å€‹é æœŸæœƒå¤±æ•—çš„ä»»å‹™
-            workflow_url = f"{SERVICES['video_service']}/api/v1/entrepreneur/create"
+            workflow_url = (
+                f"{SERVICES['video_service']}/api/v1/entrepreneur/create"
+            )
             invalid_payload = {
                 "user_id": TEST_CONFIG["user_id"],
                 "trend_keywords": [],  # ç©ºé—œéµå­—æ‡‰è©²å°è‡´å¤±æ•—
-                "video_count": 0,      # ç„¡æ•ˆæ•¸é‡
+                "video_count": 0,  # ç„¡æ•ˆæ•¸é‡
                 "categories": [],
-                "platforms": []
+                "platforms": [],
             }
-            
+
             headers = {
                 "Authorization": "Bearer test_token_e2e",
-                "Content-Type": "application/json"
+                "Content-Type": "application/json",
             }
-            
-            async with self.session.post(workflow_url, json=invalid_payload, headers=headers) as response:
+
+            async with self.session.post(
+                workflow_url, json=invalid_payload, headers=headers
+            ) as response:
                 error_response = await response.json()
-                
+
                 # é©—è­‰ç³»çµ±æ­£ç¢ºè¿”å›éŒ¯èª¤
                 if response.status == 200:
                     raise Exception("ç³»çµ±æ‡‰è©²æ‹’çµ•ç„¡æ•ˆè«‹æ±‚ä½†æ²’æœ‰")
-                    
+
                 if response.status not in [400, 422]:
                     raise Exception(f"éŒ¯èª¤ç‹€æ…‹ç¢¼ä¸æ­£ç¢º: {response.status}")
-                    
+
             # æ¸¬è©¦æ’ç¨‹å™¨å°å¤±æ•—ä»»å‹™çš„è™•ç†
             status_url = f"{SERVICES['scheduler_service']}/api/v1/entrepreneur-scheduler/status"
-            async with self.session.get(status_url, headers=headers) as response:
+            async with self.session.get(
+                status_url, headers=headers
+            ) as response:
                 if response.status == 200:
                     scheduler_status = await response.json()
-                    
+
                     # é©—è­‰æ’ç¨‹å™¨ä»åœ¨æ­£å¸¸é‹è¡Œ
                     if not scheduler_status.get("is_running", False):
                         raise Exception("æ’ç¨‹å™¨å› éŒ¯èª¤åœæ­¢é‹è¡Œ")
-                        
+
             duration = time.time() - start_time
-            self.result.add_step_result("error_recovery", True, duration, {
-                "error_handled": True,
-                "scheduler_stable": True
-            })
-            
+            self.result.add_step_result(
+                "error_recovery",
+                True,
+                duration,
+                {"error_handled": True, "scheduler_stable": True},
+            )
+
             return True
-            
+
         except Exception as e:
             duration = time.time() - start_time
             self.result.add_step_result("error_recovery", False, duration)
@@ -450,55 +531,57 @@ async def run_e2e_tests():
     """åŸ·è¡Œå®Œæ•´çš„ç«¯å°ç«¯æ¸¬è©¦"""
     logger.info("ğŸš€ é–‹å§‹å‰µæ¥­è€…æ¨¡å¼ç«¯å°ç«¯æ•´åˆæ¸¬è©¦")
     logger.info("=" * 60)
-    
+
     async with E2ETestRunner() as runner:
         try:
             # 1. æœå‹™å¥åº·æª¢æŸ¥
             if not await runner.health_check_services():
                 logger.error("âŒ æœå‹™å¥åº·æª¢æŸ¥å¤±æ•—ï¼Œåœæ­¢æ¸¬è©¦")
                 return False
-                
+
             logger.info("âœ… æ‰€æœ‰æœå‹™å¥åº·æª¢æŸ¥é€šé")
-            
+
             # 2. è¶¨å‹¢åˆ†ææµç¨‹æ¸¬è©¦
             trend_data = await runner.test_trend_analysis_flow()
             logger.info("âœ… è¶¨å‹¢åˆ†ææµç¨‹æ¸¬è©¦é€šé")
-            
+
             # 3. æ’ç¨‹å™¨é…ç½®æ¸¬è©¦
             await runner.test_scheduler_configuration()
             logger.info("âœ… æ’ç¨‹å™¨é…ç½®æ¸¬è©¦é€šé")
-            
+
             # 4. å½±ç‰‡å·¥ä½œæµç¨‹æ¸¬è©¦
             video_data = await runner.test_video_workflow_execution(trend_data)
             logger.info("âœ… å½±ç‰‡å·¥ä½œæµç¨‹æ¸¬è©¦é€šé")
-            
+
             # 5. ç™¼å¸ƒå·¥ä½œæµç¨‹æ¸¬è©¦
             publish_data = await runner.test_publishing_workflow(video_data)
             logger.info("âœ… ç™¼å¸ƒå·¥ä½œæµç¨‹æ¸¬è©¦é€šé")
-            
+
             # 6. éŒ¯èª¤æ¢å¾©æ¸¬è©¦
             await runner.test_error_recovery()
             logger.info("âœ… éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶æ¸¬è©¦é€šé")
-            
+
             # å®Œæˆæ¸¬è©¦ä¸¦ç”Ÿæˆå ±å‘Š
             runner.result.finalize()
-            
+
             logger.info("=" * 60)
             logger.info("ğŸ‰ ç«¯å°ç«¯æ•´åˆæ¸¬è©¦å®Œæˆï¼")
             logger.info(f"ğŸ“Š æ¸¬è©¦æˆåŠŸç‡: {runner.result.success_rate:.1f}%")
-            logger.info(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {runner.result.total_execution_time:.2f}ç§’")
-            
+            logger.info(
+                f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {runner.result.total_execution_time:.2f}ç§’"
+            )
+
             if runner.result.errors:
                 logger.info("âš ï¸  ç™¼ç¾çš„å•é¡Œ:")
                 for error in runner.result.errors:
                     logger.info(f"   - {error}")
-                    
+
             return runner.result.success_rate >= 80.0  # 80%æˆåŠŸç‡ç®—é€šé
-            
+
         except Exception as e:
             runner.result.add_error(f"E2Eæ¸¬è©¦åŸ·è¡Œç•°å¸¸: {str(e)}")
             runner.result.finalize()
-            
+
             logger.error("âŒ ç«¯å°ç«¯æ•´åˆæ¸¬è©¦å¤±æ•—")
             logger.error(f"éŒ¯èª¤è©³æƒ…: {str(e)}")
             return False
@@ -507,10 +590,10 @@ async def run_e2e_tests():
 if __name__ == "__main__":
     # åŸ·è¡Œç«¯å°ç«¯æ¸¬è©¦
     success = asyncio.run(run_e2e_tests())
-    
+
     if success:
         logger.info("ğŸ¯ E2Eæ¸¬è©¦é€šé - ç³»çµ±æº–å‚™å°±ç·’ï¼")
         sys.exit(0)
     else:
         logger.error("ğŸ’¥ E2Eæ¸¬è©¦å¤±æ•— - éœ€è¦ä¿®å¾©å•é¡Œ")
-        sys.exit(1)
\ No newline at end of file
+        sys.exit(1)
diff --git a/auto_generate_video_fold6/test_e2e_simple.py b/auto_generate_video_fold6/test_e2e_simple.py
index c681128..4a0dadc 100644
--- a/auto_generate_video_fold6/test_e2e_simple.py
+++ b/auto_generate_video_fold6/test_e2e_simple.py
@@ -15,48 +15,49 @@ from typing import Dict, Any
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 class SimpleE2ERunner:
     """ç°¡åŒ–ç‰ˆ E2E æ¸¬è©¦åŸ·è¡Œå™¨"""
-    
+
     def __init__(self):
         self.session = None
-        self.results = {
-            "tests_passed": 0,
-            "tests_failed": 0,
-            "errors": []
-        }
-    
+        self.results = {"tests_passed": 0, "tests_failed": 0, "errors": []}
+
     async def __aenter__(self):
         self.session = aiohttp.ClientSession(
             timeout=aiohttp.ClientTimeout(total=30)
         )
         return self
-    
+
     async def __aexit__(self, exc_type, exc_val, exc_tb):
         if self.session:
             await self.session.close()
-    
+
     async def test_service_connectivity(self):
         """æ¸¬è©¦æœå‹™é€£é€šæ€§"""
         logger.info("ğŸ”— æ¸¬è©¦æœå‹™é€£é€šæ€§...")
-        
+
         services = {
             "trend_service": "http://localhost:8001/health",
-            "video_service": "http://localhost:8003/health", 
+            "video_service": "http://localhost:8003/health",
             "social_service": "http://localhost:8004/health",
-            "scheduler_service": "http://localhost:8008/health"
+            "scheduler_service": "http://localhost:8008/health",
         }
-        
+
         all_healthy = True
         for service_name, health_url in services.items():
             try:
                 async with self.session.get(health_url) as response:
                     if response.status == 200:
                         data = await response.json()
-                        logger.info(f"âœ… {service_name}: {data.get('status', 'unknown')}")
+                        logger.info(
+                            f"âœ… {service_name}: {data.get('status', 'unknown')}"
+                        )
                         self.results["tests_passed"] += 1
                     else:
-                        logger.error(f"âŒ {service_name}: HTTP {response.status}")
+                        logger.error(
+                            f"âŒ {service_name}: HTTP {response.status}"
+                        )
                         self.results["tests_failed"] += 1
                         all_healthy = False
             except Exception as e:
@@ -64,22 +65,24 @@ class SimpleE2ERunner:
                 self.results["tests_failed"] += 1
                 self.results["errors"].append(f"{service_name}: {str(e)}")
                 all_healthy = False
-        
+
         return all_healthy
-    
+
     async def test_trend_analysis(self):
         """æ¸¬è©¦è¶¨å‹¢åˆ†æ"""
         logger.info("ğŸ“ˆ æ¸¬è©¦è¶¨å‹¢åˆ†æ...")
-        
+
         try:
-            trend_url = "http://localhost:8001/api/v1/entrepreneur/fetch-trends"
+            trend_url = (
+                "http://localhost:8001/api/v1/entrepreneur/fetch-trends"
+            )
             payload = {
                 "categories": ["technology", "ai"],
                 "platforms": ["tiktok", "youtube"],
                 "hours_back": 24,
-                "min_engagement": 1000
+                "min_engagement": 1000,
             }
-            
+
             async with self.session.post(trend_url, json=payload) as response:
                 if response.status == 200:
                     data = await response.json()
@@ -90,45 +93,47 @@ class SimpleE2ERunner:
                 else:
                     error_text = await response.text()
                     raise Exception(f"HTTP {response.status}: {error_text}")
-                    
+
         except Exception as e:
             logger.error(f"âŒ è¶¨å‹¢åˆ†æå¤±æ•—: {e}")
             self.results["tests_failed"] += 1
             self.results["errors"].append(f"è¶¨å‹¢åˆ†æ: {str(e)}")
             return None
-    
+
     async def test_video_workflow(self, trend_data):
         """æ¸¬è©¦å½±ç‰‡å·¥ä½œæµç¨‹"""
         logger.info("ğŸ¬ æ¸¬è©¦å½±ç‰‡å·¥ä½œæµç¨‹...")
-        
+
         if not trend_data or not trend_data.get("trends"):
             logger.error("âŒ ç„¡è¶¨å‹¢æ•¸æ“šå¯ç”¨æ–¼å½±ç‰‡å·¥ä½œæµç¨‹")
             self.results["tests_failed"] += 1
             return None
-        
+
         try:
             workflow_url = "http://localhost:8003/api/v1/entrepreneur/create"
             best_trend = trend_data["trends"][0]
-            
+
             payload = {
                 "user_id": "e2e_test_user",
                 "trend_keywords": [best_trend.get("keyword", "AI technology")],
                 "video_count": 1,
                 "categories": ["technology"],
-                "platforms": ["tiktok"]
+                "platforms": ["tiktok"],
             }
-            
+
             # å‰µå»ºå·¥ä½œæµç¨‹
-            async with self.session.post(workflow_url, json=payload) as response:
+            async with self.session.post(
+                workflow_url, json=payload
+            ) as response:
                 if response.status == 200:
                     data = await response.json()
                     workflow_id = data.get("workflow_id")
                     logger.info(f"âœ… å·¥ä½œæµç¨‹å‰µå»ºæˆåŠŸ: {workflow_id}")
                     self.results["tests_passed"] += 1
-                    
+
                     # ç­‰å¾…å·¥ä½œæµç¨‹å®Œæˆï¼ˆç°¡åŒ–ç‰ˆï¼‰
                     await asyncio.sleep(5)
-                    
+
                     # æª¢æŸ¥å·¥ä½œæµç¨‹ç‹€æ…‹
                     status_url = f"http://localhost:8003/api/v1/entrepreneur/status/{workflow_id}"
                     async with self.session.get(status_url) as status_response:
@@ -139,39 +144,43 @@ class SimpleE2ERunner:
                             self.results["tests_passed"] += 1
                             return status_data
                         else:
-                            raise Exception(f"ç‹€æ…‹æŸ¥è©¢å¤±æ•—: HTTP {status_response.status}")
+                            raise Exception(
+                                f"ç‹€æ…‹æŸ¥è©¢å¤±æ•—: HTTP {status_response.status}"
+                            )
                 else:
                     error_text = await response.text()
                     raise Exception(f"HTTP {response.status}: {error_text}")
-                    
+
         except Exception as e:
             logger.error(f"âŒ å½±ç‰‡å·¥ä½œæµç¨‹å¤±æ•—: {e}")
             self.results["tests_failed"] += 1
             self.results["errors"].append(f"å½±ç‰‡å·¥ä½œæµç¨‹: {str(e)}")
             return None
-    
+
     async def test_scheduler_operations(self):
         """æ¸¬è©¦æ’ç¨‹å™¨æ“ä½œ"""
         logger.info("âš™ï¸ æ¸¬è©¦æ’ç¨‹å™¨æ“ä½œ...")
-        
+
         try:
             base_url = "http://localhost:8008/api/v1/entrepreneur-scheduler"
-            
+
             # é…ç½®æ’ç¨‹å™¨
             config_payload = {
                 "enabled": True,
                 "daily_video_limit": 5,
                 "daily_budget_limit": 20.0,
-                "max_concurrent_tasks": 2
+                "max_concurrent_tasks": 2,
             }
-            
-            async with self.session.post(f"{base_url}/configure", json=config_payload) as response:
+
+            async with self.session.post(
+                f"{base_url}/configure", json=config_payload
+            ) as response:
                 if response.status == 200:
                     logger.info("âœ… æ’ç¨‹å™¨é…ç½®æˆåŠŸ")
                     self.results["tests_passed"] += 1
                 else:
                     raise Exception(f"é…ç½®å¤±æ•—: HTTP {response.status}")
-            
+
             # å•Ÿå‹•æ’ç¨‹å™¨
             async with self.session.post(f"{base_url}/start") as response:
                 if response.status == 200:
@@ -179,28 +188,30 @@ class SimpleE2ERunner:
                     self.results["tests_passed"] += 1
                 else:
                     raise Exception(f"å•Ÿå‹•å¤±æ•—: HTTP {response.status}")
-            
+
             # æª¢æŸ¥ç‹€æ…‹
             async with self.session.get(f"{base_url}/status") as response:
                 if response.status == 200:
                     data = await response.json()
                     is_running = data.get("is_running", False)
-                    logger.info(f"âœ… æ’ç¨‹å™¨ç‹€æ…‹: {'é‹è¡Œä¸­' if is_running else 'å·²åœæ­¢'}")
+                    logger.info(
+                        f"âœ… æ’ç¨‹å™¨ç‹€æ…‹: {'é‹è¡Œä¸­' if is_running else 'å·²åœæ­¢'}"
+                    )
                     self.results["tests_passed"] += 1
                     return True
                 else:
                     raise Exception(f"ç‹€æ…‹æŸ¥è©¢å¤±æ•—: HTTP {response.status}")
-                    
+
         except Exception as e:
             logger.error(f"âŒ æ’ç¨‹å™¨æ“ä½œå¤±æ•—: {e}")
             self.results["tests_failed"] += 1
             self.results["errors"].append(f"æ’ç¨‹å™¨æ“ä½œ: {str(e)}")
             return False
-    
+
     async def test_basic_publishing(self):
         """æ¸¬è©¦åŸºæœ¬ç™¼å¸ƒåŠŸèƒ½"""
         logger.info("ğŸ“¢ æ¸¬è©¦åŸºæœ¬ç™¼å¸ƒåŠŸèƒ½...")
-        
+
         try:
             publish_url = "http://localhost:8004/api/v1/entrepreneur/publish"
             payload = {
@@ -209,11 +220,13 @@ class SimpleE2ERunner:
                 "platforms": ["tiktok"],
                 "metadata": {
                     "title": "æ¸¬è©¦å½±ç‰‡",
-                    "description": "E2E æ¸¬è©¦å½±ç‰‡"
-                }
+                    "description": "E2E æ¸¬è©¦å½±ç‰‡",
+                },
             }
-            
-            async with self.session.post(publish_url, json=payload) as response:
+
+            async with self.session.post(
+                publish_url, json=payload
+            ) as response:
                 if response.status == 200:
                     data = await response.json()
                     publish_id = data.get("publish_id")
@@ -223,78 +236,86 @@ class SimpleE2ERunner:
                 else:
                     error_text = await response.text()
                     raise Exception(f"HTTP {response.status}: {error_text}")
-                    
+
         except Exception as e:
             logger.error(f"âŒ ç™¼å¸ƒåŠŸèƒ½å¤±æ•—: {e}")
             self.results["tests_failed"] += 1
             self.results["errors"].append(f"ç™¼å¸ƒåŠŸèƒ½: {str(e)}")
             return False
-    
+
     def print_results(self):
         """æ‰“å°æ¸¬è©¦çµæœ"""
-        total_tests = self.results["tests_passed"] + self.results["tests_failed"]
-        success_rate = (self.results["tests_passed"] / total_tests * 100) if total_tests > 0 else 0
-        
+        total_tests = (
+            self.results["tests_passed"] + self.results["tests_failed"]
+        )
+        success_rate = (
+            (self.results["tests_passed"] / total_tests * 100)
+            if total_tests > 0
+            else 0
+        )
+
         logger.info("=" * 50)
         logger.info("ğŸ“Š E2E æ¸¬è©¦çµæœæ‘˜è¦")
         logger.info("=" * 50)
         logger.info(f"âœ… é€šéæ¸¬è©¦: {self.results['tests_passed']}")
         logger.info(f"âŒ å¤±æ•—æ¸¬è©¦: {self.results['tests_failed']}")
         logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
-        
+
         if self.results["errors"]:
             logger.info("\nğŸš¨ éŒ¯èª¤åˆ—è¡¨:")
             for error in self.results["errors"]:
                 logger.info(f"  - {error}")
-        
+
         return success_rate >= 70.0  # 70% æˆåŠŸç‡ç®—é€šé
 
+
 async def main():
     """åŸ·è¡Œç°¡åŒ–ç‰ˆ E2E æ¸¬è©¦"""
     logger.info("ğŸš€ é–‹å§‹ç°¡åŒ–ç‰ˆå‰µæ¥­è€…æ¨¡å¼ E2E æ¸¬è©¦")
     logger.info("=" * 50)
-    
+
     async with SimpleE2ERunner() as runner:
         # 1. æœå‹™é€£é€šæ€§æ¸¬è©¦
         connectivity_ok = await runner.test_service_connectivity()
-        
+
         if not connectivity_ok:
             logger.error("âŒ æœå‹™é€£é€šæ€§æ¸¬è©¦å¤±æ•—ï¼Œè·³éå¾ŒçºŒæ¸¬è©¦")
             return False
-        
+
         # 2. è¶¨å‹¢åˆ†ææ¸¬è©¦
         trend_data = await runner.test_trend_analysis()
-        
+
         # 3. å½±ç‰‡å·¥ä½œæµç¨‹æ¸¬è©¦
         workflow_data = await runner.test_video_workflow(trend_data)
-        
+
         # 4. æ’ç¨‹å™¨æ“ä½œæ¸¬è©¦
         scheduler_ok = await runner.test_scheduler_operations()
-        
+
         # 5. åŸºæœ¬ç™¼å¸ƒæ¸¬è©¦
         publish_ok = await runner.test_basic_publishing()
-        
+
         # æ‰“å°çµæœ
         success = runner.print_results()
-        
+
         if success:
             logger.info("ğŸ‰ E2E æ¸¬è©¦é€šéï¼å‰µæ¥­è€…æ¨¡å¼æ ¸å¿ƒåŠŸèƒ½é©—è­‰æˆåŠŸ")
         else:
             logger.error("ğŸ’¥ E2E æ¸¬è©¦å¤±æ•—ï¼éœ€è¦ä¿®å¾©å•é¡Œ")
-        
+
         return success
 
+
 if __name__ == "__main__":
     # çµ¦æ¨¡æ“¬æœå‹™æ™‚é–“å•Ÿå‹•
     logger.info("â³ ç­‰å¾…æœå‹™å•Ÿå‹•...")
     time.sleep(2)
-    
+
     # åŸ·è¡Œæ¸¬è©¦
     success = asyncio.run(main())
-    
+
     if success:
         logger.info("ğŸ¯ TDD Green éšæ®µé€šé - æº–å‚™é€²å…¥ Refactor éšæ®µ")
         exit(0)
     else:
         logger.error("ğŸ”´ TDD Green éšæ®µå¤±æ•— - éœ€è¦ä¿®å¾©å¯¦ä½œ")
-        exit(1)
\ No newline at end of file
+        exit(1)
diff --git a/auto_generate_video_fold6/test_monitoring_logging_system.py b/auto_generate_video_fold6/test_monitoring_logging_system.py
index 0d77e95..cf86abc 100644
--- a/auto_generate_video_fold6/test_monitoring_logging_system.py
+++ b/auto_generate_video_fold6/test_monitoring_logging_system.py
@@ -37,32 +37,35 @@ except ImportError:
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 class MonitoringLoggingSystemTest:
     """ç›£æ§å’Œæ—¥èªŒç³»çµ± TDD æ¸¬è©¦å¥—ä»¶"""
-    
+
     def __init__(self):
         self.project_root = Path(__file__).parent
-        self.results = {
-            "tests_passed": 0,
-            "tests_failed": 0,
-            "errors": []
-        }
-        
+        self.results = {"tests_passed": 0, "tests_failed": 0, "errors": []}
+
         # Test configuration
         self.services = [
-            "frontend", "api-gateway", "trend-service", 
-            "video-service", "social-service", "scheduler-service",
-            "postgres", "redis", "minio"
+            "frontend",
+            "api-gateway",
+            "trend-service",
+            "video-service",
+            "social-service",
+            "scheduler-service",
+            "postgres",
+            "redis",
+            "minio",
         ]
-        
+
         # Expected monitoring endpoints
         self.monitoring_endpoints = {
             "prometheus": "http://localhost:9090",
-            "grafana": "http://localhost:3001", 
+            "grafana": "http://localhost:3001",
             "alertmanager": "http://localhost:9093",
-            "jaeger": "http://localhost:16686"
+            "jaeger": "http://localhost:16686",
         }
-    
+
     def _record_result(self, test_name: str, success: bool, error: str = None):
         """è¨˜éŒ„æ¸¬è©¦çµæœ"""
         if success:
@@ -79,78 +82,105 @@ class MonitoringLoggingSystemTest:
             # æª¢æŸ¥æ—¥èªŒé…ç½®æ–‡ä»¶
             log_config_files = [
                 "monitoring/logging/structured_logger.py",
-                "monitoring/logging/log_config.json",  
+                "monitoring/logging/log_config.json",
                 "monitoring/fluentd/fluent.conf",
-                "monitoring/logstash/pipeline/logstash.conf"
+                "monitoring/logstash/pipeline/logstash.conf",
             ]
-            
+
             for config_file in log_config_files:
                 config_path = self.project_root / config_file
-                assert config_path.exists(), f"æ—¥èªŒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}"
-                
+                assert config_path.exists(), (
+                    f"æ—¥èªŒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}"
+                )
+
                 # æª¢æŸ¥é…ç½®æ–‡ä»¶å…§å®¹
-                if config_file.endswith('.py'):
+                if config_file.endswith(".py"):
                     content = config_path.read_text()
-                    assert "StructuredLogger" in content, f"ç¼ºå°‘ StructuredLogger é¡åˆ¥: {config_file}"
-                    assert "correlation_id" in content, f"ç¼ºå°‘é—œè¯IDæ”¯æ´: {config_file}"
-                    assert "json" in content.lower(), f"ç¼ºå°‘JSONæ ¼å¼æ”¯æ´: {config_file}"
-                
-                elif config_file.endswith('.json'):
-                    with open(config_path, 'r') as f:
+                    assert "StructuredLogger" in content, (
+                        f"ç¼ºå°‘ StructuredLogger é¡åˆ¥: {config_file}"
+                    )
+                    assert "correlation_id" in content, (
+                        f"ç¼ºå°‘é—œè¯IDæ”¯æ´: {config_file}"
+                    )
+                    assert "json" in content.lower(), (
+                        f"ç¼ºå°‘JSONæ ¼å¼æ”¯æ´: {config_file}"
+                    )
+
+                elif config_file.endswith(".json"):
+                    with open(config_path, "r") as f:
                         config = json.load(f)
-                    assert "formatters" in config, f"ç¼ºå°‘æ ¼å¼åŒ–å™¨é…ç½®: {config_file}"
-                    assert "handlers" in config, f"ç¼ºå°‘è™•ç†å™¨é…ç½®: {config_file}"
-                    
-                elif config_file.endswith('.conf'):
+                    assert "formatters" in config, (
+                        f"ç¼ºå°‘æ ¼å¼åŒ–å™¨é…ç½®: {config_file}"
+                    )
+                    assert "handlers" in config, (
+                        f"ç¼ºå°‘è™•ç†å™¨é…ç½®: {config_file}"
+                    )
+
+                elif config_file.endswith(".conf"):
                     content = config_path.read_text()
-                    assert "source" in content, f"ç¼ºå°‘è¼¸å…¥æºé…ç½®: {config_file}"
+                    assert "source" in content, (
+                        f"ç¼ºå°‘è¼¸å…¥æºé…ç½®: {config_file}"
+                    )
                     assert "match" in content, f"ç¼ºå°‘åŒ¹é…è¦å‰‡: {config_file}"
-            
+
             self._record_result("structured_logging_configuration", True)
-            
+
         except Exception as e:
-            self._record_result("structured_logging_configuration", False, str(e))
+            self._record_result(
+                "structured_logging_configuration", False, str(e)
+            )
 
     def test_prometheus_metrics_collection(self):
         """æ¸¬è©¦ Prometheus æŒ‡æ¨™æ”¶é›†é…ç½®"""
         try:
             # æª¢æŸ¥ Prometheus é…ç½®
-            prometheus_config = self.project_root / "monitoring/prometheus/prometheus.yml"
+            prometheus_config = (
+                self.project_root / "monitoring/prometheus/prometheus.yml"
+            )
             assert prometheus_config.exists(), "Prometheus é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
-            
+
             if yaml:
-                with open(prometheus_config, 'r') as f:
+                with open(prometheus_config, "r") as f:
                     config = yaml.safe_load(f)
             else:
                 # ç°¡å–®æ–‡æœ¬æª¢æŸ¥å¦‚æœæ²’æœ‰ yaml
                 config_text = prometheus_config.read_text()
-                config = {"scrape_configs": [{"job_name": "prometheus"}]}  # æ¨¡æ“¬åŸºæœ¬çµæ§‹
-            
+                config = {
+                    "scrape_configs": [{"job_name": "prometheus"}]
+                }  # æ¨¡æ“¬åŸºæœ¬çµæ§‹
+
             # æª¢æŸ¥åŸºæœ¬é…ç½®
             assert "global" in config, "ç¼ºå°‘å…¨å±€é…ç½®"
             assert "scrape_configs" in config, "ç¼ºå°‘æŠ“å–é…ç½®"
-            
+
             # æª¢æŸ¥æœå‹™ç™¼ç¾é…ç½®
-            scrape_jobs = {job['job_name'] for job in config['scrape_configs']}
+            scrape_jobs = {job["job_name"] for job in config["scrape_configs"]}
             expected_jobs = {
-                'prometheus', 'node-exporter', 'postgres-exporter', 
-                'redis-exporter', 'docker-exporter', 'application-metrics'
+                "prometheus",
+                "node-exporter",
+                "postgres-exporter",
+                "redis-exporter",
+                "docker-exporter",
+                "application-metrics",
             }
-            
+
             missing_jobs = expected_jobs - scrape_jobs
             assert not missing_jobs, f"ç¼ºå°‘ç›£æ§ä½œæ¥­: {missing_jobs}"
-            
+
             # æª¢æŸ¥æŒ‡æ¨™ä¸­é–“ä»¶æ–‡ä»¶
-            metrics_middleware = self.project_root / "monitoring/middleware/prometheus_middleware.py"
+            metrics_middleware = (
+                self.project_root
+                / "monitoring/middleware/prometheus_middleware.py"
+            )
             assert metrics_middleware.exists(), "ç¼ºå°‘ Prometheus ä¸­é–“ä»¶"
-            
+
             middleware_content = metrics_middleware.read_text()
             assert "Counter" in middleware_content, "ç¼ºå°‘è¨ˆæ•¸å™¨æŒ‡æ¨™"
             assert "Histogram" in middleware_content, "ç¼ºå°‘ç›´æ–¹åœ–æŒ‡æ¨™"
             assert "Gauge" in middleware_content, "ç¼ºå°‘æ¸¬é‡æŒ‡æ¨™"
-            
+
             self._record_result("prometheus_metrics_collection", True)
-            
+
         except Exception as e:
             self._record_result("prometheus_metrics_collection", False, str(e))
 
@@ -160,107 +190,136 @@ class MonitoringLoggingSystemTest:
             # æª¢æŸ¥ Grafana é…ç½®ç›®éŒ„
             grafana_dir = self.project_root / "monitoring/grafana"
             assert grafana_dir.exists(), "Grafana é…ç½®ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             # æª¢æŸ¥æ•¸æ“šæºé…ç½®
-            datasources_dir = grafana_dir / "provisioning/datasources"  
+            datasources_dir = grafana_dir / "provisioning/datasources"
             assert datasources_dir.exists(), "æ•¸æ“šæºé…ç½®ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             prometheus_datasource = datasources_dir / "prometheus.yml"
-            assert prometheus_datasource.exists(), "Prometheus æ•¸æ“šæºé…ç½®ä¸å­˜åœ¨"
-            
+            assert prometheus_datasource.exists(), (
+                "Prometheus æ•¸æ“šæºé…ç½®ä¸å­˜åœ¨"
+            )
+
             if yaml:
-                with open(prometheus_datasource, 'r') as f:
+                with open(prometheus_datasource, "r") as f:
                     datasource_config = yaml.safe_load(f)
             else:
                 datasource_config = {"datasources": [{"type": "prometheus"}]}
-            
+
             assert "datasources" in datasource_config, "ç¼ºå°‘æ•¸æ“šæºå®šç¾©"
-            prometheus_ds = datasource_config['datasources'][0]
-            assert prometheus_ds['type'] == 'prometheus', "æ•¸æ“šæºé¡å‹ä¸æ­£ç¢º"
-            
+            prometheus_ds = datasource_config["datasources"][0]
+            assert prometheus_ds["type"] == "prometheus", "æ•¸æ“šæºé¡å‹ä¸æ­£ç¢º"
+
             # æª¢æŸ¥å„€è¡¨æ¿é…ç½®
             dashboards_dir = grafana_dir / "dashboards"
             assert dashboards_dir.exists(), "å„€è¡¨æ¿ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             expected_dashboards = [
                 "system-overview.json",
-                "application-performance.json", 
+                "application-performance.json",
                 "docker-containers.json",
                 "business-metrics.json",
-                "error-tracking.json"
+                "error-tracking.json",
             ]
-            
+
             for dashboard in expected_dashboards:
                 dashboard_path = dashboards_dir / dashboard
                 assert dashboard_path.exists(), f"å„€è¡¨æ¿ä¸å­˜åœ¨: {dashboard}"
-                
-                with open(dashboard_path, 'r') as f:
+
+                with open(dashboard_path, "r") as f:
                     dashboard_config = json.load(f)
-                
-                assert "dashboard" in dashboard_config, f"å„€è¡¨æ¿é…ç½®æ ¼å¼éŒ¯èª¤: {dashboard}"
-                assert "panels" in dashboard_config['dashboard'], f"ç¼ºå°‘é¢æ¿é…ç½®: {dashboard}"
-            
+
+                assert "dashboard" in dashboard_config, (
+                    f"å„€è¡¨æ¿é…ç½®æ ¼å¼éŒ¯èª¤: {dashboard}"
+                )
+                assert "panels" in dashboard_config["dashboard"], (
+                    f"ç¼ºå°‘é¢æ¿é…ç½®: {dashboard}"
+                )
+
             self._record_result("grafana_dashboard_configuration", True)
-            
+
         except Exception as e:
-            self._record_result("grafana_dashboard_configuration", False, str(e))
+            self._record_result(
+                "grafana_dashboard_configuration", False, str(e)
+            )
 
     def test_alerting_system_configuration(self):
         """æ¸¬è©¦è­¦å ±ç³»çµ±é…ç½®"""
         try:
             # æª¢æŸ¥ Alertmanager é…ç½®
-            alertmanager_config = self.project_root / "monitoring/alertmanager/alertmanager.yml"
+            alertmanager_config = (
+                self.project_root / "monitoring/alertmanager/alertmanager.yml"
+            )
             assert alertmanager_config.exists(), "Alertmanager é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
-            
+
             if yaml:
-                with open(alertmanager_config, 'r') as f:
+                with open(alertmanager_config, "r") as f:
                     config = yaml.safe_load(f)
             else:
                 config = {"global": {}, "route": {}, "receivers": []}
-            
+
             assert "global" in config, "ç¼ºå°‘å…¨å±€è­¦å ±é…ç½®"
             assert "route" in config, "ç¼ºå°‘è·¯ç”±é…ç½®"
             assert "receivers" in config, "ç¼ºå°‘æ¥æ”¶å™¨é…ç½®"
-            
+
             # æª¢æŸ¥è­¦å ±è¦å‰‡
             alert_rules = self.project_root / "monitoring/prometheus/rules"
             assert alert_rules.exists(), "è­¦å ±è¦å‰‡ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             expected_rule_files = [
                 "system-alerts.yml",
                 "application-alerts.yml",
-                "business-alerts.yml"
+                "business-alerts.yml",
             ]
-            
+
             for rule_file in expected_rule_files:
                 rule_path = alert_rules / rule_file
                 assert rule_path.exists(), f"è­¦å ±è¦å‰‡æ–‡ä»¶ä¸å­˜åœ¨: {rule_file}"
-                
+
                 if yaml:
-                    with open(rule_path, 'r') as f:
+                    with open(rule_path, "r") as f:
                         rules = yaml.safe_load(f)
                 else:
-                    rules = {"groups": [{"name": "test", "rules": [{"alert": "test", "expr": "up", "for": "1m"}]}]}
-                
+                    rules = {
+                        "groups": [
+                            {
+                                "name": "test",
+                                "rules": [
+                                    {
+                                        "alert": "test",
+                                        "expr": "up",
+                                        "for": "1m",
+                                    }
+                                ],
+                            }
+                        ]
+                    }
+
                 assert "groups" in rules, f"ç¼ºå°‘è­¦å ±çµ„: {rule_file}"
-                
-                for group in rules['groups']:
+
+                for group in rules["groups"]:
                     assert "rules" in group, f"è­¦å ±çµ„ç¼ºå°‘è¦å‰‡: {group['name']}"
-                    
-                    for rule in group['rules']:
+
+                    for rule in group["rules"]:
                         assert "alert" in rule, f"è¦å‰‡ç¼ºå°‘è­¦å ±åç¨±: {rule}"
-                        assert "expr" in rule, f"è¦å‰‡ç¼ºå°‘è¡¨é”å¼: {rule['alert']}"
-                        assert "for" in rule, f"è¦å‰‡ç¼ºå°‘æŒçºŒæ™‚é–“: {rule['alert']}"
-            
+                        assert "expr" in rule, (
+                            f"è¦å‰‡ç¼ºå°‘è¡¨é”å¼: {rule['alert']}"
+                        )
+                        assert "for" in rule, (
+                            f"è¦å‰‡ç¼ºå°‘æŒçºŒæ™‚é–“: {rule['alert']}"
+                        )
+
             # æª¢æŸ¥é€šçŸ¥ç¯„æœ¬
-            templates_dir = self.project_root / "monitoring/alertmanager/templates"
+            templates_dir = (
+                self.project_root / "monitoring/alertmanager/templates"
+            )
             assert templates_dir.exists(), "è­¦å ±ç¯„æœ¬ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             email_template = templates_dir / "email.tmpl"
             assert email_template.exists(), "é›»å­éƒµä»¶ç¯„æœ¬ä¸å­˜åœ¨"
-            
+
             self._record_result("alerting_system_configuration", True)
-            
+
         except Exception as e:
             self._record_result("alerting_system_configuration", False, str(e))
 
@@ -270,35 +329,42 @@ class MonitoringLoggingSystemTest:
             # æª¢æŸ¥ Jaeger é…ç½®
             jaeger_config = self.project_root / "monitoring/jaeger"
             assert jaeger_config.exists(), "Jaeger é…ç½®ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             jaeger_config_file = jaeger_config / "jaeger-config.yml"
             assert jaeger_config_file.exists(), "Jaeger é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
-            
+
             # æª¢æŸ¥ OpenTelemetry ä¸­é–“ä»¶
-            otel_middleware = self.project_root / "monitoring/tracing/opentelemetry_middleware.py"
+            otel_middleware = (
+                self.project_root
+                / "monitoring/tracing/opentelemetry_middleware.py"
+            )
             assert otel_middleware.exists(), "OpenTelemetry ä¸­é–“ä»¶ä¸å­˜åœ¨"
-            
+
             middleware_content = otel_middleware.read_text()
             assert "tracer" in middleware_content.lower(), "ç¼ºå°‘è¿½è¹¤å™¨é…ç½®"
             assert "span" in middleware_content.lower(), "ç¼ºå°‘è·¨åº¦è™•ç†"
-            assert "correlation" in middleware_content.lower(), "ç¼ºå°‘é—œè¯IDè™•ç†"
-            
+            assert "correlation" in middleware_content.lower(), (
+                "ç¼ºå°‘é—œè¯IDè™•ç†"
+            )
+
             # æª¢æŸ¥è¿½è¹¤é…ç½®æª”æ¡ˆ
             tracing_configs = [
                 "services/api-gateway/tracing_config.py",
-                "services/trend-service/tracing_config.py", 
-                "services/video-service/tracing_config.py"
+                "services/trend-service/tracing_config.py",
+                "services/video-service/tracing_config.py",
             ]
-            
+
             for config_file in tracing_configs:
                 config_path = self.project_root / config_file
                 # å…è¨±æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå› ç‚ºé€™æ˜¯ Red éšæ®µ
                 if config_path.exists():
                     content = config_path.read_text()
-                    assert "opentelemetry" in content.lower(), f"ç¼ºå°‘ OpenTelemetry é…ç½®: {config_file}"
-            
+                    assert "opentelemetry" in content.lower(), (
+                        f"ç¼ºå°‘ OpenTelemetry é…ç½®: {config_file}"
+                    )
+
             self._record_result("distributed_tracing_setup", True)
-            
+
         except Exception as e:
             self._record_result("distributed_tracing_setup", False, str(e))
 
@@ -308,30 +374,34 @@ class MonitoringLoggingSystemTest:
             # æª¢æŸ¥ ELK Stack é…ç½®
             logstash_config = self.project_root / "monitoring/logstash"
             assert logstash_config.exists(), "Logstash é…ç½®ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             pipeline_config = logstash_config / "pipeline/logstash.conf"
             assert pipeline_config.exists(), "Logstash ç®¡é“é…ç½®ä¸å­˜åœ¨"
-            
+
             pipeline_content = pipeline_config.read_text()
             assert "input" in pipeline_content, "ç¼ºå°‘è¼¸å…¥é…ç½®"
             assert "filter" in pipeline_content, "ç¼ºå°‘éæ¿¾å™¨é…ç½®"
             assert "output" in pipeline_content, "ç¼ºå°‘è¼¸å‡ºé…ç½®"
-            
+
             # æª¢æŸ¥ Fluent Bit é…ç½®
-            fluent_config = self.project_root / "monitoring/logging/fluent-bit.conf"
+            fluent_config = (
+                self.project_root / "monitoring/logging/fluent-bit.conf"
+            )
             assert fluent_config.exists(), "Fluent Bit é…ç½®ä¸å­˜åœ¨"
-            
+
             fluent_content = fluent_config.read_text()
             assert "[INPUT]" in fluent_content, "ç¼ºå°‘è¼¸å…¥é…ç½®"
             assert "[OUTPUT]" in fluent_content, "ç¼ºå°‘è¼¸å‡ºé…ç½®"
             assert "[FILTER]" in fluent_content, "ç¼ºå°‘éæ¿¾é…ç½®"
-            
+
             # æª¢æŸ¥è§£æå™¨é…ç½®
-            parsers_config = self.project_root / "monitoring/logging/parsers.conf"
+            parsers_config = (
+                self.project_root / "monitoring/logging/parsers.conf"
+            )
             assert parsers_config.exists(), "è§£æå™¨é…ç½®ä¸å­˜åœ¨"
-            
+
             self._record_result("log_aggregation_pipeline", True)
-            
+
         except Exception as e:
             self._record_result("log_aggregation_pipeline", False, str(e))
 
@@ -341,81 +411,99 @@ class MonitoringLoggingSystemTest:
             # æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼ç›£æ§ä¸­é–“ä»¶
             monitoring_middleware = self.project_root / "monitoring/middleware"
             assert monitoring_middleware.exists(), "ç›£æ§ä¸­é–“ä»¶ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             expected_middleware = [
                 "performance_middleware.py",
                 "health_check_middleware.py",
-                "correlation_middleware.py"
+                "correlation_middleware.py",
             ]
-            
+
             for middleware in expected_middleware:
                 middleware_path = monitoring_middleware / middleware
                 assert middleware_path.exists(), f"ä¸­é–“ä»¶ä¸å­˜åœ¨: {middleware}"
-                
+
                 content = middleware_path.read_text()
-                assert "async def" in content or "def " in content, f"ä¸­é–“ä»¶ç¼ºå°‘å‡½æ•¸å®šç¾©: {middleware}"
-                
+                assert "async def" in content or "def " in content, (
+                    f"ä¸­é–“ä»¶ç¼ºå°‘å‡½æ•¸å®šç¾©: {middleware}"
+                )
+
                 if "performance" in middleware:
-                    assert "response_time" in content.lower(), f"ç¼ºå°‘å›æ‡‰æ™‚é–“ç›£æ§: {middleware}"
-                    assert "request_count" in content.lower(), f"ç¼ºå°‘è«‹æ±‚è¨ˆæ•¸: {middleware}"
-                
+                    assert "response_time" in content.lower(), (
+                        f"ç¼ºå°‘å›æ‡‰æ™‚é–“ç›£æ§: {middleware}"
+                    )
+                    assert "request_count" in content.lower(), (
+                        f"ç¼ºå°‘è«‹æ±‚è¨ˆæ•¸: {middleware}"
+                    )
+
                 elif "health" in middleware:
-                    assert "health" in content.lower(), f"ç¼ºå°‘å¥åº·æª¢æŸ¥: {middleware}"
-                    assert "status" in content.lower(), f"ç¼ºå°‘ç‹€æ…‹æª¢æŸ¥: {middleware}"
-            
+                    assert "health" in content.lower(), (
+                        f"ç¼ºå°‘å¥åº·æª¢æŸ¥: {middleware}"
+                    )
+                    assert "status" in content.lower(), (
+                        f"ç¼ºå°‘ç‹€æ…‹æª¢æŸ¥: {middleware}"
+                    )
+
             # æª¢æŸ¥è³‡æ–™åº«ç›£æ§é…ç½®
             db_monitoring = self.project_root / "monitoring/database"
             if db_monitoring.exists():
                 pg_exporter_config = db_monitoring / "postgres_exporter.yml"
                 if pg_exporter_config.exists():
                     if yaml:
-                        with open(pg_exporter_config, 'r') as f:
+                        with open(pg_exporter_config, "r") as f:
                             config = yaml.safe_load(f)
                         assert "queries" in config, "ç¼ºå°‘è³‡æ–™åº«æŸ¥è©¢é…ç½®"
                     else:
                         logger.info("è·³é YAML é…ç½®æª¢æŸ¥ (yaml æ¨¡çµ„æœªå®‰è£)")
-            
+
             self._record_result("performance_monitoring_integration", True)
-            
+
         except Exception as e:
-            self._record_result("performance_monitoring_integration", False, str(e))
+            self._record_result(
+                "performance_monitoring_integration", False, str(e)
+            )
 
     def test_business_metrics_tracking(self):
         """æ¸¬è©¦æ¥­å‹™æŒ‡æ¨™è¿½è¹¤"""
         try:
             # æª¢æŸ¥æ¥­å‹™æŒ‡æ¨™å®šç¾©
-            business_metrics = self.project_root / "monitoring/business_metrics"
+            business_metrics = (
+                self.project_root / "monitoring/business_metrics"
+            )
             assert business_metrics.exists(), "æ¥­å‹™æŒ‡æ¨™ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             metrics_config = business_metrics / "metrics_definition.json"
             assert metrics_config.exists(), "æ¥­å‹™æŒ‡æ¨™å®šç¾©ä¸å­˜åœ¨"
-            
-            with open(metrics_config, 'r') as f:
+
+            with open(metrics_config, "r") as f:
                 metrics = json.load(f)
-            
+
             # æª¢æŸ¥é—œéµæ¥­å‹™æŒ‡æ¨™
             expected_metrics = [
                 "video_generation_count",
-                "user_engagement_rate", 
+                "user_engagement_rate",
                 "platform_publish_success_rate",
                 "trend_analysis_accuracy",
-                "system_availability"
+                "system_availability",
             ]
-            
+
             defined_metrics = set(metrics.keys())
             missing_metrics = set(expected_metrics) - defined_metrics
             assert not missing_metrics, f"ç¼ºå°‘æ¥­å‹™æŒ‡æ¨™: {missing_metrics}"
-            
+
             # æª¢æŸ¥æŒ‡æ¨™æ”¶é›†å™¨
-            metrics_collector = business_metrics / "business_metrics_collector.py"
+            metrics_collector = (
+                business_metrics / "business_metrics_collector.py"
+            )
             assert metrics_collector.exists(), "æ¥­å‹™æŒ‡æ¨™æ”¶é›†å™¨ä¸å­˜åœ¨"
-            
+
             collector_content = metrics_collector.read_text()
-            assert "BusinessMetricsCollector" in collector_content, "ç¼ºå°‘æ¥­å‹™æŒ‡æ¨™æ”¶é›†å™¨é¡åˆ¥"
+            assert "BusinessMetricsCollector" in collector_content, (
+                "ç¼ºå°‘æ¥­å‹™æŒ‡æ¨™æ”¶é›†å™¨é¡åˆ¥"
+            )
             assert "collect_metrics" in collector_content, "ç¼ºå°‘æŒ‡æ¨™æ”¶é›†æ–¹æ³•"
-            
+
             self._record_result("business_metrics_tracking", True)
-            
+
         except Exception as e:
             self._record_result("business_metrics_tracking", False, str(e))
 
@@ -423,42 +511,61 @@ class MonitoringLoggingSystemTest:
         """æ¸¬è©¦ç›£æ§ç³»çµ± Docker Compose æ•´åˆ"""
         try:
             # æª¢æŸ¥ç›£æ§ Docker Compose æ–‡ä»¶
-            monitoring_compose = self.project_root / "docker-compose.monitoring.yml"
-            assert monitoring_compose.exists(), "ç›£æ§ Docker Compose æ–‡ä»¶ä¸å­˜åœ¨"
-            
+            monitoring_compose = (
+                self.project_root / "docker-compose.monitoring.yml"
+            )
+            assert monitoring_compose.exists(), (
+                "ç›£æ§ Docker Compose æ–‡ä»¶ä¸å­˜åœ¨"
+            )
+
             if yaml:
-                with open(monitoring_compose, 'r') as f:
+                with open(monitoring_compose, "r") as f:
                     compose_config = yaml.safe_load(f)
             else:
-                compose_config = {"services": {"prometheus": {"image": "prom/prometheus"}}}
-            
+                compose_config = {
+                    "services": {"prometheus": {"image": "prom/prometheus"}}
+                }
+
             assert "services" in compose_config, "ç¼ºå°‘æœå‹™å®šç¾©"
-            
+
             # æª¢æŸ¥å¿…è¦çš„ç›£æ§æœå‹™
             expected_services = [
-                "prometheus", "grafana", "alertmanager", 
-                "jaeger", "logstash", "elasticsearch", "kibana"
+                "prometheus",
+                "grafana",
+                "alertmanager",
+                "jaeger",
+                "logstash",
+                "elasticsearch",
+                "kibana",
             ]
-            
-            defined_services = set(compose_config['services'].keys())
+
+            defined_services = set(compose_config["services"].keys())
             missing_services = set(expected_services) - defined_services
-            
+
             # å…è¨±éƒ¨åˆ†æœå‹™ç¼ºå¤±ï¼ˆRed éšæ®µæœŸæœ›å¤±æ•—ï¼‰
             if missing_services:
                 logger.warning(f"ç¼ºå°‘ç›£æ§æœå‹™: {missing_services}")
-            
+
             # æª¢æŸ¥æœå‹™é…ç½®
-            for service_name, service_config in compose_config['services'].items():
+            for service_name, service_config in compose_config[
+                "services"
+            ].items():
                 if service_name in expected_services:
-                    assert "image" in service_config or "build" in service_config, f"æœå‹™ç¼ºå°‘æ˜ åƒé…ç½®: {service_name}"
-                    
+                    assert (
+                        "image" in service_config or "build" in service_config
+                    ), f"æœå‹™ç¼ºå°‘æ˜ åƒé…ç½®: {service_name}"
+
                     if "ports" in service_config:
-                        assert len(service_config['ports']) > 0, f"æœå‹™æœªæš´éœ²ç«¯å£: {service_name}"
-            
+                        assert len(service_config["ports"]) > 0, (
+                            f"æœå‹™æœªæš´éœ²ç«¯å£: {service_name}"
+                        )
+
             self._record_result("monitoring_docker_compose_integration", True)
-            
+
         except Exception as e:
-            self._record_result("monitoring_docker_compose_integration", False, str(e))
+            self._record_result(
+                "monitoring_docker_compose_integration", False, str(e)
+            )
 
     def test_health_check_endpoints(self):
         """æ¸¬è©¦å¥åº·æª¢æŸ¥ç«¯é»"""
@@ -466,16 +573,20 @@ class MonitoringLoggingSystemTest:
             # æª¢æŸ¥å¥åº·æª¢æŸ¥å¯¦ä½œ
             health_check_files = [
                 "monitoring/health-check/health_monitor.py",
-                "shared/health/health_checker.py"
+                "shared/health/health_checker.py",
             ]
-            
+
             for health_file in health_check_files:
                 health_path = self.project_root / health_file
                 if health_path.exists():  # å…è¨±æ–‡ä»¶ä¸å­˜åœ¨
                     content = health_path.read_text()
-                    assert "health" in content.lower(), f"ç¼ºå°‘å¥åº·æª¢æŸ¥å¯¦ä½œ: {health_file}"
-                    assert "status" in content.lower(), f"ç¼ºå°‘ç‹€æ…‹æª¢æŸ¥: {health_file}"
-            
+                    assert "health" in content.lower(), (
+                        f"ç¼ºå°‘å¥åº·æª¢æŸ¥å¯¦ä½œ: {health_file}"
+                    )
+                    assert "status" in content.lower(), (
+                        f"ç¼ºå°‘ç‹€æ…‹æª¢æŸ¥: {health_file}"
+                    )
+
             # æª¢æŸ¥å„æœå‹™çš„å¥åº·æª¢æŸ¥ç«¯é»å®šç¾©
             for service in self.services[:3]:  # åªæª¢æŸ¥å‰3å€‹æœå‹™
                 service_path = self.project_root / f"services/{service}"
@@ -485,11 +596,13 @@ class MonitoringLoggingSystemTest:
                         if py_file.name in ["main.py", "app.py", "health.py"]:
                             content = py_file.read_text()
                             if "/health" in content:
-                                assert "health" in content.lower(), f"å¥åº·æª¢æŸ¥ç«¯é»å¯¦ä½œä¸å®Œæ•´: {py_file}"
+                                assert "health" in content.lower(), (
+                                    f"å¥åº·æª¢æŸ¥ç«¯é»å¯¦ä½œä¸å®Œæ•´: {py_file}"
+                                )
                                 break
-            
+
             self._record_result("health_check_endpoints", True)
-            
+
         except Exception as e:
             self._record_result("health_check_endpoints", False, str(e))
 
@@ -497,59 +610,81 @@ class MonitoringLoggingSystemTest:
         """æ¸¬è©¦ç›£æ§é…ç½®é©—è­‰"""
         try:
             # æª¢æŸ¥é…ç½®é©—è­‰è…³æœ¬
-            config_validator = self.project_root / "scripts/validate-monitoring-config.py"
+            config_validator = (
+                self.project_root / "scripts/validate-monitoring-config.py"
+            )
             if not config_validator.exists():
-                config_validator = self.project_root / "monitoring/config_validator.py"
-            
+                config_validator = (
+                    self.project_root / "monitoring/config_validator.py"
+                )
+
             if config_validator.exists():
                 content = config_validator.read_text()
                 assert "validate" in content.lower(), "ç¼ºå°‘é…ç½®é©—è­‰åŠŸèƒ½"
-                assert "prometheus" in content.lower(), "ç¼ºå°‘ Prometheus é…ç½®é©—è­‰"
+                assert "prometheus" in content.lower(), (
+                    "ç¼ºå°‘ Prometheus é…ç½®é©—è­‰"
+                )
                 assert "grafana" in content.lower(), "ç¼ºå°‘ Grafana é…ç½®é©—è­‰"
-            
+
             # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ç¯„æœ¬
             env_example = self.project_root / ".env.monitoring.example"
             if env_example.exists():
                 env_content = env_example.read_text()
                 monitoring_vars = [
-                    "PROMETHEUS_PORT", "GRAFANA_ADMIN_PASSWORD", 
-                    "ALERTMANAGER_PORT", "JAEGER_PORT"
+                    "PROMETHEUS_PORT",
+                    "GRAFANA_ADMIN_PASSWORD",
+                    "ALERTMANAGER_PORT",
+                    "JAEGER_PORT",
                 ]
-                
+
                 for var in monitoring_vars:
                     if var not in env_content:
                         logger.warning(f"ç’°å¢ƒè®Šæ•¸ç¯„æœ¬ç¼ºå°‘: {var}")
-            
+
             self._record_result("monitoring_configuration_validation", True)
-            
+
         except Exception as e:
-            self._record_result("monitoring_configuration_validation", False, str(e))
+            self._record_result(
+                "monitoring_configuration_validation", False, str(e)
+            )
 
     def print_results(self):
         """æ‰“å°æ¸¬è©¦çµæœ"""
-        total_tests = self.results["tests_passed"] + self.results["tests_failed"]
-        success_rate = (self.results["tests_passed"] / total_tests * 100) if total_tests > 0 else 0
-        
+        total_tests = (
+            self.results["tests_passed"] + self.results["tests_failed"]
+        )
+        success_rate = (
+            (self.results["tests_passed"] / total_tests * 100)
+            if total_tests > 0
+            else 0
+        )
+
         logger.info("=" * 70)
         logger.info("ğŸ”´ TDD Red éšæ®µ: ç›£æ§å’Œæ—¥èªŒç³»çµ±æ¸¬è©¦çµæœ")
         logger.info("=" * 70)
         logger.info(f"âœ… é€šéæ¸¬è©¦: {self.results['tests_passed']}")
         logger.info(f"âŒ å¤±æ•—æ¸¬è©¦: {self.results['tests_failed']}")
         logger.info(f"ğŸ“ˆ ç•¶å‰å®Œæˆç‡: {success_rate:.1f}%")
-        
+
         if self.results["errors"]:
             logger.info("\nğŸ¯ éœ€è¦å¯¦ä½œçš„åŠŸèƒ½:")
             for error in self.results["errors"]:
                 logger.info(f"  - {error}")
-        
+
         # Red éšæ®µè©•ä¼°
         if success_rate < 30:
-            logger.info("\nğŸ”´ TDD Red éšæ®µç‹€æ…‹: å®Œç¾ - å¤§éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œå®šç¾©äº†æ¸…æ™°çš„ç›®æ¨™")
+            logger.info(
+                "\nğŸ”´ TDD Red éšæ®µç‹€æ…‹: å®Œç¾ - å¤§éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œå®šç¾©äº†æ¸…æ™°çš„ç›®æ¨™"
+            )
         elif success_rate < 60:
-            logger.info("\nğŸŸ¡ TDD Red éšæ®µç‹€æ…‹: è‰¯å¥½ - æœ‰äº›åŸºç¤å·²å­˜åœ¨ï¼Œéœ€è¦æ›´å¤šå¯¦ä½œ")
+            logger.info(
+                "\nğŸŸ¡ TDD Red éšæ®µç‹€æ…‹: è‰¯å¥½ - æœ‰äº›åŸºç¤å·²å­˜åœ¨ï¼Œéœ€è¦æ›´å¤šå¯¦ä½œ"
+            )
         else:
-            logger.info("\nğŸŸ¢ TDD Red éšæ®µç‹€æ…‹: æ„å¤– - å¾ˆå¤šåŠŸèƒ½å·²å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦èª¿æ•´æ¸¬è©¦")
-        
+            logger.info(
+                "\nğŸŸ¢ TDD Red éšæ®µç‹€æ…‹: æ„å¤– - å¾ˆå¤šåŠŸèƒ½å·²å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦èª¿æ•´æ¸¬è©¦"
+            )
+
         return success_rate < 50  # Red éšæ®µæœŸæœ›ä½æˆåŠŸç‡
 
 
@@ -558,9 +693,9 @@ def main():
     logger.info("ğŸ”´ é–‹å§‹ TDD Red éšæ®µ: ç›£æ§å’Œæ—¥èªŒç³»çµ±æ¸¬è©¦")
     logger.info("ç›®æ¨™: å®šç¾©å®Œæ•´ç›£æ§å’Œæ—¥èªŒç³»çµ±çš„æœŸæœ›è¡Œç‚º")
     logger.info("=" * 70)
-    
+
     test_suite = MonitoringLoggingSystemTest()
-    
+
     try:
         # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
         test_suite.test_structured_logging_configuration()
@@ -574,10 +709,10 @@ def main():
         test_suite.test_monitoring_docker_compose_integration()
         test_suite.test_health_check_endpoints()
         test_suite.test_monitoring_configuration_validation()
-        
+
         # æ‰“å°çµæœ
         is_proper_red = test_suite.print_results()
-        
+
         if is_proper_red:
             logger.info("\nğŸ‰ TDD Red éšæ®µæˆåŠŸï¼")
             logger.info("âœ¨ å·²å®šç¾©å®Œæ•´çš„ç›£æ§å’Œæ—¥èªŒç³»çµ±éœ€æ±‚")
@@ -585,9 +720,9 @@ def main():
         else:
             logger.info("\nğŸ¤” TDD Red éšæ®µæ„å¤–é€šéè¼ƒå¤šæ¸¬è©¦")
             logger.info("ğŸ”§ å¯èƒ½éœ€è¦èª¿æ•´æ¸¬è©¦æˆ–æª¢æŸ¥ç¾æœ‰å¯¦ä½œ")
-        
+
         return is_proper_red
-        
+
     except Exception as e:
         logger.error(f"âŒ Red éšæ®µæ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}")
         return False
@@ -595,10 +730,10 @@ def main():
 
 if __name__ == "__main__":
     success = main()
-    
+
     if success:
         logger.info("ğŸ TDD Red éšæ®µå®Œæˆ - ç›£æ§å’Œæ—¥èªŒç³»çµ±éœ€æ±‚å·²å®šç¾©")
         exit(0)
     else:
         logger.error("ğŸ›‘ TDD Red éšæ®µéœ€è¦èª¿æ•´")
-        exit(1)
\ No newline at end of file
+        exit(1)
diff --git a/auto_generate_video_fold6/test_monitoring_performance_optimization.py b/auto_generate_video_fold6/test_monitoring_performance_optimization.py
index 37f1a25..f5fa898 100644
--- a/auto_generate_video_fold6/test_monitoring_performance_optimization.py
+++ b/auto_generate_video_fold6/test_monitoring_performance_optimization.py
@@ -21,28 +21,29 @@ import threading
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 class MonitoringPerformanceOptimizationTest:
     """ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ– TDD Refactor æ¸¬è©¦å¥—ä»¶"""
-    
+
     def __init__(self):
         self.project_root = Path(__file__).parent
         self.results = {
             "tests_passed": 0,
             "tests_failed": 0,
             "errors": [],
-            "performance_metrics": {}
+            "performance_metrics": {},
         }
-        
+
         # æ•ˆèƒ½åŸºæº– (å¾®ç§’)
         self.performance_thresholds = {
             "log_entry_processing": 1000,  # 1ms
-            "metric_collection": 500,      # 0.5ms
-            "dashboard_query": 2000,       # 2ms
-            "alert_evaluation": 100,       # 0.1ms
-            "trace_span_creation": 50,     # 0.05ms
-            "correlation_id_lookup": 10,   # 0.01ms
+            "metric_collection": 500,  # 0.5ms
+            "dashboard_query": 2000,  # 2ms
+            "alert_evaluation": 100,  # 0.1ms
+            "trace_span_creation": 50,  # 0.05ms
+            "correlation_id_lookup": 10,  # 0.01ms
         }
-        
+
         # å¯è§€æ¸¬æ€§ç›®æ¨™
         self.observability_targets = {
             "log_retention_days": 30,
@@ -51,8 +52,14 @@ class MonitoringPerformanceOptimizationTest:
             "dashboard_refresh_seconds": 5,
             "alert_notification_seconds": 30,
         }
-    
-    def _record_result(self, test_name: str, success: bool, error: str = None, metrics: Dict[str, float] = None):
+
+    def _record_result(
+        self,
+        test_name: str,
+        success: bool,
+        error: str = None,
+        metrics: Dict[str, float] = None,
+    ):
         """è¨˜éŒ„æ¸¬è©¦çµæœå’Œæ•ˆèƒ½æŒ‡æ¨™"""
         if success:
             self.results["tests_passed"] += 1
@@ -61,7 +68,7 @@ class MonitoringPerformanceOptimizationTest:
             self.results["tests_failed"] += 1
             self.results["errors"].append(f"{test_name}: {error}")
             logger.error(f"âŒ {test_name} å¤±æ•—: {error}")
-        
+
         if metrics:
             self.results["performance_metrics"][test_name] = metrics
 
@@ -69,138 +76,187 @@ class MonitoringPerformanceOptimizationTest:
         """æ¸¬è©¦çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„æ•ˆèƒ½"""
         try:
             # æª¢æŸ¥æ—¥èªŒè¨˜éŒ„å™¨æ˜¯å¦å­˜åœ¨
-            logger_module = self.project_root / "monitoring/logging/structured_logger.py"
+            logger_module = (
+                self.project_root / "monitoring/logging/structured_logger.py"
+            )
             assert logger_module.exists(), "çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„å™¨ä¸å­˜åœ¨"
-            
+
             # æ¨¡æ“¬é«˜ä½µç™¼æ—¥èªŒè¨˜éŒ„æ¸¬è©¦
             import importlib.util
-            spec = importlib.util.spec_from_file_location("structured_logger", logger_module)
+
+            spec = importlib.util.spec_from_file_location(
+                "structured_logger", logger_module
+            )
             structured_logger = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(structured_logger)
-            
+
             # å‰µå»ºæ¸¬è©¦æ—¥èªŒè¨˜éŒ„å™¨
             test_logger = structured_logger.get_logger("performance_test")
-            
+
             # æ•ˆèƒ½æ¸¬è©¦ï¼šå–®å€‹æ—¥èªŒæ¢ç›®è™•ç†æ™‚é–“
             log_times = []
             for i in range(100):
                 start_time = time.perf_counter()
-                test_logger.info(f"Performance test log entry {i}", extra={
-                    "test_data": {"iteration": i, "timestamp": time.time()}
-                })
+                test_logger.info(
+                    f"Performance test log entry {i}",
+                    extra={
+                        "test_data": {"iteration": i, "timestamp": time.time()}
+                    },
+                )
                 end_time = time.perf_counter()
-                log_times.append((end_time - start_time) * 1000000)  # è½‰æ›ç‚ºå¾®ç§’
-            
+                log_times.append(
+                    (end_time - start_time) * 1000000
+                )  # è½‰æ›ç‚ºå¾®ç§’
+
             avg_log_time = statistics.mean(log_times)
-            p95_log_time = statistics.quantiles(log_times, n=20)[18]  # 95th percentile
+            p95_log_time = statistics.quantiles(log_times, n=20)[
+                18
+            ]  # 95th percentile
             max_log_time = max(log_times)
-            
+
             # é©—è­‰æ•ˆèƒ½è¦æ±‚
             threshold = self.performance_thresholds["log_entry_processing"]
-            assert avg_log_time < threshold, f"å¹³å‡æ—¥èªŒè™•ç†æ™‚é–“ {avg_log_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
-            assert p95_log_time < threshold * 2, f"P95 æ—¥èªŒè™•ç†æ™‚é–“ {p95_log_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold * 2}Î¼s"
-            
+            assert avg_log_time < threshold, (
+                f"å¹³å‡æ—¥èªŒè™•ç†æ™‚é–“ {avg_log_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
+            )
+            assert p95_log_time < threshold * 2, (
+                f"P95 æ—¥èªŒè™•ç†æ™‚é–“ {p95_log_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold * 2}Î¼s"
+            )
+
             # ä½µç™¼æ¸¬è©¦
             def concurrent_logging(thread_id, log_count):
                 thread_times = []
                 for i in range(log_count):
                     start_time = time.perf_counter()
-                    test_logger.info(f"Concurrent log from thread {thread_id}, entry {i}")
+                    test_logger.info(
+                        f"Concurrent log from thread {thread_id}, entry {i}"
+                    )
                     end_time = time.perf_counter()
                     thread_times.append((end_time - start_time) * 1000000)
                 return thread_times
-            
-            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
-                futures = [executor.submit(concurrent_logging, i, 20) for i in range(10)]
+
+            with concurrent.futures.ThreadPoolExecutor(
+                max_workers=10
+            ) as executor:
+                futures = [
+                    executor.submit(concurrent_logging, i, 20)
+                    for i in range(10)
+                ]
                 concurrent_times = []
                 for future in concurrent.futures.as_completed(futures):
                     concurrent_times.extend(future.result())
-            
+
             concurrent_avg = statistics.mean(concurrent_times)
-            
+
             metrics = {
                 "avg_log_time_us": avg_log_time,
                 "p95_log_time_us": p95_log_time,
                 "max_log_time_us": max_log_time,
                 "concurrent_avg_time_us": concurrent_avg,
-                "throughput_logs_per_second": 1000000 / avg_log_time
+                "throughput_logs_per_second": 1000000 / avg_log_time,
             }
-            
-            self._record_result("structured_logging_performance", True, metrics=metrics)
-            
+
+            self._record_result(
+                "structured_logging_performance", True, metrics=metrics
+            )
+
         except Exception as e:
-            self._record_result("structured_logging_performance", False, str(e))
+            self._record_result(
+                "structured_logging_performance", False, str(e)
+            )
 
     def test_prometheus_metrics_performance(self):
         """æ¸¬è©¦ Prometheus æŒ‡æ¨™æ”¶é›†æ•ˆèƒ½"""
         try:
             # æª¢æŸ¥æ¥­å‹™æŒ‡æ¨™æ”¶é›†å™¨
-            metrics_collector_path = self.project_root / "monitoring/business_metrics/business_metrics_collector.py"
+            metrics_collector_path = (
+                self.project_root
+                / "monitoring/business_metrics/business_metrics_collector.py"
+            )
             assert metrics_collector_path.exists(), "æ¥­å‹™æŒ‡æ¨™æ”¶é›†å™¨ä¸å­˜åœ¨"
-            
+
             # å°å…¥æŒ‡æ¨™æ”¶é›†å™¨
             import importlib.util
-            spec = importlib.util.spec_from_file_location("business_metrics_collector", metrics_collector_path)
+
+            spec = importlib.util.spec_from_file_location(
+                "business_metrics_collector", metrics_collector_path
+            )
             metrics_module = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(metrics_module)
-            
+
             # å‰µå»ºæ¸¬è©¦æ”¶é›†å™¨
             collector = metrics_module.BusinessMetricsCollector()
-            
+
             # æ•ˆèƒ½æ¸¬è©¦ï¼šæŒ‡æ¨™è¨˜éŒ„æ™‚é–“
             metric_times = []
             for i in range(200):
                 start_time = time.perf_counter()
-                collector.record_metric("test_counter", i, {"iteration": str(i)})
+                collector.record_metric(
+                    "test_counter", i, {"iteration": str(i)}
+                )
                 end_time = time.perf_counter()
                 metric_times.append((end_time - start_time) * 1000000)
-            
+
             avg_metric_time = statistics.mean(metric_times)
             p95_metric_time = statistics.quantiles(metric_times, n=20)[18]
-            
+
             # æ‰¹é‡æŒ‡æ¨™æ¸¬è©¦
             batch_start = time.perf_counter()
             for i in range(1000):
                 collector.record_metric("batch_test", i, {"batch": "true"})
             batch_end = time.perf_counter()
-            batch_time = (batch_end - batch_start) * 1000000 / 1000  # æ¯å€‹æŒ‡æ¨™çš„å¹³å‡æ™‚é–“
-            
+            batch_time = (
+                (batch_end - batch_start) * 1000000 / 1000
+            )  # æ¯å€‹æŒ‡æ¨™çš„å¹³å‡æ™‚é–“
+
             # é©—è­‰æ•ˆèƒ½è¦æ±‚
             threshold = self.performance_thresholds["metric_collection"]
-            assert avg_metric_time < threshold, f"å¹³å‡æŒ‡æ¨™æ”¶é›†æ™‚é–“ {avg_metric_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
-            
+            assert avg_metric_time < threshold, (
+                f"å¹³å‡æŒ‡æ¨™æ”¶é›†æ™‚é–“ {avg_metric_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
+            )
+
             # æ¸¬è©¦æŒ‡æ¨™æ‘˜è¦ç”Ÿæˆæ•ˆèƒ½
             summary_start = time.perf_counter()
             summary = collector.get_all_metrics_summary()
             summary_end = time.perf_counter()
             summary_time = (summary_end - summary_start) * 1000
-            
+
             metrics = {
                 "avg_metric_time_us": avg_metric_time,
                 "p95_metric_time_us": p95_metric_time,
                 "batch_avg_time_us": batch_time,
                 "summary_generation_time_ms": summary_time,
-                "metrics_per_second": 1000000 / avg_metric_time
+                "metrics_per_second": 1000000 / avg_metric_time,
             }
-            
-            self._record_result("prometheus_metrics_performance", True, metrics=metrics)
-            
+
+            self._record_result(
+                "prometheus_metrics_performance", True, metrics=metrics
+            )
+
         except Exception as e:
-            self._record_result("prometheus_metrics_performance", False, str(e))
+            self._record_result(
+                "prometheus_metrics_performance", False, str(e)
+            )
 
     def test_correlation_middleware_performance(self):
         """æ¸¬è©¦é—œè¯IDä¸­é–“ä»¶æ•ˆèƒ½"""
         try:
             # æª¢æŸ¥é—œè¯ä¸­é–“ä»¶
-            middleware_path = self.project_root / "monitoring/middleware/correlation_middleware.py"
+            middleware_path = (
+                self.project_root
+                / "monitoring/middleware/correlation_middleware.py"
+            )
             assert middleware_path.exists(), "é—œè¯IDä¸­é–“ä»¶ä¸å­˜åœ¨"
-            
+
             # å°å…¥ä¸­é–“ä»¶
             import importlib.util
-            spec = importlib.util.spec_from_file_location("correlation_middleware", middleware_path)
+
+            spec = importlib.util.spec_from_file_location(
+                "correlation_middleware", middleware_path
+            )
             middleware_module = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(middleware_module)
-            
+
             # æ¸¬è©¦é—œè¯IDç”Ÿæˆå’ŒæŸ¥æ‰¾æ•ˆèƒ½
             correlation_times = []
             for i in range(1000):
@@ -208,68 +264,84 @@ class MonitoringPerformanceOptimizationTest:
                 correlation_id = middleware_module.get_correlation_id()
                 end_time = time.perf_counter()
                 correlation_times.append((end_time - start_time) * 1000000)
-            
+
             avg_correlation_time = statistics.mean(correlation_times)
             threshold = self.performance_thresholds["correlation_id_lookup"]
-            
+
             # æ¸¬è©¦è¿½è¸ªäº‹ä»¶è¨˜éŒ„æ•ˆèƒ½
             event_times = []
             for i in range(100):
                 start_time = time.perf_counter()
-                middleware_module.log_trace_event(f"test_event_{i}", iteration=i)
+                middleware_module.log_trace_event(
+                    f"test_event_{i}", iteration=i
+                )
                 end_time = time.perf_counter()
                 event_times.append((end_time - start_time) * 1000000)
-            
+
             avg_event_time = statistics.mean(event_times)
-            
+
             metrics = {
                 "avg_correlation_lookup_us": avg_correlation_time,
                 "avg_trace_event_time_us": avg_event_time,
-                "correlation_lookups_per_second": 1000000 / avg_correlation_time if avg_correlation_time > 0 else 0
+                "correlation_lookups_per_second": 1000000
+                / avg_correlation_time
+                if avg_correlation_time > 0
+                else 0,
             }
-            
-            self._record_result("correlation_middleware_performance", True, metrics=metrics)
-            
+
+            self._record_result(
+                "correlation_middleware_performance", True, metrics=metrics
+            )
+
         except Exception as e:
-            self._record_result("correlation_middleware_performance", False, str(e))
+            self._record_result(
+                "correlation_middleware_performance", False, str(e)
+            )
 
     def test_opentelemetry_span_performance(self):
         """æ¸¬è©¦ OpenTelemetry Span å‰µå»ºæ•ˆèƒ½"""
         try:
             # æª¢æŸ¥ OpenTelemetry ä¸­é–“ä»¶
-            otel_path = self.project_root / "monitoring/tracing/opentelemetry_middleware.py"
+            otel_path = (
+                self.project_root
+                / "monitoring/tracing/opentelemetry_middleware.py"
+            )
             assert otel_path.exists(), "OpenTelemetry ä¸­é–“ä»¶ä¸å­˜åœ¨"
-            
+
             # å°å…¥ OpenTelemetry æ¨¡çµ„
             import importlib.util
-            spec = importlib.util.spec_from_file_location("opentelemetry_middleware", otel_path)
+
+            spec = importlib.util.spec_from_file_location(
+                "opentelemetry_middleware", otel_path
+            )
             otel_module = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(otel_module)
-            
+
             # å‰µå»ºæ¸¬è©¦ä¸­é–“ä»¶
             otel_middleware = otel_module.OpenTelemetryMiddleware(
-                service_name="performance_test",
-                service_version="1.0.0"
+                service_name="performance_test", service_version="1.0.0"
             )
-            
+
             # æ¸¬è©¦ Span å‰µå»ºæ•ˆèƒ½
             span_times = []
             for i in range(500):
                 start_time = time.perf_counter()
-                span = otel_middleware.create_span(f"test_span_{i}", {
-                    "iteration": i,
-                    "test_type": "performance"
-                })
+                span = otel_middleware.create_span(
+                    f"test_span_{i}",
+                    {"iteration": i, "test_type": "performance"},
+                )
                 span.end()
                 end_time = time.perf_counter()
                 span_times.append((end_time - start_time) * 1000000)
-            
+
             avg_span_time = statistics.mean(span_times)
             p95_span_time = statistics.quantiles(span_times, n=20)[18]
-            
+
             threshold = self.performance_thresholds["trace_span_creation"]
-            assert avg_span_time < threshold, f"å¹³å‡ Span å‰µå»ºæ™‚é–“ {avg_span_time:.3f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
-            
+            assert avg_span_time < threshold, (
+                f"å¹³å‡ Span å‰µå»ºæ™‚é–“ {avg_span_time:.3f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
+            )
+
             # æ¸¬è©¦åµŒå¥— Span æ•ˆèƒ½
             nested_start = time.perf_counter()
             parent_span = otel_middleware.create_span("parent_span")
@@ -278,94 +350,124 @@ class MonitoringPerformanceOptimizationTest:
                 child_span.end()
             parent_span.end()
             nested_end = time.perf_counter()
-            nested_time = (nested_end - nested_start) * 1000000 / 11  # å¹³å‡æ¯å€‹ span
-            
+            nested_time = (
+                (nested_end - nested_start) * 1000000 / 11
+            )  # å¹³å‡æ¯å€‹ span
+
             metrics = {
                 "avg_span_creation_us": avg_span_time,
                 "p95_span_creation_us": p95_span_time,
                 "nested_span_avg_us": nested_time,
-                "spans_per_second": 1000000 / avg_span_time
+                "spans_per_second": 1000000 / avg_span_time,
             }
-            
-            self._record_result("opentelemetry_span_performance", True, metrics=metrics)
-            
+
+            self._record_result(
+                "opentelemetry_span_performance", True, metrics=metrics
+            )
+
         except Exception as e:
-            self._record_result("opentelemetry_span_performance", False, str(e))
+            self._record_result(
+                "opentelemetry_span_performance", False, str(e)
+            )
 
     def test_grafana_dashboard_query_optimization(self):
         """æ¸¬è©¦ Grafana å„€è¡¨æ¿æŸ¥è©¢å„ªåŒ–"""
         try:
             # æª¢æŸ¥å„€è¡¨æ¿é…ç½®
-            dashboards_dir = self.project_root / "monitoring/grafana/dashboards"
+            dashboards_dir = (
+                self.project_root / "monitoring/grafana/dashboards"
+            )
             assert dashboards_dir.exists(), "Grafana å„€è¡¨æ¿ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             dashboard_files = list(dashboards_dir.glob("*.json"))
             assert len(dashboard_files) > 0, "æ²’æœ‰æ‰¾åˆ°å„€è¡¨æ¿é…ç½®æª”æ¡ˆ"
-            
+
             # åˆ†ææ¯å€‹å„€è¡¨æ¿çš„æŸ¥è©¢è¤‡é›œåº¦
             dashboard_metrics = {}
-            
+
             for dashboard_file in dashboard_files:
-                with open(dashboard_file, 'r') as f:
+                with open(dashboard_file, "r") as f:
                     dashboard_config = json.load(f)
-                
+
                 dashboard_name = dashboard_file.stem
-                panels = dashboard_config.get('dashboard', {}).get('panels', [])
-                
+                panels = dashboard_config.get("dashboard", {}).get(
+                    "panels", []
+                )
+
                 query_count = 0
                 complex_queries = 0
-                
+
                 for panel in panels:
-                    targets = panel.get('targets', [])
+                    targets = panel.get("targets", [])
                     query_count += len(targets)
-                    
+
                     for target in targets:
-                        expr = target.get('expr', '')
+                        expr = target.get("expr", "")
                         # æª¢æŸ¥æŸ¥è©¢è¤‡é›œåº¦ï¼ˆç°¡å–®æŒ‡æ¨™ï¼‰
-                        if any(func in expr for func in ['rate(', 'increase(', 'histogram_quantile(']):
+                        if any(
+                            func in expr
+                            for func in [
+                                "rate(",
+                                "increase(",
+                                "histogram_quantile(",
+                            ]
+                        ):
                             complex_queries += 1
-                
+
                 dashboard_metrics[dashboard_name] = {
                     "total_queries": query_count,
                     "complex_queries": complex_queries,
-                    "complexity_ratio": complex_queries / query_count if query_count > 0 else 0,
-                    "panels_count": len(panels)
+                    "complexity_ratio": complex_queries / query_count
+                    if query_count > 0
+                    else 0,
+                    "panels_count": len(panels),
                 }
-            
+
             # é©—è­‰å„€è¡¨æ¿æ•ˆèƒ½ç‰¹æ€§
-            total_queries = sum(m["total_queries"] for m in dashboard_metrics.values())
+            total_queries = sum(
+                m["total_queries"] for m in dashboard_metrics.values()
+            )
             avg_queries_per_dashboard = total_queries / len(dashboard_metrics)
-            
+
             # å»ºè­°ï¼šæ¯å€‹å„€è¡¨æ¿ä¸è¶…é 20 å€‹æŸ¥è©¢ä»¥ä¿æŒæ•ˆèƒ½
-            assert avg_queries_per_dashboard <= 20, f"å¹³å‡æ¯å€‹å„€è¡¨æ¿æŸ¥è©¢æ•¸ {avg_queries_per_dashboard:.1f} éå¤šï¼Œå»ºè­°ä¸è¶…é 20"
-            
+            assert avg_queries_per_dashboard <= 20, (
+                f"å¹³å‡æ¯å€‹å„€è¡¨æ¿æŸ¥è©¢æ•¸ {avg_queries_per_dashboard:.1f} éå¤šï¼Œå»ºè­°ä¸è¶…é 20"
+            )
+
             metrics = {
                 "total_dashboards": len(dashboard_metrics),
                 "total_queries": total_queries,
                 "avg_queries_per_dashboard": avg_queries_per_dashboard,
-                "dashboard_details": dashboard_metrics
+                "dashboard_details": dashboard_metrics,
             }
-            
-            self._record_result("grafana_dashboard_query_optimization", True, metrics=metrics)
-            
+
+            self._record_result(
+                "grafana_dashboard_query_optimization", True, metrics=metrics
+            )
+
         except Exception as e:
-            self._record_result("grafana_dashboard_query_optimization", False, str(e))
+            self._record_result(
+                "grafana_dashboard_query_optimization", False, str(e)
+            )
 
     def test_log_aggregation_performance(self):
         """æ¸¬è©¦æ—¥èªŒèšåˆç®¡é“æ•ˆèƒ½"""
         try:
             # æª¢æŸ¥ Logstash é…ç½®
-            logstash_config = self.project_root / "monitoring/logstash/pipeline/logstash.conf"
+            logstash_config = (
+                self.project_root
+                / "monitoring/logstash/pipeline/logstash.conf"
+            )
             assert logstash_config.exists(), "Logstash é…ç½®ä¸å­˜åœ¨"
-            
+
             config_content = logstash_config.read_text()
-            
+
             # åˆ†æé…ç½®æ•ˆèƒ½ç‰¹æ€§
             has_grok_filter = "grok" in config_content
             has_json_filter = "json" in config_content
             has_date_filter = "date" in config_content
             has_mutate_filter = "mutate" in config_content
-            
+
             # è¨ˆç®—é æœŸè™•ç†å»¶é²ï¼ˆåŸºæ–¼éæ¿¾å™¨è¤‡é›œåº¦ï¼‰
             base_latency = 10  # åŸºç¤å»¶é² ms
             if has_grok_filter:
@@ -376,38 +478,44 @@ class MonitoringPerformanceOptimizationTest:
                 base_latency += 3  # æ—¥æœŸè§£æ
             if has_mutate_filter:
                 base_latency += 1  # å­—æ®µè®Šæ›´
-            
+
             # æª¢æŸ¥ Fluent Bit é…ç½®
-            fluent_config = self.project_root / "monitoring/logging/fluent-bit.conf"
+            fluent_config = (
+                self.project_root / "monitoring/logging/fluent-bit.conf"
+            )
             assert fluent_config.exists(), "Fluent Bit é…ç½®ä¸å­˜åœ¨"
-            
+
             fluent_content = fluent_config.read_text()
-            
+
             # è¨ˆç®—ç·©è¡é…ç½®
             buffer_chunk_size = "1MB"  # é è¨­å€¼
-            buffer_max_size = "5MB"    # é è¨­å€¼
-            
+            buffer_max_size = "5MB"  # é è¨­å€¼
+
             if "Chunk_Size" in fluent_content:
                 # æå–å¯¦éš›é…ç½®å€¼ï¼ˆç°¡åŒ–å¯¦ä½œï¼‰
-                for line in fluent_content.split('\n'):
+                for line in fluent_content.split("\n"):
                     if "Chunk_Size" in line:
                         buffer_chunk_size = line.split()[-1]
                         break
-            
+
             metrics = {
                 "estimated_processing_latency_ms": base_latency,
                 "has_grok_filter": has_grok_filter,
                 "has_json_filter": has_json_filter,
                 "has_date_filter": has_date_filter,
                 "buffer_chunk_size": buffer_chunk_size,
-                "buffer_max_size": buffer_max_size
+                "buffer_max_size": buffer_max_size,
             }
-            
+
             # æ•ˆèƒ½é©—è­‰ï¼šé æœŸå»¶é²ä¸è¶…é 50ms
-            assert base_latency <= 50, f"é æœŸæ—¥èªŒè™•ç†å»¶é² {base_latency}ms éé«˜ï¼Œå»ºè­°ä¸è¶…é 50ms"
-            
-            self._record_result("log_aggregation_performance", True, metrics=metrics)
-            
+            assert base_latency <= 50, (
+                f"é æœŸæ—¥èªŒè™•ç†å»¶é² {base_latency}ms éé«˜ï¼Œå»ºè­°ä¸è¶…é 50ms"
+            )
+
+            self._record_result(
+                "log_aggregation_performance", True, metrics=metrics
+            )
+
         except Exception as e:
             self._record_result("log_aggregation_performance", False, str(e))
 
@@ -417,69 +525,84 @@ class MonitoringPerformanceOptimizationTest:
             # æª¢æŸ¥è­¦å ±è¦å‰‡é…ç½®
             rules_dir = self.project_root / "monitoring/prometheus/rules"
             assert rules_dir.exists(), "è­¦å ±è¦å‰‡ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             rule_files = list(rules_dir.glob("*.yml"))
             assert len(rule_files) > 0, "æ²’æœ‰æ‰¾åˆ°è­¦å ±è¦å‰‡æª”æ¡ˆ"
-            
+
             total_rules = 0
             complex_rules = 0
             rule_complexity_scores = []
-            
+
             try:
                 import yaml
+
                 yaml_available = True
             except ImportError:
                 yaml_available = False
-            
+
             for rule_file in rule_files:
                 if yaml_available:
-                    with open(rule_file, 'r') as f:
+                    with open(rule_file, "r") as f:
                         rules_config = yaml.safe_load(f)
-                    
-                    for group in rules_config.get('groups', []):
-                        for rule in group.get('rules', []):
+
+                    for group in rules_config.get("groups", []):
+                        for rule in group.get("rules", []):
                             total_rules += 1
-                            expr = rule.get('expr', '')
-                            
+                            expr = rule.get("expr", "")
+
                             # è¨ˆç®—è¦å‰‡è¤‡é›œåº¦åˆ†æ•¸
                             complexity = 0
-                            complexity += expr.count('(')  # æ‹¬è™Ÿæ•¸é‡
-                            complexity += expr.count('rate(') * 2  # rate å‡½æ•¸
-                            complexity += expr.count('histogram_quantile(') * 3  # åˆ†ä½æ•¸è¨ˆç®—
-                            complexity += expr.count('by(') * 1  # åˆ†çµ„æ“ä½œ
-                            
+                            complexity += expr.count("(")  # æ‹¬è™Ÿæ•¸é‡
+                            complexity += expr.count("rate(") * 2  # rate å‡½æ•¸
+                            complexity += (
+                                expr.count("histogram_quantile(") * 3
+                            )  # åˆ†ä½æ•¸è¨ˆç®—
+                            complexity += expr.count("by(") * 1  # åˆ†çµ„æ“ä½œ
+
                             rule_complexity_scores.append(complexity)
-                            
+
                             if complexity > 5:  # è¤‡é›œè¦å‰‡é–¾å€¼
                                 complex_rules += 1
                 else:
                     # ç°¡å–®æ–‡æœ¬åˆ†æ
                     content = rule_file.read_text()
-                    total_rules += content.count('alert:')
-                    complex_rules += content.count('histogram_quantile')
-            
-            avg_complexity = statistics.mean(rule_complexity_scores) if rule_complexity_scores else 0
-            max_complexity = max(rule_complexity_scores) if rule_complexity_scores else 0
-            
+                    total_rules += content.count("alert:")
+                    complex_rules += content.count("histogram_quantile")
+
+            avg_complexity = (
+                statistics.mean(rule_complexity_scores)
+                if rule_complexity_scores
+                else 0
+            )
+            max_complexity = (
+                max(rule_complexity_scores) if rule_complexity_scores else 0
+            )
+
             # é ä¼°è­¦å ±è©•ä¼°æ™‚é–“ï¼ˆåŸºæ–¼è¤‡é›œåº¦ï¼‰
             base_eval_time = 10  # åŸºç¤è©•ä¼°æ™‚é–“ Î¼s
             estimated_eval_time = base_eval_time + (avg_complexity * 20)
-            
+
             metrics = {
                 "total_rules": total_rules,
                 "complex_rules": complex_rules,
-                "complexity_ratio": complex_rules / total_rules if total_rules > 0 else 0,
+                "complexity_ratio": complex_rules / total_rules
+                if total_rules > 0
+                else 0,
                 "avg_complexity_score": avg_complexity,
                 "max_complexity_score": max_complexity,
-                "estimated_eval_time_us": estimated_eval_time
+                "estimated_eval_time_us": estimated_eval_time,
             }
-            
+
             # æ•ˆèƒ½é©—è­‰
             threshold = self.performance_thresholds["alert_evaluation"]
-            assert estimated_eval_time <= threshold, f"é ä¼°è­¦å ±è©•ä¼°æ™‚é–“ {estimated_eval_time:.1f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
-            
-            self._record_result("alert_evaluation_performance", True, metrics=metrics)
-            
+            assert estimated_eval_time <= threshold, (
+                f"é ä¼°è­¦å ±è©•ä¼°æ™‚é–“ {estimated_eval_time:.1f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
+            )
+
+            self._record_result(
+                "alert_evaluation_performance", True, metrics=metrics
+            )
+
         except Exception as e:
             self._record_result("alert_evaluation_performance", False, str(e))
 
@@ -487,45 +610,58 @@ class MonitoringPerformanceOptimizationTest:
         """æ¸¬è©¦å¯è§€æ¸¬æ€§é…ç½®å„ªåŒ–"""
         try:
             # æª¢æŸ¥è³‡æ–™ä¿ç•™é…ç½®
-            prometheus_config = self.project_root / "monitoring/prometheus/prometheus.yml"
+            prometheus_config = (
+                self.project_root / "monitoring/prometheus/prometheus.yml"
+            )
             assert prometheus_config.exists(), "Prometheus é…ç½®ä¸å­˜åœ¨"
-            
+
             config_content = prometheus_config.read_text()
-            
+
             # æª¢æŸ¥æŠ“å–é–“éš”é…ç½®
             scrape_interval = "15s"  # é è¨­å€¼
             if "scrape_interval:" in config_content:
-                for line in config_content.split('\n'):
-                    if "scrape_interval:" in line and not line.strip().startswith('#'):
-                        scrape_interval = line.split(':')[-1].strip()
+                for line in config_content.split("\n"):
+                    if (
+                        "scrape_interval:" in line
+                        and not line.strip().startswith("#")
+                    ):
+                        scrape_interval = line.split(":")[-1].strip()
                         break
-            
+
             # è½‰æ›ç‚ºç§’
             interval_seconds = 15
-            if scrape_interval.endswith('s'):
+            if scrape_interval.endswith("s"):
                 interval_seconds = int(scrape_interval[:-1])
-            elif scrape_interval.endswith('m'):
+            elif scrape_interval.endswith("m"):
                 interval_seconds = int(scrape_interval[:-1]) * 60
-            
+
             # æª¢æŸ¥å„²å­˜é…ç½®ï¼ˆæ¨¡æ“¬ï¼‰
             retention_days = 15  # Prometheus é è¨­
             retention_size = "10GB"
-            
+
             # æª¢æŸ¥ Grafana é…ç½®
-            grafana_config = self.project_root / "monitoring/grafana/grafana.ini"
+            grafana_config = (
+                self.project_root / "monitoring/grafana/grafana.ini"
+            )
             if grafana_config.exists():
                 grafana_content = grafana_config.read_text()
                 # æª¢æŸ¥ç·©å­˜é…ç½®
                 has_query_cache = "query_caching_enabled" in grafana_content
             else:
                 has_query_cache = False
-            
+
             # è¨ˆç®—é ä¼°è³‡æ–™é‡
             services_count = 8  # åŸºæ–¼ç³»çµ±æœå‹™æ•¸é‡
             metrics_per_service = 50  # æ¯å€‹æœå‹™çš„å¹³å‡æŒ‡æ¨™æ•¸
-            data_points_per_day = services_count * metrics_per_service * (86400 / interval_seconds)
-            estimated_storage_mb_per_day = data_points_per_day * 0.1 / 1024  # ä¼°ç®—
-            
+            data_points_per_day = (
+                services_count
+                * metrics_per_service
+                * (86400 / interval_seconds)
+            )
+            estimated_storage_mb_per_day = (
+                data_points_per_day * 0.1 / 1024
+            )  # ä¼°ç®—
+
             metrics = {
                 "scrape_interval_seconds": interval_seconds,
                 "retention_days": retention_days,
@@ -533,57 +669,83 @@ class MonitoringPerformanceOptimizationTest:
                 "estimated_data_points_per_day": data_points_per_day,
                 "estimated_storage_mb_per_day": estimated_storage_mb_per_day,
                 "has_grafana_query_cache": has_query_cache,
-                "services_monitored": services_count
+                "services_monitored": services_count,
             }
-            
+
             # é©—è­‰é…ç½®åˆç†æ€§
-            target_interval = self.observability_targets["metric_resolution_seconds"]
-            assert interval_seconds <= target_interval, f"æŠ“å–é–“éš” {interval_seconds}s è¶…éç›®æ¨™ {target_interval}s"
-            
+            target_interval = self.observability_targets[
+                "metric_resolution_seconds"
+            ]
+            assert interval_seconds <= target_interval, (
+                f"æŠ“å–é–“éš” {interval_seconds}s è¶…éç›®æ¨™ {target_interval}s"
+            )
+
             target_retention = self.observability_targets["log_retention_days"]
-            assert retention_days >= target_retention, f"è³‡æ–™ä¿ç•™æœŸ {retention_days} å¤©ä¸è¶³ï¼Œç›®æ¨™ {target_retention} å¤©"
-            
-            self._record_result("observability_configuration_optimization", True, metrics=metrics)
-            
+            assert retention_days >= target_retention, (
+                f"è³‡æ–™ä¿ç•™æœŸ {retention_days} å¤©ä¸è¶³ï¼Œç›®æ¨™ {target_retention} å¤©"
+            )
+
+            self._record_result(
+                "observability_configuration_optimization",
+                True,
+                metrics=metrics,
+            )
+
         except Exception as e:
-            self._record_result("observability_configuration_optimization", False, str(e))
+            self._record_result(
+                "observability_configuration_optimization", False, str(e)
+            )
 
     def print_results(self):
         """æ‰“å°æ¸¬è©¦çµæœå’Œæ•ˆèƒ½åˆ†æ"""
-        total_tests = self.results["tests_passed"] + self.results["tests_failed"]
-        success_rate = (self.results["tests_passed"] / total_tests * 100) if total_tests > 0 else 0
-        
+        total_tests = (
+            self.results["tests_passed"] + self.results["tests_failed"]
+        )
+        success_rate = (
+            (self.results["tests_passed"] / total_tests * 100)
+            if total_tests > 0
+            else 0
+        )
+
         logger.info("=" * 70)
         logger.info("ğŸ”§ TDD Refactor éšæ®µ: ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–çµæœ")
         logger.info("=" * 70)
         logger.info(f"âœ… é€šéæ¸¬è©¦: {self.results['tests_passed']}")
         logger.info(f"âŒ å¤±æ•—æ¸¬è©¦: {self.results['tests_failed']}")
         logger.info(f"ğŸ“ˆ æ¸¬è©¦æˆåŠŸç‡: {success_rate:.1f}%")
-        
+
         if self.results["errors"]:
             logger.info("\nğŸ¯ éœ€è¦å„ªåŒ–çš„é …ç›®:")
             for error in self.results["errors"]:
                 logger.info(f"  - {error}")
-        
+
         # æ•ˆèƒ½åˆ†æå ±å‘Š
         if self.results["performance_metrics"]:
             logger.info("\nğŸ“Š æ•ˆèƒ½åˆ†æå ±å‘Š:")
-            for test_name, metrics in self.results["performance_metrics"].items():
+            for test_name, metrics in self.results[
+                "performance_metrics"
+            ].items():
                 logger.info(f"  ğŸ“ˆ {test_name}:")
                 for metric_name, value in metrics.items():
                     if isinstance(value, (int, float)):
                         logger.info(f"    - {metric_name}: {value:.3f}")
                     else:
                         logger.info(f"    - {metric_name}: {value}")
-        
+
         # Refactor éšæ®µè©•ä¼°
         if success_rate >= 90:
-            logger.info("\nğŸŸ¢ TDD Refactor éšæ®µç‹€æ…‹: å„ªç§€ - æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–æˆåŠŸ")
+            logger.info(
+                "\nğŸŸ¢ TDD Refactor éšæ®µç‹€æ…‹: å„ªç§€ - æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–æˆåŠŸ"
+            )
         elif success_rate >= 70:
-            logger.info("\nğŸŸ¡ TDD Refactor éšæ®µç‹€æ…‹: è‰¯å¥½ - å¤§éƒ¨åˆ†å„ªåŒ–æˆåŠŸï¼Œå°‘æ•¸éœ€èª¿æ•´")
+            logger.info(
+                "\nğŸŸ¡ TDD Refactor éšæ®µç‹€æ…‹: è‰¯å¥½ - å¤§éƒ¨åˆ†å„ªåŒ–æˆåŠŸï¼Œå°‘æ•¸éœ€èª¿æ•´"
+            )
         else:
-            logger.info("\nğŸ”´ TDD Refactor éšæ®µç‹€æ…‹: éœ€æ”¹é€² - å¤šæ•¸æ•ˆèƒ½æŒ‡æ¨™éœ€è¦å„ªåŒ–")
-        
+            logger.info(
+                "\nğŸ”´ TDD Refactor éšæ®µç‹€æ…‹: éœ€æ”¹é€² - å¤šæ•¸æ•ˆèƒ½æŒ‡æ¨™éœ€è¦å„ªåŒ–"
+            )
+
         return success_rate >= 70
 
 
@@ -592,9 +754,9 @@ def main():
     logger.info("ğŸ”§ é–‹å§‹ TDD Refactor éšæ®µ: ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–")
     logger.info("ç›®æ¨™: å„ªåŒ–ç›£æ§ç³»çµ±æ•ˆèƒ½ä¸¦å¢å¼·å¯è§€æ¸¬æ€§åŠŸèƒ½")
     logger.info("=" * 70)
-    
+
     test_suite = MonitoringPerformanceOptimizationTest()
-    
+
     try:
         # åŸ·è¡Œæ‰€æœ‰å„ªåŒ–æ¸¬è©¦
         test_suite.test_structured_logging_performance()
@@ -605,10 +767,10 @@ def main():
         test_suite.test_log_aggregation_performance()
         test_suite.test_alert_evaluation_performance()
         test_suite.test_observability_configuration_optimization()
-        
+
         # æ‰“å°çµæœå’Œåˆ†æ
         is_successful = test_suite.print_results()
-        
+
         if is_successful:
             logger.info("\nğŸ‰ TDD Refactor éšæ®µæˆåŠŸï¼")
             logger.info("âœ¨ ç›£æ§ç³»çµ±æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–å®Œæˆ")
@@ -616,9 +778,9 @@ def main():
         else:
             logger.info("\nğŸ”§ TDD Refactor éšæ®µéœ€è¦é€²ä¸€æ­¥å„ªåŒ–")
             logger.info("ğŸ“Š è«‹æ ¹æ“šæ•ˆèƒ½åˆ†æå ±å‘Šé€²è¡Œèª¿æ•´")
-        
+
         return is_successful
-        
+
     except Exception as e:
         logger.error(f"âŒ Refactor éšæ®µæ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}")
         return False
@@ -626,10 +788,10 @@ def main():
 
 if __name__ == "__main__":
     success = main()
-    
+
     if success:
         logger.info("ğŸ TDD Refactor éšæ®µå®Œæˆ - ç›£æ§æ•ˆèƒ½å„ªåŒ–æˆåŠŸ")
         exit(0)
     else:
         logger.error("ğŸ›‘ TDD Refactor éšæ®µéœ€è¦æ”¹é€²")
-        exit(1)
\ No newline at end of file
+        exit(1)
diff --git a/auto_generate_video_fold6/test_monitoring_performance_refactored.py b/auto_generate_video_fold6/test_monitoring_performance_refactored.py
index de70853..98547dd 100644
--- a/auto_generate_video_fold6/test_monitoring_performance_refactored.py
+++ b/auto_generate_video_fold6/test_monitoring_performance_refactored.py
@@ -23,27 +23,34 @@ import uuid
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 class MonitoringPerformanceRefactoredTest:
     """ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–é‡æ§‹æ¸¬è©¦å¥—ä»¶"""
-    
+
     def __init__(self):
         self.project_root = Path(__file__).parent
         self.results = {
             "tests_passed": 0,
             "tests_failed": 0,
             "errors": [],
-            "performance_metrics": {}
+            "performance_metrics": {},
         }
-        
+
         # æ•ˆèƒ½åŸºæº– (å¾®ç§’)
         self.performance_thresholds = {
             "log_entry_processing": 1000,  # 1ms
-            "metric_collection": 500,      # 0.5ms
-            "correlation_lookup": 50,      # 0.05ms
-            "span_creation": 100,          # 0.1ms
+            "metric_collection": 500,  # 0.5ms
+            "correlation_lookup": 50,  # 0.05ms
+            "span_creation": 100,  # 0.1ms
         }
-    
-    def _record_result(self, test_name: str, success: bool, error: str = None, metrics: Dict[str, float] = None):
+
+    def _record_result(
+        self,
+        test_name: str,
+        success: bool,
+        error: str = None,
+        metrics: Dict[str, float] = None,
+    ):
         """è¨˜éŒ„æ¸¬è©¦çµæœå’Œæ•ˆèƒ½æŒ‡æ¨™"""
         if success:
             self.results["tests_passed"] += 1
@@ -52,7 +59,7 @@ class MonitoringPerformanceRefactoredTest:
             self.results["tests_failed"] += 1
             self.results["errors"].append(f"{test_name}: {error}")
             logger.error(f"âŒ {test_name} å¤±æ•—: {error}")
-        
+
         if metrics:
             self.results["performance_metrics"][test_name] = metrics
 
@@ -60,31 +67,40 @@ class MonitoringPerformanceRefactoredTest:
         """æ¸¬è©¦å„ªåŒ–çš„æ—¥èªŒè¨˜éŒ„æ•ˆèƒ½"""
         try:
             # å°å…¥å„ªåŒ–çš„æ•ˆèƒ½æ—¥èªŒè¨˜éŒ„å™¨
-            from monitoring.logging.performance_logger import get_performance_logger, performance_monitor
-            
+            from monitoring.logging.performance_logger import (
+                get_performance_logger,
+                performance_monitor,
+            )
+
             # å‰µå»ºæ¸¬è©¦æ—¥èªŒè¨˜éŒ„å™¨
-            test_logger = get_performance_logger("performance_test", 
-                                               buffer_size=500, 
-                                               enable_async=True)
-            
+            test_logger = get_performance_logger(
+                "performance_test", buffer_size=500, enable_async=True
+            )
+
             # å–®å€‹æ—¥èªŒæ¢ç›®æ•ˆèƒ½æ¸¬è©¦
             log_times = []
             for i in range(200):
                 start_time = time.perf_counter()
-                test_logger.info(f"Performance test log {i}", 
-                               iteration=i, 
-                               test_type="performance")
+                test_logger.info(
+                    f"Performance test log {i}",
+                    iteration=i,
+                    test_type="performance",
+                )
                 end_time = time.perf_counter()
                 log_times.append((end_time - start_time) * 1000000)
-            
+
             # å¼·åˆ¶åˆ·æ–°ç·©è¡å€
             test_logger.flush()
-            
+
             # è¨ˆç®—çµ±è¨ˆ
             avg_log_time = statistics.mean(log_times)
-            p95_log_time = statistics.quantiles(log_times, n=20)[18] if len(log_times) > 20 else max(log_times)
+            p95_log_time = (
+                statistics.quantiles(log_times, n=20)[18]
+                if len(log_times) > 20
+                else max(log_times)
+            )
             max_log_time = max(log_times)
-            
+
             # ä½µç™¼æ—¥èªŒè¨˜éŒ„æ¸¬è©¦
             def concurrent_logging_task(thread_id: int, log_count: int):
                 thread_times = []
@@ -94,34 +110,47 @@ class MonitoringPerformanceRefactoredTest:
                     end_time = time.perf_counter()
                     thread_times.append((end_time - start_time) * 1000000)
                 return thread_times
-            
-            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
-                futures = [executor.submit(concurrent_logging_task, i, 50) for i in range(5)]
+
+            with concurrent.futures.ThreadPoolExecutor(
+                max_workers=5
+            ) as executor:
+                futures = [
+                    executor.submit(concurrent_logging_task, i, 50)
+                    for i in range(5)
+                ]
                 concurrent_times = []
                 for future in concurrent.futures.as_completed(futures):
                     concurrent_times.extend(future.result())
-            
+
             concurrent_avg = statistics.mean(concurrent_times)
-            
+
             # ç²å–æ—¥èªŒè¨˜éŒ„å™¨çµ±è¨ˆ
             logger_stats = test_logger.get_stats()
-            
+
             metrics = {
                 "avg_log_time_us": avg_log_time,
                 "p95_log_time_us": p95_log_time,
                 "max_log_time_us": max_log_time,
                 "concurrent_avg_time_us": concurrent_avg,
-                "throughput_logs_per_second": 1000000 / avg_log_time if avg_log_time > 0 else 0,
-                "buffer_utilization": logger_stats.get("buffer_utilization", 0),
-                "total_logs_processed": logger_stats.get("logs_processed", 0)
+                "throughput_logs_per_second": 1000000 / avg_log_time
+                if avg_log_time > 0
+                else 0,
+                "buffer_utilization": logger_stats.get(
+                    "buffer_utilization", 0
+                ),
+                "total_logs_processed": logger_stats.get("logs_processed", 0),
             }
-            
+
             # æ•ˆèƒ½é©—è­‰
             threshold = self.performance_thresholds["log_entry_processing"]
-            assert avg_log_time < threshold, f"å¹³å‡æ—¥èªŒè™•ç†æ™‚é–“ {avg_log_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
-            
-            self._record_result("optimized_logging_performance", True, metrics=metrics)
-            
+            assert avg_log_time < threshold, (
+                f"å¹³å‡æ—¥èªŒè™•ç†æ™‚é–“ {avg_log_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
+            )
+
+            self._record_result(
+                "optimized_logging_performance", True, metrics=metrics
+            )
+
         except Exception as e:
             self._record_result("optimized_logging_performance", False, str(e))
 
@@ -129,15 +158,18 @@ class MonitoringPerformanceRefactoredTest:
         """æ¸¬è©¦å„ªåŒ–çš„æŒ‡æ¨™æ”¶é›†æ•ˆèƒ½"""
         try:
             # å°å…¥å„ªåŒ–çš„æŒ‡æ¨™æ”¶é›†å™¨
-            from monitoring.metrics.optimized_metrics_collector import OptimizedMetricsCollector, MetricType
-            
+            from monitoring.metrics.optimized_metrics_collector import (
+                OptimizedMetricsCollector,
+                MetricType,
+            )
+
             # å‰µå»ºæ¸¬è©¦æ”¶é›†å™¨
             collector = OptimizedMetricsCollector(
                 buffer_size=1000,
                 flush_interval=2.0,
-                enable_sampling=False  # é—œé–‰æ¡æ¨£ä»¥ç²å¾—æº–ç¢ºçš„æ•ˆèƒ½æ¸¬è©¦
+                enable_sampling=False,  # é—œé–‰æ¡æ¨£ä»¥ç²å¾—æº–ç¢ºçš„æ•ˆèƒ½æ¸¬è©¦
             )
-            
+
             # å–®å€‹æŒ‡æ¨™è¨˜éŒ„æ•ˆèƒ½æ¸¬è©¦
             metric_times = []
             for i in range(500):
@@ -146,62 +178,86 @@ class MonitoringPerformanceRefactoredTest:
                     f"test_metric_{i % 10}",
                     i,
                     labels={"iteration": str(i), "batch": str(i // 100)},
-                    metric_type=MetricType.GAUGE
+                    metric_type=MetricType.GAUGE,
                 )
                 end_time = time.perf_counter()
                 metric_times.append((end_time - start_time) * 1000000)
-            
+
             avg_metric_time = statistics.mean(metric_times)
-            p95_metric_time = statistics.quantiles(metric_times, n=20)[18] if len(metric_times) > 20 else max(metric_times)
-            
+            p95_metric_time = (
+                statistics.quantiles(metric_times, n=20)[18]
+                if len(metric_times) > 20
+                else max(metric_times)
+            )
+
             # æ‰¹é‡æŒ‡æ¨™æ¸¬è©¦
             batch_start = time.perf_counter()
             for i in range(1000):
-                collector.increment_counter("batch_counter", 1, {"batch_id": str(i // 100)})
+                collector.increment_counter(
+                    "batch_counter", 1, {"batch_id": str(i // 100)}
+                )
             batch_end = time.perf_counter()
             batch_avg_time = (batch_end - batch_start) * 1000000 / 1000
-            
+
             # ä¸åŒé¡å‹æŒ‡æ¨™æ¸¬è©¦
             histogram_times = []
             for i in range(100):
                 start_time = time.perf_counter()
-                collector.observe_histogram("response_time", i * 0.01, {"endpoint": f"/api/v{i%3}"})
+                collector.observe_histogram(
+                    "response_time", i * 0.01, {"endpoint": f"/api/v{i % 3}"}
+                )
                 end_time = time.perf_counter()
                 histogram_times.append((end_time - start_time) * 1000000)
-            
+
             avg_histogram_time = statistics.mean(histogram_times)
-            
+
             # ç²å–æ”¶é›†å™¨çµ±è¨ˆ
             collector_stats = collector.get_performance_stats()
-            
+
             metrics = {
                 "avg_metric_time_us": avg_metric_time,
                 "p95_metric_time_us": p95_metric_time,
                 "batch_avg_time_us": batch_avg_time,
                 "histogram_avg_time_us": avg_histogram_time,
-                "metrics_per_second": 1000000 / avg_metric_time if avg_metric_time > 0 else 0,
-                "buffer_utilization": collector_stats.get("buffer_utilization", 0),
-                "total_metrics_processed": collector_stats.get("metrics_processed", 0),
-                "total_buffer_flushes": collector_stats.get("buffer_flushes", 0)
+                "metrics_per_second": 1000000 / avg_metric_time
+                if avg_metric_time > 0
+                else 0,
+                "buffer_utilization": collector_stats.get(
+                    "buffer_utilization", 0
+                ),
+                "total_metrics_processed": collector_stats.get(
+                    "metrics_processed", 0
+                ),
+                "total_buffer_flushes": collector_stats.get(
+                    "buffer_flushes", 0
+                ),
             }
-            
+
             # æ•ˆèƒ½é©—è­‰
             threshold = self.performance_thresholds["metric_collection"]
-            assert avg_metric_time < threshold, f"å¹³å‡æŒ‡æ¨™æ”¶é›†æ™‚é–“ {avg_metric_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
-            
-            self._record_result("optimized_metrics_collection_performance", True, metrics=metrics)
-            
+            assert avg_metric_time < threshold, (
+                f"å¹³å‡æŒ‡æ¨™æ”¶é›†æ™‚é–“ {avg_metric_time:.2f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
+            )
+
+            self._record_result(
+                "optimized_metrics_collection_performance",
+                True,
+                metrics=metrics,
+            )
+
         except Exception as e:
-            self._record_result("optimized_metrics_collection_performance", False, str(e))
+            self._record_result(
+                "optimized_metrics_collection_performance", False, str(e)
+            )
 
     def test_correlation_id_performance(self):
         """æ¸¬è©¦é—œè¯IDè™•ç†æ•ˆèƒ½"""
         try:
             # æ¨¡æ“¬é—œè¯IDä¸Šä¸‹æ–‡æ“ä½œ
             from contextvars import ContextVar
-            
+
             correlation_id_context = ContextVar("correlation_id", default=None)
-            
+
             # é—œè¯IDç”Ÿæˆæ•ˆèƒ½æ¸¬è©¦
             generation_times = []
             for i in range(1000):
@@ -211,12 +267,18 @@ class MonitoringPerformanceRefactoredTest:
                 retrieved_id = correlation_id_context.get()
                 end_time = time.perf_counter()
                 generation_times.append((end_time - start_time) * 1000000)
-            
+
             avg_generation_time = statistics.mean(generation_times)
-            p95_generation_time = statistics.quantiles(generation_times, n=20)[18] if len(generation_times) > 20 else max(generation_times)
-            
+            p95_generation_time = (
+                statistics.quantiles(generation_times, n=20)[18]
+                if len(generation_times) > 20
+                else max(generation_times)
+            )
+
             # ä½µç™¼é—œè¯IDæ“ä½œæ¸¬è©¦
-            def concurrent_correlation_task(task_id: int, operation_count: int):
+            def concurrent_correlation_task(
+                task_id: int, operation_count: int
+            ):
                 task_times = []
                 for i in range(operation_count):
                     start_time = time.perf_counter()
@@ -227,47 +289,58 @@ class MonitoringPerformanceRefactoredTest:
                     end_time = time.perf_counter()
                     task_times.append((end_time - start_time) * 1000000)
                 return task_times
-            
-            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
-                futures = [executor.submit(concurrent_correlation_task, i, 100) for i in range(10)]
+
+            with concurrent.futures.ThreadPoolExecutor(
+                max_workers=10
+            ) as executor:
+                futures = [
+                    executor.submit(concurrent_correlation_task, i, 100)
+                    for i in range(10)
+                ]
                 concurrent_times = []
                 for future in concurrent.futures.as_completed(futures):
                     concurrent_times.extend(future.result())
-            
+
             concurrent_avg = statistics.mean(concurrent_times)
-            
+
             # ä¸Šä¸‹æ–‡åˆ‡æ›æ•ˆèƒ½æ¸¬è©¦
             context_switch_times = []
             for i in range(500):
                 old_id = f"old-{i}"
                 new_id = f"new-{i}"
-                
+
                 correlation_id_context.set(old_id)
-                
+
                 start_time = time.perf_counter()
                 token = correlation_id_context.set(new_id)
                 retrieved = correlation_id_context.get()
                 correlation_id_context.reset(token)
                 end_time = time.perf_counter()
-                
+
                 context_switch_times.append((end_time - start_time) * 1000000)
-            
+
             avg_context_switch_time = statistics.mean(context_switch_times)
-            
+
             metrics = {
                 "avg_generation_time_us": avg_generation_time,
                 "p95_generation_time_us": p95_generation_time,
                 "concurrent_avg_time_us": concurrent_avg,
                 "context_switch_avg_time_us": avg_context_switch_time,
-                "operations_per_second": 1000000 / avg_generation_time if avg_generation_time > 0 else 0
+                "operations_per_second": 1000000 / avg_generation_time
+                if avg_generation_time > 0
+                else 0,
             }
-            
+
             # æ•ˆèƒ½é©—è­‰
             threshold = self.performance_thresholds["correlation_lookup"]
-            assert avg_generation_time < threshold, f"å¹³å‡é—œè¯IDè™•ç†æ™‚é–“ {avg_generation_time:.3f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
-            
-            self._record_result("correlation_id_performance", True, metrics=metrics)
-            
+            assert avg_generation_time < threshold, (
+                f"å¹³å‡é—œè¯IDè™•ç†æ™‚é–“ {avg_generation_time:.3f}Î¼s è¶…éé–¾å€¼ {threshold}Î¼s"
+            )
+
+            self._record_result(
+                "correlation_id_performance", True, metrics=metrics
+            )
+
         except Exception as e:
             self._record_result("correlation_id_performance", False, str(e))
 
@@ -275,34 +348,42 @@ class MonitoringPerformanceRefactoredTest:
         """æ¸¬è©¦ç›£æ§é…ç½®å„ªåŒ–"""
         try:
             # æª¢æŸ¥ Prometheus é…ç½®å„ªåŒ–
-            prometheus_config = self.project_root / "monitoring/prometheus/prometheus.yml"
+            prometheus_config = (
+                self.project_root / "monitoring/prometheus/prometheus.yml"
+            )
             assert prometheus_config.exists(), "Prometheus é…ç½®ä¸å­˜åœ¨"
-            
+
             config_content = prometheus_config.read_text()
-            
+
             # æª¢æŸ¥æ•ˆèƒ½å„ªåŒ–é…ç½®
             has_query_log = "query_log_file" in config_content
             has_retention_config = "storage.tsdb.retention" in config_content
             has_compression = "wal-compression" in config_content
             has_concurrency_config = "query.max-concurrency" in config_content
-            
+
             # æª¢æŸ¥æŠ“å–é–“éš”å„ªåŒ–
             scrape_intervals = []
-            for line in config_content.split('\n'):
-                if "scrape_interval:" in line and not line.strip().startswith('#'):
-                    interval_str = line.split(':')[-1].strip()
-                    if interval_str.endswith('s'):
+            for line in config_content.split("\n"):
+                if "scrape_interval:" in line and not line.strip().startswith(
+                    "#"
+                ):
+                    interval_str = line.split(":")[-1].strip()
+                    if interval_str.endswith("s"):
                         scrape_intervals.append(int(interval_str[:-1]))
-            
-            avg_scrape_interval = statistics.mean(scrape_intervals) if scrape_intervals else 30
-            
+
+            avg_scrape_interval = (
+                statistics.mean(scrape_intervals) if scrape_intervals else 30
+            )
+
             # æª¢æŸ¥æœå‹™æ•¸é‡ï¼ˆå½±éŸ¿è³‡æºä½¿ç”¨ï¼‰
             service_count = config_content.count("job_name:")
-            
+
             # é ä¼°æ•ˆèƒ½ç‰¹æ€§
             estimated_queries_per_second = service_count / avg_scrape_interval
-            estimated_data_points_per_hour = estimated_queries_per_second * 3600 * 50  # å‡è¨­æ¯å€‹æŸ¥è©¢50å€‹æŒ‡æ¨™
-            
+            estimated_data_points_per_hour = (
+                estimated_queries_per_second * 3600 * 50
+            )  # å‡è¨­æ¯å€‹æŸ¥è©¢50å€‹æŒ‡æ¨™
+
             metrics = {
                 "has_query_log": has_query_log,
                 "has_retention_config": has_retention_config,
@@ -311,77 +392,105 @@ class MonitoringPerformanceRefactoredTest:
                 "avg_scrape_interval_seconds": avg_scrape_interval,
                 "monitored_services": service_count,
                 "estimated_queries_per_second": estimated_queries_per_second,
-                "estimated_data_points_per_hour": estimated_data_points_per_hour
+                "estimated_data_points_per_hour": estimated_data_points_per_hour,
             }
-            
+
             # é…ç½®å„ªåŒ–é©—è­‰
             config_score = 0
             config_score += 1 if has_query_log else 0
             config_score += 1 if has_retention_config else 0
             config_score += 1 if has_compression else 0
             config_score += 1 if has_concurrency_config else 0
-            config_score += 1 if avg_scrape_interval <= 30 else 0  # åˆç†çš„æŠ“å–é–“éš”
-            
+            config_score += (
+                1 if avg_scrape_interval <= 30 else 0
+            )  # åˆç†çš„æŠ“å–é–“éš”
+
             metrics["optimization_score"] = config_score
             metrics["optimization_percentage"] = (config_score / 5) * 100
-            
+
             # è¦æ±‚è‡³å°‘60%çš„å„ªåŒ–é…ç½®
-            assert config_score >= 3, f"é…ç½®å„ªåŒ–åˆ†æ•¸ {config_score}/5 éä½ï¼Œéœ€è¦æ›´å¤šå„ªåŒ–"
-            
-            self._record_result("monitoring_configuration_optimization", True, metrics=metrics)
-            
+            assert config_score >= 3, (
+                f"é…ç½®å„ªåŒ–åˆ†æ•¸ {config_score}/5 éä½ï¼Œéœ€è¦æ›´å¤šå„ªåŒ–"
+            )
+
+            self._record_result(
+                "monitoring_configuration_optimization", True, metrics=metrics
+            )
+
         except Exception as e:
-            self._record_result("monitoring_configuration_optimization", False, str(e))
+            self._record_result(
+                "monitoring_configuration_optimization", False, str(e)
+            )
 
     def test_dashboard_query_performance_optimization(self):
         """æ¸¬è©¦å„€è¡¨æ¿æŸ¥è©¢æ•ˆèƒ½å„ªåŒ–"""
         try:
             # æª¢æŸ¥å„€è¡¨æ¿é…ç½®
-            dashboards_dir = self.project_root / "monitoring/grafana/dashboards"
+            dashboards_dir = (
+                self.project_root / "monitoring/grafana/dashboards"
+            )
             assert dashboards_dir.exists(), "Grafana å„€è¡¨æ¿ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             dashboard_files = list(dashboards_dir.glob("*.json"))
             assert len(dashboard_files) > 0, "æ²’æœ‰æ‰¾åˆ°å„€è¡¨æ¿é…ç½®æª”æ¡ˆ"
-            
+
             total_queries = 0
             total_panels = 0
             complex_queries = 0
             dashboard_performance_scores = {}
-            
+
             for dashboard_file in dashboard_files:
-                with open(dashboard_file, 'r') as f:
+                with open(dashboard_file, "r") as f:
                     dashboard_config = json.load(f)
-                
+
                 dashboard_name = dashboard_file.stem
-                panels = dashboard_config.get('dashboard', {}).get('panels', [])
-                
+                panels = dashboard_config.get("dashboard", {}).get(
+                    "panels", []
+                )
+
                 panel_count = len(panels)
                 query_count = 0
                 complex_query_count = 0
-                
+
                 for panel in panels:
-                    targets = panel.get('targets', [])
+                    targets = panel.get("targets", [])
                     query_count += len(targets)
-                    
+
                     for target in targets:
-                        expr = target.get('expr', '')
-                        
+                        expr = target.get("expr", "")
+
                         # åˆ†ææŸ¥è©¢è¤‡é›œåº¦
                         complexity_indicators = [
-                            'rate(', 'increase(', 'histogram_quantile(',
-                            'avg_over_time(', 'max_over_time(', 'min_over_time(',
-                            'by(', 'group_left', 'group_right',
-                            'join', 'on(', 'ignoring('
+                            "rate(",
+                            "increase(",
+                            "histogram_quantile(",
+                            "avg_over_time(",
+                            "max_over_time(",
+                            "min_over_time(",
+                            "by(",
+                            "group_left",
+                            "group_right",
+                            "join",
+                            "on(",
+                            "ignoring(",
                         ]
-                        
-                        complexity_score = sum(1 for indicator in complexity_indicators if indicator in expr)
+
+                        complexity_score = sum(
+                            1
+                            for indicator in complexity_indicators
+                            if indicator in expr
+                        )
                         if complexity_score >= 2:
                             complex_query_count += 1
-                
+
                 # è¨ˆç®—å„€è¡¨æ¿æ•ˆèƒ½åˆ†æ•¸
-                queries_per_panel = query_count / panel_count if panel_count > 0 else 0
-                complexity_ratio = complex_query_count / query_count if query_count > 0 else 0
-                
+                queries_per_panel = (
+                    query_count / panel_count if panel_count > 0 else 0
+                )
+                complexity_ratio = (
+                    complex_query_count / query_count if query_count > 0 else 0
+                )
+
                 # æ•ˆèƒ½åˆ†æ•¸ (0-100)
                 performance_score = 100
                 if queries_per_panel > 2:
@@ -390,29 +499,36 @@ class MonitoringPerformanceRefactoredTest:
                     performance_score -= (complexity_ratio - 0.5) * 40
                 if panel_count > 15:
                     performance_score -= (panel_count - 15) * 2
-                
+
                 performance_score = max(0, performance_score)
-                
+
                 dashboard_performance_scores[dashboard_name] = {
                     "panels": panel_count,
                     "queries": query_count,
                     "complex_queries": complex_query_count,
                     "queries_per_panel": queries_per_panel,
                     "complexity_ratio": complexity_ratio,
-                    "performance_score": performance_score
+                    "performance_score": performance_score,
                 }
-                
+
                 total_queries += query_count
                 total_panels += panel_count
                 complex_queries += complex_query_count
-            
+
             # æ•´é«”æ•ˆèƒ½åˆ†æ
             avg_queries_per_dashboard = total_queries / len(dashboard_files)
-            avg_panels_per_dashboard = total_panels / len(dashboard_files)  
-            overall_complexity_ratio = complex_queries / total_queries if total_queries > 0 else 0
-            
-            avg_performance_score = statistics.mean([d["performance_score"] for d in dashboard_performance_scores.values()])
-            
+            avg_panels_per_dashboard = total_panels / len(dashboard_files)
+            overall_complexity_ratio = (
+                complex_queries / total_queries if total_queries > 0 else 0
+            )
+
+            avg_performance_score = statistics.mean(
+                [
+                    d["performance_score"]
+                    for d in dashboard_performance_scores.values()
+                ]
+            )
+
             metrics = {
                 "total_dashboards": len(dashboard_files),
                 "total_queries": total_queries,
@@ -421,17 +537,27 @@ class MonitoringPerformanceRefactoredTest:
                 "avg_panels_per_dashboard": avg_panels_per_dashboard,
                 "overall_complexity_ratio": overall_complexity_ratio,
                 "avg_performance_score": avg_performance_score,
-                "dashboard_details": dashboard_performance_scores
+                "dashboard_details": dashboard_performance_scores,
             }
-            
+
             # æ•ˆèƒ½é©—è­‰
-            assert avg_performance_score >= 70, f"å¹³å‡å„€è¡¨æ¿æ•ˆèƒ½åˆ†æ•¸ {avg_performance_score:.1f} éä½ï¼Œéœ€è¦å„ªåŒ–"
-            assert avg_queries_per_dashboard <= 15, f"å¹³å‡æ¯å€‹å„€è¡¨æ¿æŸ¥è©¢æ•¸ {avg_queries_per_dashboard:.1f} éå¤š"
-            
-            self._record_result("dashboard_query_performance_optimization", True, metrics=metrics)
-            
+            assert avg_performance_score >= 70, (
+                f"å¹³å‡å„€è¡¨æ¿æ•ˆèƒ½åˆ†æ•¸ {avg_performance_score:.1f} éä½ï¼Œéœ€è¦å„ªåŒ–"
+            )
+            assert avg_queries_per_dashboard <= 15, (
+                f"å¹³å‡æ¯å€‹å„€è¡¨æ¿æŸ¥è©¢æ•¸ {avg_queries_per_dashboard:.1f} éå¤š"
+            )
+
+            self._record_result(
+                "dashboard_query_performance_optimization",
+                True,
+                metrics=metrics,
+            )
+
         except Exception as e:
-            self._record_result("dashboard_query_performance_optimization", False, str(e))
+            self._record_result(
+                "dashboard_query_performance_optimization", False, str(e)
+            )
 
     def test_alerting_performance_optimization(self):
         """æ¸¬è©¦è­¦å ±æ•ˆèƒ½å„ªåŒ–"""
@@ -439,84 +565,103 @@ class MonitoringPerformanceRefactoredTest:
             # æª¢æŸ¥è­¦å ±è¦å‰‡é…ç½®
             rules_dir = self.project_root / "monitoring/prometheus/rules"
             assert rules_dir.exists(), "è­¦å ±è¦å‰‡ç›®éŒ„ä¸å­˜åœ¨"
-            
+
             rule_files = list(rules_dir.glob("*.yml"))
             assert len(rule_files) > 0, "æ²’æœ‰æ‰¾åˆ°è­¦å ±è¦å‰‡æª”æ¡ˆ"
-            
+
             total_rules = 0
             total_groups = 0
             complex_rules = 0
             rule_performance_metrics = {}
-            
+
             try:
                 import yaml
+
                 yaml_available = True
             except ImportError:
                 yaml_available = False
-            
+
             for rule_file in rule_files:
                 file_rules = 0
                 file_groups = 0
                 file_complex_rules = 0
-                
+
                 if yaml_available:
                     try:
-                        with open(rule_file, 'r') as f:
+                        with open(rule_file, "r") as f:
                             rules_config = yaml.safe_load(f)
-                        
-                        for group in rules_config.get('groups', []):
+
+                        for group in rules_config.get("groups", []):
                             file_groups += 1
-                            for rule in group.get('rules', []):
+                            for rule in group.get("rules", []):
                                 file_rules += 1
-                                expr = rule.get('expr', '')
-                                
+                                expr = rule.get("expr", "")
+
                                 # åˆ†æè¦å‰‡è¤‡é›œåº¦
                                 complexity_score = 0
-                                complexity_score += expr.count('(') * 0.5
-                                complexity_score += expr.count('rate(') * 2
-                                complexity_score += expr.count('histogram_quantile(') * 3
-                                complexity_score += expr.count('by(') * 1
-                                complexity_score += expr.count('group_left') * 2
-                                complexity_score += expr.count('group_right') * 2
-                                
+                                complexity_score += expr.count("(") * 0.5
+                                complexity_score += expr.count("rate(") * 2
+                                complexity_score += (
+                                    expr.count("histogram_quantile(") * 3
+                                )
+                                complexity_score += expr.count("by(") * 1
+                                complexity_score += (
+                                    expr.count("group_left") * 2
+                                )
+                                complexity_score += (
+                                    expr.count("group_right") * 2
+                                )
+
                                 if complexity_score > 3:
                                     file_complex_rules += 1
                     except Exception as e:
                         logger.warning(f"ç„¡æ³•è§£æ YAML æ–‡ä»¶ {rule_file}: {e}")
                         # ä½¿ç”¨æ–‡æœ¬åˆ†æä½œç‚ºå‚™é¸æ–¹æ¡ˆ
                         content = rule_file.read_text()
-                        file_rules = content.count('alert:')
-                        file_groups = content.count('name:')
-                        file_complex_rules = content.count('histogram_quantile')
+                        file_rules = content.count("alert:")
+                        file_groups = content.count("name:")
+                        file_complex_rules = content.count(
+                            "histogram_quantile"
+                        )
                 else:
                     # ç°¡å–®æ–‡æœ¬åˆ†æ
                     content = rule_file.read_text()
-                    file_rules = content.count('alert:')
-                    file_groups = content.count('name:') 
-                    file_complex_rules = content.count('histogram_quantile') + content.count('rate(')
-                
+                    file_rules = content.count("alert:")
+                    file_groups = content.count("name:")
+                    file_complex_rules = content.count(
+                        "histogram_quantile"
+                    ) + content.count("rate(")
+
                 rule_performance_metrics[rule_file.stem] = {
                     "rules": file_rules,
                     "groups": file_groups,
                     "complex_rules": file_complex_rules,
-                    "complexity_ratio": file_complex_rules / file_rules if file_rules > 0 else 0
+                    "complexity_ratio": file_complex_rules / file_rules
+                    if file_rules > 0
+                    else 0,
                 }
-                
+
                 total_rules += file_rules
                 total_groups += file_groups
                 complex_rules += file_complex_rules
-            
+
             # æ•ˆèƒ½åˆ†æ
-            avg_rules_per_file = total_rules / len(rule_files) if len(rule_files) > 0 else 0
-            avg_rules_per_group = total_rules / total_groups if total_groups > 0 else 0
-            complexity_ratio = complex_rules / total_rules if total_rules > 0 else 0
-            
+            avg_rules_per_file = (
+                total_rules / len(rule_files) if len(rule_files) > 0 else 0
+            )
+            avg_rules_per_group = (
+                total_rules / total_groups if total_groups > 0 else 0
+            )
+            complexity_ratio = (
+                complex_rules / total_rules if total_rules > 0 else 0
+            )
+
             # é ä¼°è­¦å ±è©•ä¼°æ•ˆèƒ½
             base_eval_time_us = 5  # åŸºç¤è©•ä¼°æ™‚é–“
             complexity_penalty = complexity_ratio * 20
             estimated_eval_time = base_eval_time_us + complexity_penalty
             estimated_total_eval_time = estimated_eval_time * total_rules
-            
+
             metrics = {
                 "total_rules": total_rules,
                 "total_groups": total_groups,
@@ -526,39 +671,57 @@ class MonitoringPerformanceRefactoredTest:
                 "complexity_ratio": complexity_ratio,
                 "estimated_eval_time_us": estimated_eval_time,
                 "estimated_total_eval_time_us": estimated_total_eval_time,
-                "rule_file_details": rule_performance_metrics
+                "rule_file_details": rule_performance_metrics,
             }
-            
+
             # æ•ˆèƒ½é©—è­‰
-            assert estimated_eval_time <= 50, f"é ä¼°è­¦å ±è©•ä¼°æ™‚é–“ {estimated_eval_time:.1f}Î¼s éé«˜"
-            assert avg_rules_per_group <= 10, f"å¹³å‡æ¯çµ„è¦å‰‡æ•¸ {avg_rules_per_group:.1f} éå¤šï¼Œå»ºè­°æ‹†åˆ†"
-            
-            self._record_result("alerting_performance_optimization", True, metrics=metrics)
-            
+            assert estimated_eval_time <= 50, (
+                f"é ä¼°è­¦å ±è©•ä¼°æ™‚é–“ {estimated_eval_time:.1f}Î¼s éé«˜"
+            )
+            assert avg_rules_per_group <= 10, (
+                f"å¹³å‡æ¯çµ„è¦å‰‡æ•¸ {avg_rules_per_group:.1f} éå¤šï¼Œå»ºè­°æ‹†åˆ†"
+            )
+
+            self._record_result(
+                "alerting_performance_optimization", True, metrics=metrics
+            )
+
         except Exception as e:
-            self._record_result("alerting_performance_optimization", False, str(e))
+            self._record_result(
+                "alerting_performance_optimization", False, str(e)
+            )
 
     def print_results(self):
         """æ‰“å°æ¸¬è©¦çµæœå’Œæ•ˆèƒ½åˆ†æ"""
-        total_tests = self.results["tests_passed"] + self.results["tests_failed"]
-        success_rate = (self.results["tests_passed"] / total_tests * 100) if total_tests > 0 else 0
-        
+        total_tests = (
+            self.results["tests_passed"] + self.results["tests_failed"]
+        )
+        success_rate = (
+            (self.results["tests_passed"] / total_tests * 100)
+            if total_tests > 0
+            else 0
+        )
+
         logger.info("=" * 80)
-        logger.info("ğŸ”§ TDD Refactor éšæ®µ: ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–çµæœ (é‡æ§‹ç‰ˆ)")
+        logger.info(
+            "ğŸ”§ TDD Refactor éšæ®µ: ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–çµæœ (é‡æ§‹ç‰ˆ)"
+        )
         logger.info("=" * 80)
         logger.info(f"âœ… é€šéæ¸¬è©¦: {self.results['tests_passed']}")
         logger.info(f"âŒ å¤±æ•—æ¸¬è©¦: {self.results['tests_failed']}")
         logger.info(f"ğŸ“ˆ æ¸¬è©¦æˆåŠŸç‡: {success_rate:.1f}%")
-        
+
         if self.results["errors"]:
             logger.info("\nğŸ¯ éœ€è¦å„ªåŒ–çš„é …ç›®:")
             for error in self.results["errors"]:
                 logger.info(f"  - {error}")
-        
+
         # æ•ˆèƒ½åˆ†æå ±å‘Š
         if self.results["performance_metrics"]:
             logger.info("\nğŸ“Š æ•ˆèƒ½å„ªåŒ–åˆ†æå ±å‘Š:")
-            for test_name, metrics in self.results["performance_metrics"].items():
+            for test_name, metrics in self.results[
+                "performance_metrics"
+            ].items():
                 logger.info(f"  ğŸ“ˆ {test_name}:")
                 for metric_name, value in metrics.items():
                     if isinstance(value, (int, float)):
@@ -568,7 +731,10 @@ class MonitoringPerformanceRefactoredTest:
                             logger.info(f"    - {metric_name}: {value:.3f} ms")
                         elif metric_name.endswith("_seconds"):
                             logger.info(f"    - {metric_name}: {value:.1f} s")
-                        elif "percentage" in metric_name or "ratio" in metric_name:
+                        elif (
+                            "percentage" in metric_name
+                            or "ratio" in metric_name
+                        ):
                             logger.info(f"    - {metric_name}: {value:.1f}%")
                         elif "per_second" in metric_name:
                             logger.info(f"    - {metric_name}: {value:.0f} /s")
@@ -578,33 +744,39 @@ class MonitoringPerformanceRefactoredTest:
                         logger.info(f"    - {metric_name}: {value}")
                     elif not isinstance(value, dict):
                         logger.info(f"    - {metric_name}: {value}")
-        
+
         # ç¸½é«”æ•ˆèƒ½è©•ä¼°
         logger.info("\nğŸ¯ æ•ˆèƒ½å„ªåŒ–æ‘˜è¦:")
-        
+
         performance_summary = {}
         for test_name, metrics in self.results["performance_metrics"].items():
             if "avg_" in str(metrics):
                 for key, value in metrics.items():
                     if "avg_" in key and isinstance(value, (int, float)):
                         performance_summary[f"{test_name}_{key}"] = value
-        
+
         if performance_summary:
             logger.info("  ä¸»è¦æ•ˆèƒ½æŒ‡æ¨™:")
             for metric, value in performance_summary.items():
                 logger.info(f"    - {metric}: {value:.3f}")
-        
+
         # Refactor éšæ®µè©•ä¼°
         if success_rate >= 90:
-            logger.info("\nğŸŸ¢ TDD Refactor éšæ®µç‹€æ…‹: å„ªç§€ - æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–æˆåŠŸ")
+            logger.info(
+                "\nğŸŸ¢ TDD Refactor éšæ®µç‹€æ…‹: å„ªç§€ - æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–æˆåŠŸ"
+            )
             logger.info("âœ¨ ç›£æ§ç³»çµ±å·²é”åˆ°ç”Ÿç”¢ç´šæ•ˆèƒ½æ¨™æº–")
         elif success_rate >= 70:
-            logger.info("\nğŸŸ¡ TDD Refactor éšæ®µç‹€æ…‹: è‰¯å¥½ - å¤§éƒ¨åˆ†å„ªåŒ–æˆåŠŸï¼Œå°‘æ•¸éœ€èª¿æ•´")
+            logger.info(
+                "\nğŸŸ¡ TDD Refactor éšæ®µç‹€æ…‹: è‰¯å¥½ - å¤§éƒ¨åˆ†å„ªåŒ–æˆåŠŸï¼Œå°‘æ•¸éœ€èª¿æ•´"
+            )
             logger.info("ğŸ”§ å»ºè­°é€²ä¸€æ­¥èª¿æ•´æœªé”æ¨™çš„æ•ˆèƒ½æŒ‡æ¨™")
         else:
-            logger.info("\nğŸ”´ TDD Refactor éšæ®µç‹€æ…‹: éœ€æ”¹é€² - å¤šæ•¸æ•ˆèƒ½æŒ‡æ¨™éœ€è¦å„ªåŒ–")
+            logger.info(
+                "\nğŸ”´ TDD Refactor éšæ®µç‹€æ…‹: éœ€æ”¹é€² - å¤šæ•¸æ•ˆèƒ½æŒ‡æ¨™éœ€è¦å„ªåŒ–"
+            )
             logger.info("ğŸ“Š è«‹æ ¹æ“šè©³ç´°åˆ†æå ±å‘Šé€²è¡Œç³»çµ±æ€§å„ªåŒ–")
-        
+
         return success_rate >= 70
 
 
@@ -613,9 +785,9 @@ def main():
     logger.info("ğŸ”§ é–‹å§‹ TDD Refactor éšæ®µ: ç›£æ§æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ– (é‡æ§‹ç‰ˆ)")
     logger.info("ç›®æ¨™: å„ªåŒ–ç›£æ§ç³»çµ±æ•ˆèƒ½ä¸¦å¢å¼·å¯è§€æ¸¬æ€§åŠŸèƒ½")
     logger.info("=" * 80)
-    
+
     test_suite = MonitoringPerformanceRefactoredTest()
-    
+
     try:
         # åŸ·è¡Œæ‰€æœ‰å„ªåŒ–æ¸¬è©¦
         test_suite.test_optimized_logging_performance()
@@ -624,10 +796,10 @@ def main():
         test_suite.test_monitoring_configuration_optimization()
         test_suite.test_dashboard_query_performance_optimization()
         test_suite.test_alerting_performance_optimization()
-        
+
         # æ‰“å°çµæœå’Œåˆ†æ
         is_successful = test_suite.print_results()
-        
+
         if is_successful:
             logger.info("\nğŸ‰ TDD Refactor éšæ®µæˆåŠŸï¼")
             logger.info("âœ¨ ç›£æ§ç³»çµ±æ•ˆèƒ½å’Œå¯è§€æ¸¬æ€§å„ªåŒ–å®Œæˆ")
@@ -636,22 +808,23 @@ def main():
         else:
             logger.info("\nğŸ”§ TDD Refactor éšæ®µéœ€è¦é€²ä¸€æ­¥å„ªåŒ–")
             logger.info("ğŸ“Š è«‹æ ¹æ“šæ•ˆèƒ½åˆ†æå ±å‘Šé€²è¡Œé‡å°æ€§èª¿æ•´")
-        
+
         return is_successful
-        
+
     except Exception as e:
         logger.error(f"âŒ Refactor éšæ®µæ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}")
         import traceback
+
         traceback.print_exc()
         return False
 
 
 if __name__ == "__main__":
     success = main()
-    
+
     if success:
         logger.info("ğŸ TDD Refactor éšæ®µå®Œæˆ - ç›£æ§æ•ˆèƒ½å„ªåŒ–æˆåŠŸ")
         exit(0)
     else:
         logger.error("ğŸ›‘ TDD Refactor éšæ®µéœ€è¦æ”¹é€²")
-        exit(1)
\ No newline at end of file
+        exit(1)
diff --git a/auto_generate_video_fold6/test_refactor_validation.py b/auto_generate_video_fold6/test_refactor_validation.py
index f0a2ecb..28bd56e 100644
--- a/auto_generate_video_fold6/test_refactor_validation.py
+++ b/auto_generate_video_fold6/test_refactor_validation.py
@@ -16,176 +16,199 @@ import logging
 sys.path.insert(0, str(Path(__file__).parent / "services" / "common"))
 
 from base_service import (
-    BaseService, ServiceState, ServiceError, MetricsCollector, 
-    StructuredLogger, TraceContext, trace_span
+    BaseService,
+    ServiceState,
+    ServiceError,
+    MetricsCollector,
+    StructuredLogger,
+    TraceContext,
+    trace_span,
 )
 from workflow_engine_refactored import (
-    WorkflowEngine, WorkflowTemplate, WorkflowStep, WorkflowContext,
-    WorkflowState, StepState, StepResult
+    WorkflowEngine,
+    WorkflowTemplate,
+    WorkflowStep,
+    WorkflowContext,
+    WorkflowState,
+    StepState,
+    StepResult,
 )
 
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 class MockService(BaseService):
     """æ¸¬è©¦ç”¨æ¨¡æ“¬æœå‹™"""
-    
+
     def __init__(self, config: Dict[str, Any] = None):
         super().__init__("mock_service", "1.0.0", config)
         self.initialized = False
         self.started = False
-    
+
     async def _initialize(self):
         await asyncio.sleep(0.1)  # æ¨¡æ“¬åˆå§‹åŒ–å»¶é²
         self.initialized = True
-    
+
     async def _startup(self):
         await asyncio.sleep(0.1)  # æ¨¡æ“¬å•Ÿå‹•å»¶é²
         self.started = True
-    
+
     async def _shutdown(self):
         await asyncio.sleep(0.1)  # æ¨¡æ“¬é—œé–‰å»¶é²
         self.started = False
 
+
 class MockWorkflowStep(WorkflowStep):
     """æ¸¬è©¦ç”¨å·¥ä½œæµç¨‹æ­¥é©Ÿ"""
-    
-    def __init__(self, step_name: str, delay: float = 0.1, should_fail: bool = False, **kwargs):
+
+    def __init__(
+        self,
+        step_name: str,
+        delay: float = 0.1,
+        should_fail: bool = False,
+        **kwargs,
+    ):
         super().__init__(step_name, **kwargs)
         self.delay = delay
         self.should_fail = should_fail
         self.execution_count = 0
-    
+
     async def _execute_step(self, context: WorkflowContext) -> Dict[str, Any]:
         self.execution_count += 1
         await asyncio.sleep(self.delay)
-        
+
         if self.should_fail:
             raise Exception(f"Mock step {self.step_name} failed")
-        
+
         return {
             "step_name": self.step_name,
             "execution_count": self.execution_count,
-            "timestamp": time.time()
+            "timestamp": time.time(),
         }
 
+
 class RefactorValidationTest:
     """é‡æ§‹é©—è­‰æ¸¬è©¦å¥—ä»¶"""
-    
+
     def __init__(self):
-        self.results = {
-            "tests_passed": 0,
-            "tests_failed": 0,
-            "errors": []
-        }
-    
+        self.results = {"tests_passed": 0, "tests_failed": 0, "errors": []}
+
     def _record_result(self, test_name: str, success: bool, error: str = None):
         """è¨˜éŒ„æ¸¬è©¦çµæœ"""
         if success:
             self.results["tests_passed"] += 1
             logger.info(f"âœ… {test_name} é€šé")
         else:
-            self.results["tests_failed"] += 1 
+            self.results["tests_failed"] += 1
             self.results["errors"].append(f"{test_name}: {error}")
             logger.error(f"âŒ {test_name} å¤±æ•—: {error}")
-    
+
     async def test_base_service_lifecycle(self):
         """æ¸¬è©¦åŸºç¤æœå‹™ç”Ÿå‘½é€±æœŸ"""
         try:
             service = MockService({"test_config": "value"})
-            
+
             # æ¸¬è©¦åˆå§‹ç‹€æ…‹
             assert service.state == ServiceState.INITIALIZING
             assert not service.initialized
             assert not service.started
-            
+
             # æ¸¬è©¦å•Ÿå‹•
             await service.start()
             assert service.state == ServiceState.HEALTHY
             assert service.initialized
             assert service.started
-            
+
             # æ¸¬è©¦å¥åº·æª¢æŸ¥
             health = await service.health_check()
             assert health.status == ServiceState.HEALTHY
-            
+
             # æ¸¬è©¦åœæ­¢
             await service.stop()
             assert service.state == ServiceState.STOPPED
             assert not service.started
-            
+
             self._record_result("base_service_lifecycle", True)
-            
+
         except Exception as e:
             self._record_result("base_service_lifecycle", False, str(e))
-    
+
     async def test_base_service_context_manager(self):
         """æ¸¬è©¦åŸºç¤æœå‹™ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
         try:
             async with MockService() as service:
                 assert service.state == ServiceState.HEALTHY
                 assert service.started
-            
+
             # æœå‹™æ‡‰è©²è‡ªå‹•åœæ­¢
             assert service.state == ServiceState.STOPPED
-            
+
             self._record_result("base_service_context_manager", True)
-            
+
         except Exception as e:
             self._record_result("base_service_context_manager", False, str(e))
-    
+
     async def test_metrics_collection(self):
         """æ¸¬è©¦æŒ‡æ¨™æ”¶é›†"""
         try:
             metrics = MetricsCollector()
-            
+
             # æ¸¬è©¦è¨ˆæ•¸å™¨
             await metrics.increment_counter("test_counter", {"type": "test"})
-            await metrics.increment_counter("test_counter", {"type": "test"}, 2)
-            
+            await metrics.increment_counter(
+                "test_counter", {"type": "test"}, 2
+            )
+
             # æ¸¬è©¦é‡è¡¨
             await metrics.set_gauge("test_gauge", 42.5, {"unit": "percent"})
-            
+
             # æ¸¬è©¦ç›´æ–¹åœ–
             await metrics.record_histogram("test_histogram", 1.5)
             await metrics.record_histogram("test_histogram", 2.5)
             await metrics.record_histogram("test_histogram", 3.5)
-            
+
             # ç²å–æŒ‡æ¨™
             all_metrics = await metrics.get_metrics()
             assert len(all_metrics) > 0
-            
+
             # é©—è­‰æŒ‡æ¨™é¡å‹
-            counter_metrics = [m for m in all_metrics if "counter" in m["name"] or "_total" in m["name"]]
+            counter_metrics = [
+                m
+                for m in all_metrics
+                if "counter" in m["name"] or "_total" in m["name"]
+            ]
             gauge_metrics = [m for m in all_metrics if "gauge" in m["name"]]
-            histogram_metrics = [m for m in all_metrics if "histogram" in m["name"]]
-            
+            histogram_metrics = [
+                m for m in all_metrics if "histogram" in m["name"]
+            ]
+
             assert len(counter_metrics) > 0
-            assert len(gauge_metrics) > 0  
+            assert len(gauge_metrics) > 0
             assert len(histogram_metrics) > 0
-            
+
             self._record_result("metrics_collection", True)
-            
+
         except Exception as e:
             self._record_result("metrics_collection", False, str(e))
-    
+
     async def test_structured_logging(self):
         """æ¸¬è©¦çµæ§‹åŒ–æ—¥èªŒ"""
         try:
             logger = StructuredLogger("test_service", "1.0.0")
-            
+
             # æ¸¬è©¦ä¸åŒç´šåˆ¥çš„æ—¥èªŒ
             logger.info("Test info message", extra_field="value")
             logger.error("Test error message", error_code="TEST_ERROR")
             logger.warning("Test warning message", warning_type="validation")
             logger.debug("Test debug message", debug_info={"key": "value"})
-            
+
             # å¦‚æœæ²’æœ‰ç•°å¸¸ï¼Œèªç‚ºæˆåŠŸ
             self._record_result("structured_logging", True)
-            
+
         except Exception as e:
             self._record_result("structured_logging", False, str(e))
-    
+
     async def test_trace_context(self):
         """æ¸¬è©¦åˆ†æ•£å¼è¿½è¹¤"""
         try:
@@ -193,226 +216,240 @@ class RefactorValidationTest:
             trace = TraceContext()
             assert trace.trace_id is not None
             assert trace.span_id is not None
-            
+
             # æ·»åŠ æ¨™ç±¤å’Œæ—¥èªŒ
             trace.add_tag("operation", "test")
             trace.log("Test log message")
-            
+
             # æ¸¬è©¦è¿½è¹¤ span
             logger = StructuredLogger("test", "1.0.0")
             async with trace_span("test_operation", trace, logger) as span:
                 span.add_tag("test_tag", "test_value")
                 span.log("Operation started")
                 await asyncio.sleep(0.1)
-            
+
             # é©—è­‰ span æ•¸æ“š
             span_data = span.to_dict()
             assert "trace_id" in span_data
             assert "span_id" in span_data
             assert "tags" in span_data
             assert "logs" in span_data
-            
+
             self._record_result("trace_context", True)
-            
+
         except Exception as e:
             self._record_result("trace_context", False, str(e))
-    
+
     async def test_workflow_engine_basic(self):
         """æ¸¬è©¦å·¥ä½œæµç¨‹å¼•æ“åŸºæœ¬åŠŸèƒ½"""
         try:
             engine = WorkflowEngine()
-            
+
             # å•Ÿå‹•å¼•æ“
             await engine.start()
             assert engine.state == ServiceState.HEALTHY
-            
+
             # å‰µå»ºç°¡å–®å·¥ä½œæµç¨‹ç¯„æœ¬
             step1 = MockWorkflowStep("step1", delay=0.1)
             step2 = MockWorkflowStep("step2", delay=0.1)
             step1.next_step = step2
-            
+
             template = WorkflowTemplate(
                 name="test_workflow",
                 description="Test workflow template",
                 first_step=step1,
-                timeout=10.0
+                timeout=10.0,
             )
-            
+
             # è¨»å†Šç¯„æœ¬
             engine.register_workflow_template(template)
-            
+
             # å•Ÿå‹•å·¥ä½œæµç¨‹
             workflow_id = await engine.start_workflow(
-                "test_workflow",
-                "test_user",
-                {"test_input": "value"}
+                "test_workflow", "test_user", {"test_input": "value"}
             )
-            
+
             assert workflow_id is not None
-            
+
             # ç­‰å¾…å·¥ä½œæµç¨‹å®Œæˆ
             await asyncio.sleep(1.0)
-            
+
             # æª¢æŸ¥ç‹€æ…‹
             status = await engine.get_workflow_status(workflow_id)
             if status:  # å¯èƒ½å·²ç¶“å®Œæˆä¸¦æ¸…ç†
-                assert status["state"] in [WorkflowState.COMPLETED.value, WorkflowState.RUNNING.value]
-            
+                assert status["state"] in [
+                    WorkflowState.COMPLETED.value,
+                    WorkflowState.RUNNING.value,
+                ]
+
             # ç²å–å¼•æ“çµ±è¨ˆ
             stats = await engine.get_engine_stats()
             assert stats["stats"]["total_workflows"] >= 1
-            
+
             await engine.stop()
-            
+
             self._record_result("workflow_engine_basic", True)
-            
+
         except Exception as e:
             self._record_result("workflow_engine_basic", False, str(e))
-    
+
     async def test_workflow_step_chain(self):
         """æ¸¬è©¦å·¥ä½œæµç¨‹æ­¥é©Ÿéˆ"""
         try:
             # å‰µå»ºæ­¥é©Ÿéˆ
             step1 = MockWorkflowStep("step1", delay=0.05)
-            step2 = MockWorkflowStep("step2", delay=0.05, required_steps=["step1"])
-            step3 = MockWorkflowStep("step3", delay=0.05, required_steps=["step2"])
-            
+            step2 = MockWorkflowStep(
+                "step2", delay=0.05, required_steps=["step1"]
+            )
+            step3 = MockWorkflowStep(
+                "step3", delay=0.05, required_steps=["step2"]
+            )
+
             step1.next_step = step2
             step2.next_step = step3
-            
+
             # å‰µå»ºåŸ·è¡Œä¸Šä¸‹æ–‡
             context = WorkflowContext(
                 workflow_id="test_workflow",
                 user_id="test_user",
-                input_data={"test": "data"}
+                input_data={"test": "data"},
             )
-            
+
             # åŸ·è¡Œæ­¥é©Ÿéˆ
             result_context = await step1.process(context)
-            
+
             # é©—è­‰æ‰€æœ‰æ­¥é©Ÿéƒ½åŸ·è¡Œäº†
             assert "step1" in result_context.step_results
             assert "step2" in result_context.step_results
             assert "step3" in result_context.step_results
-            
+
             # é©—è­‰æ­¥é©Ÿç‹€æ…‹
             for step_name in ["step1", "step2", "step3"]:
                 result = result_context.step_results[step_name]
                 assert result.state == StepState.COMPLETED
                 assert result.duration is not None
                 assert result.duration > 0
-            
+
             # é©—è­‰æ­¥é©ŸåŸ·è¡Œé †åº
             step1_time = result_context.step_results["step1"].start_time
-            step2_time = result_context.step_results["step2"].start_time  
+            step2_time = result_context.step_results["step2"].start_time
             step3_time = result_context.step_results["step3"].start_time
-            
+
             assert step1_time < step2_time < step3_time
-            
+
             self._record_result("workflow_step_chain", True)
-            
+
         except Exception as e:
             self._record_result("workflow_step_chain", False, str(e))
-    
+
     async def test_workflow_error_handling(self):
         """æ¸¬è©¦å·¥ä½œæµç¨‹éŒ¯èª¤è™•ç†"""
         try:
             # å‰µå»ºæœƒå¤±æ•—çš„æ­¥é©Ÿ
             failing_step = MockWorkflowStep("failing_step", should_fail=True)
-            
+
             context = WorkflowContext(
                 workflow_id="test_error_workflow",
-                user_id="test_user", 
-                input_data={}
+                user_id="test_user",
+                input_data={},
             )
-            
+
             # åŸ·è¡Œæ‡‰è©²å¤±æ•—çš„æ­¥é©Ÿ
             result = await failing_step.execute(context)
-            
+
             # é©—è­‰å¤±æ•—ç‹€æ…‹
             assert result.state == StepState.FAILED
             assert result.error is not None
             assert "failed" in result.error
-            
+
             self._record_result("workflow_error_handling", True)
-            
+
         except Exception as e:
             self._record_result("workflow_error_handling", False, str(e))
-    
+
     async def test_workflow_prerequisites(self):
         """æ¸¬è©¦å·¥ä½œæµç¨‹å‰ç½®æ¢ä»¶"""
         try:
             # å‰µå»ºæœ‰å‰ç½®æ¢ä»¶çš„æ­¥é©Ÿ
-            step_with_prereq = MockWorkflowStep("step_with_prereq", 
-                                              required_steps=["missing_step"])
-            
+            step_with_prereq = MockWorkflowStep(
+                "step_with_prereq", required_steps=["missing_step"]
+            )
+
             context = WorkflowContext(
                 workflow_id="test_prereq_workflow",
                 user_id="test_user",
-                input_data={}
+                input_data={},
             )
-            
+
             # åŸ·è¡Œæ‡‰è©²è¢«è·³éçš„æ­¥é©Ÿ
             result = await step_with_prereq.execute(context)
-            
+
             # é©—è­‰è·³éç‹€æ…‹
             assert result.state == StepState.SKIPPED
             assert "Prerequisites not met" in result.error
-            
+
             self._record_result("workflow_prerequisites", True)
-            
+
         except Exception as e:
             self._record_result("workflow_prerequisites", False, str(e))
-    
+
     async def test_performance_metrics(self):
         """æ¸¬è©¦æ•ˆèƒ½æŒ‡æ¨™"""
         try:
             start_time = time.time()
-            
+
             # å‰µå»ºæœå‹™ä¸¦åŸ·è¡Œæ“ä½œ
             async with MockService() as service:
                 # è¨˜éŒ„ä¸€äº›æŒ‡æ¨™
-                await service.metrics.increment_counter("operations", {"type": "test"})
+                await service.metrics.increment_counter(
+                    "operations", {"type": "test"}
+                )
                 await service.metrics.set_gauge("cpu_usage", 45.2)
                 await service.metrics.record_histogram("response_time", 0.123)
-                
+
                 # åŸ·è¡Œè¿½è¹¤æ“ä½œ
                 async with service.trace_operation("performance_test") as span:
                     await asyncio.sleep(0.1)
                     span.add_tag("test_metric", "value")
-            
+
             end_time = time.time()
             execution_time = end_time - start_time
-            
+
             # é©—è­‰åŸ·è¡Œæ™‚é–“åˆç†ï¼ˆæ‡‰è©²å°æ–¼1ç§’ï¼‰
             assert execution_time < 1.0
-            
+
             # ç²å–æŒ‡æ¨™
             metrics = await service.metrics.get_metrics()
             assert len(metrics) > 0
-            
+
             self._record_result("performance_metrics", True)
-            
+
         except Exception as e:
             self._record_result("performance_metrics", False, str(e))
-    
+
     def print_results(self):
         """æ‰“å°æ¸¬è©¦çµæœ"""
-        total_tests = self.results["tests_passed"] + self.results["tests_failed"]
-        success_rate = (self.results["tests_passed"] / total_tests * 100) if total_tests > 0 else 0
-        
+        total_tests = (
+            self.results["tests_passed"] + self.results["tests_failed"]
+        )
+        success_rate = (
+            (self.results["tests_passed"] / total_tests * 100)
+            if total_tests > 0
+            else 0
+        )
+
         logger.info("=" * 60)
         logger.info("ğŸ” TDD Refactor é©—è­‰æ¸¬è©¦çµæœ")
         logger.info("=" * 60)
         logger.info(f"âœ… é€šéæ¸¬è©¦: {self.results['tests_passed']}")
         logger.info(f"âŒ å¤±æ•—æ¸¬è©¦: {self.results['tests_failed']}")
         logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
-        
+
         if self.results["errors"]:
             logger.info("\nğŸš¨ å¤±æ•—æ¸¬è©¦è©³æƒ…:")
             for error in self.results["errors"]:
                 logger.info(f"  - {error}")
-        
+
         # é‡æ§‹å“è³ªè©•ä¼°
         if success_rate >= 95:
             logger.info("\nğŸ† é‡æ§‹å“è³ª: å„ªç§€ - æ‰€æœ‰åŠŸèƒ½å®Œæ•´ä¿ç•™ä¸¦å¢å¼·")
@@ -422,17 +459,18 @@ class RefactorValidationTest:
             logger.info("\nâš ï¸ é‡æ§‹å“è³ª: ä¸€èˆ¬ - å­˜åœ¨ä¸€äº›å•é¡Œéœ€è¦ä¿®å¾©")
         else:
             logger.info("\nâŒ é‡æ§‹å“è³ª: ä¸ä½³ - å­˜åœ¨é‡å¤§å•é¡Œï¼Œéœ€è¦é‡æ–°æª¢è¦–")
-        
+
         return success_rate >= 85  # 85% ä»¥ä¸Šç®—é€šé
 
+
 async def main():
     """åŸ·è¡Œé‡æ§‹é©—è­‰æ¸¬è©¦"""
     logger.info("ğŸš€ é–‹å§‹ TDD Refactor éšæ®µé©—è­‰æ¸¬è©¦")
     logger.info("ç›®æ¨™: ç¢ºä¿é‡æ§‹å¾Œç³»çµ±åŠŸèƒ½å®Œæ•´ä¸”å“è³ªæå‡")
     logger.info("=" * 60)
-    
+
     test_suite = RefactorValidationTest()
-    
+
     try:
         # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
         await test_suite.test_base_service_lifecycle()
@@ -445,10 +483,10 @@ async def main():
         await test_suite.test_workflow_error_handling()
         await test_suite.test_workflow_prerequisites()
         await test_suite.test_performance_metrics()
-        
+
         # æ‰“å°çµæœ
         success = test_suite.print_results()
-        
+
         if success:
             logger.info("\nğŸ‰ TDD Refactor éšæ®µé©—è­‰æˆåŠŸï¼")
             logger.info("âœ¨ ç³»çµ±æ¶æ§‹å·²å„ªåŒ–ï¼Œå“è³ªé¡¯è‘—æå‡")
@@ -456,19 +494,20 @@ async def main():
         else:
             logger.error("\nğŸ’¥ TDD Refactor éšæ®µé©—è­‰å¤±æ•—ï¼")
             logger.error("ğŸ”§ éœ€è¦ä¿®å¾©å•é¡Œå¾Œé‡æ–°é©—è­‰")
-        
+
         return success
-        
+
     except Exception as e:
         logger.error(f"âŒ é©—è­‰æ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}")
         return False
 
+
 if __name__ == "__main__":
     success = asyncio.run(main())
-    
+
     if success:
         logger.info("ğŸ TDD Refactor éšæ®µå®Œæˆ - ç³»çµ±é‡æ§‹æˆåŠŸ")
         sys.exit(0)
     else:
         logger.error("ğŸ›‘ TDD Refactor éšæ®µå¤±æ•— - éœ€è¦ä¿®å¾©å•é¡Œ")
-        sys.exit(1)
\ No newline at end of file
+        sys.exit(1)
diff --git a/auto_generate_video_fold6/tests/conftest.py b/auto_generate_video_fold6/tests/conftest.py
index a84b875..e687c9e 100644
--- a/auto_generate_video_fold6/tests/conftest.py
+++ b/auto_generate_video_fold6/tests/conftest.py
@@ -75,11 +75,23 @@ def mock_config_manager():
                     "batch_size": 2,
                     "platforms": ["tiktok", "instagram"],
                 },
-                "cost_control": {"daily_budget_usd": 10.0, "stop_on_budget_exceeded": True},
+                "cost_control": {
+                    "daily_budget_usd": 10.0,
+                    "stop_on_budget_exceeded": True,
+                },
                 "ai_services": {
-                    "text_generation": {"provider": "openai", "model": "gpt-3.5-turbo"},
-                    "image_generation": {"provider": "stability", "model": "stable-diffusion-xl"},
-                    "voice_synthesis": {"provider": "elevenlabs", "voice_id": "test-voice"},
+                    "text_generation": {
+                        "provider": "openai",
+                        "model": "gpt-3.5-turbo",
+                    },
+                    "image_generation": {
+                        "provider": "stability",
+                        "model": "stable-diffusion-xl",
+                    },
+                    "voice_synthesis": {
+                        "provider": "elevenlabs",
+                        "voice_id": "test-voice",
+                    },
                 },
             }
 
@@ -114,7 +126,9 @@ def mock_cost_tracker():
             self.total_cost = 0.0
             self.calls = []
 
-        async def track_api_call(self, provider, model, operation_type, **kwargs):
+        async def track_api_call(
+            self, provider, model, operation_type, **kwargs
+        ):
             cost = 0.1  # å›ºå®šæ¸¬è©¦æˆæœ¬
             self.total_cost += cost
             self.calls.append(
@@ -189,8 +203,12 @@ def mock_openai_client():
             self.chat.completions = MagicMock()
             self.chat.completions.create = AsyncMock(
                 return_value=MagicMock(
-                    choices=[MagicMock(message=MagicMock(content="æ¸¬è©¦ç”Ÿæˆçš„å…§å®¹"))],
-                    usage=MagicMock(prompt_tokens=10, completion_tokens=20, total_tokens=30),
+                    choices=[
+                        MagicMock(message=MagicMock(content="æ¸¬è©¦ç”Ÿæˆçš„å…§å®¹"))
+                    ],
+                    usage=MagicMock(
+                        prompt_tokens=10, completion_tokens=20, total_tokens=30
+                    ),
                 )
             )
 
@@ -205,7 +223,11 @@ def sample_video_data():
         "description": "é€™æ˜¯ä¸€å€‹æ¸¬è©¦å½±ç‰‡çš„æè¿°",
         "script": "é€™æ˜¯æ¸¬è©¦å½±ç‰‡è…³æœ¬å…§å®¹",
         "platforms": ["tiktok", "instagram"],
-        "style": {"theme": "technology", "tone": "professional", "duration": 60},
+        "style": {
+            "theme": "technology",
+            "tone": "professional",
+            "duration": 60,
+        },
         "generated_content": {
             "images": ["image1.jpg", "image2.jpg"],
             "audio": "audio.mp3",
@@ -272,10 +294,14 @@ def mock_http_client():
             self.responses[url] = MockResponse(response_data, status_code)
 
         async def get(self, url, **kwargs):
-            return self.responses.get(url, MockResponse({"error": "Not mocked"}, 404))
+            return self.responses.get(
+                url, MockResponse({"error": "Not mocked"}, 404)
+            )
 
         async def post(self, url, **kwargs):
-            return self.responses.get(url, MockResponse({"success": True}, 200))
+            return self.responses.get(
+                url, MockResponse({"success": True}, 200)
+            )
 
         async def __aenter__(self):
             return self
@@ -346,5 +372,15 @@ def pytest_collection_modifyitems(config, items):
 
 def pytest_addoption(parser):
     """æ·»åŠ å‘½ä»¤è¡Œé¸é …"""
-    parser.addoption("--runslow", action="store_true", default=False, help="åŸ·è¡Œæ¨™è¨˜ç‚º slow çš„æ¸¬è©¦")
-    parser.addoption("--runintegration", action="store_true", default=False, help="åŸ·è¡Œæ•´åˆæ¸¬è©¦")
+    parser.addoption(
+        "--runslow",
+        action="store_true",
+        default=False,
+        help="åŸ·è¡Œæ¨™è¨˜ç‚º slow çš„æ¸¬è©¦",
+    )
+    parser.addoption(
+        "--runintegration",
+        action="store_true",
+        default=False,
+        help="åŸ·è¡Œæ•´åˆæ¸¬è©¦",
+    )
diff --git a/auto_generate_video_fold6/tests/factories/__init__.py b/auto_generate_video_fold6/tests/factories/__init__.py
index 3a51679..e2263b9 100644
--- a/auto_generate_video_fold6/tests/factories/__init__.py
+++ b/auto_generate_video_fold6/tests/factories/__init__.py
@@ -8,14 +8,14 @@ from .script_factory import ScriptFactory, CreateScriptFactory
 from .voice_factory import VoiceCloneFactory, CreateVoiceCloneFactory
 
 __all__ = [
-    'UserFactory',
-    'CreateUserFactory',
-    'ProjectFactory', 
-    'CreateProjectFactory',
-    'VideoFactory',
-    'CreateVideoFactory',
-    'ScriptFactory',
-    'CreateScriptFactory',
-    'VoiceCloneFactory',
-    'CreateVoiceCloneFactory',
-]
\ No newline at end of file
+    "UserFactory",
+    "CreateUserFactory",
+    "ProjectFactory",
+    "CreateProjectFactory",
+    "VideoFactory",
+    "CreateVideoFactory",
+    "ScriptFactory",
+    "CreateScriptFactory",
+    "VoiceCloneFactory",
+    "CreateVoiceCloneFactory",
+]
diff --git a/auto_generate_video_fold6/tests/factories/base_factory.py b/auto_generate_video_fold6/tests/factories/base_factory.py
index 15a859d..609fc3d 100644
--- a/auto_generate_video_fold6/tests/factories/base_factory.py
+++ b/auto_generate_video_fold6/tests/factories/base_factory.py
@@ -8,37 +8,39 @@ import uuid
 import factory
 from faker import Faker
 
-fake = Faker(['zh_TW', 'en_US'])  # æ”¯æ´ç¹é«”ä¸­æ–‡å’Œè‹±æ–‡
+fake = Faker(["zh_TW", "en_US"])  # æ”¯æ´ç¹é«”ä¸­æ–‡å’Œè‹±æ–‡
+
+T = TypeVar("T")
 
-T = TypeVar('T')
 
 class BaseFactory(ABC, Generic[T]):
     """
     åŸºç¤å·¥å» æŠ½è±¡é¡åˆ¥
     ç‚ºæ‰€æœ‰æ¸¬è©¦æ•¸æ“šå·¥å» æä¾›çµ±ä¸€æ¥å£
     """
-    
+
     @abstractmethod
     def create(self, **kwargs) -> T:
         """å»ºç«‹ä¸€å€‹æ¸¬è©¦å¯¦ä¾‹"""
         pass
-    
+
     @abstractmethod
     def build(self, **kwargs) -> T:
         """å»ºæ§‹ä½†ä¸ä¿å­˜çš„æ¸¬è©¦å¯¦ä¾‹ï¼ˆç”¨æ–¼å–®å…ƒæ¸¬è©¦ï¼‰"""
         pass
-    
+
     @abstractmethod
     def create_batch(self, size: int, **kwargs) -> list[T]:
         """å»ºç«‹å¤šå€‹æ¸¬è©¦å¯¦ä¾‹"""
         pass
 
+
 class TDDFactoryMixin:
     """
     TDD å·¥å» æ··åˆé¡åˆ¥
     æä¾› TDD ç‰¹å®šçš„å·¥å» æ–¹æ³•
     """
-    
+
     @classmethod
     def create_for_red_phase(cls, **kwargs):
         """
@@ -46,15 +48,15 @@ class TDDFactoryMixin:
         ä¾‹å¦‚ï¼šç„¡æ•ˆçš„è¼¸å…¥ã€é‚Šç•Œæ¢ä»¶ç­‰
         """
         return cls.create(**kwargs)
-    
+
     @classmethod
     def create_for_green_phase(cls, **kwargs):
         """
         GREEN éšæ®µï¼šå»ºç«‹è®“æ¸¬è©¦é€šéçš„æœ€ç°¡æ•¸æ“š
         """
         return cls.create(**kwargs)
-    
-    @classmethod 
+
+    @classmethod
     def create_for_refactor_phase(cls, **kwargs):
         """
         REFACTOR éšæ®µï¼šå»ºç«‹æ¸¬è©¦é‡æ§‹çš„æ•¸æ“š
@@ -62,51 +64,56 @@ class TDDFactoryMixin:
         """
         return cls.create(**kwargs)
 
+
 class CommonFieldsMixin:
     """æä¾›å¸¸ç”¨æ¬„ä½çš„æ··åˆé¡åˆ¥"""
-    
+
     id = factory.LazyFunction(lambda: str(uuid.uuid4()))
     created_at = factory.LazyFunction(lambda: datetime.now(timezone.utc))
     updated_at = factory.LazyFunction(lambda: datetime.now(timezone.utc))
-    
+
     @factory.lazy_attribute
     def name(self):
         return fake.name()
-    
+
     @factory.lazy_attribute
     def email(self):
         return fake.unique.email()
-    
+
     @factory.lazy_attribute
     def description(self):
         return fake.text(max_nb_chars=200)
 
+
 class FactoryRegistry:
     """
     å·¥å» è¨»å†Šå™¨
     ç®¡ç†æ‰€æœ‰æ¸¬è©¦å·¥å» çš„ä¸­å¤®è¨»å†Šè¡¨
     """
-    
+
     _factories: Dict[str, Any] = {}
-    
+
     @classmethod
     def register(cls, name: str, factory_class: Any):
         """è¨»å†Šå·¥å» """
         cls._factories[name] = factory_class
-    
+
     @classmethod
     def get(cls, name: str) -> Optional[Any]:
         """ç²å–å·¥å» """
         return cls._factories.get(name)
-    
+
     @classmethod
     def list_all(cls) -> Dict[str, Any]:
         """åˆ—å‡ºæ‰€æœ‰å·¥å» """
         return cls._factories.copy()
 
+
 def register_factory(name: str):
     """è£é£¾å™¨ï¼šè‡ªå‹•è¨»å†Šå·¥å» """
+
     def decorator(factory_class):
         FactoryRegistry.register(name, factory_class)
         return factory_class
-    return decorator
\ No newline at end of file
+
+    return decorator
diff --git a/auto_generate_video_fold6/tests/factories/examples.py b/auto_generate_video_fold6/tests/factories/examples.py
index 93c4eeb..eeaff1c 100644
--- a/auto_generate_video_fold6/tests/factories/examples.py
+++ b/auto_generate_video_fold6/tests/factories/examples.py
@@ -8,255 +8,267 @@ from .video_factory import VideoFactory, create_test_videos_scenario
 from .script_factory import ScriptFactory, create_test_scripts_scenario
 from .voice_factory import VoiceCloneFactory, create_test_voices_scenario
 
+
 # TDD ç¯„ä¾‹ 1: RED éšæ®µæ¸¬è©¦
 def example_red_phase_test():
     """
     RED éšæ®µç¯„ä¾‹ï¼šæ’°å¯«æœƒå¤±æ•—çš„æ¸¬è©¦
     ä½¿ç”¨å·¥å» å»ºç«‹ç„¡æ•ˆæ•¸æ“šä¾†æ¸¬è©¦é©—è­‰é‚è¼¯
     """
-    
+
     # å»ºç«‹ç„¡æ•ˆç”¨æˆ¶æ•¸æ“šä¾†æ¸¬è©¦é©—è­‰
     invalid_user = UserFactory.create_for_red_phase()
-    
+
     # é€™äº›æ•¸æ“šæ‡‰è©²å°è‡´æ¸¬è©¦å¤±æ•—
     assert invalid_user.email == "invalid-email"  # ç„¡æ•ˆ email æ ¼å¼
     assert invalid_user.username == ""  # ç©ºç”¨æˆ¶å
-    
+
     print("RED éšæ®µï¼šå»ºç«‹äº†æœƒå°è‡´æ¸¬è©¦å¤±æ•—çš„ç„¡æ•ˆæ•¸æ“š")
 
+
 # TDD ç¯„ä¾‹ 2: GREEN éšæ®µæ¸¬è©¦
 def example_green_phase_test():
     """
     GREEN éšæ®µç¯„ä¾‹ï¼šå»ºç«‹è®“æ¸¬è©¦é€šéçš„æœ€ç°¡æ•¸æ“š
     """
-    
+
     # å»ºç«‹æœ€ç°¡æœ‰æ•ˆæ•¸æ“š
     valid_user = UserFactory.create_for_green_phase()
     valid_project = ProjectFactory.create_for_green_phase()
     valid_script = ScriptFactory.create_for_green_phase()
-    
+
     # é€™äº›æ•¸æ“šæ‡‰è©²è®“æ¸¬è©¦é€šé
     assert valid_user.username == "testuser"
     assert valid_user.email == "test@example.com"
     assert valid_project.title == "æ¸¬è©¦å°ˆæ¡ˆ"
     assert valid_script.content == "é€™æ˜¯ä¸€å€‹æ¸¬è©¦è…³æœ¬çš„å…§å®¹ã€‚"
-    
+
     print("GREEN éšæ®µï¼šå»ºç«‹äº†è®“æ¸¬è©¦é€šéçš„æœ€ç°¡æ•¸æ“š")
 
+
 # TDD ç¯„ä¾‹ 3: REFACTOR éšæ®µæ¸¬è©¦
 def example_refactor_phase_test():
     """
     REFACTOR éšæ®µç¯„ä¾‹ï¼šç¢ºä¿é‡æ§‹å¾Œè¡Œç‚ºä¸è®Š
     """
-    
+
     # å»ºç«‹ç©©å®šçš„æ¸¬è©¦æ•¸æ“šç¢ºä¿é‡æ§‹ä¸ç ´å£åŠŸèƒ½
     user = UserFactory.create_for_refactor_phase()
     project = ProjectFactory.create_for_refactor_phase()
-    
+
     # åœ¨é‡æ§‹å‰å¾Œï¼Œé€™äº›æ–·è¨€æ‡‰è©²ä¿æŒç›¸åŒçµæœ
     assert user.is_active is True
-    assert project.status in ['draft', 'in_progress', 'completed']
-    
+    assert project.status in ["draft", "in_progress", "completed"]
+
     print("REFACTOR éšæ®µï¼šç¢ºä¿é‡æ§‹å¾Œæ¸¬è©¦ä»ç„¶é€šé")
 
+
 # è¤‡é›œå ´æ™¯ç¯„ä¾‹ï¼šå®Œæ•´ç”¨æˆ¶å·¥ä½œæµç¨‹æ¸¬è©¦
 def example_complete_user_workflow():
     """
     è¤‡é›œå ´æ™¯ç¯„ä¾‹ï¼šæ¨¡æ“¬å®Œæ•´çš„ç”¨æˆ¶å·¥ä½œæµç¨‹
     å¾ç”¨æˆ¶å»ºç«‹åˆ°å°ˆæ¡ˆå®Œæˆçš„ç«¯åˆ°ç«¯æ¸¬è©¦
     """
-    
+
     # 1. å»ºç«‹æ¸¬è©¦ç”¨æˆ¶
     user = UserFactory.create_premium_user()
-    
+
     # 2. ç‚ºç”¨æˆ¶å»ºç«‹å°ˆæ¡ˆ
     project = ProjectFactory.create_draft_project(user_id=user.id)
-    
+
     # 3. ç‚ºå°ˆæ¡ˆå»ºç«‹è…³æœ¬
     script = ScriptFactory.create_educational_script(
-        project_id=project.id,
-        user_id=user.id
+        project_id=project.id, user_id=user.id
     )
-    
+
     # 4. å»ºç«‹èªéŸ³å…‹éš†
     voice = VoiceCloneFactory.create_ready_voice(user_id=user.id)
-    
+
     # 5. ç”Ÿæˆå½±ç‰‡
     video = VideoFactory.create_completed_video(project_id=project.id)
-    
+
     # é©—è­‰å®Œæ•´å·¥ä½œæµç¨‹
     assert user.is_premium is True
     assert project.user_id == user.id
     assert script.project_id == project.id
     assert voice.user_id == user.id
     assert video.project_id == project.id
-    
+
     print(f"å®Œæ•´å·¥ä½œæµç¨‹æ¸¬è©¦ï¼šç”¨æˆ¶ {user.username} æˆåŠŸå®Œæˆå½±ç‰‡è£½ä½œ")
-    
+
     return {
-        'user': user,
-        'project': project,
-        'script': script,
-        'voice': voice,
-        'video': video
+        "user": user,
+        "project": project,
+        "script": script,
+        "voice": voice,
+        "video": video,
     }
 
+
 # æ‰¹æ¬¡æ•¸æ“šæ¸¬è©¦ç¯„ä¾‹
 def example_batch_data_test():
     """
     æ‰¹æ¬¡æ•¸æ“šæ¸¬è©¦ç¯„ä¾‹ï¼šæ¸¬è©¦å¤§é‡æ•¸æ“šçš„è™•ç†
     """
-    
+
     # å»ºç«‹æ¸¬è©¦ç”¨æˆ¶
     user = UserFactory.create()
-    
+
     # æ‰¹æ¬¡å»ºç«‹å°ˆæ¡ˆ
     projects = [
         ProjectFactory.create(user_id=user.id, title=f"æ‰¹æ¬¡å°ˆæ¡ˆ {i}")
         for i in range(5)
     ]
-    
+
     # ç‚ºæ¯å€‹å°ˆæ¡ˆå»ºç«‹å½±ç‰‡
     videos = []
     for project in projects:
         video = VideoFactory.create_completed_video(project_id=project.id)
         videos.append(video)
-    
+
     # é©—è­‰æ‰¹æ¬¡æ•¸æ“š
     assert len(projects) == 5
     assert len(videos) == 5
     assert all(p.user_id == user.id for p in projects)
     assert all(v.project_id in [p.id for p in projects] for v in videos)
-    
-    print(f"æ‰¹æ¬¡æ•¸æ“šæ¸¬è©¦ï¼šç‚ºç”¨æˆ¶å»ºç«‹äº† {len(projects)} å€‹å°ˆæ¡ˆå’Œ {len(videos)} å€‹å½±ç‰‡")
+
+    print(
+        f"æ‰¹æ¬¡æ•¸æ“šæ¸¬è©¦ï¼šç‚ºç”¨æˆ¶å»ºç«‹äº† {len(projects)} å€‹å°ˆæ¡ˆå’Œ {len(videos)} å€‹å½±ç‰‡"
+    )
+
 
 # é‚Šç•Œæ¢ä»¶æ¸¬è©¦ç¯„ä¾‹
 def example_edge_case_test():
     """
     é‚Šç•Œæ¢ä»¶æ¸¬è©¦ç¯„ä¾‹ï¼šæ¸¬è©¦æ¥µé™æƒ…æ³
     """
-    
+
     # å»ºç«‹æœ€å°æœ‰æ•ˆæ•¸æ“š
     minimal_user = UserFactory.create(
         username="a",  # æœ€çŸ­ç”¨æˆ¶å
         video_count=0,  # æœ€å°‘å½±ç‰‡æ•¸
-        follower_count=0  # æœ€å°‘è¿½è¹¤è€…
+        follower_count=0,  # æœ€å°‘è¿½è¹¤è€…
     )
-    
+
     # å»ºç«‹æœ€å¤§æœ‰æ•ˆæ•¸æ“š
     maximal_user = UserFactory.create(
         username="a" * 50,  # æœ€é•·ç”¨æˆ¶å
         video_count=9999,  # æœ€å¤šå½±ç‰‡æ•¸
-        follower_count=999999  # æœ€å¤šè¿½è¹¤è€…
+        follower_count=999999,  # æœ€å¤šè¿½è¹¤è€…
     )
-    
+
     # é©—è­‰é‚Šç•Œæ¢ä»¶
     assert len(minimal_user.username) == 1
     assert len(maximal_user.username) == 50
     assert minimal_user.video_count == 0
     assert maximal_user.video_count == 9999
-    
+
     print("é‚Šç•Œæ¢ä»¶æ¸¬è©¦ï¼šé©—è­‰äº†æœ€å°å’Œæœ€å¤§æœ‰æ•ˆå€¼")
 
+
 # å·¥å» è¨»å†Šè¡¨ç¯„ä¾‹
 def example_factory_registry_usage():
     """
     å·¥å» è¨»å†Šè¡¨ä½¿ç”¨ç¯„ä¾‹ï¼šå‹•æ…‹å–å¾—å·¥å» 
     """
     from .base_factory import FactoryRegistry
-    
+
     # åˆ—å‡ºæ‰€æœ‰è¨»å†Šçš„å·¥å» 
     factories = FactoryRegistry.list_all()
     print(f"å·²è¨»å†Šçš„å·¥å» : {list(factories.keys())}")
-    
+
     # å‹•æ…‹å–å¾—å·¥å» ä¸¦å»ºç«‹æ•¸æ“š
-    user_factory = FactoryRegistry.get('user')
+    user_factory = FactoryRegistry.get("user")
     if user_factory:
         user = user_factory.create()
         print(f"é€éè¨»å†Šè¡¨å»ºç«‹ç”¨æˆ¶: {user.username}")
 
+
 # æ¸¬è©¦æ•¸æ“šæ¸…ç†ç¯„ä¾‹
 def example_test_cleanup():
     """
     æ¸¬è©¦æ•¸æ“šæ¸…ç†ç¯„ä¾‹ï¼šç¢ºä¿æ¸¬è©¦é–“çš„éš”é›¢
     """
-    
+
     # å»ºç«‹æ¸¬è©¦å ´æ™¯
     users_scenario = create_test_users_scenario()
     projects_scenario = create_test_projects_scenario()
-    
+
     # åŸ·è¡Œæ¸¬è©¦é‚è¼¯...
-    
+
     # æ¸…ç†æ¸¬è©¦æ•¸æ“š
     from .user_factory import cleanup_test_users
     from .project_factory import cleanup_test_projects
-    
+
     cleanup_test_users(users_scenario)
     cleanup_test_projects(projects_scenario)
-    
+
     print("æ¸¬è©¦æ•¸æ“šå·²æ¸…ç†ï¼Œç¢ºä¿æ¸¬è©¦éš”é›¢")
 
+
 # æ•ˆèƒ½æ¸¬è©¦æ•¸æ“šç¯„ä¾‹
 def example_performance_test_data():
     """
     æ•ˆèƒ½æ¸¬è©¦æ•¸æ“šç¯„ä¾‹ï¼šå»ºç«‹å¤§é‡æ•¸æ“šé€²è¡Œæ•ˆèƒ½æ¸¬è©¦
     """
-    
+
     import time
-    
+
     start_time = time.time()
-    
+
     # å¿«é€Ÿå»ºç«‹å¤§é‡æ¸¬è©¦æ•¸æ“š
     users = [UserFactory.create() for _ in range(100)]
     projects = [ProjectFactory.create() for _ in range(500)]
     videos = [VideoFactory.create() for _ in range(1000)]
-    
+
     end_time = time.time()
-    
+
     print(f"æ•ˆèƒ½æ¸¬è©¦ï¼šåœ¨ {end_time - start_time:.2f} ç§’å…§å»ºç«‹äº†:")
     print(f"- {len(users)} å€‹ç”¨æˆ¶")
-    print(f"- {len(projects)} å€‹å°ˆæ¡ˆ") 
+    print(f"- {len(projects)} å€‹å°ˆæ¡ˆ")
     print(f"- {len(videos)} å€‹å½±ç‰‡")
 
+
 # ä¸»è¦ç¯„ä¾‹åŸ·è¡Œå‡½æ•¸
 def run_all_examples():
     """åŸ·è¡Œæ‰€æœ‰ç¯„ä¾‹"""
-    
+
     print("ğŸ§¬ TDD æ¸¬è©¦æ•¸æ“šå·¥å» ç¯„ä¾‹")
     print("=" * 50)
-    
+
     try:
         example_red_phase_test()
         print()
-        
+
         example_green_phase_test()
         print()
-        
+
         example_refactor_phase_test()
         print()
-        
+
         workflow_data = example_complete_user_workflow()
         print()
-        
+
         example_batch_data_test()
         print()
-        
+
         example_edge_case_test()
         print()
-        
+
         example_factory_registry_usage()
         print()
-        
+
         example_test_cleanup()
         print()
-        
+
         example_performance_test_data()
-        
+
         print("=" * 50)
         print("âœ… æ‰€æœ‰æ¸¬è©¦æ•¸æ“šå·¥å» ç¯„ä¾‹åŸ·è¡Œå®Œæˆï¼")
-        
+
     except Exception as e:
         print(f"âŒ ç¯„ä¾‹åŸ·è¡Œå¤±æ•—: {e}")
 
+
 if __name__ == "__main__":
-    run_all_examples()
\ No newline at end of file
+    run_all_examples()
diff --git a/auto_generate_video_fold6/tests/factories/project_factory.py b/auto_generate_video_fold6/tests/factories/project_factory.py
index fc3c8b7..0da8ac5 100644
--- a/auto_generate_video_fold6/tests/factories/project_factory.py
+++ b/auto_generate_video_fold6/tests/factories/project_factory.py
@@ -8,13 +8,14 @@ from typing import Dict, Any, List
 from enum import Enum
 
 from .base_factory import (
-    BaseFactory, 
-    TDDFactoryMixin, 
+    BaseFactory,
+    TDDFactoryMixin,
     CommonFieldsMixin,
-    register_factory
+    register_factory,
 )
 
-fake = Faker(['zh_TW', 'en_US'])
+fake = Faker(["zh_TW", "en_US"])
+
 
 class ProjectStatus(Enum):
     DRAFT = "draft"
@@ -23,67 +24,90 @@ class ProjectStatus(Enum):
     PUBLISHED = "published"
     ARCHIVED = "archived"
 
+
 class ProjectData:
     """å°ˆæ¡ˆæ•¸æ“šé¡åˆ¥"""
+
     def __init__(self, **kwargs):
         for key, value in kwargs.items():
             setattr(self, key, value)
 
-@register_factory('project')
+
+@register_factory("project")
 class ProjectFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
     """
     å°ˆæ¡ˆå·¥å» 
     ç‚º TDD æ¸¬è©¦æä¾›å„ç¨®å°ˆæ¡ˆæ•¸æ“šæƒ…å¢ƒ
     """
-    
+
     class Meta:
         model = ProjectData
-    
+
     # åŸºæœ¬æ¬„ä½
     title = factory.LazyAttribute(lambda o: fake.sentence(nb_words=4))
     description = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=300))
-    
+
     # ç”¨æˆ¶é—œè¯
     user_id = factory.LazyFunction(lambda: fake.uuid4())
-    
+
     # å°ˆæ¡ˆè¨­å®š
-    target_platform = factory.Iterator(['tiktok', 'instagram_reels', 'youtube_shorts'])
-    target_audience = factory.LazyAttribute(lambda o: fake.random_element([
-        '18-24æ­²å¹´è¼•äºº', '25-34æ­²ä¸Šç­æ—', '35-44æ­²å®¶é•·', '45æ­²ä»¥ä¸Š'
-    ]))
-    content_style = factory.LazyAttribute(lambda o: fake.random_element([
-        'educational', 'entertainment', 'promotional', 'informational'
-    ]))
-    
+    target_platform = factory.Iterator(
+        ["tiktok", "instagram_reels", "youtube_shorts"]
+    )
+    target_audience = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["18-24æ­²å¹´è¼•äºº", "25-34æ­²ä¸Šç­æ—", "35-44æ­²å®¶é•·", "45æ­²ä»¥ä¸Š"]
+        )
+    )
+    content_style = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["educational", "entertainment", "promotional", "informational"]
+        )
+    )
+
     # ç‹€æ…‹
-    status = factory.LazyAttribute(lambda o: fake.random_element([
-        ProjectStatus.DRAFT.value,
-        ProjectStatus.IN_PROGRESS.value,
-        ProjectStatus.COMPLETED.value
-    ]))
-    
+    status = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            [
+                ProjectStatus.DRAFT.value,
+                ProjectStatus.IN_PROGRESS.value,
+                ProjectStatus.COMPLETED.value,
+            ]
+        )
+    )
+
     # å…§å®¹è¨­å®š
-    script_content = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=500))
-    voice_settings = factory.LazyAttribute(lambda o: {
-        'voice_id': fake.uuid4(),
-        'speed': fake.random.uniform(0.8, 1.2),
-        'pitch': fake.random.uniform(-0.2, 0.2),
-        'emotion': fake.random_element(['neutral', 'happy', 'sad', 'excited'])
-    })
-    
+    script_content = factory.LazyAttribute(
+        lambda o: fake.text(max_nb_chars=500)
+    )
+    voice_settings = factory.LazyAttribute(
+        lambda o: {
+            "voice_id": fake.uuid4(),
+            "speed": fake.random.uniform(0.8, 1.2),
+            "pitch": fake.random.uniform(-0.2, 0.2),
+            "emotion": fake.random_element(
+                ["neutral", "happy", "sad", "excited"]
+            ),
+        }
+    )
+
     # è¦–è¦ºè¨­å®š
-    visual_style = factory.LazyAttribute(lambda o: fake.random_element([
-        'modern', 'minimalist', 'colorful', 'dark', 'vintage'
-    ]))
-    background_music = factory.LazyAttribute(lambda o: fake.random_element([
-        'upbeat', 'relaxing', 'epic', 'acoustic', 'electronic'
-    ]))
-    
+    visual_style = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["modern", "minimalist", "colorful", "dark", "vintage"]
+        )
+    )
+    background_music = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["upbeat", "relaxing", "epic", "acoustic", "electronic"]
+        )
+    )
+
     # çµ±è¨ˆæ•¸æ“š
     view_count = factory.LazyFunction(lambda: fake.random_int(0, 10000))
     like_count = factory.LazyFunction(lambda: fake.random_int(0, 1000))
     share_count = factory.LazyFunction(lambda: fake.random_int(0, 100))
-    
+
     # æª”æ¡ˆè·¯å¾‘
     output_video_path = factory.LazyAttribute(
         lambda o: f"/videos/{o.id}/output.mp4"
@@ -91,7 +115,7 @@ class ProjectFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
     thumbnail_path = factory.LazyAttribute(
         lambda o: f"/thumbnails/{o.id}/thumb.jpg"
     )
-    
+
     # æ™‚é–“è¨­å®š
     duration_seconds = factory.LazyFunction(lambda: fake.random_int(15, 180))
     scheduled_publish_at = None
@@ -104,9 +128,9 @@ class ProjectFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             status=ProjectStatus.DRAFT.value,
             user_id=user_id or fake.uuid4(),
             published_at=None,
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_completed_project(cls, user_id: str = None, **kwargs):
         """å»ºç«‹å·²å®Œæˆå°ˆæ¡ˆ"""
@@ -114,9 +138,9 @@ class ProjectFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             status=ProjectStatus.COMPLETED.value,
             user_id=user_id or fake.uuid4(),
             output_video_path=f"/videos/{fake.uuid4()}/completed.mp4",
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_published_project(cls, user_id: str = None, **kwargs):
         """å»ºç«‹å·²ç™¼å¸ƒå°ˆæ¡ˆ"""
@@ -125,9 +149,9 @@ class ProjectFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             user_id=user_id or fake.uuid4(),
             published_at=datetime.now(timezone.utc),
             view_count=fake.random_int(100, 5000),
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_for_red_phase(cls, **kwargs):
         """RED éšæ®µï¼šå»ºç«‹æœƒå°è‡´æ¸¬è©¦å¤±æ•—çš„å°ˆæ¡ˆæ•¸æ“š"""
@@ -135,9 +159,9 @@ class ProjectFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             title="",  # ç„¡æ•ˆï¼šç©ºæ¨™é¡Œ
             user_id="",  # ç„¡æ•ˆï¼šç©ºç”¨æˆ¶ID
             duration_seconds=-1,  # ç„¡æ•ˆï¼šè² æ•¸æŒçºŒæ™‚é–“
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_for_green_phase(cls, **kwargs):
         """GREEN éšæ®µï¼šå»ºç«‹è®“æ¸¬è©¦é€šéçš„æœ€ç°¡å°ˆæ¡ˆæ•¸æ“š"""
@@ -145,88 +169,101 @@ class ProjectFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             title="æ¸¬è©¦å°ˆæ¡ˆ",
             user_id=fake.uuid4(),
             status=ProjectStatus.DRAFT.value,
-            **kwargs
+            **kwargs,
         )
 
-@register_factory('create_project')
+
+@register_factory("create_project")
 class CreateProjectFactory(factory.Factory, TDDFactoryMixin):
     """
     å»ºç«‹å°ˆæ¡ˆè«‹æ±‚æ•¸æ“šå·¥å» 
     æ¨¡æ“¬ API è«‹æ±‚ä¸­çš„å°ˆæ¡ˆå»ºç«‹æ•¸æ“š
     """
-    
+
     class Meta:
         model = dict
-    
+
     title = factory.LazyAttribute(lambda o: fake.sentence(nb_words=3))
     description = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=200))
-    target_platform = factory.Iterator(['tiktok', 'instagram_reels', 'youtube_shorts'])
-    content_style = factory.Iterator(['educational', 'entertainment', 'promotional'])
+    target_platform = factory.Iterator(
+        ["tiktok", "instagram_reels", "youtube_shorts"]
+    )
+    content_style = factory.Iterator(
+        ["educational", "entertainment", "promotional"]
+    )
     target_audience = "18-34æ­²å¹´è¼•äºº"
     visual_style = "modern"
-    
+
     @classmethod
     def create_invalid_request(cls, **kwargs):
         """å»ºç«‹ç„¡æ•ˆçš„å°ˆæ¡ˆå»ºç«‹è«‹æ±‚"""
         return cls.build(
             title="",  # ç„¡æ•ˆï¼šç©ºæ¨™é¡Œ
             target_platform="invalid_platform",  # ç„¡æ•ˆï¼šä¸æ”¯æ´çš„å¹³å°
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_minimal_valid_request(cls, **kwargs):
         """å»ºç«‹æœ€ç°¡æœ‰æ•ˆçš„å°ˆæ¡ˆå»ºç«‹è«‹æ±‚"""
         return cls.build(
-            title="æœ€ç°¡æ¸¬è©¦å°ˆæ¡ˆ",
-            target_platform="tiktok",
-            **kwargs
+            title="æœ€ç°¡æ¸¬è©¦å°ˆæ¡ˆ", target_platform="tiktok", **kwargs
         )
 
+
 class ProjectBatchFactory:
     """å°ˆæ¡ˆæ‰¹æ¬¡å»ºç«‹å·¥å» """
-    
+
     @staticmethod
-    def create_user_projects(user_id: str, count: int = 5) -> List[ProjectData]:
+    def create_user_projects(
+        user_id: str, count: int = 5
+    ) -> List[ProjectData]:
         """ç‚ºç‰¹å®šç”¨æˆ¶å»ºç«‹å¤šå€‹å°ˆæ¡ˆ"""
         projects = []
         for i in range(count):
             project = ProjectFactory.create(
-                user_id=user_id,
-                title=f"ç”¨æˆ¶å°ˆæ¡ˆ {i+1}"
+                user_id=user_id, title=f"ç”¨æˆ¶å°ˆæ¡ˆ {i + 1}"
             )
             projects.append(project)
         return projects
-    
+
     @staticmethod
     def create_mixed_status_projects(user_id: str) -> Dict[str, ProjectData]:
         """å»ºç«‹ä¸åŒç‹€æ…‹çš„å°ˆæ¡ˆ"""
         return {
-            'draft': ProjectFactory.create_draft_project(user_id),
-            'completed': ProjectFactory.create_completed_project(user_id),
-            'published': ProjectFactory.create_published_project(user_id)
+            "draft": ProjectFactory.create_draft_project(user_id),
+            "completed": ProjectFactory.create_completed_project(user_id),
+            "published": ProjectFactory.create_published_project(user_id),
         }
 
+
 # TDD è¼”åŠ©å‡½æ•¸
-def create_test_projects_scenario(user_id: str = None) -> Dict[str, ProjectData]:
+def create_test_projects_scenario(
+    user_id: str = None,
+) -> Dict[str, ProjectData]:
     """
     å»ºç«‹å®Œæ•´çš„æ¸¬è©¦å°ˆæ¡ˆæƒ…å¢ƒ
     ç”¨æ–¼è¤‡é›œçš„ TDD æ¸¬è©¦å ´æ™¯
     """
     test_user_id = user_id or fake.uuid4()
-    
+
     return {
-        'draft_project': ProjectFactory.create_draft_project(test_user_id),
-        'completed_project': ProjectFactory.create_completed_project(test_user_id),
-        'published_project': ProjectFactory.create_published_project(test_user_id),
-        'invalid_project': ProjectFactory.create_for_red_phase(),
-        'valid_project': ProjectFactory.create_for_green_phase()
+        "draft_project": ProjectFactory.create_draft_project(test_user_id),
+        "completed_project": ProjectFactory.create_completed_project(
+            test_user_id
+        ),
+        "published_project": ProjectFactory.create_published_project(
+            test_user_id
+        ),
+        "invalid_project": ProjectFactory.create_for_red_phase(),
+        "valid_project": ProjectFactory.create_for_green_phase(),
     }
 
+
 def cleanup_test_projects(projects: Dict[str, ProjectData]):
     """
     æ¸…ç†æ¸¬è©¦å°ˆæ¡ˆæ•¸æ“š
     åœ¨æ¸¬è©¦å®Œæˆå¾Œå‘¼å«
     """
     # æ¸…ç†é‚è¼¯
-    pass
\ No newline at end of file
+    pass
diff --git a/auto_generate_video_fold6/tests/factories/script_factory.py b/auto_generate_video_fold6/tests/factories/script_factory.py
index dd4816b..2b354c1 100644
--- a/auto_generate_video_fold6/tests/factories/script_factory.py
+++ b/auto_generate_video_fold6/tests/factories/script_factory.py
@@ -8,13 +8,14 @@ from typing import Dict, Any, List
 from enum import Enum
 
 from .base_factory import (
-    BaseFactory, 
-    TDDFactoryMixin, 
+    BaseFactory,
+    TDDFactoryMixin,
     CommonFieldsMixin,
-    register_factory
+    register_factory,
 )
 
-fake = Faker(['zh_TW', 'en_US'])
+fake = Faker(["zh_TW", "en_US"])
+
 
 class ScriptType(Enum):
     EDUCATIONAL = "educational"
@@ -23,6 +24,7 @@ class ScriptType(Enum):
     NEWS = "news"
     TUTORIAL = "tutorial"
 
+
 class ScriptStatus(Enum):
     DRAFT = "draft"
     GENERATED = "generated"
@@ -30,92 +32,125 @@ class ScriptStatus(Enum):
     APPROVED = "approved"
     USED = "used"
 
+
 class ScriptData:
     """è…³æœ¬æ•¸æ“šé¡åˆ¥"""
+
     def __init__(self, **kwargs):
         for key, value in kwargs.items():
             setattr(self, key, value)
 
-@register_factory('script')
+
+@register_factory("script")
 class ScriptFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
     """
     è…³æœ¬å·¥å» 
     ç‚º TDD æ¸¬è©¦æä¾›å„ç¨®è…³æœ¬æ•¸æ“šæƒ…å¢ƒ
     """
-    
+
     class Meta:
         model = ScriptData
-    
+
     # åŸºæœ¬æ¬„ä½
     title = factory.LazyAttribute(lambda o: fake.sentence(nb_words=6))
     content = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=800))
-    
+
     # é—œè¯æ¬„ä½
     project_id = factory.LazyFunction(lambda: fake.uuid4())
     user_id = factory.LazyFunction(lambda: fake.uuid4())
-    
+
     # è…³æœ¬é¡å‹å’Œè¨­å®š
-    script_type = factory.LazyAttribute(lambda o: fake.random_element([
-        ScriptType.EDUCATIONAL.value,
-        ScriptType.ENTERTAINMENT.value,
-        ScriptType.PROMOTIONAL.value
-    ]))
-    
-    target_platform = factory.Iterator(['tiktok', 'instagram_reels', 'youtube_shorts'])
-    target_audience = factory.LazyAttribute(lambda o: fake.random_element([
-        '18-24æ­²å¹´è¼•äºº', '25-34æ­²ä¸Šç­æ—', '35-44æ­²å®¶é•·'
-    ]))
-    
+    script_type = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            [
+                ScriptType.EDUCATIONAL.value,
+                ScriptType.ENTERTAINMENT.value,
+                ScriptType.PROMOTIONAL.value,
+            ]
+        )
+    )
+
+    target_platform = factory.Iterator(
+        ["tiktok", "instagram_reels", "youtube_shorts"]
+    )
+    target_audience = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["18-24æ­²å¹´è¼•äºº", "25-34æ­²ä¸Šç­æ—", "35-44æ­²å®¶é•·"]
+        )
+    )
+
     # èªè¨€å’Œé¢¨æ ¼
-    language = 'zh-TW'
-    tone = factory.LazyAttribute(lambda o: fake.random_element([
-        'professional', 'casual', 'humorous', 'serious', 'friendly'
-    ]))
-    style = factory.LazyAttribute(lambda o: fake.random_element([
-        'conversational', 'narrative', 'instructional', 'persuasive'
-    ]))
-    
+    language = "zh-TW"
+    tone = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["professional", "casual", "humorous", "serious", "friendly"]
+        )
+    )
+    style = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["conversational", "narrative", "instructional", "persuasive"]
+        )
+    )
+
     # å…§å®¹çµ±è¨ˆ
-    word_count = factory.LazyAttribute(lambda o: len(o.content.split()) if hasattr(o, 'content') else fake.random_int(50, 200))
-    estimated_duration = factory.LazyAttribute(lambda o: o.word_count * 0.4 if hasattr(o, 'word_count') else fake.random_int(30, 120))  # ç§’
-    
+    word_count = factory.LazyAttribute(
+        lambda o: len(o.content.split())
+        if hasattr(o, "content")
+        else fake.random_int(50, 200)
+    )
+    estimated_duration = factory.LazyAttribute(
+        lambda o: o.word_count * 0.4
+        if hasattr(o, "word_count")
+        else fake.random_int(30, 120)
+    )  # ç§’
+
     # ç‹€æ…‹
-    status = factory.LazyAttribute(lambda o: fake.random_element([
-        ScriptStatus.DRAFT.value,
-        ScriptStatus.GENERATED.value,
-        ScriptStatus.REVIEWED.value
-    ]))
-    
+    status = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            [
+                ScriptStatus.DRAFT.value,
+                ScriptStatus.GENERATED.value,
+                ScriptStatus.REVIEWED.value,
+            ]
+        )
+    )
+
     # AI ç”Ÿæˆè³‡è¨Š
     generated_by_ai = True
-    ai_model_used = factory.LazyAttribute(lambda o: fake.random_element([
-        'gpt-4', 'claude-3', 'gemini-pro'
-    ]))
-    generation_prompt = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=200))
-    
+    ai_model_used = factory.LazyAttribute(
+        lambda o: fake.random_element(["gpt-4", "claude-3", "gemini-pro"])
+    )
+    generation_prompt = factory.LazyAttribute(
+        lambda o: fake.text(max_nb_chars=200)
+    )
+
     # é—œéµå­—å’Œæ¨™ç±¤
-    keywords = factory.LazyAttribute(lambda o: [
-        fake.word() for _ in range(fake.random_int(3, 8))
-    ])
-    hashtags = factory.LazyAttribute(lambda o: [
-        f"#{fake.word()}" for _ in range(fake.random_int(3, 6))
-    ])
-    
+    keywords = factory.LazyAttribute(
+        lambda o: [fake.word() for _ in range(fake.random_int(3, 8))]
+    )
+    hashtags = factory.LazyAttribute(
+        lambda o: [f"#{fake.word()}" for _ in range(fake.random_int(3, 6))]
+    )
+
     # çµæ§‹åŒ–å…§å®¹
-    hook = factory.LazyAttribute(lambda o: fake.sentence(nb_words=8))  # é–‹å ´å¸å¼•å¥
-    main_points = factory.LazyAttribute(lambda o: [
-        fake.sentence(nb_words=12) for _ in range(fake.random_int(2, 5))
-    ])
+    hook = factory.LazyAttribute(
+        lambda o: fake.sentence(nb_words=8)
+    )  # é–‹å ´å¸å¼•å¥
+    main_points = factory.LazyAttribute(
+        lambda o: [
+            fake.sentence(nb_words=12) for _ in range(fake.random_int(2, 5))
+        ]
+    )
     call_to_action = factory.LazyAttribute(lambda o: fake.sentence(nb_words=6))
-    
+
     # ç‰ˆæœ¬æ§åˆ¶
     version = factory.LazyFunction(lambda: fake.random_int(1, 5))
     parent_script_id = None
-    
+
     # ä½¿ç”¨çµ±è¨ˆ
     usage_count = factory.LazyFunction(lambda: fake.random_int(0, 10))
     last_used_at = None
-    
+
     # è©•åˆ†å’Œåé¥‹
     quality_score = factory.LazyFunction(lambda: fake.random.uniform(7.0, 9.5))
     user_rating = None
@@ -126,42 +161,42 @@ class ScriptFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
         """å»ºç«‹æ•™è‚²é¡å‹è…³æœ¬"""
         return cls.create(
             script_type=ScriptType.EDUCATIONAL.value,
-            tone='professional',
-            style='instructional',
+            tone="professional",
+            style="instructional",
             content=fake.text(max_nb_chars=600),
             main_points=[
                 "å­¸ç¿’çš„ç¬¬ä¸€æ­¥æ˜¯ç†è§£åŸºæœ¬æ¦‚å¿µ",
                 "å¯¦è¸æ˜¯æŒæ¡æŠ€èƒ½çš„é—œéµ",
-                "æŒçºŒç·´ç¿’èƒ½å¤ éå›ºçŸ¥è­˜"
+                "æŒçºŒç·´ç¿’èƒ½å¤ éå›ºçŸ¥è­˜",
             ],
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_entertainment_script(cls, **kwargs):
         """å»ºç«‹å¨›æ¨‚é¡å‹è…³æœ¬"""
         return cls.create(
             script_type=ScriptType.ENTERTAINMENT.value,
-            tone='humorous',
-            style='conversational',
+            tone="humorous",
+            style="conversational",
             content=fake.text(max_nb_chars=400),
             hook="ä»Šå¤©è¦è·Ÿå¤§å®¶åˆ†äº«ä¸€å€‹è¶…æœ‰è¶£çš„æ•…äº‹ï¼",
             call_to_action="è¨˜å¾—æŒ‰è®šåˆ†äº«çµ¦æœ‹å‹å€‘ï¼",
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_promotional_script(cls, **kwargs):
         """å»ºç«‹æ¨å»£é¡å‹è…³æœ¬"""
         return cls.create(
             script_type=ScriptType.PROMOTIONAL.value,
-            tone='persuasive',
-            style='persuasive',
+            tone="persuasive",
+            style="persuasive",
             content=fake.text(max_nb_chars=500),
             call_to_action="ç«‹å³é»æ“Šé€£çµäº†è§£æ›´å¤šï¼",
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_approved_script(cls, **kwargs):
         """å»ºç«‹å·²å¯©æ ¸é€šéçš„è…³æœ¬"""
@@ -169,9 +204,9 @@ class ScriptFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             status=ScriptStatus.APPROVED.value,
             quality_score=fake.random.uniform(8.5, 9.5),
             user_rating=fake.random_int(4, 5),
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_for_red_phase(cls, **kwargs):
         """RED éšæ®µï¼šå»ºç«‹æœƒå°è‡´æ¸¬è©¦å¤±æ•—çš„è…³æœ¬æ•¸æ“š"""
@@ -180,9 +215,9 @@ class ScriptFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             content="",  # ç„¡æ•ˆï¼šç©ºå…§å®¹
             word_count=-1,  # ç„¡æ•ˆï¼šè² æ•¸å­—æ•¸
             estimated_duration=-1,  # ç„¡æ•ˆï¼šè² æ•¸æŒçºŒæ™‚é–“
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_for_green_phase(cls, **kwargs):
         """GREEN éšæ®µï¼šå»ºç«‹è®“æ¸¬è©¦é€šéçš„æœ€ç°¡è…³æœ¬æ•¸æ“š"""
@@ -190,19 +225,20 @@ class ScriptFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             title="æ¸¬è©¦è…³æœ¬",
             content="é€™æ˜¯ä¸€å€‹æ¸¬è©¦è…³æœ¬çš„å…§å®¹ã€‚",
             status=ScriptStatus.GENERATED.value,
-            **kwargs
+            **kwargs,
         )
 
-@register_factory('create_script')
+
+@register_factory("create_script")
 class CreateScriptFactory(factory.Factory, TDDFactoryMixin):
     """
     å»ºç«‹è…³æœ¬è«‹æ±‚æ•¸æ“šå·¥å» 
     æ¨¡æ“¬ API è«‹æ±‚ä¸­çš„è…³æœ¬å»ºç«‹æ•¸æ“š
     """
-    
+
     class Meta:
         model = dict
-    
+
     title = factory.LazyAttribute(lambda o: fake.sentence(nb_words=5))
     project_id = factory.LazyFunction(lambda: fake.uuid4())
     script_type = ScriptType.EDUCATIONAL.value
@@ -210,12 +246,14 @@ class CreateScriptFactory(factory.Factory, TDDFactoryMixin):
     target_audience = "18-34æ­²å¹´è¼•äºº"
     tone = "friendly"
     style = "conversational"
-    
+
     # ç”Ÿæˆè¨­å®š
-    generation_prompt = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=150))
+    generation_prompt = factory.LazyAttribute(
+        lambda o: fake.text(max_nb_chars=150)
+    )
     keywords = factory.LazyAttribute(lambda o: [fake.word() for _ in range(3)])
     desired_length = "medium"  # short, medium, long
-    
+
     @classmethod
     def create_invalid_request(cls, **kwargs):
         """å»ºç«‹ç„¡æ•ˆçš„è…³æœ¬å»ºç«‹è«‹æ±‚"""
@@ -223,9 +261,9 @@ class CreateScriptFactory(factory.Factory, TDDFactoryMixin):
             title="",  # ç„¡æ•ˆï¼šç©ºæ¨™é¡Œ
             project_id="invalid-id",  # ç„¡æ•ˆï¼šéŒ¯èª¤æ ¼å¼çš„ID
             script_type="invalid_type",  # ç„¡æ•ˆï¼šä¸æ”¯æ´çš„é¡å‹
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_minimal_valid_request(cls, **kwargs):
         """å»ºç«‹æœ€ç°¡æœ‰æ•ˆçš„è…³æœ¬å»ºç«‹è«‹æ±‚"""
@@ -233,65 +271,70 @@ class CreateScriptFactory(factory.Factory, TDDFactoryMixin):
             title="ç°¡å–®æ¸¬è©¦è…³æœ¬",
             project_id=fake.uuid4(),
             generation_prompt="è«‹ç”Ÿæˆä¸€å€‹ç°¡å–®çš„æ¸¬è©¦è…³æœ¬",
-            **kwargs
+            **kwargs,
         )
 
+
 class ScriptBatchFactory:
     """è…³æœ¬æ‰¹æ¬¡å»ºç«‹å·¥å» """
-    
+
     @staticmethod
-    def create_project_scripts(project_id: str, count: int = 3) -> List[ScriptData]:
+    def create_project_scripts(
+        project_id: str, count: int = 3
+    ) -> List[ScriptData]:
         """ç‚ºç‰¹å®šå°ˆæ¡ˆå»ºç«‹å¤šå€‹è…³æœ¬"""
         scripts = []
         for i in range(count):
             script = ScriptFactory.create(
-                project_id=project_id,
-                title=f"å°ˆæ¡ˆè…³æœ¬ {i+1}",
-                version=i+1
+                project_id=project_id, title=f"å°ˆæ¡ˆè…³æœ¬ {i + 1}", version=i + 1
             )
             scripts.append(script)
         return scripts
-    
+
     @staticmethod
     def create_different_types_scripts() -> Dict[str, ScriptData]:
         """å»ºç«‹ä¸åŒé¡å‹çš„è…³æœ¬"""
         return {
-            'educational': ScriptFactory.create_educational_script(),
-            'entertainment': ScriptFactory.create_entertainment_script(),
-            'promotional': ScriptFactory.create_promotional_script()
+            "educational": ScriptFactory.create_educational_script(),
+            "entertainment": ScriptFactory.create_entertainment_script(),
+            "promotional": ScriptFactory.create_promotional_script(),
         }
 
+
 # TDD è¼”åŠ©å‡½æ•¸
-def create_test_scripts_scenario(project_id: str = None) -> Dict[str, ScriptData]:
+def create_test_scripts_scenario(
+    project_id: str = None,
+) -> Dict[str, ScriptData]:
     """
     å»ºç«‹å®Œæ•´çš„æ¸¬è©¦è…³æœ¬æƒ…å¢ƒ
     ç”¨æ–¼è¤‡é›œçš„ TDD æ¸¬è©¦å ´æ™¯
     """
     test_project_id = project_id or fake.uuid4()
-    
+
     return {
-        'draft_script': ScriptFactory.create(
-            project_id=test_project_id,
-            status=ScriptStatus.DRAFT.value
+        "draft_script": ScriptFactory.create(
+            project_id=test_project_id, status=ScriptStatus.DRAFT.value
         ),
-        'generated_script': ScriptFactory.create(
-            project_id=test_project_id,
-            status=ScriptStatus.GENERATED.value
+        "generated_script": ScriptFactory.create(
+            project_id=test_project_id, status=ScriptStatus.GENERATED.value
         ),
-        'approved_script': ScriptFactory.create_approved_script(
+        "approved_script": ScriptFactory.create_approved_script(
             project_id=test_project_id
         ),
-        'educational_script': ScriptFactory.create_educational_script(
+        "educational_script": ScriptFactory.create_educational_script(
             project_id=test_project_id
         ),
-        'entertainment_script': ScriptFactory.create_entertainment_script(
+        "entertainment_script": ScriptFactory.create_entertainment_script(
             project_id=test_project_id
         ),
-        'invalid_script': ScriptFactory.create_for_red_phase(),
-        'valid_script': ScriptFactory.create_for_green_phase()
+        "invalid_script": ScriptFactory.create_for_red_phase(),
+        "valid_script": ScriptFactory.create_for_green_phase(),
     }
 
-def create_script_versions(base_script: ScriptData, versions: int = 3) -> List[ScriptData]:
+
+def create_script_versions(
+    base_script: ScriptData, versions: int = 3
+) -> List[ScriptData]:
     """
     å»ºç«‹è…³æœ¬ç‰ˆæœ¬åºåˆ—
     ç”¨æ–¼æ¸¬è©¦ç‰ˆæœ¬æ§åˆ¶åŠŸèƒ½
@@ -300,18 +343,19 @@ def create_script_versions(base_script: ScriptData, versions: int = 3) -> List[S
     for i in range(versions):
         version_script = ScriptFactory.create(
             project_id=base_script.project_id,
-            title=f"{base_script.title} v{i+2}",
+            title=f"{base_script.title} v{i + 2}",
             parent_script_id=base_script.id,
-            version=i+2,
-            content=fake.text(max_nb_chars=600)
+            version=i + 2,
+            content=fake.text(max_nb_chars=600),
         )
         versions_list.append(version_script)
     return versions_list
 
+
 def cleanup_test_scripts(scripts: Dict[str, ScriptData]):
     """
     æ¸…ç†æ¸¬è©¦è…³æœ¬æ•¸æ“š
     åœ¨æ¸¬è©¦å®Œæˆå¾Œå‘¼å«
     """
     # æ¸…ç†è…³æœ¬æ•¸æ“šå’Œç›¸é—œæª”æ¡ˆ
-    pass
\ No newline at end of file
+    pass
diff --git a/auto_generate_video_fold6/tests/factories/user_factory.py b/auto_generate_video_fold6/tests/factories/user_factory.py
index 1e84e7d..bba79e7 100644
--- a/auto_generate_video_fold6/tests/factories/user_factory.py
+++ b/auto_generate_video_fold6/tests/factories/user_factory.py
@@ -7,62 +7,65 @@ from datetime import datetime, timezone
 from typing import Dict, Any
 
 from .base_factory import (
-    BaseFactory, 
-    TDDFactoryMixin, 
+    BaseFactory,
+    TDDFactoryMixin,
     CommonFieldsMixin,
-    register_factory
+    register_factory,
 )
 
-fake = Faker(['zh_TW', 'en_US'])
+fake = Faker(["zh_TW", "en_US"])
+
 
 class UserData:
     """ç”¨æˆ¶æ•¸æ“šé¡åˆ¥ï¼ˆç”¨æ–¼ä¸éœ€è¦ ORM çš„æ¸¬è©¦ï¼‰"""
+
     def __init__(self, **kwargs):
         for key, value in kwargs.items():
             setattr(self, key, value)
 
-@register_factory('user')
+
+@register_factory("user")
 class UserFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
     """
     ç”¨æˆ¶å·¥å» 
     ç‚º TDD æ¸¬è©¦æä¾›å„ç¨®ç”¨æˆ¶æ•¸æ“šæƒ…å¢ƒ
     """
-    
+
     class Meta:
         model = UserData
-    
+
     # åŸºæœ¬æ¬„ä½
     username = factory.LazyAttribute(lambda o: fake.user_name())
     email = factory.LazyAttribute(lambda o: fake.unique.email())
     full_name = factory.LazyAttribute(lambda o: fake.name())
-    
+
     # å¯†ç¢¼ç›¸é—œ
     hashed_password = factory.LazyFunction(
         lambda: "$2b$12$test_hash_for_password_123"
     )
-    
+
     # ç‹€æ…‹æ¬„ä½
     is_active = True
     is_verified = True
     is_premium = False
-    
+
     # å€‹äººè³‡è¨Š
     avatar_url = factory.LazyAttribute(
         lambda o: f"https://example.com/avatars/{o.username}.jpg"
     )
     bio = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=150))
     website = factory.LazyAttribute(lambda o: fake.url())
-    
+
     # åå¥½è¨­å®š
-    language = 'zh-TW'
-    timezone = 'Asia/Taipei'
-    theme = 'light'
-    
+    language = "zh-TW"
+    timezone = "Asia/Taipei"
+    theme = "light"
+
     # çµ±è¨ˆæ•¸æ“š
     video_count = factory.LazyFunction(lambda: fake.random_int(0, 100))
     follower_count = factory.LazyFunction(lambda: fake.random_int(0, 1000))
     following_count = factory.LazyFunction(lambda: fake.random_int(0, 500))
-    
+
     # æ™‚é–“æˆ³è¨˜
     last_login_at = factory.LazyFunction(lambda: datetime.now(timezone.utc))
     premium_expires_at = None
@@ -74,9 +77,9 @@ class UserFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             username="admin",
             email="admin@example.com",
             is_premium=True,
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_premium_user(cls, **kwargs):
         """å»ºç«‹é«˜ç´šç”¨æˆ¶"""
@@ -85,18 +88,14 @@ class UserFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             premium_expires_at=factory.LazyFunction(
                 lambda: datetime.now(timezone.utc).replace(year=2025)
             ),
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_inactive_user(cls, **kwargs):
         """å»ºç«‹æœªå•Ÿç”¨ç”¨æˆ¶"""
-        return cls.create(
-            is_active=False,
-            is_verified=False,
-            **kwargs
-        )
-    
+        return cls.create(is_active=False, is_verified=False, **kwargs)
+
     @classmethod
     def create_for_red_phase(cls, **kwargs):
         """RED éšæ®µï¼šå»ºç«‹æœƒå°è‡´æ¸¬è©¦å¤±æ•—çš„ç”¨æˆ¶æ•¸æ“š"""
@@ -104,35 +103,34 @@ class UserFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
         return cls.create(
             email="invalid-email",  # ç„¡æ•ˆ email
             username="",  # ç©ºç”¨æˆ¶å
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_for_green_phase(cls, **kwargs):
         """GREEN éšæ®µï¼šå»ºç«‹è®“æ¸¬è©¦é€šéçš„æœ€ç°¡ç”¨æˆ¶æ•¸æ“š"""
         return cls.create(
-            username="testuser",
-            email="test@example.com",
-            **kwargs
+            username="testuser", email="test@example.com", **kwargs
         )
 
-@register_factory('create_user')
+
+@register_factory("create_user")
 class CreateUserFactory(factory.Factory, TDDFactoryMixin):
     """
     å»ºç«‹ç”¨æˆ¶è«‹æ±‚æ•¸æ“šå·¥å» 
     æ¨¡æ“¬ API è«‹æ±‚ä¸­çš„ç”¨æˆ¶å»ºç«‹æ•¸æ“š
     """
-    
+
     class Meta:
         model = dict
-    
+
     username = factory.LazyAttribute(lambda o: fake.user_name())
     email = factory.LazyAttribute(lambda o: fake.unique.email())
     password = "SecurePassword123!"
     full_name = factory.LazyAttribute(lambda o: fake.name())
     language = "zh-TW"
     timezone = "Asia/Taipei"
-    
+
     @classmethod
     def create_invalid_request(cls, **kwargs):
         """å»ºç«‹ç„¡æ•ˆçš„ç”¨æˆ¶å»ºç«‹è«‹æ±‚"""
@@ -140,18 +138,19 @@ class CreateUserFactory(factory.Factory, TDDFactoryMixin):
             username="",  # ç„¡æ•ˆï¼šç©ºç”¨æˆ¶å
             email="invalid-email",  # ç„¡æ•ˆï¼šéŒ¯èª¤æ ¼å¼
             password="123",  # ç„¡æ•ˆï¼šå¯†ç¢¼å¤ªçŸ­
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_duplicate_request(cls, existing_user: UserData, **kwargs):
         """å»ºç«‹é‡è¤‡çš„ç”¨æˆ¶å»ºç«‹è«‹æ±‚"""
         return cls.build(
             username=existing_user.username,  # é‡è¤‡ç”¨æˆ¶å
             email=existing_user.email,  # é‡è¤‡ email
-            **kwargs
+            **kwargs,
         )
 
+
 # TDD è¼”åŠ©å‡½æ•¸
 def create_test_users_scenario() -> Dict[str, UserData]:
     """
@@ -159,13 +158,14 @@ def create_test_users_scenario() -> Dict[str, UserData]:
     ç”¨æ–¼è¤‡é›œçš„ TDD æ¸¬è©¦å ´æ™¯
     """
     return {
-        'active_user': UserFactory.create_for_green_phase(),
-        'inactive_user': UserFactory.create_inactive_user(),
-        'premium_user': UserFactory.create_premium_user(),
-        'admin_user': UserFactory.create_admin_user(),
-        'invalid_user': UserFactory.create_for_red_phase()
+        "active_user": UserFactory.create_for_green_phase(),
+        "inactive_user": UserFactory.create_inactive_user(),
+        "premium_user": UserFactory.create_premium_user(),
+        "admin_user": UserFactory.create_admin_user(),
+        "invalid_user": UserFactory.create_for_red_phase(),
     }
 
+
 def cleanup_test_users(users: Dict[str, UserData]):
     """
     æ¸…ç†æ¸¬è©¦ç”¨æˆ¶æ•¸æ“š
@@ -173,4 +173,4 @@ def cleanup_test_users(users: Dict[str, UserData]):
     """
     # é€™è£¡å¯ä»¥åŠ å…¥æ¸…ç†é‚è¼¯
     # ä¾‹å¦‚ï¼šå¾è³‡æ–™åº«åˆªé™¤æ¸¬è©¦æ•¸æ“š
-    pass
\ No newline at end of file
+    pass
diff --git a/auto_generate_video_fold6/tests/factories/video_factory.py b/auto_generate_video_fold6/tests/factories/video_factory.py
index 3194a94..9e7f573 100644
--- a/auto_generate_video_fold6/tests/factories/video_factory.py
+++ b/auto_generate_video_fold6/tests/factories/video_factory.py
@@ -8,13 +8,14 @@ from typing import Dict, Any, List
 from enum import Enum
 
 from .base_factory import (
-    BaseFactory, 
-    TDDFactoryMixin, 
+    BaseFactory,
+    TDDFactoryMixin,
     CommonFieldsMixin,
-    register_factory
+    register_factory,
 )
 
-fake = Faker(['zh_TW', 'en_US'])
+fake = Faker(["zh_TW", "en_US"])
+
 
 class VideoStatus(Enum):
     PROCESSING = "processing"
@@ -22,45 +23,50 @@ class VideoStatus(Enum):
     FAILED = "failed"
     PUBLISHED = "published"
 
+
 class VideoQuality(Enum):
     LOW = "360p"
-    MEDIUM = "720p" 
+    MEDIUM = "720p"
     HIGH = "1080p"
     ULTRA = "4K"
 
+
 class VideoData:
     """å½±ç‰‡æ•¸æ“šé¡åˆ¥"""
+
     def __init__(self, **kwargs):
         for key, value in kwargs.items():
             setattr(self, key, value)
 
-@register_factory('video')
+
+@register_factory("video")
 class VideoFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
     """
     å½±ç‰‡å·¥å» 
     ç‚º TDD æ¸¬è©¦æä¾›å„ç¨®å½±ç‰‡æ•¸æ“šæƒ…å¢ƒ
     """
-    
+
     class Meta:
         model = VideoData
-    
+
     # åŸºæœ¬æ¬„ä½
     title = factory.LazyAttribute(lambda o: fake.sentence(nb_words=5))
     description = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=400))
-    
+
     # é—œè¯æ¬„ä½
     project_id = factory.LazyFunction(lambda: fake.uuid4())
     user_id = factory.LazyFunction(lambda: fake.uuid4())
-    
+
     # å½±ç‰‡å±¬æ€§
     duration = factory.LazyFunction(lambda: fake.random_int(15, 300))  # ç§’
-    resolution = factory.Iterator([
-        VideoQuality.MEDIUM.value,
-        VideoQuality.HIGH.value
-    ])
+    resolution = factory.Iterator(
+        [VideoQuality.MEDIUM.value, VideoQuality.HIGH.value]
+    )
     fps = factory.Iterator([24, 30, 60])
-    file_size = factory.LazyFunction(lambda: fake.random_int(1024*1024, 100*1024*1024))  # bytes
-    
+    file_size = factory.LazyFunction(
+        lambda: fake.random_int(1024 * 1024, 100 * 1024 * 1024)
+    )  # bytes
+
     # æª”æ¡ˆè·¯å¾‘
     file_path = factory.LazyAttribute(
         lambda o: f"/videos/{o.project_id}/{o.id}.mp4"
@@ -68,50 +74,55 @@ class VideoFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
     thumbnail_path = factory.LazyAttribute(
         lambda o: f"/thumbnails/{o.project_id}/{o.id}_thumb.jpg"
     )
-    
+
     # ç‹€æ…‹
-    status = factory.LazyAttribute(lambda o: fake.random_element([
-        VideoStatus.PROCESSING.value,
-        VideoStatus.COMPLETED.value
-    ]))
-    
+    status = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            [VideoStatus.PROCESSING.value, VideoStatus.COMPLETED.value]
+        )
+    )
+
     # è™•ç†è³‡è¨Š
-    processing_started_at = factory.LazyFunction(lambda: datetime.now(timezone.utc))
+    processing_started_at = factory.LazyFunction(
+        lambda: datetime.now(timezone.utc)
+    )
     processing_completed_at = None
     processing_progress = factory.LazyFunction(lambda: fake.random_int(0, 100))
-    
+
     # éŒ¯èª¤è³‡è¨Š
     error_message = None
     retry_count = 0
-    
+
     # åª’é«”è³‡è¨Š
-    video_codec = factory.Iterator(['h264', 'h265', 'vp9'])
-    audio_codec = factory.Iterator(['aac', 'mp3', 'opus'])
+    video_codec = factory.Iterator(["h264", "h265", "vp9"])
+    audio_codec = factory.Iterator(["aac", "mp3", "opus"])
     bitrate = factory.LazyFunction(lambda: fake.random_int(1000, 8000))  # kbps
-    
+
     # AI ç”Ÿæˆè³‡è¨Š
     script_used = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=500))
-    voice_settings = factory.LazyAttribute(lambda o: {
-        'voice_id': fake.uuid4(),
-        'speed': fake.random.uniform(0.8, 1.2),
-        'pitch': fake.random.uniform(-0.2, 0.2),
-        'volume': fake.random.uniform(0.7, 1.0)
-    })
-    
+    voice_settings = factory.LazyAttribute(
+        lambda o: {
+            "voice_id": fake.uuid4(),
+            "speed": fake.random.uniform(0.8, 1.2),
+            "pitch": fake.random.uniform(-0.2, 0.2),
+            "volume": fake.random.uniform(0.7, 1.0),
+        }
+    )
+
     # è¦–è¦ºè¨­å®š
-    background_images = factory.LazyAttribute(lambda o: [
-        f"/images/bg_{i}.jpg" for i in range(fake.random_int(3, 8))
-    ])
-    transition_effects = factory.LazyAttribute(lambda o: fake.random_element([
-        'fade', 'slide', 'zoom', 'dissolve'
-    ]))
-    
+    background_images = factory.LazyAttribute(
+        lambda o: [f"/images/bg_{i}.jpg" for i in range(fake.random_int(3, 8))]
+    )
+    transition_effects = factory.LazyAttribute(
+        lambda o: fake.random_element(["fade", "slide", "zoom", "dissolve"])
+    )
+
     # çµ±è¨ˆæ•¸æ“š
     view_count = factory.LazyFunction(lambda: fake.random_int(0, 10000))
     like_count = factory.LazyFunction(lambda: fake.random_int(0, 1000))
     share_count = factory.LazyFunction(lambda: fake.random_int(0, 100))
     comment_count = factory.LazyFunction(lambda: fake.random_int(0, 200))
-    
+
     # ç™¼å¸ƒè³‡è¨Š
     published_at = None
     published_platforms = factory.LazyAttribute(lambda o: [])
@@ -124,9 +135,9 @@ class VideoFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             project_id=project_id or fake.uuid4(),
             processing_progress=fake.random_int(10, 90),
             processing_completed_at=None,
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_completed_video(cls, project_id: str = None, **kwargs):
         """å»ºç«‹å·²å®Œæˆçš„å½±ç‰‡"""
@@ -136,9 +147,9 @@ class VideoFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             processing_progress=100,
             processing_completed_at=datetime.now(timezone.utc),
             file_path=f"/videos/{project_id or fake.uuid4()}/completed.mp4",
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_failed_video(cls, project_id: str = None, **kwargs):
         """å»ºç«‹è™•ç†å¤±æ•—çš„å½±ç‰‡"""
@@ -148,9 +159,9 @@ class VideoFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             processing_progress=fake.random_int(0, 50),
             error_message="å½±ç‰‡è™•ç†å¤±æ•—ï¼šç·¨ç¢¼éŒ¯èª¤",
             retry_count=fake.random_int(1, 3),
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_published_video(cls, project_id: str = None, **kwargs):
         """å»ºç«‹å·²ç™¼å¸ƒçš„å½±ç‰‡"""
@@ -160,11 +171,11 @@ class VideoFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             processing_progress=100,
             processing_completed_at=datetime.now(timezone.utc),
             published_at=datetime.now(timezone.utc),
-            published_platforms=['tiktok', 'instagram_reels'],
+            published_platforms=["tiktok", "instagram_reels"],
             view_count=fake.random_int(100, 5000),
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_high_quality_video(cls, **kwargs):
         """å»ºç«‹é«˜å“è³ªå½±ç‰‡"""
@@ -172,11 +183,11 @@ class VideoFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             resolution=VideoQuality.HIGH.value,
             fps=60,
             bitrate=fake.random_int(4000, 8000),
-            video_codec='h265',
-            audio_codec='aac',
-            **kwargs
+            video_codec="h265",
+            audio_codec="aac",
+            **kwargs,
         )
-    
+
     @classmethod
     def create_for_red_phase(cls, **kwargs):
         """RED éšæ®µï¼šå»ºç«‹æœƒå°è‡´æ¸¬è©¦å¤±æ•—çš„å½±ç‰‡æ•¸æ“š"""
@@ -185,9 +196,9 @@ class VideoFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             duration=-1,  # ç„¡æ•ˆï¼šè² æ•¸æŒçºŒæ™‚é–“
             file_size=0,  # ç„¡æ•ˆï¼šæª”æ¡ˆå¤§å°ç‚º0
             project_id="",  # ç„¡æ•ˆï¼šç©ºå°ˆæ¡ˆID
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_for_green_phase(cls, **kwargs):
         """GREEN éšæ®µï¼šå»ºç«‹è®“æ¸¬è©¦é€šéçš„æœ€ç°¡å½±ç‰‡æ•¸æ“š"""
@@ -196,36 +207,37 @@ class VideoFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             duration=30,
             status=VideoStatus.COMPLETED.value,
             processing_progress=100,
-            **kwargs
+            **kwargs,
         )
 
-@register_factory('create_video')
+
+@register_factory("create_video")
 class CreateVideoFactory(factory.Factory, TDDFactoryMixin):
     """
     å»ºç«‹å½±ç‰‡è«‹æ±‚æ•¸æ“šå·¥å» 
     æ¨¡æ“¬ API è«‹æ±‚ä¸­çš„å½±ç‰‡å»ºç«‹æ•¸æ“š
     """
-    
+
     class Meta:
         model = dict
-    
+
     title = factory.LazyAttribute(lambda o: fake.sentence(nb_words=4))
     description = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=200))
     project_id = factory.LazyFunction(lambda: fake.uuid4())
-    
+
     # å½±ç‰‡è¨­å®š
     resolution = VideoQuality.HIGH.value
     fps = 30
     quality_preset = "medium"
-    
+
     # å…§å®¹è¨­å®š
-    script_content = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=300))
-    voice_settings = factory.LazyAttribute(lambda o: {
-        'voice_id': fake.uuid4(),
-        'speed': 1.0,
-        'pitch': 0.0
-    })
-    
+    script_content = factory.LazyAttribute(
+        lambda o: fake.text(max_nb_chars=300)
+    )
+    voice_settings = factory.LazyAttribute(
+        lambda o: {"voice_id": fake.uuid4(), "speed": 1.0, "pitch": 0.0}
+    )
+
     @classmethod
     def create_invalid_request(cls, **kwargs):
         """å»ºç«‹ç„¡æ•ˆçš„å½±ç‰‡å»ºç«‹è«‹æ±‚"""
@@ -233,9 +245,9 @@ class CreateVideoFactory(factory.Factory, TDDFactoryMixin):
             title="",  # ç„¡æ•ˆï¼šç©ºæ¨™é¡Œ
             project_id="invalid-id",  # ç„¡æ•ˆï¼šéŒ¯èª¤æ ¼å¼çš„ID
             resolution="invalid-resolution",  # ç„¡æ•ˆï¼šä¸æ”¯æ´çš„è§£æåº¦
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_minimal_valid_request(cls, **kwargs):
         """å»ºç«‹æœ€ç°¡æœ‰æ•ˆçš„å½±ç‰‡å»ºç«‹è«‹æ±‚"""
@@ -243,56 +255,70 @@ class CreateVideoFactory(factory.Factory, TDDFactoryMixin):
             title="ç°¡å–®æ¸¬è©¦å½±ç‰‡",
             project_id=fake.uuid4(),
             script_content="é€™æ˜¯ä¸€å€‹æ¸¬è©¦è…³æœ¬",
-            **kwargs
+            **kwargs,
         )
 
+
 class VideoBatchFactory:
     """å½±ç‰‡æ‰¹æ¬¡å»ºç«‹å·¥å» """
-    
+
     @staticmethod
-    def create_project_videos(project_id: str, count: int = 3) -> List[VideoData]:
+    def create_project_videos(
+        project_id: str, count: int = 3
+    ) -> List[VideoData]:
         """ç‚ºç‰¹å®šå°ˆæ¡ˆå»ºç«‹å¤šå€‹å½±ç‰‡"""
         videos = []
         for i in range(count):
             video = VideoFactory.create_completed_video(
-                project_id=project_id,
-                title=f"å°ˆæ¡ˆå½±ç‰‡ {i+1}"
+                project_id=project_id, title=f"å°ˆæ¡ˆå½±ç‰‡ {i + 1}"
             )
             videos.append(video)
         return videos
-    
+
     @staticmethod
     def create_mixed_status_videos(project_id: str) -> Dict[str, VideoData]:
         """å»ºç«‹ä¸åŒç‹€æ…‹çš„å½±ç‰‡"""
         return {
-            'processing': VideoFactory.create_processing_video(project_id),
-            'completed': VideoFactory.create_completed_video(project_id),
-            'failed': VideoFactory.create_failed_video(project_id),
-            'published': VideoFactory.create_published_video(project_id)
+            "processing": VideoFactory.create_processing_video(project_id),
+            "completed": VideoFactory.create_completed_video(project_id),
+            "failed": VideoFactory.create_failed_video(project_id),
+            "published": VideoFactory.create_published_video(project_id),
         }
 
+
 # TDD è¼”åŠ©å‡½æ•¸
-def create_test_videos_scenario(project_id: str = None) -> Dict[str, VideoData]:
+def create_test_videos_scenario(
+    project_id: str = None,
+) -> Dict[str, VideoData]:
     """
     å»ºç«‹å®Œæ•´çš„æ¸¬è©¦å½±ç‰‡æƒ…å¢ƒ
     ç”¨æ–¼è¤‡é›œçš„ TDD æ¸¬è©¦å ´æ™¯
     """
     test_project_id = project_id or fake.uuid4()
-    
+
     return {
-        'processing_video': VideoFactory.create_processing_video(test_project_id),
-        'completed_video': VideoFactory.create_completed_video(test_project_id),
-        'failed_video': VideoFactory.create_failed_video(test_project_id),
-        'published_video': VideoFactory.create_published_video(test_project_id),
-        'high_quality_video': VideoFactory.create_high_quality_video(project_id=test_project_id),
-        'invalid_video': VideoFactory.create_for_red_phase(),
-        'valid_video': VideoFactory.create_for_green_phase()
+        "processing_video": VideoFactory.create_processing_video(
+            test_project_id
+        ),
+        "completed_video": VideoFactory.create_completed_video(
+            test_project_id
+        ),
+        "failed_video": VideoFactory.create_failed_video(test_project_id),
+        "published_video": VideoFactory.create_published_video(
+            test_project_id
+        ),
+        "high_quality_video": VideoFactory.create_high_quality_video(
+            project_id=test_project_id
+        ),
+        "invalid_video": VideoFactory.create_for_red_phase(),
+        "valid_video": VideoFactory.create_for_green_phase(),
     }
 
+
 def cleanup_test_videos(videos: Dict[str, VideoData]):
     """
     æ¸…ç†æ¸¬è©¦å½±ç‰‡æ•¸æ“š
     åœ¨æ¸¬è©¦å®Œæˆå¾Œå‘¼å«
     """
     # æ¸…ç†å½±ç‰‡æª”æ¡ˆå’Œæ•¸æ“šåº«è¨˜éŒ„
-    pass
\ No newline at end of file
+    pass
diff --git a/auto_generate_video_fold6/tests/factories/voice_factory.py b/auto_generate_video_fold6/tests/factories/voice_factory.py
index e49b6b3..36345de 100644
--- a/auto_generate_video_fold6/tests/factories/voice_factory.py
+++ b/auto_generate_video_fold6/tests/factories/voice_factory.py
@@ -8,19 +8,21 @@ from typing import Dict, Any, List
 from enum import Enum
 
 from .base_factory import (
-    BaseFactory, 
-    TDDFactoryMixin, 
+    BaseFactory,
+    TDDFactoryMixin,
     CommonFieldsMixin,
-    register_factory
+    register_factory,
 )
 
-fake = Faker(['zh_TW', 'en_US'])
+fake = Faker(["zh_TW", "en_US"])
+
 
 class VoiceGender(Enum):
     MALE = "male"
     FEMALE = "female"
     NEUTRAL = "neutral"
 
+
 class VoiceLanguage(Enum):
     ZH_TW = "zh-TW"
     ZH_CN = "zh-CN"
@@ -29,121 +31,165 @@ class VoiceLanguage(Enum):
     JA_JP = "ja-JP"
     KO_KR = "ko-KR"
 
+
 class VoiceStatus(Enum):
     TRAINING = "training"
     READY = "ready"
     FAILED = "failed"
     ARCHIVED = "archived"
 
+
 class VoiceCloneData:
     """èªéŸ³å…‹éš†æ•¸æ“šé¡åˆ¥"""
+
     def __init__(self, **kwargs):
         for key, value in kwargs.items():
             setattr(self, key, value)
 
-@register_factory('voice_clone')
+
+@register_factory("voice_clone")
 class VoiceCloneFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
     """
     èªéŸ³å…‹éš†å·¥å» 
     ç‚º TDD æ¸¬è©¦æä¾›å„ç¨®èªéŸ³å…‹éš†æ•¸æ“šæƒ…å¢ƒ
     """
-    
+
     class Meta:
         model = VoiceCloneData
-    
+
     # åŸºæœ¬æ¬„ä½
     name = factory.LazyAttribute(lambda o: fake.name())
     description = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=200))
-    
+
     # é—œè¯æ¬„ä½
     user_id = factory.LazyFunction(lambda: fake.uuid4())
-    
+
     # èªéŸ³å±¬æ€§
-    gender = factory.LazyAttribute(lambda o: fake.random_element([
-        VoiceGender.MALE.value,
-        VoiceGender.FEMALE.value
-    ]))
-    
-    language = factory.LazyAttribute(lambda o: fake.random_element([
-        VoiceLanguage.ZH_TW.value,
-        VoiceLanguage.EN_US.value
-    ]))
-    
-    accent = factory.LazyAttribute(lambda o: fake.random_element([
-        'standard', 'northern', 'southern', 'american', 'british'
-    ]))
-    
+    gender = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            [VoiceGender.MALE.value, VoiceGender.FEMALE.value]
+        )
+    )
+
+    language = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            [VoiceLanguage.ZH_TW.value, VoiceLanguage.EN_US.value]
+        )
+    )
+
+    accent = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["standard", "northern", "southern", "american", "british"]
+        )
+    )
+
     # éŸ³è‰²ç‰¹å¾µ
-    pitch_range = factory.LazyAttribute(lambda o: {
-        'min': fake.random.uniform(80, 150),
-        'max': fake.random.uniform(200, 300),
-        'average': fake.random.uniform(120, 250)
-    })
-    
-    tone_characteristics = factory.LazyAttribute(lambda o: fake.random_element([
-        'warm', 'professional', 'friendly', 'authoritative', 'calm', 'energetic'
-    ]))
-    
-    speaking_rate = factory.LazyFunction(lambda: fake.random.uniform(0.8, 1.3))  # ç›¸å°æ–¼æ¨™æº–èªé€Ÿ
-    
+    pitch_range = factory.LazyAttribute(
+        lambda o: {
+            "min": fake.random.uniform(80, 150),
+            "max": fake.random.uniform(200, 300),
+            "average": fake.random.uniform(120, 250),
+        }
+    )
+
+    tone_characteristics = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            [
+                "warm",
+                "professional",
+                "friendly",
+                "authoritative",
+                "calm",
+                "energetic",
+            ]
+        )
+    )
+
+    speaking_rate = factory.LazyFunction(
+        lambda: fake.random.uniform(0.8, 1.3)
+    )  # ç›¸å°æ–¼æ¨™æº–èªé€Ÿ
+
     # è¨“ç·´è³‡æ–™
-    training_audio_files = factory.LazyAttribute(lambda o: [
-        f"/audio/training/{o.id}/sample_{i}.wav" 
-        for i in range(fake.random_int(5, 20))
-    ])
-    
-    total_training_duration = factory.LazyFunction(lambda: fake.random_int(300, 3600))  # ç§’
-    training_quality_score = factory.LazyFunction(lambda: fake.random.uniform(7.0, 9.5))
-    
+    training_audio_files = factory.LazyAttribute(
+        lambda o: [
+            f"/audio/training/{o.id}/sample_{i}.wav"
+            for i in range(fake.random_int(5, 20))
+        ]
+    )
+
+    total_training_duration = factory.LazyFunction(
+        lambda: fake.random_int(300, 3600)
+    )  # ç§’
+    training_quality_score = factory.LazyFunction(
+        lambda: fake.random.uniform(7.0, 9.5)
+    )
+
     # æ¨¡å‹è³‡è¨Š
     model_file_path = factory.LazyAttribute(
         lambda o: f"/models/voices/{o.id}/voice_model.pt"
     )
-    model_size = factory.LazyFunction(lambda: fake.random_int(50*1024*1024, 500*1024*1024))  # bytes
+    model_size = factory.LazyFunction(
+        lambda: fake.random_int(50 * 1024 * 1024, 500 * 1024 * 1024)
+    )  # bytes
     model_version = factory.LazyFunction(lambda: fake.random.uniform(1.0, 3.0))
-    
+
     # ç‹€æ…‹å’Œé€²åº¦
-    status = factory.LazyAttribute(lambda o: fake.random_element([
-        VoiceStatus.READY.value,
-        VoiceStatus.TRAINING.value
-    ]))
-    
+    status = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            [VoiceStatus.READY.value, VoiceStatus.TRAINING.value]
+        )
+    )
+
     training_progress = factory.LazyFunction(lambda: fake.random_int(0, 100))
-    training_started_at = factory.LazyFunction(lambda: datetime.now(timezone.utc))
+    training_started_at = factory.LazyFunction(
+        lambda: datetime.now(timezone.utc)
+    )
     training_completed_at = None
-    
+
     # å“è³ªæŒ‡æ¨™
-    similarity_score = factory.LazyFunction(lambda: fake.random.uniform(0.8, 0.95))  # èˆ‡åŸè²ç›¸ä¼¼åº¦
-    naturalness_score = factory.LazyFunction(lambda: fake.random.uniform(0.7, 0.9))  # è‡ªç„¶åº¦
-    clarity_score = factory.LazyFunction(lambda: fake.random.uniform(0.8, 0.95))  # æ¸…æ™°åº¦
-    
+    similarity_score = factory.LazyFunction(
+        lambda: fake.random.uniform(0.8, 0.95)
+    )  # èˆ‡åŸè²ç›¸ä¼¼åº¦
+    naturalness_score = factory.LazyFunction(
+        lambda: fake.random.uniform(0.7, 0.9)
+    )  # è‡ªç„¶åº¦
+    clarity_score = factory.LazyFunction(
+        lambda: fake.random.uniform(0.8, 0.95)
+    )  # æ¸…æ™°åº¦
+
     # ä½¿ç”¨çµ±è¨ˆ
     usage_count = factory.LazyFunction(lambda: fake.random_int(0, 100))
     last_used_at = None
-    
+
     # éŒ¯èª¤è³‡è¨Š
     error_message = None
-    
+
     # åŸå§‹è²éŸ³ä¾†æºè³‡è¨Š
     source_speaker_name = factory.LazyAttribute(lambda o: fake.name())
-    source_description = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=100))
+    source_description = factory.LazyAttribute(
+        lambda o: fake.text(max_nb_chars=100)
+    )
     consent_verified = True
-    
+
     # æ¨™ç±¤å’Œåˆ†é¡
-    tags = factory.LazyAttribute(lambda o: [
-        fake.word() for _ in range(fake.random_int(2, 5))
-    ])
-    
-    category = factory.LazyAttribute(lambda o: fake.random_element([
-        'personal', 'commercial', 'character', 'narrator', 'celebrity'
-    ]))
-    
+    tags = factory.LazyAttribute(
+        lambda o: [fake.word() for _ in range(fake.random_int(2, 5))]
+    )
+
+    category = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["personal", "commercial", "character", "narrator", "celebrity"]
+        )
+    )
+
     # éš±ç§å’Œæˆæ¬Š
     is_public = factory.LazyFunction(lambda: fake.boolean())
     is_commercial_use_allowed = factory.LazyFunction(lambda: fake.boolean())
-    license_type = factory.LazyAttribute(lambda o: fake.random_element([
-        'personal', 'commercial', 'educational', 'research'
-    ]))
+    license_type = factory.LazyAttribute(
+        lambda o: fake.random_element(
+            ["personal", "commercial", "educational", "research"]
+        )
+    )
 
     @classmethod
     def create_male_voice(cls, **kwargs):
@@ -151,26 +197,26 @@ class VoiceCloneFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
         return cls.create(
             gender=VoiceGender.MALE.value,
             pitch_range={
-                'min': fake.random.uniform(80, 120),
-                'max': fake.random.uniform(150, 200),
-                'average': fake.random.uniform(100, 160)
+                "min": fake.random.uniform(80, 120),
+                "max": fake.random.uniform(150, 200),
+                "average": fake.random.uniform(100, 160),
             },
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_female_voice(cls, **kwargs):
         """å»ºç«‹å¥³è²èªéŸ³å…‹éš†"""
         return cls.create(
             gender=VoiceGender.FEMALE.value,
             pitch_range={
-                'min': fake.random.uniform(150, 200),
-                'max': fake.random.uniform(250, 350),
-                'average': fake.random.uniform(180, 280)
+                "min": fake.random.uniform(150, 200),
+                "max": fake.random.uniform(250, 350),
+                "average": fake.random.uniform(180, 280),
             },
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_training_voice(cls, **kwargs):
         """å»ºç«‹æ­£åœ¨è¨“ç·´çš„èªéŸ³å…‹éš†"""
@@ -179,9 +225,9 @@ class VoiceCloneFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             training_progress=fake.random_int(10, 90),
             training_completed_at=None,
             model_file_path=None,
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_ready_voice(cls, **kwargs):
         """å»ºç«‹å·²å®Œæˆè¨“ç·´çš„èªéŸ³å…‹éš†"""
@@ -191,9 +237,9 @@ class VoiceCloneFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             training_completed_at=datetime.now(timezone.utc),
             similarity_score=fake.random.uniform(0.85, 0.95),
             naturalness_score=fake.random.uniform(0.8, 0.9),
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_failed_voice(cls, **kwargs):
         """å»ºç«‹è¨“ç·´å¤±æ•—çš„èªéŸ³å…‹éš†"""
@@ -202,9 +248,9 @@ class VoiceCloneFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             training_progress=fake.random_int(5, 70),
             error_message="è¨“ç·´æ•¸æ“šå“è³ªä¸è¶³ï¼Œç„¡æ³•å®Œæˆæ¨¡å‹è¨“ç·´",
             model_file_path=None,
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_high_quality_voice(cls, **kwargs):
         """å»ºç«‹é«˜å“è³ªèªéŸ³å…‹éš†"""
@@ -215,9 +261,9 @@ class VoiceCloneFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             naturalness_score=fake.random.uniform(0.85, 0.9),
             clarity_score=fake.random.uniform(0.9, 0.95),
             total_training_duration=fake.random_int(1800, 3600),  # 30-60åˆ†é˜
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_for_red_phase(cls, **kwargs):
         """RED éšæ®µï¼šå»ºç«‹æœƒå°è‡´æ¸¬è©¦å¤±æ•—çš„èªéŸ³å…‹éš†æ•¸æ“š"""
@@ -226,9 +272,9 @@ class VoiceCloneFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             total_training_duration=-1,  # ç„¡æ•ˆï¼šè² æ•¸æ™‚é•·
             similarity_score=2.0,  # ç„¡æ•ˆï¼šè¶…å‡ºç¯„åœçš„åˆ†æ•¸
             training_audio_files=[],  # ç„¡æ•ˆï¼šæ²’æœ‰è¨“ç·´éŸ³æª”
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_for_green_phase(cls, **kwargs):
         """GREEN éšæ®µï¼šå»ºç«‹è®“æ¸¬è©¦é€šéçš„æœ€ç°¡èªéŸ³å…‹éš†æ•¸æ“š"""
@@ -236,37 +282,38 @@ class VoiceCloneFactory(factory.Factory, TDDFactoryMixin, CommonFieldsMixin):
             name="æ¸¬è©¦èªéŸ³",
             status=VoiceStatus.READY.value,
             training_progress=100,
-            **kwargs
+            **kwargs,
         )
 
-@register_factory('create_voice_clone')
+
+@register_factory("create_voice_clone")
 class CreateVoiceCloneFactory(factory.Factory, TDDFactoryMixin):
     """
     å»ºç«‹èªéŸ³å…‹éš†è«‹æ±‚æ•¸æ“šå·¥å» 
     æ¨¡æ“¬ API è«‹æ±‚ä¸­çš„èªéŸ³å…‹éš†å»ºç«‹æ•¸æ“š
     """
-    
+
     class Meta:
         model = dict
-    
+
     name = factory.LazyAttribute(lambda o: fake.name())
     description = factory.LazyAttribute(lambda o: fake.text(max_nb_chars=150))
-    
+
     # è²éŸ³å±¬æ€§
     gender = VoiceGender.FEMALE.value
     language = VoiceLanguage.ZH_TW.value
     tone_characteristics = "friendly"
-    
+
     # è¨“ç·´è¨­å®š
-    training_audio_files = factory.LazyAttribute(lambda o: [
-        f"audio_sample_{i}.wav" for i in range(10)
-    ])
-    
+    training_audio_files = factory.LazyAttribute(
+        lambda o: [f"audio_sample_{i}.wav" for i in range(10)]
+    )
+
     # æˆæ¬Šè¨­å®š
     consent_verified = True
     is_commercial_use_allowed = False
     license_type = "personal"
-    
+
     @classmethod
     def create_invalid_request(cls, **kwargs):
         """å»ºç«‹ç„¡æ•ˆçš„èªéŸ³å…‹éš†å»ºç«‹è«‹æ±‚"""
@@ -274,9 +321,9 @@ class CreateVoiceCloneFactory(factory.Factory, TDDFactoryMixin):
             name="",  # ç„¡æ•ˆï¼šç©ºåç¨±
             training_audio_files=[],  # ç„¡æ•ˆï¼šæ²’æœ‰è¨“ç·´æª”æ¡ˆ
             consent_verified=False,  # ç„¡æ•ˆï¼šæœªé©—è­‰æˆæ¬Š
-            **kwargs
+            **kwargs,
         )
-    
+
     @classmethod
     def create_minimal_valid_request(cls, **kwargs):
         """å»ºç«‹æœ€ç°¡æœ‰æ•ˆçš„èªéŸ³å…‹éš†å»ºç«‹è«‹æ±‚"""
@@ -284,52 +331,72 @@ class CreateVoiceCloneFactory(factory.Factory, TDDFactoryMixin):
             name="ç°¡å–®æ¸¬è©¦èªéŸ³",
             training_audio_files=["test_sample.wav"],
             consent_verified=True,
-            **kwargs
+            **kwargs,
         )
 
+
 class VoiceBatchFactory:
     """èªéŸ³å…‹éš†æ‰¹æ¬¡å»ºç«‹å·¥å» """
-    
+
     @staticmethod
-    def create_user_voices(user_id: str, count: int = 3) -> List[VoiceCloneData]:
+    def create_user_voices(
+        user_id: str, count: int = 3
+    ) -> List[VoiceCloneData]:
         """ç‚ºç‰¹å®šç”¨æˆ¶å»ºç«‹å¤šå€‹èªéŸ³å…‹éš†"""
         voices = []
         for i in range(count):
             voice = VoiceCloneFactory.create_ready_voice(
-                user_id=user_id,
-                name=f"ç”¨æˆ¶èªéŸ³ {i+1}"
+                user_id=user_id, name=f"ç”¨æˆ¶èªéŸ³ {i + 1}"
             )
             voices.append(voice)
         return voices
-    
+
     @staticmethod
     def create_mixed_gender_voices() -> Dict[str, VoiceCloneData]:
         """å»ºç«‹ä¸åŒæ€§åˆ¥çš„èªéŸ³å…‹éš†"""
         return {
-            'male': VoiceCloneFactory.create_male_voice(),
-            'female': VoiceCloneFactory.create_female_voice()
+            "male": VoiceCloneFactory.create_male_voice(),
+            "female": VoiceCloneFactory.create_female_voice(),
         }
 
+
 # TDD è¼”åŠ©å‡½æ•¸
-def create_test_voices_scenario(user_id: str = None) -> Dict[str, VoiceCloneData]:
+def create_test_voices_scenario(
+    user_id: str = None,
+) -> Dict[str, VoiceCloneData]:
     """
     å»ºç«‹å®Œæ•´çš„æ¸¬è©¦èªéŸ³å…‹éš†æƒ…å¢ƒ
     ç”¨æ–¼è¤‡é›œçš„ TDD æ¸¬è©¦å ´æ™¯
     """
     test_user_id = user_id or fake.uuid4()
-    
+
     return {
-        'training_voice': VoiceCloneFactory.create_training_voice(user_id=test_user_id),
-        'ready_voice': VoiceCloneFactory.create_ready_voice(user_id=test_user_id),
-        'failed_voice': VoiceCloneFactory.create_failed_voice(user_id=test_user_id),
-        'male_voice': VoiceCloneFactory.create_male_voice(user_id=test_user_id),
-        'female_voice': VoiceCloneFactory.create_female_voice(user_id=test_user_id),
-        'high_quality_voice': VoiceCloneFactory.create_high_quality_voice(user_id=test_user_id),
-        'invalid_voice': VoiceCloneFactory.create_for_red_phase(),
-        'valid_voice': VoiceCloneFactory.create_for_green_phase()
+        "training_voice": VoiceCloneFactory.create_training_voice(
+            user_id=test_user_id
+        ),
+        "ready_voice": VoiceCloneFactory.create_ready_voice(
+            user_id=test_user_id
+        ),
+        "failed_voice": VoiceCloneFactory.create_failed_voice(
+            user_id=test_user_id
+        ),
+        "male_voice": VoiceCloneFactory.create_male_voice(
+            user_id=test_user_id
+        ),
+        "female_voice": VoiceCloneFactory.create_female_voice(
+            user_id=test_user_id
+        ),
+        "high_quality_voice": VoiceCloneFactory.create_high_quality_voice(
+            user_id=test_user_id
+        ),
+        "invalid_voice": VoiceCloneFactory.create_for_red_phase(),
+        "valid_voice": VoiceCloneFactory.create_for_green_phase(),
     }
 
-def create_voice_training_sequence(base_voice: VoiceCloneData) -> List[VoiceCloneData]:
+
+def create_voice_training_sequence(
+    base_voice: VoiceCloneData,
+) -> List[VoiceCloneData]:
     """
     å»ºç«‹èªéŸ³è¨“ç·´é€²åº¦åºåˆ—
     ç”¨æ–¼æ¸¬è©¦è¨“ç·´éç¨‹
@@ -340,16 +407,19 @@ def create_voice_training_sequence(base_voice: VoiceCloneData) -> List[VoiceClon
             id=base_voice.id,
             name=base_voice.name,
             user_id=base_voice.user_id,
-            status=VoiceStatus.TRAINING.value if progress < 100 else VoiceStatus.READY.value,
-            training_progress=progress
+            status=VoiceStatus.TRAINING.value
+            if progress < 100
+            else VoiceStatus.READY.value,
+            training_progress=progress,
         )
         sequences.append(voice_snapshot)
     return sequences
 
+
 def cleanup_test_voices(voices: Dict[str, VoiceCloneData]):
     """
     æ¸…ç†æ¸¬è©¦èªéŸ³å…‹éš†æ•¸æ“š
     åœ¨æ¸¬è©¦å®Œæˆå¾Œå‘¼å«
     """
     # æ¸…ç†èªéŸ³æ¨¡å‹æª”æ¡ˆå’Œæ•¸æ“šåº«è¨˜éŒ„
-    pass
\ No newline at end of file
+    pass
diff --git a/auto_generate_video_fold6/tests/fixtures/test_data.py b/auto_generate_video_fold6/tests/fixtures/test_data.py
index e082bbf..fb1b226 100644
--- a/auto_generate_video_fold6/tests/fixtures/test_data.py
+++ b/auto_generate_video_fold6/tests/fixtures/test_data.py
@@ -43,8 +43,14 @@ def multiple_users_data():
                 "full_name": f"User {i}",
                 "password": "Test123456!",
                 "is_verified": i % 2 == 0,  # å¶æ•¸ç”¨æˆ¶å·²é©—è­‰
-                "created_at": (datetime.utcnow() - timedelta(days=i)).isoformat(),
-                "credits": {"total": 1000 + i * 100, "used": i * 50, "remaining": 1000 + i * 50},
+                "created_at": (
+                    datetime.utcnow() - timedelta(days=i)
+                ).isoformat(),
+                "credits": {
+                    "total": 1000 + i * 100,
+                    "used": i * 50,
+                    "remaining": 1000 + i * 50,
+                },
             }
         )
     return users
@@ -185,7 +191,11 @@ These tools aren't just cool - they're game-changers! Which one will you try fir
         "style": "engaging",
         "status": "generated",
         "generation_time_seconds": 3.2,
-        "metadata": {"model_used": "gpt-4", "temperature": 0.8, "tokens_used": 456},
+        "metadata": {
+            "model_used": "gpt-4",
+            "temperature": 0.8,
+            "tokens_used": 456,
+        },
     }
 
 
@@ -252,7 +262,12 @@ def trending_keywords_data():
                 "platforms": ["tiktok", "youtube"],
                 "engagement_rate": 6.8,
                 "competition": "very_high",
-                "suggested_tags": ["crypto", "trading", "bitcoin", "investment"],
+                "suggested_tags": [
+                    "crypto",
+                    "trading",
+                    "bitcoin",
+                    "investment",
+                ],
             },
         ],
         "generated_at": datetime.utcnow().isoformat(),
@@ -414,7 +429,11 @@ class TestDataSeeder:
                 description=f"Description for test project {i}",
                 user_id=user_id,
                 status="draft" if i % 2 == 0 else "completed",
-                settings={"aspect_ratio": "9:16", "duration": 60 + i * 10, "style": "engaging"},
+                settings={
+                    "aspect_ratio": "9:16",
+                    "duration": 60 + i * 10,
+                    "style": "engaging",
+                },
             )
             projects.append(project)
             db_session.add(project)
@@ -484,7 +503,9 @@ class MockDataGenerator:
         return data
 
     @staticmethod
-    def generate_project(user_id: str = None, overrides: Dict[str, Any] = None) -> Dict[str, Any]:
+    def generate_project(
+        user_id: str = None, overrides: Dict[str, Any] = None
+    ) -> Dict[str, Any]:
         """ç”Ÿæˆå°ˆæ¡ˆæ•¸æ“š"""
         data = {
             "id": str(uuid.uuid4()),
@@ -502,7 +523,9 @@ class MockDataGenerator:
         return data
 
     @staticmethod
-    def generate_api_response(success: bool = True, data: Any = None) -> Dict[str, Any]:
+    def generate_api_response(
+        success: bool = True, data: Any = None
+    ) -> Dict[str, Any]:
         """ç”Ÿæˆ API å›æ‡‰æ•¸æ“š"""
         if success:
             return {
diff --git a/auto_generate_video_fold6/tests/test_config_manager.py b/auto_generate_video_fold6/tests/test_config_manager.py
index f6e231c..5295189 100644
--- a/auto_generate_video_fold6/tests/test_config_manager.py
+++ b/auto_generate_video_fold6/tests/test_config_manager.py
@@ -16,7 +16,9 @@ except ImportError:
     CONFIG_MANAGER_AVAILABLE = False
 
 
-@pytest.mark.skipif(not CONFIG_MANAGER_AVAILABLE, reason="é…ç½®ç®¡ç†å™¨æ¨¡çµ„ä¸å¯ç”¨")
+@pytest.mark.skipif(
+    not CONFIG_MANAGER_AVAILABLE, reason="é…ç½®ç®¡ç†å™¨æ¨¡çµ„ä¸å¯ç”¨"
+)
 class TestConfigManager:
     """é…ç½®ç®¡ç†å™¨æ¸¬è©¦é¡"""
 
@@ -35,7 +37,10 @@ class TestConfigManager:
                 "batch_size": 3,
                 "platforms": ["tiktok", "instagram"],
             },
-            "cost_control": {"daily_budget_usd": 50.0, "stop_on_budget_exceeded": True},
+            "cost_control": {
+                "daily_budget_usd": 50.0,
+                "stop_on_budget_exceeded": True,
+            },
         }
 
         with open(config_dir / "base-config.json", "w") as f:
@@ -218,24 +223,34 @@ class TestConfigManager:
         cm2 = ConfigManager(str(temp_config_dir))
 
         # é©—è­‰é…ç½®ä¸€è‡´æ€§
-        assert cm2.get("generation.daily_video_limit") == cm1.get("generation.daily_video_limit")
+        assert cm2.get("generation.daily_video_limit") == cm1.get(
+            "generation.daily_video_limit"
+        )
 
 
 @pytest.mark.unit
 class TestConfigManagerUtils:
     """é…ç½®ç®¡ç†å™¨å·¥å…·å‡½æ•¸æ¸¬è©¦"""
 
-    @pytest.mark.skipif(not CONFIG_MANAGER_AVAILABLE, reason="é…ç½®ç®¡ç†å™¨æ¨¡çµ„ä¸å¯ç”¨")
+    @pytest.mark.skipif(
+        not CONFIG_MANAGER_AVAILABLE, reason="é…ç½®ç®¡ç†å™¨æ¨¡çµ„ä¸å¯ç”¨"
+    )
     def test_get_config_function(self, mock_config_manager):
         """æ¸¬è©¦ get_config ä¾¿åˆ©å‡½æ•¸"""
-        with patch("config.config_manager.config_manager", mock_config_manager):
+        with patch(
+            "config.config_manager.config_manager", mock_config_manager
+        ):
             value = get_config("generation.daily_video_limit", 0)
             assert value == 5
 
-    @pytest.mark.skipif(not CONFIG_MANAGER_AVAILABLE, reason="é…ç½®ç®¡ç†å™¨æ¨¡çµ„ä¸å¯ç”¨")
+    @pytest.mark.skipif(
+        not CONFIG_MANAGER_AVAILABLE, reason="é…ç½®ç®¡ç†å™¨æ¨¡çµ„ä¸å¯ç”¨"
+    )
     def test_set_mode_function(self, mock_config_manager):
         """æ¸¬è©¦ set_mode ä¾¿åˆ©å‡½æ•¸"""
-        with patch("config.config_manager.config_manager", mock_config_manager):
+        with patch(
+            "config.config_manager.config_manager", mock_config_manager
+        ):
             set_mode("enterprise")
             # é€™è£¡æ‡‰è©²æª¢æŸ¥æ¨¡å¼æ˜¯å¦æ”¹è®Šï¼Œä½†éœ€è¦ mock å¯¦ç¾
             assert True  # ç°¡åŒ–çš„æ¸¬è©¦
@@ -246,7 +261,9 @@ class TestConfigManagerUtils:
 class TestConfigManagerIntegration:
     """é…ç½®ç®¡ç†å™¨æ•´åˆæ¸¬è©¦"""
 
-    @pytest.mark.skipif(not CONFIG_MANAGER_AVAILABLE, reason="é…ç½®ç®¡ç†å™¨æ¨¡çµ„ä¸å¯ç”¨")
+    @pytest.mark.skipif(
+        not CONFIG_MANAGER_AVAILABLE, reason="é…ç½®ç®¡ç†å™¨æ¨¡çµ„ä¸å¯ç”¨"
+    )
     def test_full_config_workflow(self, temp_config_dir):
         """æ¸¬è©¦å®Œæ•´é…ç½®å·¥ä½œæµç¨‹"""
         # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
diff --git a/auto_generate_video_fold6/tests/test_cost_tracker.py b/auto_generate_video_fold6/tests/test_cost_tracker.py
index 733a1a0..2896df6 100644
--- a/auto_generate_video_fold6/tests/test_cost_tracker.py
+++ b/auto_generate_video_fold6/tests/test_cost_tracker.py
@@ -16,7 +16,10 @@ try:
         APICallRecord,
         DailyCostSummary,
     )
-    from monitoring.budget_controller import BudgetController, get_budget_controller
+    from monitoring.budget_controller import (
+        BudgetController,
+        get_budget_controller,
+    )
 
     COST_MONITORING_AVAILABLE = True
 except ImportError:
@@ -45,7 +48,9 @@ class TestCostTracker:
 
         # æª¢æŸ¥è³‡æ–™åº«è¡¨æ˜¯å¦å‰µå»º
         with sqlite3.connect(cost_tracker.db_path) as conn:
-            cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
+            cursor = conn.execute(
+                "SELECT name FROM sqlite_master WHERE type='table'"
+            )
             tables = [row[0] for row in cursor.fetchall()]
             assert "api_calls" in tables
             assert "daily_summaries" in tables
@@ -113,9 +118,14 @@ class TestCostTracker:
     async def test_daily_summary(self, cost_tracker):
         """æ¸¬è©¦æ¯æ—¥æ‘˜è¦åŠŸèƒ½"""
         # æ·»åŠ ä¸€äº›æ¸¬è©¦å‘¼å«
-        await cost_tracker.track_api_call("openai", "gpt-4", "text_generation", tokens_used=500)
         await cost_tracker.track_api_call(
-            "stability_ai", "stable-diffusion-xl", "image_generation", images_generated=2
+            "openai", "gpt-4", "text_generation", tokens_used=500
+        )
+        await cost_tracker.track_api_call(
+            "stability_ai",
+            "stable-diffusion-xl",
+            "image_generation",
+            images_generated=2,
         )
 
         summary = await cost_tracker.get_daily_summary()
@@ -131,7 +141,9 @@ class TestCostTracker:
     async def test_weekly_report(self, cost_tracker):
         """æ¸¬è©¦é€±å ±å‘ŠåŠŸèƒ½"""
         # æ·»åŠ æ¸¬è©¦æ•¸æ“š
-        await cost_tracker.track_api_call("openai", "gpt-4", "text_generation", tokens_used=500)
+        await cost_tracker.track_api_call(
+            "openai", "gpt-4", "text_generation", tokens_used=500
+        )
 
         report = await cost_tracker.get_weekly_report()
 
@@ -145,7 +157,9 @@ class TestCostTracker:
     async def test_budget_status_check(self, cost_tracker):
         """æ¸¬è©¦é ç®—ç‹€æ…‹æª¢æŸ¥"""
         # æ·»åŠ ä¸€äº›æˆæœ¬
-        await cost_tracker.track_api_call("openai", "gpt-4", "text_generation", tokens_used=1000)
+        await cost_tracker.track_api_call(
+            "openai", "gpt-4", "text_generation", tokens_used=1000
+        )
 
         status = await cost_tracker.check_budget_status()
 
@@ -171,9 +185,14 @@ class TestCostTracker:
     async def test_export_cost_data(self, cost_tracker):
         """æ¸¬è©¦æˆæœ¬æ•¸æ“šåŒ¯å‡º"""
         # æ·»åŠ æ¸¬è©¦æ•¸æ“š
-        await cost_tracker.track_api_call("openai", "gpt-4", "text_generation", tokens_used=500)
         await cost_tracker.track_api_call(
-            "stability_ai", "stable-diffusion-xl", "image_generation", images_generated=1
+            "openai", "gpt-4", "text_generation", tokens_used=500
+        )
+        await cost_tracker.track_api_call(
+            "stability_ai",
+            "stable-diffusion-xl",
+            "image_generation",
+            images_generated=1,
         )
 
         export_file = await cost_tracker.export_cost_data(days=7)
@@ -211,7 +230,9 @@ class TestBudgetController:
     @pytest.mark.asyncio
     async def test_budget_decision_normal(self, budget_controller):
         """æ¸¬è©¦æ­£å¸¸é ç®—æƒ…æ³ä¸‹çš„æ±ºç­–"""
-        decision = await budget_controller.check_budget_and_decide(estimated_cost=1.0)
+        decision = await budget_controller.check_budget_and_decide(
+            estimated_cost=1.0
+        )
 
         assert decision.can_continue == True
         assert decision.status.value in ["normal", "warning"]
@@ -222,7 +243,9 @@ class TestBudgetController:
     async def test_budget_decision_critical(self, budget_controller):
         """æ¸¬è©¦è‡¨ç•Œé ç®—æƒ…æ³ä¸‹çš„æ±ºç­–"""
         # æ¨¡æ“¬é«˜é ç®—ä½¿ç”¨ç‡
-        with patch.object(budget_controller.cost_tracker, "check_budget_status") as mock_status:
+        with patch.object(
+            budget_controller.cost_tracker, "check_budget_status"
+        ) as mock_status:
             mock_status.return_value = {
                 "current_cost": 9.5,
                 "budget_limit": 10.0,
@@ -232,9 +255,14 @@ class TestBudgetController:
                 "can_continue": True,
             }
 
-            decision = await budget_controller.check_budget_and_decide(estimated_cost=1.0)
+            decision = await budget_controller.check_budget_and_decide(
+                estimated_cost=1.0
+            )
 
-            assert decision.can_continue == False or decision.action.value in ["pause", "stop"]
+            assert decision.can_continue == False or decision.action.value in [
+                "pause",
+                "stop",
+            ]
             assert decision.status.value in ["critical", "exceeded"]
 
     @pytest.mark.asyncio
@@ -252,7 +280,9 @@ class TestBudgetController:
     async def test_post_operation_update(self, budget_controller):
         """æ¸¬è©¦æ“ä½œå¾Œæ›´æ–°"""
         # é€™æ‡‰è©²ä¸æœƒæ‹‹å‡ºä¾‹å¤–
-        await budget_controller.post_operation_update(actual_cost=1.5, operation_result=True)
+        await budget_controller.post_operation_update(
+            actual_cost=1.5, operation_result=True
+        )
 
         # é©—è­‰æˆæœ¬è¿½è¹¤å™¨ç‹€æ…‹æ›´æ–°
         # å…·é«”é©—è­‰é‚è¼¯å–æ±ºæ–¼å¯¦ç¾
@@ -261,7 +291,9 @@ class TestBudgetController:
     async def test_dynamic_budget_adjustment(self, budget_controller):
         """æ¸¬è©¦å‹•æ…‹é ç®—èª¿æ•´"""
         if budget_controller.config_manager:
-            old_budget = budget_controller.config_manager.get("cost_control.daily_budget_usd", 10.0)
+            old_budget = budget_controller.config_manager.get(
+                "cost_control.daily_budget_usd", 10.0
+            )
 
             result = await budget_controller.adjust_budget_dynamically(
                 new_budget=20.0, reason="æ¸¬è©¦èª¿æ•´"
@@ -299,7 +331,9 @@ class TestBudgetController:
 class TestCostCalculations:
     """æˆæœ¬è¨ˆç®—æ¸¬è©¦"""
 
-    @pytest.mark.skipif(not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨")
+    @pytest.mark.skipif(
+        not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨"
+    )
     def test_openai_cost_calculation(self, mock_config_manager):
         """æ¸¬è©¦ OpenAI æˆæœ¬è¨ˆç®—"""
         tracker = CostTracker(mock_config_manager, ":memory:")
@@ -319,7 +353,9 @@ class TestCostCalculations:
         expected_cost = (500 / 1000 * 0.03) + (500 / 1000 * 0.06)
         assert cost == pytest.approx(expected_cost, rel=0.01)
 
-    @pytest.mark.skipif(not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨")
+    @pytest.mark.skipif(
+        not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨"
+    )
     def test_stability_cost_calculation(self, mock_config_manager):
         """æ¸¬è©¦ Stability AI æˆæœ¬è¨ˆç®—"""
         tracker = CostTracker(mock_config_manager, ":memory:")
@@ -337,7 +373,9 @@ class TestCostCalculations:
         expected_cost = 5 * 0.04
         assert cost == pytest.approx(expected_cost, rel=0.01)
 
-    @pytest.mark.skipif(not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨")
+    @pytest.mark.skipif(
+        not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨"
+    )
     def test_elevenlabs_cost_calculation(self, mock_config_manager):
         """æ¸¬è©¦ ElevenLabs æˆæœ¬è¨ˆç®—"""
         tracker = CostTracker(mock_config_manager, ":memory:")
@@ -361,9 +399,13 @@ class TestCostCalculations:
 class TestCostMonitoringIntegration:
     """æˆæœ¬ç›£æ§æ•´åˆæ¸¬è©¦"""
 
-    @pytest.mark.skipif(not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨")
+    @pytest.mark.skipif(
+        not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨"
+    )
     @pytest.mark.asyncio
-    async def test_full_cost_tracking_workflow(self, temp_dir, mock_config_manager):
+    async def test_full_cost_tracking_workflow(
+        self, temp_dir, mock_config_manager
+    ):
         """æ¸¬è©¦å®Œæ•´çš„æˆæœ¬è¿½è¹¤å·¥ä½œæµç¨‹"""
         db_path = temp_dir / "integration_test.db"
 
@@ -374,9 +416,24 @@ class TestCostMonitoringIntegration:
         # æ¨¡æ“¬ä¸€ç³»åˆ— API å‘¼å«
         api_calls = [
             ("openai", "gpt-4", "text_generation", {"tokens_used": 800}),
-            ("stability_ai", "stable-diffusion-xl", "image_generation", {"images_generated": 2}),
-            ("elevenlabs", "voice_synthesis", "voice_synthesis", {"characters_used": 1500}),
-            ("openai", "gpt-3.5-turbo", "text_generation", {"tokens_used": 1200}),
+            (
+                "stability_ai",
+                "stable-diffusion-xl",
+                "image_generation",
+                {"images_generated": 2},
+            ),
+            (
+                "elevenlabs",
+                "voice_synthesis",
+                "voice_synthesis",
+                {"characters_used": 1500},
+            ),
+            (
+                "openai",
+                "gpt-3.5-turbo",
+                "text_generation",
+                {"tokens_used": 1200},
+            ),
         ]
 
         total_cost = 0
@@ -389,7 +446,10 @@ class TestCostMonitoringIntegration:
             if can_proceed:
                 # åŸ·è¡Œ API å‘¼å«ä¸¦è¿½è¹¤æˆæœ¬
                 cost = await cost_tracker.track_api_call(
-                    provider=provider, model=model, operation_type=operation, **kwargs
+                    provider=provider,
+                    model=model,
+                    operation_type=operation,
+                    **kwargs,
                 )
                 total_cost += cost
 
@@ -406,11 +466,15 @@ class TestCostMonitoringIntegration:
 
         # æª¢æŸ¥é ç®—ç‹€æ…‹
         budget_status = await cost_tracker.check_budget_status()
-        assert budget_status["current_cost"] == pytest.approx(total_cost, rel=0.01)
+        assert budget_status["current_cost"] == pytest.approx(
+            total_cost, rel=0.01
+        )
 
         # ç²å–é ç®—å ±å‘Š
         budget_report = await budget_controller.get_daily_budget_report()
-        assert budget_report["budget_status"]["current_cost"] == pytest.approx(total_cost, rel=0.01)
+        assert budget_report["budget_status"]["current_cost"] == pytest.approx(
+            total_cost, rel=0.01
+        )
 
         # æ¸…ç†
         if db_path.exists():
@@ -421,7 +485,9 @@ class TestCostMonitoringIntegration:
 class TestGlobalFunctions:
     """å…¨åŸŸå‡½æ•¸æ¸¬è©¦"""
 
-    @pytest.mark.skipif(not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨")
+    @pytest.mark.skipif(
+        not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨"
+    )
     def test_get_cost_tracker(self, mock_config_manager):
         """æ¸¬è©¦ç²å–æˆæœ¬è¿½è¹¤å™¨å¯¦ä¾‹"""
         tracker1 = get_cost_tracker(mock_config_manager)
@@ -430,7 +496,9 @@ class TestGlobalFunctions:
         # æ‡‰è©²è¿”å›åŒä¸€å€‹å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
         assert tracker1 is tracker2
 
-    @pytest.mark.skipif(not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨")
+    @pytest.mark.skipif(
+        not COST_MONITORING_AVAILABLE, reason="æˆæœ¬ç›£æ§æ¨¡çµ„ä¸å¯ç”¨"
+    )
     def test_get_budget_controller(self, mock_config_manager):
         """æ¸¬è©¦ç²å–é ç®—æ§åˆ¶å™¨å¯¦ä¾‹"""
         controller1 = get_budget_controller(mock_config_manager)
diff --git a/services/api-gateway/app/auth.py b/services/api-gateway/app/auth.py
index 1aced64..69c2eb5 100644
--- a/services/api-gateway/app/auth.py
+++ b/services/api-gateway/app/auth.py
@@ -29,14 +29,18 @@ async def verify_token_with_auth_service(token: str) -> dict:
                 return None
 
         except httpx.RequestError as e:
-            logger.error("Failed to verify token with auth service", error=str(e))
+            logger.error(
+                "Failed to verify token with auth service", error=str(e)
+            )
             return None
 
 
 def verify_token_locally(token: str) -> dict:
     """Verify JWT token locally (fallback)"""
     try:
-        payload = jwt.decode(token, settings.jwt_secret_key, algorithms=[settings.jwt_algorithm])
+        payload = jwt.decode(
+            token, settings.jwt_secret_key, algorithms=[settings.jwt_algorithm]
+        )
         email: str = payload.get("sub")
         if email is None:
             return None
@@ -45,7 +49,9 @@ def verify_token_locally(token: str) -> dict:
         return None
 
 
-async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security_scheme)):
+async def get_current_user(
+    credentials: HTTPAuthorizationCredentials = Depends(security_scheme),
+):
     """Get current authenticated user"""
     credentials_exception = HTTPException(
         status_code=status.HTTP_401_UNAUTHORIZED,
@@ -61,7 +67,9 @@ async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(s
 
         # If auth service is unavailable, verify locally
         if user_data is None:
-            logger.warning("Auth service unavailable, using local token verification")
+            logger.warning(
+                "Auth service unavailable, using local token verification"
+            )
             user_data = verify_token_locally(token)
 
         if user_data is None:
@@ -75,7 +83,9 @@ async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(s
 
 
 async def get_optional_user(
-    credentials: HTTPAuthorizationCredentials = Depends(HTTPBearer(auto_error=False)),
+    credentials: HTTPAuthorizationCredentials = Depends(
+        HTTPBearer(auto_error=False)
+    ),
 ):
     """Get current user if token is provided (optional)"""
     if not credentials:
diff --git a/services/api-gateway/app/config.py b/services/api-gateway/app/config.py
index e9e750e..b64cde3 100644
--- a/services/api-gateway/app/config.py
+++ b/services/api-gateway/app/config.py
@@ -10,7 +10,10 @@ class Settings(BaseSettings):
 
     # CORS
     allowed_hosts: List[str] = ["localhost", "127.0.0.1"]
-    cors_origins: List[str] = ["http://localhost:3000", "http://localhost:5173"]
+    cors_origins: List[str] = [
+        "http://localhost:3000",
+        "http://localhost:5173",
+    ]
 
     # Logging
     log_level: str = "INFO"
diff --git a/services/api-gateway/app/main.py b/services/api-gateway/app/main.py
index fa9ae75..6276b08 100644
--- a/services/api-gateway/app/main.py
+++ b/services/api-gateway/app/main.py
@@ -50,7 +50,9 @@ app = FastAPI(
 
 # Add rate limiting
 app.state.limiter = limiter
-app.add_exception_handler(RateLimitExceeded, custom_rate_limit_exceeded_handler)
+app.add_exception_handler(
+    RateLimitExceeded, custom_rate_limit_exceeded_handler
+)
 
 # Add middleware
 app.add_middleware(SecurityHeadersMiddleware)
diff --git a/services/api-gateway/app/middleware.py b/services/api-gateway/app/middleware.py
index 9c77eac..4e26dfd 100644
--- a/services/api-gateway/app/middleware.py
+++ b/services/api-gateway/app/middleware.py
@@ -15,7 +15,9 @@ class LoggingMiddleware(BaseHTTPMiddleware):
     def __init__(self, app: ASGIApp):
         super().__init__(app)
 
-    async def dispatch(self, request: Request, call_next: Callable) -> Response:
+    async def dispatch(
+        self, request: Request, call_next: Callable
+    ) -> Response:
         # Generate unique request ID
         request_id = str(uuid.uuid4())
 
@@ -76,14 +78,18 @@ class LoggingMiddleware(BaseHTTPMiddleware):
 class SecurityHeadersMiddleware(BaseHTTPMiddleware):
     """Middleware to add security headers"""
 
-    async def dispatch(self, request: Request, call_next: Callable) -> Response:
+    async def dispatch(
+        self, request: Request, call_next: Callable
+    ) -> Response:
         response = await call_next(request)
 
         # Add security headers
         response.headers["X-Content-Type-Options"] = "nosniff"
         response.headers["X-Frame-Options"] = "DENY"
         response.headers["X-XSS-Protection"] = "1; mode=block"
-        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
+        response.headers["Strict-Transport-Security"] = (
+            "max-age=31536000; includeSubDomains"
+        )
         response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
 
         return response
diff --git a/services/api-gateway/app/proxy.py b/services/api-gateway/app/proxy.py
index f99682a..52782ad 100644
--- a/services/api-gateway/app/proxy.py
+++ b/services/api-gateway/app/proxy.py
@@ -33,7 +33,8 @@ class ServiceProxy:
 
         if service not in self.service_urls:
             raise HTTPException(
-                status_code=status.HTTP_404_NOT_FOUND, detail=f"Service '{service}' not found"
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail=f"Service '{service}' not found",
             )
 
         service_url = self.service_urls[service]
@@ -104,7 +105,12 @@ class ServiceProxy:
             )
 
         except httpx.RequestError as e:
-            logger.error("Service request error", service=service, path=path, error=str(e))
+            logger.error(
+                "Service request error",
+                service=service,
+                path=path,
+                error=str(e),
+            )
             raise HTTPException(
                 status_code=status.HTTP_502_BAD_GATEWAY,
                 detail=f"Service '{service}' is unavailable",
@@ -124,13 +130,19 @@ class ServiceProxy:
             return False
 
     async def forward_file_request(
-        self, service: str, path: str, method: str, headers: Dict[str, str] = None, file=None
+        self,
+        service: str,
+        path: str,
+        method: str,
+        headers: Dict[str, str] = None,
+        file=None,
     ) -> Dict[str, Any]:
         """Forward file upload request to internal service"""
 
         if service not in self.service_urls:
             raise HTTPException(
-                status_code=status.HTTP_404_NOT_FOUND, detail=f"Service '{service}' not found"
+                status_code=status.HTTP_404_NOT_FOUND,
+                detail=f"Service '{service}' not found",
             )
 
         service_url = self.service_urls[service]
@@ -139,7 +151,12 @@ class ServiceProxy:
         # Prepare headers (remove content-type for multipart)
         request_headers = {}
         if headers:
-            forwarded_headers = ["authorization", "user-agent", "x-forwarded-for", "x-real-ip"]
+            forwarded_headers = [
+                "authorization",
+                "user-agent",
+                "x-forwarded-for",
+                "x-real-ip",
+            ]
             for header in forwarded_headers:
                 if header in headers:
                     request_headers[header] = headers[header]
@@ -150,7 +167,10 @@ class ServiceProxy:
 
             async with httpx.AsyncClient(timeout=self.timeout) as client:
                 response = await client.request(
-                    method=method, url=full_url, headers=request_headers, files=files
+                    method=method,
+                    url=full_url,
+                    headers=request_headers,
+                    files=files,
                 )
 
                 logger.info(
@@ -196,7 +216,8 @@ class ServiceProxy:
                 error=str(e),
             )
             raise HTTPException(
-                status_code=status.HTTP_502_BAD_GATEWAY, detail=f"File upload to '{service}' failed"
+                status_code=status.HTTP_502_BAD_GATEWAY,
+                detail=f"File upload to '{service}' failed",
             )
 
 
diff --git a/services/api-gateway/app/rate_limiter.py b/services/api-gateway/app/rate_limiter.py
index ca1bc38..ca80902 100644
--- a/services/api-gateway/app/rate_limiter.py
+++ b/services/api-gateway/app/rate_limiter.py
@@ -40,7 +40,9 @@ limiter = Limiter(
 )
 
 
-def custom_rate_limit_exceeded_handler(request: Request, exc: RateLimitExceeded):
+def custom_rate_limit_exceeded_handler(
+    request: Request, exc: RateLimitExceeded
+):
     """Custom rate limit exceeded handler"""
     response = HTTPException(
         status_code=status.HTTP_429_TOO_MANY_REQUESTS,
diff --git a/services/api-gateway/app/routers.py b/services/api-gateway/app/routers.py
index a385576..26d1f2a 100644
--- a/services/api-gateway/app/routers.py
+++ b/services/api-gateway/app/routers.py
@@ -1,4 +1,12 @@
-from fastapi import APIRouter, Request, Depends, HTTPException, status, UploadFile, File
+from fastapi import (
+    APIRouter,
+    Request,
+    Depends,
+    HTTPException,
+    status,
+    UploadFile,
+    File,
+)
 from fastapi.responses import JSONResponse
 from typing import Dict, Any, Optional
 import structlog
@@ -32,7 +40,9 @@ async def register(request: Request):
     )
 
     if result["status_code"] == 201:
-        return JSONResponse(status_code=result["status_code"], content=result["data"])
+        return JSONResponse(
+            status_code=result["status_code"], content=result["data"]
+        )
     else:
         raise HTTPException(
             status_code=result["status_code"],
@@ -54,27 +64,39 @@ async def login(request: Request):
     )
 
     if result["status_code"] == 200:
-        return JSONResponse(status_code=result["status_code"], content=result["data"])
+        return JSONResponse(
+            status_code=result["status_code"], content=result["data"]
+        )
     else:
         raise HTTPException(
-            status_code=result["status_code"], detail=result["data"].get("detail", "Login failed")
+            status_code=result["status_code"],
+            detail=result["data"].get("detail", "Login failed"),
         )
 
 
 @auth_router.get("/me")
 @limiter.limit("30/minute")
-async def get_profile(request: Request, current_user: dict = Depends(get_current_user)):
+async def get_profile(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Get current user profile"""
     result = await proxy.forward_request(
-        service="auth", path="/api/v1/me", method="GET", headers=dict(request.headers)
+        service="auth",
+        path="/api/v1/me",
+        method="GET",
+        headers=dict(request.headers),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @auth_router.put("/me")
 @limiter.limit("10/minute")
-async def update_profile(request: Request, current_user: dict = Depends(get_current_user)):
+async def update_profile(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Update user profile"""
     body = await request.json()
     result = await proxy.forward_request(
@@ -85,12 +107,16 @@ async def update_profile(request: Request, current_user: dict = Depends(get_curr
         json_data=body,
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @auth_router.post("/change-password")
 @limiter.limit("5/minute")
-async def change_password(request: Request, current_user: dict = Depends(get_current_user)):
+async def change_password(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Change user password"""
     body = await request.json()
     result = await proxy.forward_request(
@@ -101,14 +127,18 @@ async def change_password(request: Request, current_user: dict = Depends(get_cur
         json_data=body,
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 # Data ingestion routes
 @data_router.post("/upload")
 @limiter.limit("10/minute")
 async def upload_audio(
-    request: Request, file: UploadFile = File(...), current_user: dict = Depends(get_current_user)
+    request: Request,
+    file: UploadFile = File(...),
+    current_user: dict = Depends(get_current_user),
 ):
     """Upload audio file for training"""
     # Forward to data service
@@ -120,12 +150,16 @@ async def upload_audio(
         file=file,
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @data_router.get("/files")
 @limiter.limit("30/minute")
-async def get_user_files(request: Request, current_user: dict = Depends(get_current_user)):
+async def get_user_files(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Get user's uploaded files"""
     result = await proxy.forward_request(
         service="data",
@@ -135,13 +169,17 @@ async def get_user_files(request: Request, current_user: dict = Depends(get_curr
         params=dict(request.query_params),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @data_router.delete("/files/{file_id}")
 @limiter.limit("10/minute")
 async def delete_file(
-    file_id: int, request: Request, current_user: dict = Depends(get_current_user)
+    file_id: int,
+    request: Request,
+    current_user: dict = Depends(get_current_user),
 ):
     """Delete uploaded file"""
     result = await proxy.forward_request(
@@ -151,13 +189,17 @@ async def delete_file(
         headers=dict(request.headers),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @data_router.post("/process/{file_id}")
 @limiter.limit("20/minute")
 async def start_processing(
-    file_id: int, request: Request, current_user: dict = Depends(get_current_user)
+    file_id: int,
+    request: Request,
+    current_user: dict = Depends(get_current_user),
 ):
     """Start processing job for uploaded file"""
     body = await request.json()
@@ -169,12 +211,16 @@ async def start_processing(
         json_data=body,
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @data_router.get("/jobs")
 @limiter.limit("30/minute")
-async def get_processing_jobs(request: Request, current_user: dict = Depends(get_current_user)):
+async def get_processing_jobs(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Get user's processing jobs"""
     result = await proxy.forward_request(
         service="data",
@@ -184,25 +230,38 @@ async def get_processing_jobs(request: Request, current_user: dict = Depends(get
         params=dict(request.query_params),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @data_router.get("/jobs/{job_id}")
 @limiter.limit("30/minute")
 async def get_job_status(
-    job_id: int, request: Request, current_user: dict = Depends(get_current_user)
+    job_id: int,
+    request: Request,
+    current_user: dict = Depends(get_current_user),
 ):
     """Get processing job status"""
     result = await proxy.forward_request(
-        service="data", path=f"/api/v1/jobs/{job_id}", method="GET", headers=dict(request.headers)
+        service="data",
+        path=f"/api/v1/jobs/{job_id}",
+        method="GET",
+        headers=dict(request.headers),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @data_router.delete("/jobs/{job_id}")
 @limiter.limit("10/minute")
-async def cancel_job(job_id: int, request: Request, current_user: dict = Depends(get_current_user)):
+async def cancel_job(
+    job_id: int,
+    request: Request,
+    current_user: dict = Depends(get_current_user),
+):
     """Cancel processing job"""
     result = await proxy.forward_request(
         service="data",
@@ -211,13 +270,17 @@ async def cancel_job(job_id: int, request: Request, current_user: dict = Depends
         headers=dict(request.headers),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 # Inference routes
 @inference_router.post("/synthesize")
 @limiter.limit("20/minute")
-async def synthesize_voice(request: Request, current_user: dict = Depends(get_current_user)):
+async def synthesize_voice(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Synthesize voice from text"""
     body = await request.json()
     result = await proxy.forward_request(
@@ -228,12 +291,16 @@ async def synthesize_voice(request: Request, current_user: dict = Depends(get_cu
         json_data=body,
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @inference_router.post("/synthesize/batch")
 @limiter.limit("10/minute")
-async def batch_synthesize_voice(request: Request, current_user: dict = Depends(get_current_user)):
+async def batch_synthesize_voice(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Batch synthesize multiple texts"""
     body = await request.json()
     result = await proxy.forward_request(
@@ -244,13 +311,17 @@ async def batch_synthesize_voice(request: Request, current_user: dict = Depends(
         json_data=body,
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @inference_router.get("/synthesize/audio/{job_id}")
 @limiter.limit("30/minute")
 async def get_synthesis_audio(
-    job_id: int, request: Request, current_user: dict = Depends(get_current_user)
+    job_id: int,
+    request: Request,
+    current_user: dict = Depends(get_current_user),
 ):
     """Get synthesized audio file"""
     result = await proxy.forward_request(
@@ -260,12 +331,16 @@ async def get_synthesis_audio(
         headers=dict(request.headers),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @inference_router.get("/jobs")
 @limiter.limit("30/minute")
-async def get_synthesis_jobs(request: Request, current_user: dict = Depends(get_current_user)):
+async def get_synthesis_jobs(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Get user's synthesis jobs"""
     result = await proxy.forward_request(
         service="inference",
@@ -275,13 +350,17 @@ async def get_synthesis_jobs(request: Request, current_user: dict = Depends(get_
         params=dict(request.query_params),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @inference_router.get("/jobs/{job_id}")
 @limiter.limit("30/minute")
 async def get_synthesis_job(
-    job_id: int, request: Request, current_user: dict = Depends(get_current_user)
+    job_id: int,
+    request: Request,
+    current_user: dict = Depends(get_current_user),
 ):
     """Get synthesis job details"""
     result = await proxy.forward_request(
@@ -291,12 +370,16 @@ async def get_synthesis_job(
         headers=dict(request.headers),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @inference_router.get("/models")
 @limiter.limit("30/minute")
-async def get_available_models(request: Request, current_user: dict = Depends(get_current_user)):
+async def get_available_models(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Get available voice models for user"""
     result = await proxy.forward_request(
         service="inference",
@@ -306,12 +389,16 @@ async def get_available_models(request: Request, current_user: dict = Depends(ge
         params=dict(request.query_params),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @inference_router.get("/models/ready")
 @limiter.limit("30/minute")
-async def get_ready_models(request: Request, current_user: dict = Depends(get_current_user)):
+async def get_ready_models(
+    request: Request, current_user: dict = Depends(get_current_user)
+):
     """Get ready-to-use voice models"""
     result = await proxy.forward_request(
         service="inference",
@@ -320,13 +407,17 @@ async def get_ready_models(request: Request, current_user: dict = Depends(get_cu
         headers=dict(request.headers),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @inference_router.get("/models/{model_id}")
 @limiter.limit("30/minute")
 async def get_model_details(
-    model_id: int, request: Request, current_user: dict = Depends(get_current_user)
+    model_id: int,
+    request: Request,
+    current_user: dict = Depends(get_current_user),
 ):
     """Get voice model details"""
     result = await proxy.forward_request(
@@ -336,13 +427,17 @@ async def get_model_details(
         headers=dict(request.headers),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 @inference_router.post("/models/{model_id}/preload")
 @limiter.limit("10/minute")
 async def preload_model(
-    model_id: int, request: Request, current_user: dict = Depends(get_current_user)
+    model_id: int,
+    request: Request,
+    current_user: dict = Depends(get_current_user),
 ):
     """Preload model into cache"""
     result = await proxy.forward_request(
@@ -352,7 +447,9 @@ async def preload_model(
         headers=dict(request.headers),
     )
 
-    return JSONResponse(status_code=result["status_code"], content=result["data"])
+    return JSONResponse(
+        status_code=result["status_code"], content=result["data"]
+    )
 
 
 # Admin routes
diff --git a/services/auth-service/app/config.py b/services/auth-service/app/config.py
index 6e13b22..b0704b5 100644
--- a/services/auth-service/app/config.py
+++ b/services/auth-service/app/config.py
@@ -4,7 +4,9 @@ from typing import List
 
 class Settings(BaseSettings):
     # Database
-    database_url: str = "postgresql://voiceclone_user:password@postgres:5432/voiceclone_db"
+    database_url: str = (
+        "postgresql://voiceclone_user:password@postgres:5432/voiceclone_db"
+    )
 
     # JWT
     jwt_secret_key: str = "your-secret-key-change-in-production"
@@ -18,7 +20,10 @@ class Settings(BaseSettings):
 
     # CORS
     allowed_hosts: List[str] = ["localhost", "127.0.0.1"]
-    cors_origins: List[str] = ["http://localhost:3000", "http://localhost:5173"]
+    cors_origins: List[str] = [
+        "http://localhost:3000",
+        "http://localhost:5173",
+    ]
 
     # Logging
     log_level: str = "INFO"
diff --git a/services/auth-service/app/crud.py b/services/auth-service/app/crud.py
index 0e9ffbb..6abfb1e 100644
--- a/services/auth-service/app/crud.py
+++ b/services/auth-service/app/crud.py
@@ -17,7 +17,9 @@ def get_user_by_email(db: Session, email: str) -> Optional[models.User]:
 
 def get_user_by_username(db: Session, username: str) -> Optional[models.User]:
     """Get user by username"""
-    return db.query(models.User).filter(models.User.username == username).first()
+    return (
+        db.query(models.User).filter(models.User.username == username).first()
+    )
 
 
 def get_users(db: Session, skip: int = 0, limit: int = 100):
@@ -70,7 +72,9 @@ def delete_user(db: Session, user_id: int) -> bool:
     return True
 
 
-def authenticate_user(db: Session, email: str, password: str) -> Optional[models.User]:
+def authenticate_user(
+    db: Session, email: str, password: str
+) -> Optional[models.User]:
     """Authenticate user with email and password"""
     user = get_user_by_email(db, email)
     if not user:
@@ -107,7 +111,9 @@ def increment_api_calls(db: Session, user_id: int) -> None:
         db.commit()
 
 
-def check_user_exists(db: Session, email: str = None, username: str = None) -> bool:
+def check_user_exists(
+    db: Session, email: str = None, username: str = None
+) -> bool:
     """Check if user exists by email or username"""
     query = db.query(models.User)
 
diff --git a/services/auth-service/app/database.py b/services/auth-service/app/database.py
index a72d8fd..20d2099 100644
--- a/services/auth-service/app/database.py
+++ b/services/auth-service/app/database.py
@@ -4,7 +4,10 @@ from sqlalchemy.orm import sessionmaker
 from .config import settings
 
 engine = create_engine(
-    settings.database_url, pool_pre_ping=True, pool_recycle=300, echo=settings.debug
+    settings.database_url,
+    pool_pre_ping=True,
+    pool_recycle=300,
+    echo=settings.debug,
 )
 
 SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
diff --git a/services/auth-service/app/dependencies.py b/services/auth-service/app/dependencies.py
index 72873a9..6f8fb4f 100644
--- a/services/auth-service/app/dependencies.py
+++ b/services/auth-service/app/dependencies.py
@@ -36,12 +36,17 @@ def get_current_user(
 def get_current_active_user(current_user=Depends(get_current_user)):
     """Get current active user"""
     if not current_user.is_active:
-        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")
+        raise HTTPException(
+            status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user"
+        )
     return current_user
 
 
 def get_current_superuser(current_user=Depends(get_current_user)):
     """Get current superuser"""
     if not current_user.is_superuser:
-        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Not enough permissions")
+        raise HTTPException(
+            status_code=status.HTTP_403_FORBIDDEN,
+            detail="Not enough permissions",
+        )
     return current_user
diff --git a/services/auth-service/app/routers.py b/services/auth-service/app/routers.py
index 6172676..b7c484f 100644
--- a/services/auth-service/app/routers.py
+++ b/services/auth-service/app/routers.py
@@ -5,28 +5,41 @@ from typing import List
 
 from . import crud, schemas, security
 from .database import get_db
-from .dependencies import get_current_user, get_current_active_user, get_current_superuser
+from .dependencies import (
+    get_current_user,
+    get_current_active_user,
+    get_current_superuser,
+)
 from .config import settings
 
 router = APIRouter()
 
 
-@router.post("/register", response_model=schemas.User, status_code=status.HTTP_201_CREATED)
+@router.post(
+    "/register",
+    response_model=schemas.User,
+    status_code=status.HTTP_201_CREATED,
+)
 def register_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
     """Register a new user"""
     # Check if user already exists
     if crud.check_user_exists(db, email=user.email, username=user.username):
         raise HTTPException(
-            status_code=status.HTTP_400_BAD_REQUEST, detail="Email or username already registered"
+            status_code=status.HTTP_400_BAD_REQUEST,
+            detail="Email or username already registered",
         )
 
     return crud.create_user(db=db, user=user)
 
 
 @router.post("/login", response_model=schemas.Token)
-def login_user(user_credentials: schemas.UserLogin, db: Session = Depends(get_db)):
+def login_user(
+    user_credentials: schemas.UserLogin, db: Session = Depends(get_db)
+):
     """Login user and return access token"""
-    user = crud.authenticate_user(db, user_credentials.email, user_credentials.password)
+    user = crud.authenticate_user(
+        db, user_credentials.email, user_credentials.password
+    )
     if not user:
         raise HTTPException(
             status_code=status.HTTP_401_UNAUTHORIZED,
@@ -35,13 +48,17 @@ def login_user(user_credentials: schemas.UserLogin, db: Session = Depends(get_db
         )
 
     if not user.is_active:
-        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user")
+        raise HTTPException(
+            status_code=status.HTTP_400_BAD_REQUEST, detail="Inactive user"
+        )
 
     # Update last login
     crud.update_last_login(db, user.id)
 
     # Create access token
-    access_token_expires = timedelta(minutes=settings.jwt_access_token_expire_minutes)
+    access_token_expires = timedelta(
+        minutes=settings.jwt_access_token_expire_minutes
+    )
     access_token = security.create_access_token(
         data={"sub": user.email}, expires_delta=access_token_expires
     )
@@ -71,13 +88,15 @@ def update_user_me(
     if user_update.email and user_update.email != current_user.email:
         if crud.get_user_by_email(db, user_update.email):
             raise HTTPException(
-                status_code=status.HTTP_400_BAD_REQUEST, detail="Email already registered"
+                status_code=status.HTTP_400_BAD_REQUEST,
+                detail="Email already registered",
             )
 
     if user_update.username and user_update.username != current_user.username:
         if crud.get_user_by_username(db, user_update.username):
             raise HTTPException(
-                status_code=status.HTTP_400_BAD_REQUEST, detail="Username already taken"
+                status_code=status.HTTP_400_BAD_REQUEST,
+                detail="Username already taken",
             )
 
     updated_user = crud.update_user(db, current_user.id, user_update)
@@ -92,28 +111,38 @@ def change_password(
 ):
     """Change user password"""
     # Verify current password
-    if not security.verify_password(password_change.current_password, current_user.hashed_password):
+    if not security.verify_password(
+        password_change.current_password, current_user.hashed_password
+    ):
         raise HTTPException(
-            status_code=status.HTTP_400_BAD_REQUEST, detail="Incorrect current password"
+            status_code=status.HTTP_400_BAD_REQUEST,
+            detail="Incorrect current password",
         )
 
     # Update password
-    success = crud.update_user_password(db, current_user.id, password_change.new_password)
+    success = crud.update_user_password(
+        db, current_user.id, password_change.new_password
+    )
     if not success:
         raise HTTPException(
-            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to update password"
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail="Failed to update password",
         )
 
     return {"message": "Password updated successfully"}
 
 
 @router.post("/password-reset")
-def request_password_reset(password_reset: schemas.PasswordReset, db: Session = Depends(get_db)):
+def request_password_reset(
+    password_reset: schemas.PasswordReset, db: Session = Depends(get_db)
+):
     """Request password reset token"""
     user = crud.get_user_by_email(db, password_reset.email)
     if not user:
         # Don't reveal if email exists or not
-        return {"message": "If the email exists, a password reset link has been sent"}
+        return {
+            "message": "If the email exists, a password reset link has been sent"
+        }
 
     reset_token = security.create_password_reset_token(user.email)
 
@@ -127,24 +156,31 @@ def request_password_reset(password_reset: schemas.PasswordReset, db: Session =
 
 @router.post("/password-reset-confirm")
 def confirm_password_reset(
-    password_reset_confirm: schemas.PasswordResetConfirm, db: Session = Depends(get_db)
+    password_reset_confirm: schemas.PasswordResetConfirm,
+    db: Session = Depends(get_db),
 ):
     """Confirm password reset with token"""
     email = security.verify_password_reset_token(password_reset_confirm.token)
     if not email:
         raise HTTPException(
-            status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid or expired reset token"
+            status_code=status.HTTP_400_BAD_REQUEST,
+            detail="Invalid or expired reset token",
         )
 
     user = crud.get_user_by_email(db, email)
     if not user:
-        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found")
+        raise HTTPException(
+            status_code=status.HTTP_404_NOT_FOUND, detail="User not found"
+        )
 
     # Update password
-    success = crud.update_user_password(db, user.id, password_reset_confirm.new_password)
+    success = crud.update_user_password(
+        db, user.id, password_reset_confirm.new_password
+    )
     if not success:
         raise HTTPException(
-            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to update password"
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail="Failed to update password",
         )
 
     return {"message": "Password reset successfully"}
@@ -165,21 +201,29 @@ def read_users(
 
 @router.get("/users/{user_id}", response_model=schemas.User)
 def read_user(
-    user_id: int, current_user=Depends(get_current_superuser), db: Session = Depends(get_db)
+    user_id: int,
+    current_user=Depends(get_current_superuser),
+    db: Session = Depends(get_db),
 ):
     """Get user by ID (admin only)"""
     user = crud.get_user(db, user_id=user_id)
     if user is None:
-        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found")
+        raise HTTPException(
+            status_code=status.HTTP_404_NOT_FOUND, detail="User not found"
+        )
     return user
 
 
 @router.delete("/users/{user_id}")
 def delete_user(
-    user_id: int, current_user=Depends(get_current_superuser), db: Session = Depends(get_db)
+    user_id: int,
+    current_user=Depends(get_current_superuser),
+    db: Session = Depends(get_db),
 ):
     """Delete user (admin only)"""
     success = crud.delete_user(db, user_id)
     if not success:
-        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found")
+        raise HTTPException(
+            status_code=status.HTTP_404_NOT_FOUND, detail="User not found"
+        )
     return {"message": "User deleted successfully"}
diff --git a/services/auth-service/app/schemas.py b/services/auth-service/app/schemas.py
index ce8fb33..7ce120f 100644
--- a/services/auth-service/app/schemas.py
+++ b/services/auth-service/app/schemas.py
@@ -25,7 +25,9 @@ class UserCreate(UserBase):
         if len(v) < 3:
             raise ValueError("Username must be at least 3 characters long")
         if not v.isalnum():
-            raise ValueError("Username must contain only alphanumeric characters")
+            raise ValueError(
+                "Username must contain only alphanumeric characters"
+            )
         return v.lower()
 
 
@@ -41,7 +43,9 @@ class UserUpdate(BaseModel):
         if v and len(v) < 3:
             raise ValueError("Username must be at least 3 characters long")
         if v and not v.isalnum():
-            raise ValueError("Username must contain only alphanumeric characters")
+            raise ValueError(
+                "Username must contain only alphanumeric characters"
+            )
         return v.lower() if v else v
 
 
diff --git a/services/auth-service/app/security.py b/services/auth-service/app/security.py
index 53b9808..e3b50e8 100644
--- a/services/auth-service/app/security.py
+++ b/services/auth-service/app/security.py
@@ -19,23 +19,31 @@ def get_password_hash(password: str) -> str:
     return pwd_context.hash(password)
 
 
-def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
+def create_access_token(
+    data: dict, expires_delta: Optional[timedelta] = None
+) -> str:
     """Create JWT access token"""
     to_encode = data.copy()
     if expires_delta:
         expire = datetime.utcnow() + expires_delta
     else:
-        expire = datetime.utcnow() + timedelta(minutes=settings.jwt_access_token_expire_minutes)
+        expire = datetime.utcnow() + timedelta(
+            minutes=settings.jwt_access_token_expire_minutes
+        )
 
     to_encode.update({"exp": expire})
-    encoded_jwt = jwt.encode(to_encode, settings.jwt_secret_key, algorithm=settings.jwt_algorithm)
+    encoded_jwt = jwt.encode(
+        to_encode, settings.jwt_secret_key, algorithm=settings.jwt_algorithm
+    )
     return encoded_jwt
 
 
 def verify_token(token: str) -> Optional[str]:
     """Verify JWT token and return email"""
     try:
-        payload = jwt.decode(token, settings.jwt_secret_key, algorithms=[settings.jwt_algorithm])
+        payload = jwt.decode(
+            token, settings.jwt_secret_key, algorithms=[settings.jwt_algorithm]
+        )
         email: str = payload.get("sub")
         if email is None:
             return None
@@ -47,14 +55,18 @@ def verify_token(token: str) -> Optional[str]:
 def create_password_reset_token(email: str) -> str:
     """Create password reset token"""
     data = {"sub": email, "type": "password_reset"}
-    expires_delta = timedelta(hours=1)  # Password reset tokens expire in 1 hour
+    expires_delta = timedelta(
+        hours=1
+    )  # Password reset tokens expire in 1 hour
     return create_access_token(data, expires_delta)
 
 
 def verify_password_reset_token(token: str) -> Optional[str]:
     """Verify password reset token"""
     try:
-        payload = jwt.decode(token, settings.jwt_secret_key, algorithms=[settings.jwt_algorithm])
+        payload = jwt.decode(
+            token, settings.jwt_secret_key, algorithms=[settings.jwt_algorithm]
+        )
         email: str = payload.get("sub")
         token_type: str = payload.get("type")
         if email is None or token_type != "password_reset":
diff --git a/services/auth-service/tests/conftest.py b/services/auth-service/tests/conftest.py
index b1aa7cb..97106d9 100644
--- a/services/auth-service/tests/conftest.py
+++ b/services/auth-service/tests/conftest.py
@@ -17,7 +17,9 @@ engine = create_engine(
     poolclass=StaticPool,
 )
 
-TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
+TestingSessionLocal = sessionmaker(
+    autocommit=False, autoflush=False, bind=engine
+)
 
 
 def override_get_db():
diff --git a/services/auth-service/tests/test_auth.py b/services/auth-service/tests/test_auth.py
index fc239fd..1346b87 100644
--- a/services/auth-service/tests/test_auth.py
+++ b/services/auth-service/tests/test_auth.py
@@ -18,7 +18,9 @@ class TestUserRegistration:
         assert "id" in data
         assert "hashed_password" not in data
 
-    def test_register_user_duplicate_email(self, client: TestClient, sample_user_data):
+    def test_register_user_duplicate_email(
+        self, client: TestClient, sample_user_data
+    ):
         """Test registration with duplicate email"""
         # Register first user
         client.post("/api/v1/register", json=sample_user_data)
@@ -31,7 +33,9 @@ class TestUserRegistration:
         assert response.status_code == 400
         assert "already registered" in response.json()["detail"]
 
-    def test_register_user_duplicate_username(self, client: TestClient, sample_user_data):
+    def test_register_user_duplicate_username(
+        self, client: TestClient, sample_user_data
+    ):
         """Test registration with duplicate username"""
         # Register first user
         client.post("/api/v1/register", json=sample_user_data)
@@ -44,7 +48,9 @@ class TestUserRegistration:
         assert response.status_code == 400
         assert "already registered" in response.json()["detail"]
 
-    def test_register_user_invalid_password(self, client: TestClient, sample_user_data):
+    def test_register_user_invalid_password(
+        self, client: TestClient, sample_user_data
+    ):
         """Test registration with invalid password"""
         invalid_data = sample_user_data.copy()
         invalid_data["password"] = "123"  # Too short
@@ -62,7 +68,10 @@ class TestUserLogin:
         client.post("/api/v1/register", json=sample_user_data)
 
         # Login
-        login_data = {"email": sample_user_data["email"], "password": sample_user_data["password"]}
+        login_data = {
+            "email": sample_user_data["email"],
+            "password": sample_user_data["password"],
+        }
         response = client.post("/api/v1/login", json=login_data)
 
         assert response.status_code == 200
@@ -73,19 +82,27 @@ class TestUserLogin:
 
     def test_login_invalid_email(self, client: TestClient, sample_user_data):
         """Test login with invalid email"""
-        login_data = {"email": "nonexistent@example.com", "password": "somepassword"}
+        login_data = {
+            "email": "nonexistent@example.com",
+            "password": "somepassword",
+        }
         response = client.post("/api/v1/login", json=login_data)
 
         assert response.status_code == 401
         assert "Incorrect email or password" in response.json()["detail"]
 
-    def test_login_invalid_password(self, client: TestClient, sample_user_data):
+    def test_login_invalid_password(
+        self, client: TestClient, sample_user_data
+    ):
         """Test login with invalid password"""
         # Register user
         client.post("/api/v1/register", json=sample_user_data)
 
         # Try login with wrong password
-        login_data = {"email": sample_user_data["email"], "password": "wrongpassword"}
+        login_data = {
+            "email": sample_user_data["email"],
+            "password": "wrongpassword",
+        }
         response = client.post("/api/v1/login", json=login_data)
 
         assert response.status_code == 401
@@ -101,12 +118,17 @@ class TestUserProfile:
         client.post("/api/v1/register", json=sample_user_data)
         login_response = client.post(
             "/api/v1/login",
-            json={"email": sample_user_data["email"], "password": sample_user_data["password"]},
+            json={
+                "email": sample_user_data["email"],
+                "password": sample_user_data["password"],
+            },
         )
         token = login_response.json()["access_token"]
 
         # Get profile
-        response = client.get("/api/v1/me", headers={"Authorization": f"Bearer {token}"})
+        response = client.get(
+            "/api/v1/me", headers={"Authorization": f"Bearer {token}"}
+        )
 
         assert response.status_code == 200
         data = response.json()
@@ -120,7 +142,9 @@ class TestUserProfile:
 
     def test_get_current_user_invalid_token(self, client: TestClient):
         """Test getting current user with invalid token"""
-        response = client.get("/api/v1/me", headers={"Authorization": "Bearer invalid-token"})
+        response = client.get(
+            "/api/v1/me", headers={"Authorization": "Bearer invalid-token"}
+        )
         assert response.status_code == 401
 
     def test_update_user_profile(self, client: TestClient, sample_user_data):
@@ -129,14 +153,19 @@ class TestUserProfile:
         client.post("/api/v1/register", json=sample_user_data)
         login_response = client.post(
             "/api/v1/login",
-            json={"email": sample_user_data["email"], "password": sample_user_data["password"]},
+            json={
+                "email": sample_user_data["email"],
+                "password": sample_user_data["password"],
+            },
         )
         token = login_response.json()["access_token"]
 
         # Update profile
         update_data = {"full_name": "Updated Name", "bio": "This is my bio"}
         response = client.put(
-            "/api/v1/me", json=update_data, headers={"Authorization": f"Bearer {token}"}
+            "/api/v1/me",
+            json=update_data,
+            headers={"Authorization": f"Bearer {token}"},
         )
 
         assert response.status_code == 200
@@ -148,13 +177,18 @@ class TestUserProfile:
 class TestPasswordManagement:
     """Test password management functionality"""
 
-    def test_change_password_success(self, client: TestClient, sample_user_data):
+    def test_change_password_success(
+        self, client: TestClient, sample_user_data
+    ):
         """Test successful password change"""
         # Register and login
         client.post("/api/v1/register", json=sample_user_data)
         login_response = client.post(
             "/api/v1/login",
-            json={"email": sample_user_data["email"], "password": sample_user_data["password"]},
+            json={
+                "email": sample_user_data["email"],
+                "password": sample_user_data["password"],
+            },
         )
         token = login_response.json()["access_token"]
 
@@ -174,17 +208,26 @@ class TestPasswordManagement:
 
         # Test login with new password
         login_response = client.post(
-            "/api/v1/login", json={"email": sample_user_data["email"], "password": "newpassword123"}
+            "/api/v1/login",
+            json={
+                "email": sample_user_data["email"],
+                "password": "newpassword123",
+            },
         )
         assert login_response.status_code == 200
 
-    def test_change_password_wrong_current(self, client: TestClient, sample_user_data):
+    def test_change_password_wrong_current(
+        self, client: TestClient, sample_user_data
+    ):
         """Test password change with wrong current password"""
         # Register and login
         client.post("/api/v1/register", json=sample_user_data)
         login_response = client.post(
             "/api/v1/login",
-            json={"email": sample_user_data["email"], "password": sample_user_data["password"]},
+            json={
+                "email": sample_user_data["email"],
+                "password": sample_user_data["password"],
+            },
         )
         token = login_response.json()["access_token"]
 
diff --git a/services/data-ingestion/main.py b/services/data-ingestion/main.py
index 225c0d1..f4db433 100644
--- a/services/data-ingestion/main.py
+++ b/services/data-ingestion/main.py
@@ -58,18 +58,28 @@ class IngestionResult(BaseModel):
 @app.get("/health")
 async def health_check():
     """Health check endpoint"""
-    return {"status": "healthy", "service": "data-ingestion", "timestamp": datetime.utcnow()}
+    return {
+        "status": "healthy",
+        "service": "data-ingestion",
+        "timestamp": datetime.utcnow(),
+    }
 
 
 @app.post("/api/v1/ingest/trends", response_model=IngestionResult)
-async def ingest_trends(request: IngestionRequest, background_tasks: BackgroundTasks):
+async def ingest_trends(
+    request: IngestionRequest, background_tasks: BackgroundTasks
+):
     """Ingest trending data from social platforms"""
 
     try:
-        logger.info(f"Starting trend ingestion for platform: {request.platform}")
+        logger.info(
+            f"Starting trend ingestion for platform: {request.platform}"
+        )
 
         # Start background ingestion task
-        background_tasks.add_task(process_trend_ingestion, request.platform, request.parameters)
+        background_tasks.add_task(
+            process_trend_ingestion, request.platform, request.parameters
+        )
 
         return IngestionResult(
             success=True,
@@ -84,13 +94,19 @@ async def ingest_trends(request: IngestionRequest, background_tasks: BackgroundT
 
 
 @app.post("/api/v1/ingest/keywords", response_model=IngestionResult)
-async def ingest_keywords(request: IngestionRequest, background_tasks: BackgroundTasks):
+async def ingest_keywords(
+    request: IngestionRequest, background_tasks: BackgroundTasks
+):
     """Ingest keyword data and performance metrics"""
 
     try:
-        logger.info(f"Starting keyword ingestion for platform: {request.platform}")
+        logger.info(
+            f"Starting keyword ingestion for platform: {request.platform}"
+        )
 
-        background_tasks.add_task(process_keyword_ingestion, request.platform, request.parameters)
+        background_tasks.add_task(
+            process_keyword_ingestion, request.platform, request.parameters
+        )
 
         return IngestionResult(
             success=True,
@@ -142,7 +158,9 @@ async def process_trend_ingestion(platform: str, parameters: Dict[str, Any]):
         # Store trends in database
         await store_trends_in_database(platform, trends)
 
-        logger.info(f"Successfully ingested {len(trends)} trends for {platform}")
+        logger.info(
+            f"Successfully ingested {len(trends)} trends for {platform}"
+        )
 
     except Exception as e:
         logger.error(f"Trend ingestion failed for {platform}: {str(e)}")
@@ -162,7 +180,9 @@ async def process_keyword_ingestion(platform: str, parameters: Dict[str, Any]):
         # Store in database
         await store_keywords_in_database(platform, keyword_metrics)
 
-        logger.info(f"Successfully ingested {len(keywords)} keywords for {platform}")
+        logger.info(
+            f"Successfully ingested {len(keywords)} keywords for {platform}"
+        )
 
     except Exception as e:
         logger.error(f"Keyword ingestion failed for {platform}: {str(e)}")
@@ -191,7 +211,9 @@ async def ingest_youtube_trends(parameters: Dict[str, Any]) -> List[TrendData]:
     return trends
 
 
-async def ingest_instagram_trends(parameters: Dict[str, Any]) -> List[TrendData]:
+async def ingest_instagram_trends(
+    parameters: Dict[str, Any],
+) -> List[TrendData]:
     """Ingest trending data from Instagram"""
 
     trends = []
@@ -202,7 +224,9 @@ async def ingest_instagram_trends(parameters: Dict[str, Any]) -> List[TrendData]
     return trends
 
 
-async def fetch_trending_keywords(platform: str, parameters: Dict[str, Any]) -> List[str]:
+async def fetch_trending_keywords(
+    platform: str, parameters: Dict[str, Any]
+) -> List[str]:
     """Fetch trending keywords for a platform"""
 
     # Placeholder implementation
@@ -222,7 +246,11 @@ async def analyze_keyword_performance(keywords: List[str]) -> Dict[str, Any]:
 
     # Placeholder - would use analytics APIs
     metrics = {
-        keyword: {"search_volume": 1000, "competition": 0.5, "trend_score": 0.8}
+        keyword: {
+            "search_volume": 1000,
+            "competition": 0.5,
+            "trend_score": 0.8,
+        }
         for keyword in keywords
     }
 
@@ -236,14 +264,18 @@ async def store_trends_in_database(platform: str, trends: List[TrendData]):
     logger.info(f"Storing {len(trends)} trends for {platform} in database")
 
 
-async def store_keywords_in_database(platform: str, keyword_metrics: Dict[str, Any]):
+async def store_keywords_in_database(
+    platform: str, keyword_metrics: Dict[str, Any]
+):
     """Store keyword metrics in database"""
 
     # Placeholder - would use database connection
     logger.info(f"Storing keyword metrics for {platform} in database")
 
 
-async def fetch_trends_from_database(platform: str, limit: int) -> List[Dict[str, Any]]:
+async def fetch_trends_from_database(
+    platform: str, limit: int
+) -> List[Dict[str, Any]]:
     """Fetch trends from database"""
 
     # Placeholder - would query database
diff --git a/services/data-service/app/audio_validator.py b/services/data-service/app/audio_validator.py
index a00c35b..0731cdc 100644
--- a/services/data-service/app/audio_validator.py
+++ b/services/data-service/app/audio_validator.py
@@ -21,7 +21,9 @@ class AudioValidator:
         self.min_duration = settings.min_duration
         self.max_duration = settings.max_duration
 
-    async def validate_file_upload(self, file_path: str, original_filename: str) -> Dict[str, Any]:
+    async def validate_file_upload(
+        self, file_path: str, original_filename: str
+    ) -> Dict[str, Any]:
         """
         Comprehensive file validation
         Returns metadata dict or raises FileValidationError
@@ -29,14 +31,19 @@ class AudioValidator:
         try:
             # Check file exists
             if not os.path.exists(file_path):
-                raise FileValidationError(error="File not found", details={"file_path": file_path})
+                raise FileValidationError(
+                    error="File not found", details={"file_path": file_path}
+                )
 
             # Check file size
             file_size = os.path.getsize(file_path)
             if file_size > self.max_file_size:
                 raise FileValidationError(
                     error="File too large",
-                    details={"file_size": file_size, "max_size": self.max_file_size},
+                    details={
+                        "file_size": file_size,
+                        "max_size": self.max_file_size,
+                    },
                 )
 
             # Check file format by extension
@@ -44,7 +51,10 @@ class AudioValidator:
             if file_extension not in self.allowed_formats:
                 raise FileValidationError(
                     error="Unsupported file format",
-                    details={"format": file_extension, "allowed_formats": self.allowed_formats},
+                    details={
+                        "format": file_extension,
+                        "allowed_formats": self.allowed_formats,
+                    },
                 )
 
             # Validate MIME type
@@ -69,7 +79,11 @@ class AudioValidator:
                 "validation_passed": True,
             }
 
-            logger.info("File validation successful", filename=original_filename, metadata=metadata)
+            logger.info(
+                "File validation successful",
+                filename=original_filename,
+                metadata=metadata,
+            )
 
             return metadata
 
@@ -77,9 +91,13 @@ class AudioValidator:
             raise
         except Exception as e:
             logger.error(
-                "Unexpected error during file validation", filename=original_filename, error=str(e)
+                "Unexpected error during file validation",
+                filename=original_filename,
+                error=str(e),
+            )
+            raise FileValidationError(
+                error="Validation failed", details={"error": str(e)}
             )
-            raise FileValidationError(error="Validation failed", details={"error": str(e)})
 
     async def _analyze_audio(self, file_path: str) -> Dict[str, Any]:
         """Analyze audio file properties using librosa"""
@@ -93,12 +111,18 @@ class AudioValidator:
 
             # Audio quality analysis
             rms_energy = float(librosa.feature.rms(y=y).mean())
-            spectral_centroid = float(librosa.feature.spectral_centroid(y=y, sr=sr).mean())
-            zero_crossing_rate = float(librosa.feature.zero_crossing_rate(y).mean())
+            spectral_centroid = float(
+                librosa.feature.spectral_centroid(y=y, sr=sr).mean()
+            )
+            zero_crossing_rate = float(
+                librosa.feature.zero_crossing_rate(y).mean()
+            )
 
             # Silence detection
             silence_threshold = 0.01
-            silence_ratio = float((librosa.util.normalize(y) < silence_threshold).mean())
+            silence_ratio = float(
+                (librosa.util.normalize(y) < silence_threshold).mean()
+            )
 
             return {
                 "duration": duration,
@@ -113,7 +137,9 @@ class AudioValidator:
 
         except Exception as e:
             logger.warning(
-                "Audio analysis failed, using basic file info", file_path=file_path, error=str(e)
+                "Audio analysis failed, using basic file info",
+                file_path=file_path,
+                error=str(e),
             )
 
             # Fallback to basic file info
@@ -134,30 +160,40 @@ class AudioValidator:
                     error=str(fallback_error),
                 )
                 raise FileValidationError(
-                    error="Unable to analyze audio file", details={"error": str(fallback_error)}
+                    error="Unable to analyze audio file",
+                    details={"error": str(fallback_error)},
                 )
 
-    def _validate_audio_properties(self, audio_metadata: Dict[str, Any]) -> None:
+    def _validate_audio_properties(
+        self, audio_metadata: Dict[str, Any]
+    ) -> None:
         """Validate audio properties against requirements"""
         duration = audio_metadata.get("duration", 0)
 
         if duration < self.min_duration:
             raise FileValidationError(
                 error="Audio too short",
-                details={"duration": duration, "min_duration": self.min_duration},
+                details={
+                    "duration": duration,
+                    "min_duration": self.min_duration,
+                },
             )
 
         if duration > self.max_duration:
             raise FileValidationError(
                 error="Audio too long",
-                details={"duration": duration, "max_duration": self.max_duration},
+                details={
+                    "duration": duration,
+                    "max_duration": self.max_duration,
+                },
             )
 
         # Check for excessive silence
         silence_ratio = audio_metadata.get("silence_ratio", 0)
         if silence_ratio > 0.8:  # More than 80% silence
             raise FileValidationError(
-                error="Audio contains too much silence", details={"silence_ratio": silence_ratio}
+                error="Audio contains too much silence",
+                details={"silence_ratio": silence_ratio},
             )
 
         # Check for very low energy (likely corrupt or empty)
@@ -189,8 +225,12 @@ class AudioValidator:
         self, audio_metadata: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Determine optimal preprocessing parameters based on audio analysis"""
-        current_sr = audio_metadata.get("sample_rate", settings.target_sample_rate)
-        current_channels = audio_metadata.get("channels", settings.target_channels)
+        current_sr = audio_metadata.get(
+            "sample_rate", settings.target_sample_rate
+        )
+        current_channels = audio_metadata.get(
+            "channels", settings.target_channels
+        )
 
         # Determine if resampling is needed
         needs_resampling = current_sr != settings.target_sample_rate
@@ -221,7 +261,8 @@ class AudioValidator:
             "preprocessing_steps": preprocessing_steps,
             "needs_resampling": needs_resampling,
             "needs_channel_conversion": needs_channel_conversion,
-            "estimated_processing_time": len(preprocessing_steps) * 2,  # rough estimate in seconds
+            "estimated_processing_time": len(preprocessing_steps)
+            * 2,  # rough estimate in seconds
         }
 
 
diff --git a/services/data-service/app/auth.py b/services/data-service/app/auth.py
index cbed3d1..ea05ee2 100644
--- a/services/data-service/app/auth.py
+++ b/services/data-service/app/auth.py
@@ -12,7 +12,8 @@ async def get_current_user(token: str) -> int:
     try:
         async with httpx.AsyncClient() as client:
             response = await client.get(
-                f"{AUTH_SERVICE_URL}/api/v1/me", headers={"Authorization": f"Bearer {token}"}
+                f"{AUTH_SERVICE_URL}/api/v1/me",
+                headers={"Authorization": f"Bearer {token}"},
             )
 
             if response.status_code == 200:
@@ -23,13 +24,21 @@ async def get_current_user(token: str) -> int:
             elif response.status_code == 403:
                 raise HTTPException(status_code=403, detail="Token required")
             else:
-                raise HTTPException(status_code=401, detail="Authentication failed")
+                raise HTTPException(
+                    status_code=401, detail="Authentication failed"
+                )
 
     except httpx.RequestError as e:
         logger.error("Failed to verify token with auth service", error=str(e))
-        raise HTTPException(status_code=503, detail="Authentication service unavailable")
+        raise HTTPException(
+            status_code=503, detail="Authentication service unavailable"
+        )
     except HTTPException:
         raise
     except Exception as e:
-        logger.error("Unexpected error during token verification", error=str(e))
-        raise HTTPException(status_code=500, detail="Internal authentication error")
+        logger.error(
+            "Unexpected error during token verification", error=str(e)
+        )
+        raise HTTPException(
+            status_code=500, detail="Internal authentication error"
+        )
diff --git a/services/data-service/app/celery_tasks.py b/services/data-service/app/celery_tasks.py
index 8423134..b4a5bbc 100644
--- a/services/data-service/app/celery_tasks.py
+++ b/services/data-service/app/celery_tasks.py
@@ -15,7 +15,11 @@ logger = structlog.get_logger(__name__)
 
 @app.task(bind=True)
 def start_preprocessing_task(
-    self, job_id: int, file_id: int, s3_key: str, preprocessing_params: Dict[str, Any]
+    self,
+    job_id: int,
+    file_id: int,
+    s3_key: str,
+    preprocessing_params: Dict[str, Any],
 ):
     """Celery task for audio preprocessing"""
 
@@ -23,7 +27,12 @@ def start_preprocessing_task(
     asyncio.run(update_job_status(job_id, "running", 0))
 
     try:
-        logger.info("Starting audio preprocessing", job_id=job_id, file_id=file_id, s3_key=s3_key)
+        logger.info(
+            "Starting audio preprocessing",
+            job_id=job_id,
+            file_id=file_id,
+            s3_key=s3_key,
+        )
 
         # Download file from S3
         local_filename = f"temp_{job_id}_{file_id}.wav"
@@ -38,8 +47,12 @@ def start_preprocessing_task(
 
         # Apply preprocessing steps
         processed_audio = y
-        target_sr = preprocessing_params.get("target_sample_rate", settings.target_sample_rate)
-        target_channels = preprocessing_params.get("target_channels", settings.target_channels)
+        target_sr = preprocessing_params.get(
+            "target_sample_rate", settings.target_sample_rate
+        )
+        target_channels = preprocessing_params.get(
+            "target_channels", settings.target_channels
+        )
         steps = preprocessing_params.get("preprocessing_steps", [])
 
         step_progress = 40
@@ -47,9 +60,13 @@ def start_preprocessing_task(
 
         for step in steps:
             if step == "resample" and sr != target_sr:
-                processed_audio = librosa.resample(processed_audio, orig_sr=sr, target_sr=target_sr)
+                processed_audio = librosa.resample(
+                    processed_audio, orig_sr=sr, target_sr=target_sr
+                )
                 sr = target_sr
-                logger.info("Audio resampled", job_id=job_id, target_sr=target_sr)
+                logger.info(
+                    "Audio resampled", job_id=job_id, target_sr=target_sr
+                )
 
             elif step == "channel_conversion":
                 if processed_audio.ndim > 1 and target_channels == 1:
@@ -58,7 +75,9 @@ def start_preprocessing_task(
 
             elif step == "silence_removal":
                 # Trim silence from beginning and end
-                processed_audio, _ = librosa.effects.trim(processed_audio, top_db=20)
+                processed_audio, _ = librosa.effects.trim(
+                    processed_audio, top_db=20
+                )
                 logger.info("Silence trimmed", job_id=job_id)
 
             elif step == "normalize":
@@ -114,11 +133,16 @@ def start_preprocessing_task(
     except Exception as e:
         error_message = str(e)
         logger.error(
-            "Audio preprocessing failed", job_id=job_id, file_id=file_id, error=error_message
+            "Audio preprocessing failed",
+            job_id=job_id,
+            file_id=file_id,
+            error=error_message,
         )
 
         # Update job as failed
-        asyncio.run(update_job_status(job_id, "failed", 0, None, error_message))
+        asyncio.run(
+            update_job_status(job_id, "failed", 0, None, error_message)
+        )
 
         # Cleanup temp files if they exist
         try:
@@ -144,7 +168,11 @@ async def update_job_status(
         # Import here to avoid circular imports
         from app.database import database, processing_jobs
 
-        update_data = {"status": status, "progress": progress, "updated_at": datetime.utcnow()}
+        update_data = {
+            "status": status,
+            "progress": progress,
+            "updated_at": datetime.utcnow(),
+        }
 
         if status == "running" and progress == 0:
             update_data["started_at"] = datetime.utcnow()
@@ -158,9 +186,18 @@ async def update_job_status(
         if error_message:
             update_data["error_message"] = error_message
 
-        query = processing_jobs.update().where(processing_jobs.c.id == job_id).values(**update_data)
+        query = (
+            processing_jobs.update()
+            .where(processing_jobs.c.id == job_id)
+            .values(**update_data)
+        )
 
         await database.execute(query)
 
     except Exception as e:
-        logger.error("Failed to update job status", job_id=job_id, status=status, error=str(e))
+        logger.error(
+            "Failed to update job status",
+            job_id=job_id,
+            status=status,
+            error=str(e),
+        )
diff --git a/services/data-service/app/config.py b/services/data-service/app/config.py
index f78cab9..d1e0ff8 100644
--- a/services/data-service/app/config.py
+++ b/services/data-service/app/config.py
@@ -4,7 +4,9 @@ from typing import List
 
 class Settings(BaseSettings):
     # Database
-    database_url: str = "postgresql://voiceclone:voiceclone@postgres:5432/voiceclone"
+    database_url: str = (
+        "postgresql://voiceclone:voiceclone@postgres:5432/voiceclone"
+    )
 
     # Redis
     redis_url: str = "redis://redis:6379/0"
@@ -29,7 +31,10 @@ class Settings(BaseSettings):
 
     # Service settings
     debug: bool = False
-    allowed_origins: List[str] = ["http://localhost:3000", "http://localhost:8000"]
+    allowed_origins: List[str] = [
+        "http://localhost:3000",
+        "http://localhost:8000",
+    ]
 
     # Celery
     celery_broker_url: str = "redis://redis:6379/0"
diff --git a/services/data-service/app/database.py b/services/data-service/app/database.py
index 1881143..e8a7baf 100644
--- a/services/data-service/app/database.py
+++ b/services/data-service/app/database.py
@@ -38,7 +38,9 @@ voice_files = Table(
     Column("duration", Float, nullable=True),
     Column("sample_rate", Integer, nullable=True),
     Column("channels", Integer, nullable=True),
-    Column("status", String(50), default="pending"),  # pending, processing, processed, failed
+    Column(
+        "status", String(50), default="pending"
+    ),  # pending, processing, processed, failed
     Column("metadata", Text, nullable=True),  # JSON metadata
     Column("created_at", DateTime, default=func.now()),
     Column("updated_at", DateTime, default=func.now(), onupdate=func.now()),
@@ -50,9 +52,15 @@ processing_jobs = Table(
     metadata,
     Column("id", Integer, primary_key=True, index=True),
     Column("user_id", Integer, ForeignKey("users.id"), nullable=False),
-    Column("voice_file_id", Integer, ForeignKey("voice_files.id"), nullable=False),
-    Column("job_type", String(50), nullable=False),  # preprocessing, training, inference
-    Column("status", String(50), default="pending"),  # pending, running, completed, failed
+    Column(
+        "voice_file_id", Integer, ForeignKey("voice_files.id"), nullable=False
+    ),
+    Column(
+        "job_type", String(50), nullable=False
+    ),  # preprocessing, training, inference
+    Column(
+        "status", String(50), default="pending"
+    ),  # pending, running, completed, failed
     Column("progress", Integer, default=0),  # 0-100
     Column("result_data", Text, nullable=True),  # JSON result data
     Column("error_message", Text, nullable=True),
diff --git a/services/data-service/app/routers/process.py b/services/data-service/app/routers/process.py
index fc34d17..1866e49 100644
--- a/services/data-service/app/routers/process.py
+++ b/services/data-service/app/routers/process.py
@@ -4,7 +4,12 @@ from typing import Dict, Any
 import structlog
 
 from app.database import database, voice_files, processing_jobs
-from app.schemas import ProcessingJobCreate, ProcessingResponse, JobType, JobStatus
+from app.schemas import (
+    ProcessingJobCreate,
+    ProcessingResponse,
+    JobType,
+    JobStatus,
+)
 from app.audio_validator import audio_validator
 from app.celery_tasks import start_preprocessing_task
 from app.auth import get_current_user
@@ -16,7 +21,9 @@ security = HTTPBearer()
 
 @router.post("/process/{file_id}", response_model=ProcessingResponse)
 async def start_processing(
-    file_id: int, job_type: JobType, credentials: HTTPAuthorizationCredentials = Depends(security)
+    file_id: int,
+    job_type: JobType,
+    credentials: HTTPAuthorizationCredentials = Depends(security),
 ):
     """Start processing job for uploaded file"""
 
@@ -54,7 +61,9 @@ async def start_processing(
 
     try:
         # Create processing job record
-        job_data = ProcessingJobCreate(user_id=user_id, voice_file_id=file_id, job_type=job_type)
+        job_data = ProcessingJobCreate(
+            user_id=user_id, voice_file_id=file_id, job_type=job_type
+        )
 
         insert_query = processing_jobs.insert().values(**job_data.dict())
         job_id = await database.execute(insert_query)
@@ -63,7 +72,11 @@ async def start_processing(
         if job_type == JobType.PREPROCESSING:
             # Get preprocessing parameters
             metadata = file_record.metadata or {}
-            preprocessing_params = await audio_validator.get_optimal_preprocessing_params(metadata)
+            preprocessing_params = (
+                await audio_validator.get_optimal_preprocessing_params(
+                    metadata
+                )
+            )
 
             # Start Celery task
             task = start_preprocessing_task.delay(
@@ -91,11 +104,15 @@ async def start_processing(
 
         elif job_type == JobType.TRAINING:
             # Training logic will be implemented in Phase 3
-            raise HTTPException(status_code=501, detail="Training jobs not yet implemented")
+            raise HTTPException(
+                status_code=501, detail="Training jobs not yet implemented"
+            )
 
         elif job_type == JobType.INFERENCE:
             # Inference logic will be implemented in Phase 2
-            raise HTTPException(status_code=501, detail="Inference jobs not yet implemented")
+            raise HTTPException(
+                status_code=501, detail="Inference jobs not yet implemented"
+            )
 
         return ProcessingResponse(
             message=f"{job_type.value} job started successfully",
@@ -111,7 +128,9 @@ async def start_processing(
             job_type=job_type,
             error=str(e),
         )
-        raise HTTPException(status_code=500, detail="Failed to start processing job")
+        raise HTTPException(
+            status_code=500, detail="Failed to start processing job"
+        )
 
 
 @router.get("/jobs/{job_id}")
@@ -123,7 +142,8 @@ async def get_job_status(
     user_id = await get_current_user(credentials.credentials)
 
     query = processing_jobs.select().where(
-        (processing_jobs.c.id == job_id) & (processing_jobs.c.user_id == user_id)
+        (processing_jobs.c.id == job_id)
+        & (processing_jobs.c.user_id == user_id)
     )
     job = await database.fetch_one(query)
 
@@ -145,7 +165,9 @@ async def list_user_jobs(
 
     user_id = await get_current_user(credentials.credentials)
 
-    query = processing_jobs.select().where(processing_jobs.c.user_id == user_id)
+    query = processing_jobs.select().where(
+        processing_jobs.c.user_id == user_id
+    )
 
     if job_type:
         query = query.where(processing_jobs.c.job_type == job_type)
@@ -153,22 +175,34 @@ async def list_user_jobs(
     if status:
         query = query.where(processing_jobs.c.status == status)
 
-    query = query.offset(skip).limit(limit).order_by(processing_jobs.c.created_at.desc())
+    query = (
+        query.offset(skip)
+        .limit(limit)
+        .order_by(processing_jobs.c.created_at.desc())
+    )
 
     jobs = await database.fetch_all(query)
 
-    return {"jobs": [dict(job) for job in jobs], "total": len(jobs), "skip": skip, "limit": limit}
+    return {
+        "jobs": [dict(job) for job in jobs],
+        "total": len(jobs),
+        "skip": skip,
+        "limit": limit,
+    }
 
 
 @router.delete("/jobs/{job_id}")
-async def cancel_job(job_id: int, credentials: HTTPAuthorizationCredentials = Depends(security)):
+async def cancel_job(
+    job_id: int, credentials: HTTPAuthorizationCredentials = Depends(security)
+):
     """Cancel processing job"""
 
     user_id = await get_current_user(credentials.credentials)
 
     # Check if job exists and belongs to user
     query = processing_jobs.select().where(
-        (processing_jobs.c.id == job_id) & (processing_jobs.c.user_id == user_id)
+        (processing_jobs.c.id == job_id)
+        & (processing_jobs.c.user_id == user_id)
     )
     job = await database.fetch_one(query)
 
@@ -176,7 +210,10 @@ async def cancel_job(job_id: int, credentials: HTTPAuthorizationCredentials = De
         raise HTTPException(status_code=404, detail="Job not found")
 
     if job.status in ["completed", "failed"]:
-        raise HTTPException(status_code=400, detail=f"Cannot cancel job with status: {job.status}")
+        raise HTTPException(
+            status_code=400,
+            detail=f"Cannot cancel job with status: {job.status}",
+        )
 
     # Cancel Celery task if it exists
     if job.celery_task_id:
@@ -193,7 +230,10 @@ async def cancel_job(job_id: int, credentials: HTTPAuthorizationCredentials = De
     await database.execute(update_query)
 
     logger.info(
-        "Processing job cancelled", user_id=user_id, job_id=job_id, task_id=job.celery_task_id
+        "Processing job cancelled",
+        user_id=user_id,
+        job_id=job_id,
+        task_id=job.celery_task_id,
     )
 
     return {"message": "Job cancelled successfully"}
diff --git a/services/data-service/app/routers/upload.py b/services/data-service/app/routers/upload.py
index 18080ea..1050b84 100644
--- a/services/data-service/app/routers/upload.py
+++ b/services/data-service/app/routers/upload.py
@@ -31,12 +31,16 @@ async def upload_voice_file(
         raise HTTPException(status_code=400, detail="No filename provided")
 
     # Generate unique filename
-    file_extension = file.filename.split(".")[-1] if "." in file.filename else ""
+    file_extension = (
+        file.filename.split(".")[-1] if "." in file.filename else ""
+    )
     unique_filename = f"{uuid.uuid4()}.{file_extension}"
 
     try:
         # Save file temporarily
-        temp_file_path = await local_storage.save_upload(file.file, unique_filename)
+        temp_file_path = await local_storage.save_upload(
+            file.file, unique_filename
+        )
 
         logger.info(
             "File upload started",
@@ -48,7 +52,9 @@ async def upload_voice_file(
 
         # Validate file
         try:
-            metadata = await audio_validator.validate_file_upload(temp_file_path, file.filename)
+            metadata = await audio_validator.validate_file_upload(
+                temp_file_path, file.filename
+            )
         except FileValidationError as e:
             # Clean up temp file
             await local_storage.delete_file(temp_file_path)
@@ -77,9 +83,16 @@ async def upload_voice_file(
             # Clean up temp file
             await local_storage.delete_file(temp_file_path)
 
-            logger.error("S3 upload failed", user_id=user_id, filename=file.filename, error=str(e))
+            logger.error(
+                "S3 upload failed",
+                user_id=user_id,
+                filename=file.filename,
+                error=str(e),
+            )
 
-            raise HTTPException(status_code=500, detail="Failed to upload file to storage")
+            raise HTTPException(
+                status_code=500, detail="Failed to upload file to storage"
+            )
 
         # Save file record to database
         file_data = VoiceFileCreate(
@@ -127,12 +140,16 @@ async def upload_voice_file(
             filename=file.filename,
             error=str(e),
         )
-        raise HTTPException(status_code=500, detail="Internal server error during file upload")
+        raise HTTPException(
+            status_code=500, detail="Internal server error during file upload"
+        )
 
 
 @router.get("/files")
 async def list_user_files(
-    credentials: HTTPAuthorizationCredentials = Depends(security), skip: int = 0, limit: int = 10
+    credentials: HTTPAuthorizationCredentials = Depends(security),
+    skip: int = 0,
+    limit: int = 10,
 ):
     """List user's uploaded files"""
 
@@ -157,7 +174,9 @@ async def list_user_files(
 
 
 @router.delete("/files/{file_id}")
-async def delete_file(file_id: int, credentials: HTTPAuthorizationCredentials = Depends(security)):
+async def delete_file(
+    file_id: int, credentials: HTTPAuthorizationCredentials = Depends(security)
+):
     """Delete user's uploaded file"""
 
     user_id = await get_current_user(credentials.credentials)
@@ -176,7 +195,9 @@ async def delete_file(file_id: int, credentials: HTTPAuthorizationCredentials =
         await s3_storage.delete_file(file_record.s3_key)
 
     # Delete local file if it exists
-    if file_record.file_path and local_storage.file_exists(file_record.file_path):
+    if file_record.file_path and local_storage.file_exists(
+        file_record.file_path
+    ):
         await local_storage.delete_file(file_record.file_path)
 
     # Delete from database
@@ -184,7 +205,10 @@ async def delete_file(file_id: int, credentials: HTTPAuthorizationCredentials =
     await database.execute(delete_query)
 
     logger.info(
-        "File deleted successfully", user_id=user_id, file_id=file_id, filename=file_record.filename
+        "File deleted successfully",
+        user_id=user_id,
+        file_id=file_id,
+        filename=file_record.filename,
     )
 
     return {"message": "File deleted successfully"}
diff --git a/services/data-service/app/storage.py b/services/data-service/app/storage.py
index 89ea598..cc5762e 100644
--- a/services/data-service/app/storage.py
+++ b/services/data-service/app/storage.py
@@ -38,11 +38,17 @@ class S3Storage:
                     logger.info("Created S3 bucket", bucket=self.bucket)
                 except ClientError as create_error:
                     logger.error(
-                        "Failed to create S3 bucket", bucket=self.bucket, error=str(create_error)
+                        "Failed to create S3 bucket",
+                        bucket=self.bucket,
+                        error=str(create_error),
                     )
                     raise
             else:
-                logger.error("Error checking S3 bucket", bucket=self.bucket, error=str(e))
+                logger.error(
+                    "Error checking S3 bucket",
+                    bucket=self.bucket,
+                    error=str(e),
+                )
                 raise
 
     async def upload_file(self, file_path: str, s3_key: str) -> str:
@@ -54,13 +60,21 @@ class S3Storage:
             # Generate URL
             url = f"{settings.s3_endpoint}/{self.bucket}/{s3_key}"
 
-            logger.info("File uploaded to S3", file_path=file_path, s3_key=s3_key, url=url)
+            logger.info(
+                "File uploaded to S3",
+                file_path=file_path,
+                s3_key=s3_key,
+                url=url,
+            )
 
             return url
 
         except ClientError as e:
             logger.error(
-                "Failed to upload file to S3", file_path=file_path, s3_key=s3_key, error=str(e)
+                "Failed to upload file to S3",
+                file_path=file_path,
+                s3_key=s3_key,
+                error=str(e),
             )
             raise
 
@@ -73,7 +87,9 @@ class S3Storage:
             # Download file
             self.client.download_file(self.bucket, s3_key, local_path)
 
-            logger.info("File downloaded from S3", s3_key=s3_key, local_path=local_path)
+            logger.info(
+                "File downloaded from S3", s3_key=s3_key, local_path=local_path
+            )
 
             return local_path
 
@@ -95,7 +111,9 @@ class S3Storage:
             return True
 
         except ClientError as e:
-            logger.error("Failed to delete file from S3", s3_key=s3_key, error=str(e))
+            logger.error(
+                "Failed to delete file from S3", s3_key=s3_key, error=str(e)
+            )
             return False
 
     def file_exists(self, s3_key: str) -> bool:
@@ -135,7 +153,11 @@ class LocalStorage:
             logger.info("File deleted locally", file_path=file_path)
             return True
         except OSError as e:
-            logger.error("Failed to delete local file", file_path=file_path, error=str(e))
+            logger.error(
+                "Failed to delete local file",
+                file_path=file_path,
+                error=str(e),
+            )
             return False
 
     def file_exists(self, file_path: str) -> bool:
diff --git a/services/data-service/tests/conftest.py b/services/data-service/tests/conftest.py
index 722a195..eec858e 100644
--- a/services/data-service/tests/conftest.py
+++ b/services/data-service/tests/conftest.py
@@ -33,7 +33,9 @@ def mock_auth_service():
 def mock_s3_storage():
     """Mock S3 storage operations"""
     with patch("app.storage.s3_storage") as mock:
-        mock.upload_file = AsyncMock(return_value="http://minio:9000/voice-data/test-file.wav")
+        mock.upload_file = AsyncMock(
+            return_value="http://minio:9000/voice-data/test-file.wav"
+        )
         mock.delete_file = AsyncMock(return_value=True)
         mock.file_exists = AsyncMock(return_value=True)
         yield mock
diff --git a/services/inference-service/app/auth.py b/services/inference-service/app/auth.py
index eb713d1..6d46b61 100644
--- a/services/inference-service/app/auth.py
+++ b/services/inference-service/app/auth.py
@@ -14,7 +14,9 @@ async def verify_token(token: str) -> dict:
     """Verify JWT token with auth service"""
     try:
         # First try to decode locally
-        payload = jwt.decode(token, settings.jwt_secret_key, algorithms=[settings.jwt_algorithm])
+        payload = jwt.decode(
+            token, settings.jwt_secret_key, algorithms=[settings.jwt_algorithm]
+        )
         return payload
     except JWTError:
         # If local verification fails, check with auth service
@@ -40,7 +42,9 @@ async def verify_token(token: str) -> dict:
             )
 
 
-async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)) -> dict:
+async def get_current_user(
+    credentials: HTTPAuthorizationCredentials = Depends(security),
+) -> dict:
     """Get current authenticated user"""
     try:
         token = credentials.credentials
@@ -49,7 +53,8 @@ async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(s
         user_id = payload.get("sub")
         if user_id is None:
             raise HTTPException(
-                status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token payload"
+                status_code=status.HTTP_401_UNAUTHORIZED,
+                detail="Invalid token payload",
             )
 
         return {
@@ -59,11 +64,14 @@ async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(s
         }
     except ValueError:
         raise HTTPException(
-            status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid user ID in token"
+            status_code=status.HTTP_401_UNAUTHORIZED,
+            detail="Invalid user ID in token",
         )
 
 
-async def get_optional_user(credentials: HTTPAuthorizationCredentials = Depends(security)) -> dict:
+async def get_optional_user(
+    credentials: HTTPAuthorizationCredentials = Depends(security),
+) -> dict:
     """Get current user if authenticated, otherwise return None"""
     try:
         return await get_current_user(credentials)
diff --git a/services/inference-service/app/config.py b/services/inference-service/app/config.py
index 616c7aa..35728be 100644
--- a/services/inference-service/app/config.py
+++ b/services/inference-service/app/config.py
@@ -28,7 +28,9 @@ class Settings(BaseSettings):
     jwt_algorithm: str = os.getenv("JWT_ALGORITHM", "HS256")
 
     # Auth Service
-    auth_service_url: str = os.getenv("AUTH_SERVICE_URL", "http://auth-service:8001")
+    auth_service_url: str = os.getenv(
+        "AUTH_SERVICE_URL", "http://auth-service:8001"
+    )
 
     # CORS
     cors_origins: List[str] = [
@@ -44,11 +46,17 @@ class Settings(BaseSettings):
 
     # Synthesis settings
     max_text_length: int = int(os.getenv("MAX_TEXT_LENGTH", "1000"))
-    max_audio_duration: int = int(os.getenv("MAX_AUDIO_DURATION", "300"))  # 5 minutes
+    max_audio_duration: int = int(
+        os.getenv("MAX_AUDIO_DURATION", "300")
+    )  # 5 minutes
 
     # Performance
-    synthesis_timeout: int = int(os.getenv("SYNTHESIS_TIMEOUT", "60"))  # 60 seconds
-    model_load_timeout: int = int(os.getenv("MODEL_LOAD_TIMEOUT", "120"))  # 2 minutes
+    synthesis_timeout: int = int(
+        os.getenv("SYNTHESIS_TIMEOUT", "60")
+    )  # 60 seconds
+    model_load_timeout: int = int(
+        os.getenv("MODEL_LOAD_TIMEOUT", "120")
+    )  # 2 minutes
 
 
 @lru_cache()
diff --git a/services/inference-service/app/database.py b/services/inference-service/app/database.py
index de7d095..d1536b4 100644
--- a/services/inference-service/app/database.py
+++ b/services/inference-service/app/database.py
@@ -33,7 +33,9 @@ voice_models = Table(
     Column("description", Text),
     Column("model_type", String(50), nullable=False, default="tacotron2"),
     Column("language", String(10), nullable=False, default="en"),
-    Column("status", String(20), nullable=False, default="training"),  # training, ready, failed
+    Column(
+        "status", String(20), nullable=False, default="training"
+    ),  # training, ready, failed
     Column("model_path", String(500)),  # S3 path to model files
     Column("config_data", Text),  # JSON config for model
     Column("training_data_size", Integer, default=0),
diff --git a/services/inference-service/app/main.py b/services/inference-service/app/main.py
index 075b9d0..b3b250b 100644
--- a/services/inference-service/app/main.py
+++ b/services/inference-service/app/main.py
@@ -33,7 +33,9 @@ logger = structlog.get_logger()
 @asynccontextmanager
 async def lifespan(app: FastAPI):
     """Application lifespan management"""
-    logger.info("Starting voice inference service", service="inference-service")
+    logger.info(
+        "Starting voice inference service", service="inference-service"
+    )
 
     # Create database tables
     metadata.create_all(bind=engine)
@@ -89,4 +91,8 @@ async def health_check():
 @app.get("/")
 async def root():
     """Root endpoint"""
-    return {"service": "voice-cloning-inference", "version": "1.0.0", "status": "running"}
+    return {
+        "service": "voice-cloning-inference",
+        "version": "1.0.0",
+        "status": "running",
+    }
diff --git a/services/inference-service/app/routers/models.py b/services/inference-service/app/routers/models.py
index d8e7b74..ce15862 100644
--- a/services/inference-service/app/routers/models.py
+++ b/services/inference-service/app/routers/models.py
@@ -51,12 +51,18 @@ async def get_user_models(
 ):
     """Get user's voice models"""
 
-    query = voice_models.select().where(voice_models.c.user_id == current_user["id"])
+    query = voice_models.select().where(
+        voice_models.c.user_id == current_user["id"]
+    )
 
     if status_filter:
         query = query.where(voice_models.c.status == status_filter)
 
-    query = query.order_by(voice_models.c.created_at.desc()).limit(limit).offset(offset)
+    query = (
+        query.order_by(voice_models.c.created_at.desc())
+        .limit(limit)
+        .offset(offset)
+    )
 
     models = await database.fetch_all(query)
 
@@ -79,17 +85,23 @@ async def get_user_models(
 
 
 @router.get("/models/{model_id}", response_model=VoiceModelResponse)
-async def get_model_details(model_id: int, current_user: dict = Depends(get_current_user)):
+async def get_model_details(
+    model_id: int, current_user: dict = Depends(get_current_user)
+):
     """Get voice model details"""
 
     query = voice_models.select().where(
-        (voice_models.c.id == model_id) & (voice_models.c.user_id == current_user["id"])
+        (voice_models.c.id == model_id)
+        & (voice_models.c.user_id == current_user["id"])
     )
 
     model = await database.fetch_one(query)
 
     if not model:
-        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Voice model not found")
+        raise HTTPException(
+            status_code=status.HTTP_404_NOT_FOUND,
+            detail="Voice model not found",
+        )
 
     return VoiceModelResponse(
         id=model["id"],
@@ -107,7 +119,9 @@ async def get_model_details(model_id: int, current_user: dict = Depends(get_curr
 
 
 @router.post("/models/{model_id}/preload")
-async def preload_model(model_id: int, current_user: dict = Depends(get_current_user)):
+async def preload_model(
+    model_id: int, current_user: dict = Depends(get_current_user)
+):
     """Preload model into cache"""
 
     # Verify model ownership and readiness
@@ -121,7 +135,8 @@ async def preload_model(model_id: int, current_user: dict = Depends(get_current_
 
     if not model_result:
         raise HTTPException(
-            status_code=status.HTTP_404_NOT_FOUND, detail="Voice model not found or not ready"
+            status_code=status.HTTP_404_NOT_FOUND,
+            detail="Voice model not found or not ready",
         )
 
     try:
@@ -133,12 +148,22 @@ async def preload_model(model_id: int, current_user: dict = Depends(get_current_
 
         await model_manager.preload_model(model_id, model_config)
 
-        logger.info("Model preloaded successfully", model_id=model_id, user_id=current_user["id"])
+        logger.info(
+            "Model preloaded successfully",
+            model_id=model_id,
+            user_id=current_user["id"],
+        )
 
-        return {"message": "Model preloaded successfully", "model_id": model_id, "status": "cached"}
+        return {
+            "message": "Model preloaded successfully",
+            "model_id": model_id,
+            "status": "cached",
+        }
 
     except Exception as e:
-        logger.error("Failed to preload model", model_id=model_id, error=str(e))
+        logger.error(
+            "Failed to preload model", model_id=model_id, error=str(e)
+        )
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
             detail=f"Failed to preload model: {str(e)}",
@@ -146,18 +171,24 @@ async def preload_model(model_id: int, current_user: dict = Depends(get_current_
 
 
 @router.get("/models/{model_id}/usage", response_model=ModelUsageStats)
-async def get_model_usage_stats(model_id: int, current_user: dict = Depends(get_current_user)):
+async def get_model_usage_stats(
+    model_id: int, current_user: dict = Depends(get_current_user)
+):
     """Get model usage statistics"""
 
     # Verify model ownership
     model_query = voice_models.select().where(
-        (voice_models.c.id == model_id) & (voice_models.c.user_id == current_user["id"])
+        (voice_models.c.id == model_id)
+        & (voice_models.c.user_id == current_user["id"])
     )
 
     model_result = await database.fetch_one(model_query)
 
     if not model_result:
-        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Voice model not found")
+        raise HTTPException(
+            status_code=status.HTTP_404_NOT_FOUND,
+            detail="Voice model not found",
+        )
 
     # Get usage stats
     stats_query = model_usage_stats.select().where(
@@ -191,7 +222,10 @@ async def get_ready_models(current_user: dict = Depends(get_current_user)):
 
     query = (
         voice_models.select()
-        .where((voice_models.c.user_id == current_user["id"]) & (voice_models.c.status == "ready"))
+        .where(
+            (voice_models.c.user_id == current_user["id"])
+            & (voice_models.c.status == "ready")
+        )
         .order_by(voice_models.c.created_at.desc())
     )
 
@@ -272,5 +306,6 @@ async def clear_model_cache(current_user: dict = Depends(get_current_user)):
     except Exception as e:
         logger.error("Failed to clear cache", error=str(e))
         raise HTTPException(
-            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to clear model cache"
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail="Failed to clear model cache",
         )
diff --git a/services/inference-service/app/routers/synthesis.py b/services/inference-service/app/routers/synthesis.py
index 140c0dd..6ea07fd 100644
--- a/services/inference-service/app/routers/synthesis.py
+++ b/services/inference-service/app/routers/synthesis.py
@@ -17,17 +17,31 @@ router = APIRouter()
 
 
 class SynthesisRequest(BaseModel):
-    text: str = Field(..., min_length=1, max_length=1000, description="Text to synthesize")
+    text: str = Field(
+        ..., min_length=1, max_length=1000, description="Text to synthesize"
+    )
     model_id: int = Field(..., description="Voice model ID to use")
-    speed: Optional[float] = Field(default=1.0, ge=0.5, le=2.0, description="Speech speed")
-    pitch: Optional[float] = Field(default=1.0, ge=0.5, le=2.0, description="Speech pitch")
-    volume: Optional[float] = Field(default=1.0, ge=0.1, le=2.0, description="Speech volume")
-    emotion: Optional[str] = Field(default="neutral", description="Emotion style")
-    return_audio: bool = Field(default=False, description="Return audio data directly")
+    speed: Optional[float] = Field(
+        default=1.0, ge=0.5, le=2.0, description="Speech speed"
+    )
+    pitch: Optional[float] = Field(
+        default=1.0, ge=0.5, le=2.0, description="Speech pitch"
+    )
+    volume: Optional[float] = Field(
+        default=1.0, ge=0.1, le=2.0, description="Speech volume"
+    )
+    emotion: Optional[str] = Field(
+        default="neutral", description="Emotion style"
+    )
+    return_audio: bool = Field(
+        default=False, description="Return audio data directly"
+    )
 
 
 class BatchSynthesisRequest(BaseModel):
-    texts: List[str] = Field(..., max_items=10, description="List of texts to synthesize")
+    texts: List[str] = Field(
+        ..., max_items=10, description="List of texts to synthesize"
+    )
     model_id: int = Field(..., description="Voice model ID to use")
     speed: Optional[float] = Field(default=1.0, ge=0.5, le=2.0)
     pitch: Optional[float] = Field(default=1.0, ge=0.5, le=2.0)
@@ -65,7 +79,8 @@ async def synthesize_voice(
 
     if not model_result:
         raise HTTPException(
-            status_code=status.HTTP_404_NOT_FOUND, detail="Voice model not found or not ready"
+            status_code=status.HTTP_404_NOT_FOUND,
+            detail="Voice model not found or not ready",
         )
 
     # Create synthesis job record
@@ -121,7 +136,9 @@ async def synthesize_voice(
 
             # Upload audio to S3
             audio_key = f"synthesized/{current_user['id']}/{job_id}.wav"
-            audio_url = await s3_storage.upload_audio(synthesis_result["audio_data"], audio_key)
+            audio_url = await s3_storage.upload_audio(
+                synthesis_result["audio_data"], audio_key
+            )
 
             # Update job with results
             await database.execute(
@@ -153,7 +170,11 @@ async def synthesize_voice(
             await database.execute(
                 synthesis_jobs.update()
                 .where(synthesis_jobs.c.id == job_id)
-                .values(status="failed", error_message=str(e), completed_at=datetime.utcnow())
+                .values(
+                    status="failed",
+                    error_message=str(e),
+                    completed_at=datetime.utcnow(),
+                )
             )
             raise HTTPException(
                 status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
@@ -198,7 +219,8 @@ async def batch_synthesize_voice(
 
     if not model_result:
         raise HTTPException(
-            status_code=status.HTTP_404_NOT_FOUND, detail="Voice model not found or not ready"
+            status_code=status.HTTP_404_NOT_FOUND,
+            detail="Voice model not found or not ready",
         )
 
     # Create synthesis jobs for each text
@@ -229,25 +251,36 @@ async def batch_synthesize_voice(
         current_user["id"],
     )
 
-    return {"job_ids": job_ids, "status": "pending", "batch_size": len(request.texts)}
+    return {
+        "job_ids": job_ids,
+        "status": "pending",
+        "batch_size": len(request.texts),
+    }
 
 
 @router.get("/synthesize/audio/{job_id}")
-async def get_synthesis_audio(job_id: int, current_user: dict = Depends(get_current_user)):
+async def get_synthesis_audio(
+    job_id: int, current_user: dict = Depends(get_current_user)
+):
     """Get synthesized audio file"""
 
     # Get job details
     job_query = synthesis_jobs.select().where(
-        (synthesis_jobs.c.id == job_id) & (synthesis_jobs.c.user_id == current_user["id"])
+        (synthesis_jobs.c.id == job_id)
+        & (synthesis_jobs.c.user_id == current_user["id"])
     )
     job = await database.fetch_one(job_query)
 
     if not job:
-        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Synthesis job not found")
+        raise HTTPException(
+            status_code=status.HTTP_404_NOT_FOUND,
+            detail="Synthesis job not found",
+        )
 
     if job["status"] != "completed" or not job["audio_url"]:
         raise HTTPException(
-            status_code=status.HTTP_400_BAD_REQUEST, detail="Audio not ready or synthesis failed"
+            status_code=status.HTTP_400_BAD_REQUEST,
+            detail="Audio not ready or synthesis failed",
         )
 
     # Stream audio file
@@ -256,24 +289,31 @@ async def get_synthesis_audio(job_id: int, current_user: dict = Depends(get_curr
         audio_key = job["audio_url"].split("/")[-1]
 
         # For demonstration, return mock audio
-        mock_audio = b"RIFF\x24\x08\x00\x00WAVEfmt \x10\x00\x00\x00" + b"\x00" * 1000
+        mock_audio = (
+            b"RIFF\x24\x08\x00\x00WAVEfmt \x10\x00\x00\x00" + b"\x00" * 1000
+        )
 
         return StreamingResponse(
             io.BytesIO(mock_audio),
             media_type="audio/wav",
-            headers={"Content-Disposition": f"attachment; filename=synthesis_{job_id}.wav"},
+            headers={
+                "Content-Disposition": f"attachment; filename=synthesis_{job_id}.wav"
+            },
         )
 
     except Exception as e:
         logger.error("Failed to stream audio", job_id=job_id, error=str(e))
         raise HTTPException(
-            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve audio"
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail="Failed to retrieve audio",
         )
 
 
 @router.get("/jobs", response_model=List[SynthesisResponse])
 async def get_synthesis_jobs(
-    current_user: dict = Depends(get_current_user), limit: int = 20, offset: int = 0
+    current_user: dict = Depends(get_current_user),
+    limit: int = 20,
+    offset: int = 0,
 ):
     """Get user's synthesis jobs"""
 
@@ -304,17 +344,23 @@ async def get_synthesis_jobs(
 
 
 @router.get("/jobs/{job_id}", response_model=SynthesisResponse)
-async def get_synthesis_job(job_id: int, current_user: dict = Depends(get_current_user)):
+async def get_synthesis_job(
+    job_id: int, current_user: dict = Depends(get_current_user)
+):
     """Get synthesis job details"""
 
     query = synthesis_jobs.select().where(
-        (synthesis_jobs.c.id == job_id) & (synthesis_jobs.c.user_id == current_user["id"])
+        (synthesis_jobs.c.id == job_id)
+        & (synthesis_jobs.c.user_id == current_user["id"])
     )
 
     job = await database.fetch_one(query)
 
     if not job:
-        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Synthesis job not found")
+        raise HTTPException(
+            status_code=status.HTTP_404_NOT_FOUND,
+            detail="Synthesis job not found",
+        )
 
     return SynthesisResponse(
         job_id=job["id"],
@@ -330,13 +376,19 @@ async def get_synthesis_job(job_id: int, current_user: dict = Depends(get_curren
 
 
 async def process_synthesis_job(
-    job_id: int, model_id: int, model_result, request: SynthesisRequest, user_id: int
+    job_id: int,
+    model_id: int,
+    model_result,
+    request: SynthesisRequest,
+    user_id: int,
 ):
     """Background task to process synthesis job"""
     try:
         # Update job status
         await database.execute(
-            synthesis_jobs.update().where(synthesis_jobs.c.id == job_id).values(status="processing")
+            synthesis_jobs.update()
+            .where(synthesis_jobs.c.id == job_id)
+            .values(status="processing")
         )
 
         # Perform synthesis
@@ -360,7 +412,9 @@ async def process_synthesis_job(
 
         # Upload audio to S3
         audio_key = f"synthesized/{user_id}/{job_id}.wav"
-        audio_url = await s3_storage.upload_audio(synthesis_result["audio_data"], audio_key)
+        audio_url = await s3_storage.upload_audio(
+            synthesis_result["audio_data"], audio_key
+        )
 
         # Update job with results
         await database.execute(
@@ -382,16 +436,26 @@ async def process_synthesis_job(
         await database.execute(
             synthesis_jobs.update()
             .where(synthesis_jobs.c.id == job_id)
-            .values(status="failed", error_message=str(e), completed_at=datetime.utcnow())
+            .values(
+                status="failed",
+                error_message=str(e),
+                completed_at=datetime.utcnow(),
+            )
         )
         logger.error("Synthesis job failed", job_id=job_id, error=str(e))
 
 
 async def process_batch_synthesis(
-    job_ids: List[int], model_id: int, model_result, request: BatchSynthesisRequest, user_id: int
+    job_ids: List[int],
+    model_id: int,
+    model_result,
+    request: BatchSynthesisRequest,
+    user_id: int,
 ):
     """Background task to process batch synthesis"""
-    logger.info("Starting batch synthesis", job_ids=job_ids, batch_size=len(job_ids))
+    logger.info(
+        "Starting batch synthesis", job_ids=job_ids, batch_size=len(job_ids)
+    )
 
     for i, job_id in enumerate(job_ids):
         try:
@@ -410,4 +474,6 @@ async def process_batch_synthesis(
                 user_id,
             )
         except Exception as e:
-            logger.error("Batch synthesis item failed", job_id=job_id, error=str(e))
+            logger.error(
+                "Batch synthesis item failed", job_id=job_id, error=str(e)
+            )
diff --git a/services/inference-service/app/services/model_manager.py b/services/inference-service/app/services/model_manager.py
index cc50698..f37d9de 100644
--- a/services/inference-service/app/services/model_manager.py
+++ b/services/inference-service/app/services/model_manager.py
@@ -37,7 +37,9 @@ class MockVoiceModel:
         t = np.linspace(0, duration, int(sample_rate * duration))
 
         # Create a simple sine wave with varying frequency
-        frequency = 440 + (hash(text) % 200)  # Base frequency with text-based variation
+        frequency = 440 + (
+            hash(text) % 200
+        )  # Base frequency with text-based variation
         audio = np.sin(2 * np.pi * frequency * t) * 0.3
 
         # Convert to 16-bit PCM
@@ -67,19 +69,30 @@ class ModelManager:
 
     async def initialize(self):
         """Initialize the model manager"""
-        logger.info("Initializing model manager", max_cache_size=settings.max_model_cache_size)
-
-    async def get_model(self, model_id: int, model_config: Dict[str, Any]) -> MockVoiceModel:
+        logger.info(
+            "Initializing model manager",
+            max_cache_size=settings.max_model_cache_size,
+        )
+
+    async def get_model(
+        self, model_id: int, model_config: Dict[str, Any]
+    ) -> MockVoiceModel:
         """Get a model from cache or load it"""
 
         # Check if model is already cached and not expired
         if model_id in self.model_cache:
             cache_age = time.time() - self.cache_timestamps[model_id]
             if cache_age < settings.model_cache_ttl:
-                logger.debug("Model cache hit", model_id=model_id, cache_age=cache_age)
+                logger.debug(
+                    "Model cache hit", model_id=model_id, cache_age=cache_age
+                )
                 return self.model_cache[model_id]
             else:
-                logger.debug("Model cache expired", model_id=model_id, cache_age=cache_age)
+                logger.debug(
+                    "Model cache expired",
+                    model_id=model_id,
+                    cache_age=cache_age,
+                )
                 await self._unload_model(model_id)
 
         # Get or create loading lock for this model
@@ -103,15 +116,21 @@ class ModelManager:
             self.cache_timestamps[model_id] = time.time()
 
             logger.info(
-                "Model loaded successfully", model_id=model_id, cache_size=len(self.model_cache)
+                "Model loaded successfully",
+                model_id=model_id,
+                cache_size=len(self.model_cache),
             )
 
             return model
 
-    async def _load_model(self, model_id: int, model_config: Dict[str, Any]) -> MockVoiceModel:
+    async def _load_model(
+        self, model_id: int, model_config: Dict[str, Any]
+    ) -> MockVoiceModel:
         """Load a model from storage"""
         try:
-            model_path = model_config.get("model_path", f"models/model_{model_id}")
+            model_path = model_config.get(
+                "model_path", f"models/model_{model_id}"
+            )
 
             # Simulate model loading time
             await asyncio.sleep(0.5)
@@ -122,7 +141,9 @@ class ModelManager:
             return model
 
         except Exception as e:
-            logger.error("Failed to load model", model_id=model_id, error=str(e))
+            logger.error(
+                "Failed to load model", model_id=model_id, error=str(e)
+            )
             raise RuntimeError(f"Failed to load model {model_id}: {str(e)}")
 
     async def _unload_model(self, model_id: int):
@@ -141,7 +162,8 @@ class ModelManager:
         while len(self.model_cache) >= settings.max_model_cache_size:
             # Find oldest model to evict
             oldest_model_id = min(
-                self.cache_timestamps.keys(), key=lambda k: self.cache_timestamps[k]
+                self.cache_timestamps.keys(),
+                key=lambda k: self.cache_timestamps[k],
             )
             await self._unload_model(oldest_model_id)
             logger.debug("Evicted model from cache", model_id=oldest_model_id)
@@ -152,7 +174,9 @@ class ModelManager:
             await self.get_model(model_id, model_config)
             logger.info("Model preloaded", model_id=model_id)
         except Exception as e:
-            logger.error("Failed to preload model", model_id=model_id, error=str(e))
+            logger.error(
+                "Failed to preload model", model_id=model_id, error=str(e)
+            )
 
     async def get_cache_stats(self) -> Dict[str, Any]:
         """Get cache statistics"""
@@ -162,7 +186,9 @@ class ModelManager:
             "cache_ttl": settings.model_cache_ttl,
             "model_ids": list(self.model_cache.keys()),
             "oldest_cache_age": (
-                time.time() - min(self.cache_timestamps.values()) if self.cache_timestamps else 0
+                time.time() - min(self.cache_timestamps.values())
+                if self.cache_timestamps
+                else 0
             ),
         }
 
diff --git a/services/inference-service/app/services/synthesis_engine.py b/services/inference-service/app/services/synthesis_engine.py
index 8913930..75f36e2 100644
--- a/services/inference-service/app/services/synthesis_engine.py
+++ b/services/inference-service/app/services/synthesis_engine.py
@@ -30,7 +30,9 @@ class SynthesisEngine:
 
         # Validate input
         if len(text) > settings.max_text_length:
-            raise ValueError(f"Text too long: {len(text)} > {settings.max_text_length}")
+            raise ValueError(
+                f"Text too long: {len(text)} > {settings.max_text_length}"
+            )
 
         if not text.strip():
             raise ValueError("Text cannot be empty")
@@ -67,7 +69,8 @@ class SynthesisEngine:
 
             # Perform synthesis
             audio_data = await asyncio.wait_for(
-                model.synthesize(text, **synthesis_params), timeout=settings.synthesis_timeout
+                model.synthesize(text, **synthesis_params),
+                timeout=settings.synthesis_timeout,
             )
 
             processing_time = time.time() - start_time
@@ -92,13 +95,22 @@ class SynthesisEngine:
             }
 
         except asyncio.TimeoutError:
-            logger.error("Synthesis timeout", job_id=job_id, timeout=settings.synthesis_timeout)
-            raise RuntimeError(f"Synthesis timed out after {settings.synthesis_timeout} seconds")
+            logger.error(
+                "Synthesis timeout",
+                job_id=job_id,
+                timeout=settings.synthesis_timeout,
+            )
+            raise RuntimeError(
+                f"Synthesis timed out after {settings.synthesis_timeout} seconds"
+            )
 
         except Exception as e:
             processing_time = time.time() - start_time
             logger.error(
-                "Synthesis failed", job_id=job_id, error=str(e), processing_time=processing_time
+                "Synthesis failed",
+                job_id=job_id,
+                error=str(e),
+                processing_time=processing_time,
             )
             raise RuntimeError(f"Synthesis failed: {str(e)}")
 
@@ -116,7 +128,10 @@ class SynthesisEngine:
             raise ValueError("Batch size too large: maximum 10 texts")
 
         logger.info(
-            "Starting batch synthesis", batch_size=len(texts), model_id=model_id, user_id=user_id
+            "Starting batch synthesis",
+            batch_size=len(texts),
+            model_id=model_id,
+            user_id=user_id,
         )
 
         # Create synthesis tasks
@@ -140,11 +155,21 @@ class SynthesisEngine:
             for i, result in enumerate(results):
                 if isinstance(result, Exception):
                     batch_results.append(
-                        {"index": i, "text": texts[i], "error": str(result), "success": False}
+                        {
+                            "index": i,
+                            "text": texts[i],
+                            "error": str(result),
+                            "success": False,
+                        }
                     )
                 else:
                     batch_results.append(
-                        {"index": i, "text": texts[i], "result": result, "success": True}
+                        {
+                            "index": i,
+                            "text": texts[i],
+                            "result": result,
+                            "success": True,
+                        }
                     )
 
             logger.info(
diff --git a/services/inference-service/app/storage.py b/services/inference-service/app/storage.py
index 7078c18..d92fa5b 100644
--- a/services/inference-service/app/storage.py
+++ b/services/inference-service/app/storage.py
@@ -48,12 +48,18 @@ class S3Storage:
     async def download_model(self, key: str, local_path: str) -> bool:
         """Download model file from S3"""
         try:
-            self.client.download_file(Bucket=self.bucket_name, Key=key, Filename=local_path)
-            logger.info("Model downloaded from S3", key=key, local_path=local_path)
+            self.client.download_file(
+                Bucket=self.bucket_name, Key=key, Filename=local_path
+            )
+            logger.info(
+                "Model downloaded from S3", key=key, local_path=local_path
+            )
             return True
 
         except (ClientError, BotoCoreError) as e:
-            logger.error("Failed to download model from S3", key=key, error=str(e))
+            logger.error(
+                "Failed to download model from S3", key=key, error=str(e)
+            )
             return False
 
     async def file_exists(self, key: str) -> bool:
@@ -71,13 +77,17 @@ class S3Storage:
             logger.info("File deleted from S3", key=key)
             return True
         except (ClientError, BotoCoreError) as e:
-            logger.error("Failed to delete file from S3", key=key, error=str(e))
+            logger.error(
+                "Failed to delete file from S3", key=key, error=str(e)
+            )
             return False
 
     async def list_model_files(self, prefix: str) -> list[str]:
         """List model files with given prefix"""
         try:
-            response = self.client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)
+            response = self.client.list_objects_v2(
+                Bucket=self.bucket_name, Prefix=prefix
+            )
 
             files = []
             if "Contents" in response:
@@ -86,7 +96,9 @@ class S3Storage:
             return files
 
         except (ClientError, BotoCoreError) as e:
-            logger.error("Failed to list model files", prefix=prefix, error=str(e))
+            logger.error(
+                "Failed to list model files", prefix=prefix, error=str(e)
+            )
             return []
 
 
@@ -105,11 +117,15 @@ class LocalStorage:
             async with aiofiles.open(file_path, "wb") as f:
                 await f.write(audio_data)
 
-            logger.debug("Temp audio saved", file_path=file_path, size=len(audio_data))
+            logger.debug(
+                "Temp audio saved", file_path=file_path, size=len(audio_data)
+            )
             return file_path
 
         except Exception as e:
-            logger.error("Failed to save temp audio", filename=filename, error=str(e))
+            logger.error(
+                "Failed to save temp audio", filename=filename, error=str(e)
+            )
             raise RuntimeError(f"Failed to save temporary audio: {str(e)}")
 
     async def read_temp_file(self, file_path: str) -> Optional[bytes]:
@@ -124,7 +140,9 @@ class LocalStorage:
             return data
 
         except Exception as e:
-            logger.error("Failed to read temp file", file_path=file_path, error=str(e))
+            logger.error(
+                "Failed to read temp file", file_path=file_path, error=str(e)
+            )
             return None
 
     async def cleanup_temp_file(self, file_path: str) -> bool:
@@ -135,7 +153,11 @@ class LocalStorage:
                 logger.debug("Temp file cleaned up", file_path=file_path)
             return True
         except Exception as e:
-            logger.error("Failed to cleanup temp file", file_path=file_path, error=str(e))
+            logger.error(
+                "Failed to cleanup temp file",
+                file_path=file_path,
+                error=str(e),
+            )
             return False
 
 
diff --git a/services/inference-service/tests/conftest.py b/services/inference-service/tests/conftest.py
index 927f6c9..6412ed5 100644
--- a/services/inference-service/tests/conftest.py
+++ b/services/inference-service/tests/conftest.py
@@ -25,7 +25,11 @@ def client():
 def mock_auth_service():
     """Mock authentication service responses"""
     with patch("app.auth.get_current_user") as mock:
-        mock.return_value = {"id": 1, "username": "testuser", "email": "test@example.com"}
+        mock.return_value = {
+            "id": 1,
+            "username": "testuser",
+            "email": "test@example.com",
+        }
         yield mock
 
 
@@ -43,7 +47,9 @@ def mock_database():
 def mock_s3_storage():
     """Mock S3 storage operations"""
     with patch("app.storage.s3_storage") as mock:
-        mock.upload_audio = AsyncMock(return_value="http://minio:9000/voice-models/test-audio.wav")
+        mock.upload_audio = AsyncMock(
+            return_value="http://minio:9000/voice-models/test-audio.wav"
+        )
         mock.delete_file = AsyncMock(return_value=True)
         mock.file_exists = AsyncMock(return_value=True)
         yield mock
diff --git a/services/inference-service/tests/test_synthesis.py b/services/inference-service/tests/test_synthesis.py
index 8a45bc1..7b87033 100644
--- a/services/inference-service/tests/test_synthesis.py
+++ b/services/inference-service/tests/test_synthesis.py
@@ -17,7 +17,9 @@ class TestSynthesisEndpoints:
 
     def test_synthesis_without_auth(self, client):
         """Test synthesis endpoint without authentication"""
-        response = client.post("/api/v1/synthesize", json={"text": "Hello world", "model_id": 1})
+        response = client.post(
+            "/api/v1/synthesize", json={"text": "Hello world", "model_id": 1}
+        )
         assert response.status_code == 403  # Unauthorized
 
     def test_synthesis_with_auth(
@@ -36,7 +38,11 @@ class TestSynthesisEndpoints:
         mock_database.execute.return_value = 1  # job_id
 
         headers = {"Authorization": "Bearer fake-token"}
-        response = client.post("/api/v1/synthesize", json=sample_synthesis_request, headers=headers)
+        response = client.post(
+            "/api/v1/synthesize",
+            json=sample_synthesis_request,
+            headers=headers,
+        )
 
         assert response.status_code == 200
         data = response.json()
@@ -44,14 +50,18 @@ class TestSynthesisEndpoints:
         assert data["model_id"] == 1
         assert data["text"] == sample_synthesis_request["text"]
 
-    def test_synthesis_model_not_found(self, client, mock_auth_service, mock_database):
+    def test_synthesis_model_not_found(
+        self, client, mock_auth_service, mock_database
+    ):
         """Test synthesis with non-existent model"""
         # Mock database to return None (model not found)
         mock_database.fetch_one.return_value = None
 
         headers = {"Authorization": "Bearer fake-token"}
         response = client.post(
-            "/api/v1/synthesize", json={"text": "Hello world", "model_id": 999}, headers=headers
+            "/api/v1/synthesize",
+            json={"text": "Hello world", "model_id": 999},
+            headers=headers,
         )
 
         assert response.status_code == 404
@@ -84,7 +94,9 @@ class TestSynthesisEndpoints:
         assert "audio_url" in data
         assert "processing_time" in data
 
-    def test_batch_synthesis(self, client, mock_auth_service, mock_database, sample_voice_model):
+    def test_batch_synthesis(
+        self, client, mock_auth_service, mock_database, sample_voice_model
+    ):
         """Test batch synthesis"""
         # Mock database responses
         mock_database.fetch_one.return_value = sample_voice_model
@@ -103,7 +115,9 @@ class TestSynthesisEndpoints:
         assert data["batch_size"] == 2
         assert len(data["job_ids"]) == 2
 
-    def test_get_synthesis_jobs(self, client, mock_auth_service, mock_database):
+    def test_get_synthesis_jobs(
+        self, client, mock_auth_service, mock_database
+    ):
         """Test getting synthesis jobs"""
         # Mock database response
         mock_database.fetch_all.return_value = [
@@ -129,7 +143,9 @@ class TestSynthesisEndpoints:
         assert data[0]["job_id"] == 1
         assert data[0]["status"] == "completed"
 
-    def test_get_synthesis_job_by_id(self, client, mock_auth_service, mock_database):
+    def test_get_synthesis_job_by_id(
+        self, client, mock_auth_service, mock_database
+    ):
         """Test getting specific synthesis job"""
         # Mock database response
         mock_database.fetch_one.return_value = {
@@ -152,7 +168,9 @@ class TestSynthesisEndpoints:
         assert data["job_id"] == 1
         assert data["status"] == "completed"
 
-    def test_get_synthesis_job_not_found(self, client, mock_auth_service, mock_database):
+    def test_get_synthesis_job_not_found(
+        self, client, mock_auth_service, mock_database
+    ):
         """Test getting non-existent synthesis job"""
         # Mock database to return None
         mock_database.fetch_one.return_value = None
@@ -169,14 +187,19 @@ class TestSynthesisEndpoints:
 
         # Test empty text
         response = client.post(
-            "/api/v1/synthesize", json={"text": "", "model_id": 1}, headers=headers
+            "/api/v1/synthesize",
+            json={"text": "", "model_id": 1},
+            headers=headers,
         )
         assert response.status_code == 422
 
         # Test text too long
         response = client.post(
             "/api/v1/synthesize",
-            json={"text": "x" * 1001, "model_id": 1},  # Over the 1000 character limit
+            json={
+                "text": "x" * 1001,
+                "model_id": 1,
+            },  # Over the 1000 character limit
             headers=headers,
         )
         assert response.status_code == 422
@@ -184,7 +207,11 @@ class TestSynthesisEndpoints:
         # Test invalid speed
         response = client.post(
             "/api/v1/synthesize",
-            json={"text": "Hello world", "model_id": 1, "speed": 3.0},  # Over the 2.0 limit
+            json={
+                "text": "Hello world",
+                "model_id": 1,
+                "speed": 3.0,
+            },  # Over the 2.0 limit
             headers=headers,
         )
         assert response.status_code == 422
diff --git a/services/training-worker/main.py b/services/training-worker/main.py
index f8d60dc..8ffd08f 100644
--- a/services/training-worker/main.py
+++ b/services/training-worker/main.py
@@ -91,7 +91,11 @@ def train_voice_model(self, task_data: Dict[str, Any]):
         # Update task status
         self.update_state(
             state=TrainingStatus.STARTED,
-            meta={"progress": 0, "status": "Training started", "current_step": "initializing"},
+            meta={
+                "progress": 0,
+                "status": "Training started",
+                "current_step": "initializing",
+            },
         )
 
         # Step 1: Preprocess training data
@@ -105,7 +109,9 @@ def train_voice_model(self, task_data: Dict[str, Any]):
             },
         )
 
-        processed_data = preprocess_training_data(task.training_data_urls, task.config)
+        processed_data = preprocess_training_data(
+            task.training_data_urls, task.config
+        )
 
         # Step 2: Initialize training
         self.update_state(
@@ -138,7 +144,9 @@ def train_voice_model(self, task_data: Dict[str, Any]):
             )
 
             # Simulate training step
-            train_step_result = simulate_training_step(processed_data, model_config, epoch)
+            train_step_result = simulate_training_step(
+                processed_data, model_config, epoch
+            )
 
             # Check for early stopping or cancellation
             if self.is_aborted():
@@ -156,16 +164,24 @@ def train_voice_model(self, task_data: Dict[str, Any]):
             },
         )
 
-        evaluation_metrics = evaluate_trained_model(processed_data, model_config)
+        evaluation_metrics = evaluate_trained_model(
+            processed_data, model_config
+        )
         task.model_metrics = evaluation_metrics
 
         # Step 5: Save and finalize model
         self.update_state(
             state=TrainingStatus.COMPLETED,
-            meta={"progress": 95, "status": "Saving trained model", "current_step": "saving"},
+            meta={
+                "progress": 95,
+                "status": "Saving trained model",
+                "current_step": "saving",
+            },
         )
 
-        model_path = save_trained_model(task.model_name, model_config, evaluation_metrics)
+        model_path = save_trained_model(
+            task.model_name, model_config, evaluation_metrics
+        )
 
         # Complete training
         task.status = TrainingStatus.COMPLETED
@@ -179,12 +195,16 @@ def train_voice_model(self, task_data: Dict[str, Any]):
             "task_id": task.task_id,
             "model_path": model_path,
             "metrics": evaluation_metrics,
-            "training_time": (task.completed_at - task.started_at).total_seconds(),
+            "training_time": (
+                task.completed_at - task.started_at
+            ).total_seconds(),
             "progress": 100,
         }
 
     except Exception as e:
-        logger.error(f"Training failed for task {task_data.get('task_id', 'unknown')}: {str(e)}")
+        logger.error(
+            f"Training failed for task {task_data.get('task_id', 'unknown')}: {str(e)}"
+        )
 
         task.status = TrainingStatus.FAILED
         task.error_message = str(e)
@@ -192,14 +212,20 @@ def train_voice_model(self, task_data: Dict[str, Any]):
 
         self.update_state(
             state=TrainingStatus.FAILED,
-            meta={"progress": task.progress, "status": "Training failed", "error": str(e)},
+            meta={
+                "progress": task.progress,
+                "status": "Training failed",
+                "error": str(e),
+            },
         )
 
         raise
 
 
 @celery_app.task(name="training_worker.evaluate_model")
-def evaluate_model(model_path: str, test_data_urls: List[str]) -> Dict[str, float]:
+def evaluate_model(
+    model_path: str, test_data_urls: List[str]
+) -> Dict[str, float]:
     """
     Evaluate a trained model against test data
 
@@ -259,7 +285,9 @@ def cleanup_training_data(task_id: str, keep_model: bool = True):
         raise
 
 
-def preprocess_training_data(data_urls: List[str], config: Dict[str, Any]) -> Dict[str, Any]:
+def preprocess_training_data(
+    data_urls: List[str], config: Dict[str, Any]
+) -> Dict[str, Any]:
     """Preprocess training data for model training"""
 
     logger.info(f"Preprocessing {len(data_urls)} training files")
@@ -309,7 +337,9 @@ def simulate_training_step(
     return {"loss": loss, "accuracy": accuracy, "epoch": epoch}
 
 
-def evaluate_trained_model(data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, float]:
+def evaluate_trained_model(
+    data: Dict[str, Any], config: Dict[str, Any]
+) -> Dict[str, float]:
     """Evaluate the trained model"""
 
     # Placeholder evaluation metrics
@@ -326,10 +356,14 @@ def evaluate_trained_model(data: Dict[str, Any], config: Dict[str, Any]) -> Dict
     return metrics
 
 
-def save_trained_model(model_name: str, config: Dict[str, Any], metrics: Dict[str, float]) -> str:
+def save_trained_model(
+    model_name: str, config: Dict[str, Any], metrics: Dict[str, float]
+) -> str:
     """Save the trained model"""
 
-    model_path = f"/models/{model_name}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
+    model_path = (
+        f"/models/{model_name}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
+    )
 
     # Create model directory
     os.makedirs(model_path, exist_ok=True)
diff --git a/services/video-service/ai/__init__.py b/services/video-service/ai/__init__.py
index f5d894b..460657d 100644
--- a/services/video-service/ai/__init__.py
+++ b/services/video-service/ai/__init__.py
@@ -7,7 +7,11 @@ This module provides AI service clients for video generation:
 - Stable Diffusion for image generation and visual content
 """
 
-from .suno_client import SunoAIClient, VoiceGenerationRequest, VoiceGenerationResponse
+from .suno_client import (
+    SunoAIClient,
+    VoiceGenerationRequest,
+    VoiceGenerationResponse,
+)
 from .gemini_client import GeminiClient, ScriptScene, ScriptGenerationResponse
 from .stable_diffusion_client import (
     StableDiffusionClient,
diff --git a/services/video-service/ai/gemini_client.py b/services/video-service/ai/gemini_client.py
index 07adb40..f9133cb 100644
--- a/services/video-service/ai/gemini_client.py
+++ b/services/video-service/ai/gemini_client.py
@@ -51,9 +51,14 @@ class GeminiClient:
     async def _get_session(self) -> aiohttp.ClientSession:
         """Get or create aiohttp session"""
         if self.session is None or self.session.closed:
-            headers = {"Content-Type": "application/json", "User-Agent": "AutoVideoGeneration/1.0"}
+            headers = {
+                "Content-Type": "application/json",
+                "User-Agent": "AutoVideoGeneration/1.0",
+            }
             timeout = aiohttp.ClientTimeout(total=120)  # 2 minutes timeout
-            self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
+            self.session = aiohttp.ClientSession(
+                headers=headers, timeout=timeout
+            )
         return self.session
 
     async def health_check(self) -> Dict[str, Any]:
@@ -72,7 +77,11 @@ class GeminiClient:
                         "error": f"HTTP {response.status}",
                     }
         except Exception as e:
-            return {"status": "unhealthy", "service": "gemini", "error": str(e)}
+            return {
+                "status": "unhealthy",
+                "service": "gemini",
+                "error": str(e),
+            }
 
     async def generate_script(
         self,
@@ -85,9 +94,13 @@ class GeminiClient:
 
         try:
             # Create detailed prompt for script generation
-            prompt = self._create_script_prompt(theme, duration, style, target_platform)
+            prompt = self._create_script_prompt(
+                theme, duration, style, target_platform
+            )
 
-            logger.info(f"Generating script with Gemini Pro: theme='{theme}', duration={duration}s")
+            logger.info(
+                f"Generating script with Gemini Pro: theme='{theme}', duration={duration}s"
+            )
 
             # Generate script using Gemini Pro
             response_text = await self._generate_content(prompt)
@@ -126,7 +139,9 @@ class GeminiClient:
             logger.error(f"Script generation failed: {str(e)}")
             raise Exception(f"Failed to generate script: {str(e)}")
 
-    def _create_script_prompt(self, theme: str, duration: int, style: str, platform: str) -> str:
+    def _create_script_prompt(
+        self, theme: str, duration: int, style: str, platform: str
+    ) -> str:
         """Create detailed prompt for script generation"""
 
         platform_specs = {
@@ -189,8 +204,14 @@ Make sure the total duration of all scenes equals {duration} seconds.
                 "stopSequences": [],
             },
             "safetySettings": [
-                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
-                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
+                {
+                    "category": "HARM_CATEGORY_HARASSMENT",
+                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
+                },
+                {
+                    "category": "HARM_CATEGORY_HATE_SPEECH",
+                    "threshold": "BLOCK_MEDIUM_AND_ABOVE",
+                },
                 {
                     "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                     "threshold": "BLOCK_MEDIUM_AND_ABOVE",
@@ -205,7 +226,9 @@ Make sure the total duration of all scenes equals {duration} seconds.
         async with session.post(url, json=payload) as response:
             if response.status != 200:
                 error_text = await response.text()
-                raise Exception(f"Gemini API error: {response.status} - {error_text}")
+                raise Exception(
+                    f"Gemini API error: {response.status} - {error_text}"
+                )
 
             result = await response.json()
 
@@ -243,7 +266,9 @@ Make sure the total duration of all scenes equals {duration} seconds.
 
         except json.JSONDecodeError as e:
             # Fallback: create simple structure from text
-            logger.warning(f"Failed to parse JSON response, creating fallback structure: {e}")
+            logger.warning(
+                f"Failed to parse JSON response, creating fallback structure: {e}"
+            )
             return self._create_fallback_script(response_text)
         except Exception as e:
             logger.error(f"Error parsing script response: {e}")
@@ -261,11 +286,21 @@ Make sure the total duration of all scenes equals {duration} seconds.
 
         for i in range(scene_count):
             start_idx = i * sentences_per_scene
-            end_idx = start_idx + sentences_per_scene if i < scene_count - 1 else len(sentences)
+            end_idx = (
+                start_idx + sentences_per_scene
+                if i < scene_count - 1
+                else len(sentences)
+            )
 
             scene_text = ". ".join(sentences[start_idx:end_idx])
 
-            scene_type = "intro" if i == 0 else "outro" if i == scene_count - 1 else "main"
+            scene_type = (
+                "intro"
+                if i == 0
+                else "outro"
+                if i == scene_count - 1
+                else "main"
+            )
 
             scenes.append(
                 {
@@ -279,7 +314,9 @@ Make sure the total duration of all scenes equals {duration} seconds.
 
         return {"full_script": text, "scenes": scenes}
 
-    async def generate_caption_text(self, narration: str, style: str = "modern") -> List[str]:
+    async def generate_caption_text(
+        self, narration: str, style: str = "modern"
+    ) -> List[str]:
         """Generate formatted captions for video"""
 
         prompt = f"""
@@ -317,13 +354,17 @@ Please provide only the JSON array of caption segments.
             else:
                 # Fallback: split narration into chunks
                 words = narration.split()
-                return [" ".join(words[i : i + 5]) for i in range(0, len(words), 5)]
+                return [
+                    " ".join(words[i : i + 5]) for i in range(0, len(words), 5)
+                ]
 
         except Exception as e:
             logger.error(f"Caption generation failed: {e}")
             # Fallback: simple word chunking
             words = narration.split()
-            return [" ".join(words[i : i + 5]) for i in range(0, len(words), 5)]
+            return [
+                " ".join(words[i : i + 5]) for i in range(0, len(words), 5)
+            ]
 
     async def close(self):
         """Close the HTTP session"""
diff --git a/services/video-service/ai/stable_diffusion_client.py b/services/video-service/ai/stable_diffusion_client.py
index 61c23ec..f8d70b2 100644
--- a/services/video-service/ai/stable_diffusion_client.py
+++ b/services/video-service/ai/stable_diffusion_client.py
@@ -88,14 +88,18 @@ class StableDiffusionClient:
                 "User-Agent": "AutoVideoGeneration/1.0",
             }
             timeout = aiohttp.ClientTimeout(total=180)  # 3 minutes timeout
-            self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
+            self.session = aiohttp.ClientSession(
+                headers=headers, timeout=timeout
+            )
         return self.session
 
     async def health_check(self) -> Dict[str, Any]:
         """Check Stable Diffusion API health status"""
         try:
             session = await self._get_session()
-            async with session.get(f"{self.base_url}/user/account") as response:
+            async with session.get(
+                f"{self.base_url}/user/account"
+            ) as response:
                 if response.status == 200:
                     return {"status": "healthy", "service": "stable-diffusion"}
                 else:
@@ -105,7 +109,11 @@ class StableDiffusionClient:
                         "error": f"HTTP {response.status}",
                     }
         except Exception as e:
-            return {"status": "unhealthy", "service": "stable-diffusion", "error": str(e)}
+            return {
+                "status": "unhealthy",
+                "service": "stable-diffusion",
+                "error": str(e),
+            }
 
     async def generate_image(
         self,
@@ -120,7 +128,9 @@ class StableDiffusionClient:
             session = await self._get_session()
 
             # Get style configuration
-            style_config = self.style_presets.get(style, self.style_presets["modern"])
+            style_config = self.style_presets.get(
+                style, self.style_presets["modern"]
+            )
 
             # Convert aspect ratio to dimensions
             width, height = self._get_dimensions(aspect_ratio)
@@ -147,17 +157,20 @@ class StableDiffusionClient:
                 "sampler": "K_DPM_2_ANCESTRAL",
             }
 
-            logger.info(f"Generating image with Stable Diffusion: {enhanced_prompt[:100]}...")
+            logger.info(
+                f"Generating image with Stable Diffusion: {enhanced_prompt[:100]}..."
+            )
 
             # Submit generation request
             async with session.post(
                 f"{self.base_url}/generation/stable-diffusion-xl-1024-v1-0/text-to-image",
                 json=payload,
             ) as response:
-
                 if response.status != 200:
                     error_text = await response.text()
-                    raise Exception(f"Stable Diffusion API error: {response.status} - {error_text}")
+                    raise Exception(
+                        f"Stable Diffusion API error: {response.status} - {error_text}"
+                    )
 
                 result = await response.json()
 
@@ -170,7 +183,9 @@ class StableDiffusionClient:
 
                 # Save image and create thumbnail
                 generation_id = f"sd_{datetime.utcnow().timestamp()}"
-                image_url, thumbnail_url = await self._save_image(image_data, generation_id)
+                image_url, thumbnail_url = await self._save_image(
+                    image_data, generation_id
+                )
 
                 return ImageGenerationResponse(
                     url=image_url,
@@ -188,7 +203,10 @@ class StableDiffusionClient:
             raise Exception(f"Failed to generate image: {str(e)}")
 
     async def generate_image_batch(
-        self, prompts: List[str], style: str = "modern", aspect_ratio: str = "16:9"
+        self,
+        prompts: List[str],
+        style: str = "modern",
+        aspect_ratio: str = "16:9",
     ) -> List[ImageGenerationResponse]:
         """Generate multiple images in batch"""
 
@@ -203,7 +221,9 @@ class StableDiffusionClient:
         results = []
         for i in range(0, len(tasks), 3):  # Process 3 at a time
             batch = tasks[i : i + 3]
-            batch_results = await asyncio.gather(*batch, return_exceptions=True)
+            batch_results = await asyncio.gather(
+                *batch, return_exceptions=True
+            )
 
             for result in batch_results:
                 if isinstance(result, Exception):
@@ -243,13 +263,17 @@ class StableDiffusionClient:
 
         keywords = style_keywords.get(style, "")
         if keywords:
-            enhanced = f"{prompt}, {keywords}, high quality, detailed, sharp focus"
+            enhanced = (
+                f"{prompt}, {keywords}, high quality, detailed, sharp focus"
+            )
         else:
             enhanced = f"{prompt}, high quality, detailed, sharp focus"
 
         return enhanced
 
-    async def _save_image(self, image_data: bytes, generation_id: str) -> Tuple[str, str]:
+    async def _save_image(
+        self, image_data: bytes, generation_id: str
+    ) -> Tuple[str, str]:
         """Save image and create thumbnail, return URLs"""
 
         # Create directories if they don't exist
@@ -280,7 +304,9 @@ class StableDiffusionClient:
         base_url = os.getenv("MEDIA_BASE_URL", "http://localhost:8003")
         image_url = f"{base_url}/media/images/{image_filename}"
         thumbnail_url = (
-            f"{base_url}/media/thumbnails/{thumbnail_filename}" if thumbnail_path else None
+            f"{base_url}/media/thumbnails/{thumbnail_filename}"
+            if thumbnail_path
+            else None
         )
 
         return image_url, thumbnail_url
@@ -296,7 +322,9 @@ class StableDiffusionClient:
             # Download the original image
             async with session.get(image_url) as response:
                 if response.status != 200:
-                    raise Exception(f"Failed to download image: {response.status}")
+                    raise Exception(
+                        f"Failed to download image: {response.status}"
+                    )
 
                 image_data = await response.read()
                 image_b64 = base64.b64encode(image_data).decode()
@@ -308,12 +336,14 @@ class StableDiffusionClient:
             }
 
             async with session.post(
-                f"{self.base_url}/generation/esrgan-v1-x2plus/image-to-image/upscale", json=payload
+                f"{self.base_url}/generation/esrgan-v1-x2plus/image-to-image/upscale",
+                json=payload,
             ) as response:
-
                 if response.status != 200:
                     error_text = await response.text()
-                    raise Exception(f"Upscale API error: {response.status} - {error_text}")
+                    raise Exception(
+                        f"Upscale API error: {response.status} - {error_text}"
+                    )
 
                 result = await response.json()
                 artifact = result["artifacts"][0]
@@ -321,7 +351,9 @@ class StableDiffusionClient:
 
                 # Save upscaled image
                 generation_id = f"upscale_{datetime.utcnow().timestamp()}"
-                upscaled_url, thumbnail_url = await self._save_image(upscaled_data, generation_id)
+                upscaled_url, thumbnail_url = await self._save_image(
+                    upscaled_data, generation_id
+                )
 
                 return ImageGenerationResponse(
                     url=upscaled_url,
diff --git a/services/video-service/ai/suno_client.py b/services/video-service/ai/suno_client.py
index f743aa7..6200688 100644
--- a/services/video-service/ai/suno_client.py
+++ b/services/video-service/ai/suno_client.py
@@ -50,7 +50,9 @@ class SunoAIClient:
                 "User-Agent": "AutoVideoGeneration/1.0",
             }
             timeout = aiohttp.ClientTimeout(total=300)  # 5 minutes timeout
-            self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
+            self.session = aiohttp.ClientSession(
+                headers=headers, timeout=timeout
+            )
         return self.session
 
     async def health_check(self) -> Dict[str, Any]:
@@ -67,7 +69,11 @@ class SunoAIClient:
                         "error": f"HTTP {response.status}",
                     }
         except Exception as e:
-            return {"status": "unhealthy", "service": "suno.ai", "error": str(e)}
+            return {
+                "status": "unhealthy",
+                "service": "suno.ai",
+                "error": str(e),
+            }
 
     async def generate_voice(
         self,
@@ -102,19 +108,27 @@ class SunoAIClient:
                 "quality": "high",
             }
 
-            logger.info(f"Generating voice with Suno.ai: {len(text)} characters")
+            logger.info(
+                f"Generating voice with Suno.ai: {len(text)} characters"
+            )
 
             # Submit generation request
-            async with session.post(f"{self.base_url}/generate", json=payload) as response:
+            async with session.post(
+                f"{self.base_url}/generate", json=payload
+            ) as response:
                 if response.status != 200:
                     error_text = await response.text()
-                    raise Exception(f"Suno.ai API error: {response.status} - {error_text}")
+                    raise Exception(
+                        f"Suno.ai API error: {response.status} - {error_text}"
+                    )
 
                 result = await response.json()
                 generation_id = result["generation_id"]
 
             # Poll for completion
-            audio_url, music_url, duration = await self._wait_for_completion(generation_id)
+            audio_url, music_url, duration = await self._wait_for_completion(
+                generation_id
+            )
 
             return VoiceGenerationResponse(
                 audio_url=audio_url,
@@ -138,27 +152,41 @@ class SunoAIClient:
 
         while True:
             try:
-                async with session.get(f"{self.base_url}/generate/{generation_id}") as response:
+                async with session.get(
+                    f"{self.base_url}/generate/{generation_id}"
+                ) as response:
                     if response.status != 200:
-                        raise Exception(f"Failed to check generation status: {response.status}")
+                        raise Exception(
+                            f"Failed to check generation status: {response.status}"
+                        )
 
                     result = await response.json()
                     status = result["status"]
 
                     if status == "completed":
-                        return (result["audio_url"], result.get("music_url"), result["duration"])
+                        return (
+                            result["audio_url"],
+                            result.get("music_url"),
+                            result["duration"],
+                        )
                     elif status == "failed":
                         raise Exception(
                             f"Voice generation failed: {result.get('error', 'Unknown error')}"
                         )
                     elif status in ["queued", "processing"]:
                         # Check timeout
-                        elapsed = (datetime.utcnow() - start_time).total_seconds()
+                        elapsed = (
+                            datetime.utcnow() - start_time
+                        ).total_seconds()
                         if elapsed > max_wait:
-                            raise Exception(f"Voice generation timeout after {max_wait} seconds")
+                            raise Exception(
+                                f"Voice generation timeout after {max_wait} seconds"
+                            )
 
                         logger.info(f"Voice generation in progress: {status}")
-                        await asyncio.sleep(5)  # Wait 5 seconds before next check
+                        await asyncio.sleep(
+                            5
+                        )  # Wait 5 seconds before next check
                     else:
                         raise Exception(f"Unknown generation status: {status}")
 
@@ -185,9 +213,13 @@ class SunoAIClient:
         """Get list of available music genres"""
         try:
             session = await self._get_session()
-            async with session.get(f"{self.base_url}/music/genres") as response:
+            async with session.get(
+                f"{self.base_url}/music/genres"
+            ) as response:
                 if response.status != 200:
-                    raise Exception(f"Failed to get music genres: {response.status}")
+                    raise Exception(
+                        f"Failed to get music genres: {response.status}"
+                    )
 
                 result = await response.json()
                 return result["genres"]
@@ -196,7 +228,9 @@ class SunoAIClient:
             logger.error(f"Failed to get available music genres: {str(e)}")
             return ["ambient", "cinematic", "upbeat", "calm", "dramatic"]
 
-    async def clone_voice(self, voice_sample_url: str, text: str) -> VoiceGenerationResponse:
+    async def clone_voice(
+        self, voice_sample_url: str, text: str
+    ) -> VoiceGenerationResponse:
         """Clone a voice from a sample and generate speech"""
 
         try:
@@ -205,23 +239,35 @@ class SunoAIClient:
             payload = {
                 "voice_sample_url": voice_sample_url,
                 "text": text,
-                "clone_settings": {"similarity_boost": True, "stability": 0.75, "clarity": 0.75},
+                "clone_settings": {
+                    "similarity_boost": True,
+                    "stability": 0.75,
+                    "clarity": 0.75,
+                },
                 "output_format": "mp3",
                 "quality": "high",
             }
 
-            logger.info(f"Cloning voice and generating speech: {len(text)} characters")
+            logger.info(
+                f"Cloning voice and generating speech: {len(text)} characters"
+            )
 
-            async with session.post(f"{self.base_url}/clone", json=payload) as response:
+            async with session.post(
+                f"{self.base_url}/clone", json=payload
+            ) as response:
                 if response.status != 200:
                     error_text = await response.text()
-                    raise Exception(f"Voice cloning API error: {response.status} - {error_text}")
+                    raise Exception(
+                        f"Voice cloning API error: {response.status} - {error_text}"
+                    )
 
                 result = await response.json()
                 generation_id = result["generation_id"]
 
             # Wait for completion
-            audio_url, _, duration = await self._wait_for_completion(generation_id)
+            audio_url, _, duration = await self._wait_for_completion(
+                generation_id
+            )
 
             return VoiceGenerationResponse(
                 audio_url=audio_url,
diff --git a/services/video-service/auth.py b/services/video-service/auth.py
index 9292b37..ff46522 100644
--- a/services/video-service/auth.py
+++ b/services/video-service/auth.py
@@ -37,7 +37,9 @@ async def verify_token(token: str) -> Optional[str]:
         # Get JWT settings from environment
         jwt_secret = os.getenv("JWT_SECRET_KEY")
         jwt_algorithm = os.getenv("JWT_ALGORITHM", "HS256")
-        auth_service_url = os.getenv("AUTH_SERVICE_URL", "http://localhost:8001")
+        auth_service_url = os.getenv(
+            "AUTH_SERVICE_URL", "http://localhost:8001"
+        )
 
         if not jwt_secret:
             # If no local JWT secret, validate with auth service
@@ -46,7 +48,10 @@ async def verify_token(token: str) -> Optional[str]:
         # Decode and verify token locally
         try:
             payload = jwt.decode(
-                token, jwt_secret, algorithms=[jwt_algorithm], options={"verify_exp": True}
+                token,
+                jwt_secret,
+                algorithms=[jwt_algorithm],
+                options={"verify_exp": True},
             )
 
             # Extract user ID from payload
@@ -76,7 +81,9 @@ async def verify_token(token: str) -> Optional[str]:
         return None
 
 
-async def verify_token_remote(token: str, auth_service_url: str) -> Optional[str]:
+async def verify_token_remote(
+    token: str, auth_service_url: str
+) -> Optional[str]:
     """
     Verify token with remote authentication service
 
@@ -97,16 +104,19 @@ async def verify_token_remote(token: str, auth_service_url: str) -> Optional[str
                 headers=headers,
                 timeout=aiohttp.ClientTimeout(total=10),
             ) as response:
-
                 if response.status == 200:
                     result = await response.json()
                     user_id = result.get("user_id")
 
                     if user_id:
-                        logger.debug(f"Remote token validated for user: {user_id}")
+                        logger.debug(
+                            f"Remote token validated for user: {user_id}"
+                        )
                         return user_id
                     else:
-                        logger.warning("Remote verification returned no user ID")
+                        logger.warning(
+                            "Remote verification returned no user ID"
+                        )
                         return None
 
                 elif response.status == 401:
@@ -136,7 +146,9 @@ async def get_user_info(token: str) -> Optional[Dict[str, Any]]:
     """
 
     try:
-        auth_service_url = os.getenv("AUTH_SERVICE_URL", "http://localhost:8001")
+        auth_service_url = os.getenv(
+            "AUTH_SERVICE_URL", "http://localhost:8001"
+        )
 
         async with aiohttp.ClientSession() as session:
             headers = {"Authorization": f"Bearer {token}"}
@@ -146,13 +158,16 @@ async def get_user_info(token: str) -> Optional[Dict[str, Any]]:
                 headers=headers,
                 timeout=aiohttp.ClientTimeout(total=10),
             ) as response:
-
                 if response.status == 200:
                     user_info = await response.json()
-                    logger.debug(f"Retrieved user info for: {user_info.get('id')}")
+                    logger.debug(
+                        f"Retrieved user info for: {user_info.get('id')}"
+                    )
                     return user_info
                 else:
-                    logger.warning(f"Failed to get user info: {response.status}")
+                    logger.warning(
+                        f"Failed to get user info: {response.status}"
+                    )
                     return None
 
     except Exception as e:
@@ -185,7 +200,9 @@ def create_service_token(service_name: str, expires_in: int = 3600) -> str:
             "service": service_name,
         }
 
-        token = jwt.encode(payload, jwt_secret, algorithm=os.getenv("JWT_ALGORITHM", "HS256"))
+        token = jwt.encode(
+            payload, jwt_secret, algorithm=os.getenv("JWT_ALGORITHM", "HS256")
+        )
 
         logger.debug(f"Created service token for: {service_name}")
         return token
@@ -195,7 +212,9 @@ def create_service_token(service_name: str, expires_in: int = 3600) -> str:
         raise AuthenticationError(f"Failed to create service token: {str(e)}")
 
 
-async def check_user_permissions(user_id: str, resource: str, action: str) -> bool:
+async def check_user_permissions(
+    user_id: str, resource: str, action: str
+) -> bool:
     """
     Check if user has permission to perform action on resource
 
@@ -209,13 +228,19 @@ async def check_user_permissions(user_id: str, resource: str, action: str) -> bo
     """
 
     try:
-        auth_service_url = os.getenv("AUTH_SERVICE_URL", "http://localhost:8001")
+        auth_service_url = os.getenv(
+            "AUTH_SERVICE_URL", "http://localhost:8001"
+        )
         service_token = create_service_token("video-service")
 
         async with aiohttp.ClientSession() as session:
             headers = {"Authorization": f"Bearer {service_token}"}
 
-            payload = {"user_id": user_id, "resource": resource, "action": action}
+            payload = {
+                "user_id": user_id,
+                "resource": resource,
+                "action": action,
+            }
 
             async with session.post(
                 f"{auth_service_url}/api/v1/auth/permissions/check",
@@ -223,7 +248,6 @@ async def check_user_permissions(user_id: str, resource: str, action: str) -> bo
                 json=payload,
                 timeout=aiohttp.ClientTimeout(total=10),
             ) as response:
-
                 if response.status == 200:
                     result = await response.json()
                     has_permission = result.get("allowed", False)
@@ -233,7 +257,9 @@ async def check_user_permissions(user_id: str, resource: str, action: str) -> bo
                     )
                     return has_permission
                 else:
-                    logger.warning(f"Permission check failed: {response.status}")
+                    logger.warning(
+                        f"Permission check failed: {response.status}"
+                    )
                     return False
 
     except Exception as e:
@@ -300,19 +326,28 @@ class TokenValidator:
 
         try:
             payload = jwt.decode(
-                token, self.jwt_secret, algorithms=[self.algorithm], options={"verify_exp": True}
+                token,
+                self.jwt_secret,
+                algorithms=[self.algorithm],
+                options={"verify_exp": True},
             )
 
             return payload
 
         except jwt.ExpiredSignatureError:
             raise HTTPException(
-                status_code=status.HTTP_401_UNAUTHORIZED, detail="Token has expired"
+                status_code=status.HTTP_401_UNAUTHORIZED,
+                detail="Token has expired",
             )
         except jwt.InvalidTokenError:
-            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")
+            raise HTTPException(
+                status_code=status.HTTP_401_UNAUTHORIZED,
+                detail="Invalid token",
+            )
 
-    async def validate_service_token(self, token: str, required_service: str) -> bool:
+    async def validate_service_token(
+        self, token: str, required_service: str
+    ) -> bool:
         """
         Validate service-to-service token
 
diff --git a/services/video-service/database.py b/services/video-service/database.py
index 44760b1..a07504f 100644
--- a/services/video-service/database.py
+++ b/services/video-service/database.py
@@ -33,7 +33,9 @@ class DatabaseManager:
 
         return f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
 
-    async def initialize(self, min_connections: int = 5, max_connections: int = 20):
+    async def initialize(
+        self, min_connections: int = 5, max_connections: int = 20
+    ):
         """Initialize database connection pool"""
 
         try:
@@ -70,7 +72,9 @@ class DatabaseManager:
         try:
             async with self.pool.acquire() as conn:
                 # Create extensions if needed
-                await conn.execute('CREATE EXTENSION IF NOT EXISTS "uuid-ossp";')
+                await conn.execute(
+                    'CREATE EXTENSION IF NOT EXISTS "uuid-ossp";'
+                )
                 await conn.execute('CREATE EXTENSION IF NOT EXISTS "pg_trgm";')
 
                 # Import and create tables
@@ -125,7 +129,11 @@ class DatabaseManager:
                 }
 
         except Exception as e:
-            return {"status": "unhealthy", "database": "postgresql", "error": str(e)}
+            return {
+                "status": "unhealthy",
+                "database": "postgresql",
+                "error": str(e),
+            }
 
 
 # Global database manager instance
@@ -243,7 +251,12 @@ async def run_migrations():
             )
 
             # Get current version
-            current_version = await conn.fetchval("SELECT MAX(version) FROM schema_migrations") or 0
+            current_version = (
+                await conn.fetchval(
+                    "SELECT MAX(version) FROM schema_migrations"
+                )
+                or 0
+            )
 
             # Define migrations
             migrations = {
@@ -273,10 +286,13 @@ async def run_migrations():
                     async with conn.transaction():
                         await conn.execute(sql)
                         await conn.execute(
-                            "INSERT INTO schema_migrations (version) VALUES ($1)", version
+                            "INSERT INTO schema_migrations (version) VALUES ($1)",
+                            version,
                         )
 
-                    logger.info(f"Migration version {version} applied successfully")
+                    logger.info(
+                        f"Migration version {version} applied successfully"
+                    )
 
             logger.info("All migrations completed")
 
diff --git a/services/video-service/main.py b/services/video-service/main.py
index d6222a2..45b00a3 100644
--- a/services/video-service/main.py
+++ b/services/video-service/main.py
@@ -49,7 +49,9 @@ security = HTTPBearer()
 # AI Clients initialization
 suno_client = SunoAIClient(api_key=os.getenv("SUNO_API_KEY"))
 gemini_client = GeminiClient(api_key=os.getenv("GEMINI_API_KEY"))
-stable_diffusion_client = StableDiffusionClient(api_key=os.getenv("STABLE_DIFFUSION_API_KEY"))
+stable_diffusion_client = StableDiffusionClient(
+    api_key=os.getenv("STABLE_DIFFUSION_API_KEY")
+)
 video_composer = VideoComposer()
 
 
@@ -57,11 +59,21 @@ class VideoGenerationRequest(BaseModel):
     title: str = Field(..., min_length=1, max_length=200)
     description: Optional[str] = Field(None, max_length=1000)
     theme: str = Field(..., description="Video theme/topic")
-    style: Optional[str] = Field("modern", description="Visual style preference")
-    duration: Optional[int] = Field(60, ge=15, le=300, description="Video duration in seconds")
-    voice_type: Optional[str] = Field("default", description="Voice style for narration")
-    music_genre: Optional[str] = Field("ambient", description="Background music genre")
-    include_captions: bool = Field(True, description="Whether to include captions")
+    style: Optional[str] = Field(
+        "modern", description="Visual style preference"
+    )
+    duration: Optional[int] = Field(
+        60, ge=15, le=300, description="Video duration in seconds"
+    )
+    voice_type: Optional[str] = Field(
+        "default", description="Voice style for narration"
+    )
+    music_genre: Optional[str] = Field(
+        "ambient", description="Background music genre"
+    )
+    include_captions: bool = Field(
+        True, description="Whether to include captions"
+    )
     target_platform: Optional[str] = Field(
         "youtube", description="Target platform (youtube, tiktok, instagram)"
     )
@@ -105,7 +117,9 @@ async def create_video_project(
     # Verify user authentication
     user_id = await verify_token(credentials.credentials)
     if not user_id:
-        raise HTTPException(status_code=401, detail="Invalid authentication token")
+        raise HTTPException(
+            status_code=401, detail="Invalid authentication token"
+        )
 
     # Create project record
     project_id = str(uuid.uuid4())
@@ -146,7 +160,9 @@ async def create_video_project(
     )
 
 
-@app.get("/api/v1/video/projects/{project_id}", response_model=VideoProjectResponse)
+@app.get(
+    "/api/v1/video/projects/{project_id}", response_model=VideoProjectResponse
+)
 async def get_video_project(
     project_id: str,
     credentials: HTTPAuthorizationCredentials = Depends(security),
@@ -156,7 +172,9 @@ async def get_video_project(
 
     user_id = await verify_token(credentials.credentials)
     if not user_id:
-        raise HTTPException(status_code=401, detail="Invalid authentication token")
+        raise HTTPException(
+            status_code=401, detail="Invalid authentication token"
+        )
 
     project = await VideoProject.get_by_id(db, project_id)
     if not project:
@@ -188,9 +206,13 @@ async def list_user_projects(
 
     user_id = await verify_token(credentials.credentials)
     if not user_id:
-        raise HTTPException(status_code=401, detail="Invalid authentication token")
+        raise HTTPException(
+            status_code=401, detail="Invalid authentication token"
+        )
 
-    projects = await VideoProject.get_by_user(db, user_id, limit=limit, offset=offset)
+    projects = await VideoProject.get_by_user(
+        db, user_id, limit=limit, offset=offset
+    )
 
     return [
         VideoProjectResponse(
@@ -217,7 +239,9 @@ async def delete_video_project(
 
     user_id = await verify_token(credentials.credentials)
     if not user_id:
-        raise HTTPException(status_code=401, detail="Invalid authentication token")
+        raise HTTPException(
+            status_code=401, detail="Invalid authentication token"
+        )
 
     project = await VideoProject.get_by_id(db, project_id)
     if not project:
@@ -233,7 +257,9 @@ async def delete_video_project(
     return {"message": "Project deleted successfully"}
 
 
-async def generate_video_pipeline(project_id: str, request: VideoGenerationRequest):
+async def generate_video_pipeline(
+    project_id: str, request: VideoGenerationRequest
+):
     """Background video generation pipeline"""
 
     try:
@@ -322,9 +348,13 @@ async def generate_video_pipeline(project_id: str, request: VideoGenerationReque
         logger.info(f"Video generation completed for project {project_id}")
 
     except Exception as e:
-        logger.error(f"Video generation failed for project {project_id}: {str(e)}")
+        logger.error(
+            f"Video generation failed for project {project_id}: {str(e)}"
+        )
         try:
-            await project.update_status(db, VideoStatus.FAILED, project.progress)
+            await project.update_status(
+                db, VideoStatus.FAILED, project.progress
+            )
             project.error_message = str(e)
             await project.save(db)
         except:
@@ -333,7 +363,12 @@ async def generate_video_pipeline(project_id: str, request: VideoGenerationReque
 
 def get_aspect_ratio(platform: str) -> str:
     """Get aspect ratio based on target platform"""
-    ratios = {"youtube": "16:9", "tiktok": "9:16", "instagram": "1:1", "default": "16:9"}
+    ratios = {
+        "youtube": "16:9",
+        "tiktok": "9:16",
+        "instagram": "1:1",
+        "default": "16:9",
+    }
     return ratios.get(platform, "16:9")
 
 
diff --git a/services/video-service/models/video_project.py b/services/video-service/models/video_project.py
index 8a856dd..71752d4 100644
--- a/services/video-service/models/video_project.py
+++ b/services/video-service/models/video_project.py
@@ -247,12 +247,16 @@ class VideoProject(BaseModel):
         )
 
     @classmethod
-    async def get_by_id(cls, db_pool, project_id: str) -> Optional["VideoProject"]:
+    async def get_by_id(
+        cls, db_pool, project_id: str
+    ) -> Optional["VideoProject"]:
         """Get video project by ID"""
 
         try:
             async with db_pool.acquire() as conn:
-                row = await conn.fetchrow("SELECT * FROM video_projects WHERE id = $1", project_id)
+                row = await conn.fetchrow(
+                    "SELECT * FROM video_projects WHERE id = $1", project_id
+                )
 
                 if not row:
                     return None
@@ -292,7 +296,7 @@ class VideoProject(BaseModel):
                 SELECT * FROM video_projects 
                 {where_clause}
                 ORDER BY created_at DESC 
-                LIMIT {param_nums.split(', ')[0]} OFFSET {param_nums.split(', ')[1]}
+                LIMIT {param_nums.split(", ")[0]} OFFSET {param_nums.split(", ")[1]}
                 """
 
                 rows = await conn.fetch(query, *params)
@@ -300,7 +304,9 @@ class VideoProject(BaseModel):
                 return [cls._from_db_row(row) for row in rows]
 
         except Exception as e:
-            logger.error(f"Failed to get video projects for user {user_id}: {str(e)}")
+            logger.error(
+                f"Failed to get video projects for user {user_id}: {str(e)}"
+            )
             return []
 
     @classmethod
@@ -323,10 +329,14 @@ class VideoProject(BaseModel):
             progress=row["progress"],
             error_message=row["error_message"],
             script_content=row["script_content"],
-            script_scenes=json.loads(row["script_scenes"]) if row["script_scenes"] else None,
+            script_scenes=json.loads(row["script_scenes"])
+            if row["script_scenes"]
+            else None,
             voice_url=row["voice_url"],
             music_url=row["music_url"],
-            image_urls=json.loads(row["image_urls"]) if row["image_urls"] else None,
+            image_urls=json.loads(row["image_urls"])
+            if row["image_urls"]
+            else None,
             preview_url=row["preview_url"],
             final_url=row["final_url"],
             thumbnail_url=row["thumbnail_url"],
@@ -340,7 +350,11 @@ class VideoProject(BaseModel):
         )
 
     async def update_status(
-        self, db_pool, status: VideoStatus, progress: int, error_message: Optional[str] = None
+        self,
+        db_pool,
+        status: VideoStatus,
+        progress: int,
+        error_message: Optional[str] = None,
     ):
         """Update project status and progress"""
 
@@ -362,8 +376,12 @@ class VideoProject(BaseModel):
             VideoStatus.RENDERING,
         ]:
             # Rough estimation based on current progress
-            remaining_time = max(0, (100 - progress) * 2)  # 2 seconds per percent
-            self.estimated_completion = datetime.utcnow() + timedelta(seconds=remaining_time)
+            remaining_time = max(
+                0, (100 - progress) * 2
+            )  # 2 seconds per percent
+            self.estimated_completion = datetime.utcnow() + timedelta(
+                seconds=remaining_time
+            )
 
         await self.save(db_pool)
 
@@ -372,7 +390,9 @@ class VideoProject(BaseModel):
 
         try:
             async with db_pool.acquire() as conn:
-                await conn.execute("DELETE FROM video_projects WHERE id = $1", self.id)
+                await conn.execute(
+                    "DELETE FROM video_projects WHERE id = $1", self.id
+                )
 
                 logger.info(f"Deleted video project: {self.id}")
 
@@ -386,13 +406,16 @@ class VideoProject(BaseModel):
         try:
             async with db_pool.acquire() as conn:
                 await conn.execute(
-                    "UPDATE video_projects SET view_count = view_count + 1 WHERE id = $1", self.id
+                    "UPDATE video_projects SET view_count = view_count + 1 WHERE id = $1",
+                    self.id,
                 )
 
                 self.view_count += 1
 
         except Exception as e:
-            logger.error(f"Failed to increment view count for {self.id}: {str(e)}")
+            logger.error(
+                f"Failed to increment view count for {self.id}: {str(e)}"
+            )
 
     async def increment_download_count(self, db_pool):
         """Increment download count"""
@@ -407,7 +430,9 @@ class VideoProject(BaseModel):
                 self.download_count += 1
 
         except Exception as e:
-            logger.error(f"Failed to increment download count for {self.id}: {str(e)}")
+            logger.error(
+                f"Failed to increment download count for {self.id}: {str(e)}"
+            )
 
     async def toggle_like(self, db_pool, increment: bool = True):
         """Toggle like count"""
@@ -446,11 +471,19 @@ class VideoProject(BaseModel):
             "preview_url": self.preview_url,
             "final_url": self.final_url,
             "thumbnail_url": self.thumbnail_url,
-            "created_at": self.created_at.isoformat() if self.created_at else None,
-            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
-            "completed_at": self.completed_at.isoformat() if self.completed_at else None,
+            "created_at": self.created_at.isoformat()
+            if self.created_at
+            else None,
+            "updated_at": self.updated_at.isoformat()
+            if self.updated_at
+            else None,
+            "completed_at": self.completed_at.isoformat()
+            if self.completed_at
+            else None,
             "estimated_completion": (
-                self.estimated_completion.isoformat() if self.estimated_completion else None
+                self.estimated_completion.isoformat()
+                if self.estimated_completion
+                else None
             ),
             "view_count": self.view_count,
             "download_count": self.download_count,
diff --git a/services/video-service/social/platforms.py b/services/video-service/social/platforms.py
index c0c9533..62e0d84 100644
--- a/services/video-service/social/platforms.py
+++ b/services/video-service/social/platforms.py
@@ -101,10 +101,14 @@ class TikTokClient(SocialPlatform):
             upload_result = await self._upload_video(request.video_url)
 
             if not upload_result.get("success"):
-                raise Exception(f"Video upload failed: {upload_result.get('error')}")
+                raise Exception(
+                    f"Video upload failed: {upload_result.get('error')}"
+                )
 
             # Step 2: Create post
-            post_result = await self._create_post(upload_result["video_id"], request)
+            post_result = await self._create_post(
+                upload_result["video_id"], request
+            )
 
             return PublishResult(
                 platform="tiktok",
@@ -141,26 +145,36 @@ class TikTokClient(SocialPlatform):
 
         # Upload to TikTok
         data = aiohttp.FormData()
-        data.add_field("video", video_data, filename="video.mp4", content_type="video/mp4")
+        data.add_field(
+            "video", video_data, filename="video.mp4", content_type="video/mp4"
+        )
 
-        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "multipart/form-data"}
+        headers = {
+            "Authorization": f"Bearer {self.api_key}",
+            "Content-Type": "multipart/form-data",
+        }
 
         async with session.post(
             f"{self.upload_url}/video/upload", data=data, headers=headers
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
-                raise Exception(f"TikTok upload API error: {response.status} - {error_text}")
+                raise Exception(
+                    f"TikTok upload API error: {response.status} - {error_text}"
+                )
 
             result = await response.json()
 
             if result.get("error"):
-                raise Exception(f"TikTok upload error: {result['error']['message']}")
+                raise Exception(
+                    f"TikTok upload error: {result['error']['message']}"
+                )
 
             return {"success": True, "video_id": result["data"]["video_id"]}
 
-    async def _create_post(self, video_id: str, request: PublishRequest) -> Dict[str, Any]:
+    async def _create_post(
+        self, video_id: str, request: PublishRequest
+    ) -> Dict[str, Any]:
         """Create TikTok post"""
 
         session = await self._get_session()
@@ -168,27 +182,35 @@ class TikTokClient(SocialPlatform):
         payload = {
             "video_id": video_id,
             "text": request.description or request.title,
-            "privacy_level": "PUBLIC_TO_EVERYONE" if request.privacy == "public" else "SELF_ONLY",
+            "privacy_level": "PUBLIC_TO_EVERYONE"
+            if request.privacy == "public"
+            else "SELF_ONLY",
             "disable_duet": False,
             "disable_comment": False,
             "disable_stitch": False,
             "video_cover_timestamp_ms": 1000,
         }
 
-        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
+        headers = {
+            "Authorization": f"Bearer {self.api_key}",
+            "Content-Type": "application/json",
+        }
 
         async with session.post(
             f"{self.base_url}/video/publish", json=payload, headers=headers
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
-                raise Exception(f"TikTok publish API error: {response.status} - {error_text}")
+                raise Exception(
+                    f"TikTok publish API error: {response.status} - {error_text}"
+                )
 
             result = await response.json()
 
             if result.get("error"):
-                raise Exception(f"TikTok publish error: {result['error']['message']}")
+                raise Exception(
+                    f"TikTok publish error: {result['error']['message']}"
+                )
 
             return {
                 "video_id": result["data"]["video_id"],
@@ -207,9 +229,10 @@ class TikTokClient(SocialPlatform):
             }
 
             async with session.get(
-                f"{self.base_url}/video/query", params={"video_id": platform_id}, headers=headers
+                f"{self.base_url}/video/query",
+                params={"video_id": platform_id},
+                headers=headers,
             ) as response:
-
                 if response.status != 200:
                     return {"error": f"API error: {response.status}"}
 
@@ -244,9 +267,10 @@ class TikTokClient(SocialPlatform):
             }
 
             async with session.delete(
-                f"{self.base_url}/video/delete", params={"video_id": platform_id}, headers=headers
+                f"{self.base_url}/video/delete",
+                params={"video_id": platform_id},
+                headers=headers,
             ) as response:
-
                 return response.status == 200
 
         except Exception as e:
@@ -290,7 +314,9 @@ class YouTubeClient(SocialPlatform):
                 published_at=datetime.utcnow(),
             )
 
-    async def _upload_video_with_metadata(self, request: PublishRequest) -> Dict[str, Any]:
+    async def _upload_video_with_metadata(
+        self, request: PublishRequest
+    ) -> Dict[str, Any]:
         """Upload video to YouTube with metadata"""
 
         session = await self._get_session()
@@ -303,7 +329,10 @@ class YouTubeClient(SocialPlatform):
             "categoryId": "22",  # People & Blogs
         }
 
-        status = {"privacyStatus": request.privacy, "selfDeclaredMadeForKids": False}
+        status = {
+            "privacyStatus": request.privacy,
+            "selfDeclaredMadeForKids": False,
+        }
 
         metadata = {"snippet": snippet, "status": status}
 
@@ -348,17 +377,20 @@ class YouTubeClient(SocialPlatform):
             data=body,
             headers=headers,
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
-                raise Exception(f"YouTube upload API error: {response.status} - {error_text}")
+                raise Exception(
+                    f"YouTube upload API error: {response.status} - {error_text}"
+                )
 
             result = await response.json()
 
             return {
                 "video_id": result["id"],
                 "upload_status": result.get("status", {}).get("uploadStatus"),
-                "privacy_status": result.get("status", {}).get("privacyStatus"),
+                "privacy_status": result.get("status", {}).get(
+                    "privacyStatus"
+                ),
             }
 
     async def get_video_stats(self, platform_id: str) -> Dict[str, Any]:
@@ -367,9 +399,15 @@ class YouTubeClient(SocialPlatform):
         try:
             session = await self._get_session()
 
-            params = {"part": "statistics,status", "id": platform_id, "key": self.api_key}
+            params = {
+                "part": "statistics,status",
+                "id": platform_id,
+                "key": self.api_key,
+            }
 
-            async with session.get(f"{self.base_url}/videos", params=params) as response:
+            async with session.get(
+                f"{self.base_url}/videos", params=params
+            ) as response:
                 if response.status != 200:
                     return {"error": f"API error: {response.status}"}
 
@@ -404,9 +442,10 @@ class YouTubeClient(SocialPlatform):
             }
 
             async with session.delete(
-                f"{self.base_url}/videos", params={"id": platform_id}, headers=headers
+                f"{self.base_url}/videos",
+                params={"id": platform_id},
+                headers=headers,
             ) as response:
-
                 return response.status == 204
 
         except Exception as e:
@@ -440,7 +479,10 @@ class InstagramClient(SocialPlatform):
                 platform_id=result["media_id"],
                 url=result.get("permalink"),
                 published_at=datetime.utcnow(),
-                metadata={"container_id": container_id, "media_id": result["media_id"]},
+                metadata={
+                    "container_id": container_id,
+                    "media_id": result["media_id"],
+                },
             )
 
         except Exception as e:
@@ -459,7 +501,9 @@ class InstagramClient(SocialPlatform):
 
         params = {
             "video_url": request.video_url,
-            "media_type": "REELS" if request.custom_metadata.get("is_reels") else "VIDEO",
+            "media_type": "REELS"
+            if request.custom_metadata.get("is_reels")
+            else "VIDEO",
             "caption": f"{request.title}\n\n{request.description or ''}",
             "access_token": self.api_key,
         }
@@ -471,15 +515,18 @@ class InstagramClient(SocialPlatform):
         async with session.post(
             f"{self.base_url}/{self.business_account_id}/media", params=params
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
-                raise Exception(f"Instagram media creation error: {response.status} - {error_text}")
+                raise Exception(
+                    f"Instagram media creation error: {response.status} - {error_text}"
+                )
 
             result = await response.json()
 
             if "error" in result:
-                raise Exception(f"Instagram API error: {result['error']['message']}")
+                raise Exception(
+                    f"Instagram API error: {result['error']['message']}"
+                )
 
             return result["id"]
 
@@ -491,17 +538,21 @@ class InstagramClient(SocialPlatform):
         params = {"creation_id": container_id, "access_token": self.api_key}
 
         async with session.post(
-            f"{self.base_url}/{self.business_account_id}/media_publish", params=params
+            f"{self.base_url}/{self.business_account_id}/media_publish",
+            params=params,
         ) as response:
-
             if response.status != 200:
                 error_text = await response.text()
-                raise Exception(f"Instagram publish error: {response.status} - {error_text}")
+                raise Exception(
+                    f"Instagram publish error: {response.status} - {error_text}"
+                )
 
             result = await response.json()
 
             if "error" in result:
-                raise Exception(f"Instagram API error: {result['error']['message']}")
+                raise Exception(
+                    f"Instagram API error: {result['error']['message']}"
+                )
 
             # Get media permalink
             media_id = result["id"]
@@ -517,8 +568,9 @@ class InstagramClient(SocialPlatform):
 
             params = {"fields": "permalink", "access_token": self.api_key}
 
-            async with session.get(f"{self.base_url}/{media_id}", params=params) as response:
-
+            async with session.get(
+                f"{self.base_url}/{media_id}", params=params
+            ) as response:
                 if response.status == 200:
                     result = await response.json()
                     return result.get("permalink")
@@ -540,8 +592,9 @@ class InstagramClient(SocialPlatform):
                 "access_token": self.api_key,
             }
 
-            async with session.get(f"{self.base_url}/{platform_id}", params=params) as response:
-
+            async with session.get(
+                f"{self.base_url}/{platform_id}", params=params
+            ) as response:
                 if response.status != 200:
                     return {"error": f"API error: {response.status}"}
 
@@ -576,8 +629,9 @@ class InstagramClient(SocialPlatform):
 
             params = {"access_token": self.api_key}
 
-            async with session.delete(f"{self.base_url}/{platform_id}", params=params) as response:
-
+            async with session.delete(
+                f"{self.base_url}/{platform_id}", params=params
+            ) as response:
                 return response.status == 200
 
         except Exception as e:
@@ -610,16 +664,22 @@ class SocialMediaManager:
         youtube_oauth_token = os.getenv("YOUTUBE_OAUTH_TOKEN")
 
         if all([youtube_api_key, youtube_oauth_token]):
-            self.platforms["youtube"] = YouTubeClient(youtube_api_key, youtube_oauth_token)
+            self.platforms["youtube"] = YouTubeClient(
+                youtube_api_key, youtube_oauth_token
+            )
 
         # Instagram
         instagram_api_key = os.getenv("INSTAGRAM_API_KEY")
         instagram_business_id = os.getenv("INSTAGRAM_BUSINESS_ACCOUNT_ID")
 
         if all([instagram_api_key, instagram_business_id]):
-            self.platforms["instagram"] = InstagramClient(instagram_api_key, instagram_business_id)
+            self.platforms["instagram"] = InstagramClient(
+                instagram_api_key, instagram_business_id
+            )
 
-    async def publish_to_platform(self, platform: str, request: PublishRequest) -> PublishResult:
+    async def publish_to_platform(
+        self, platform: str, request: PublishRequest
+    ) -> PublishResult:
         """Publish video to specific platform"""
 
         if platform not in self.platforms:
@@ -669,7 +729,9 @@ class SocialMediaManager:
 
         return []
 
-    async def get_platform_stats(self, platform: str, platform_id: str) -> Dict[str, Any]:
+    async def get_platform_stats(
+        self, platform: str, platform_id: str
+    ) -> Dict[str, Any]:
         """Get video statistics from platform"""
 
         if platform not in self.platforms:
@@ -677,7 +739,9 @@ class SocialMediaManager:
 
         return await self.platforms[platform].get_video_stats(platform_id)
 
-    async def delete_from_platform(self, platform: str, platform_id: str) -> bool:
+    async def delete_from_platform(
+        self, platform: str, platform_id: str
+    ) -> bool:
         """Delete video from platform"""
 
         if platform not in self.platforms:
diff --git a/services/video-service/tests/test_video_service.py b/services/video-service/tests/test_video_service.py
index 488a3d3..9ae8898 100644
--- a/services/video-service/tests/test_video_service.py
+++ b/services/video-service/tests/test_video_service.py
@@ -14,8 +14,15 @@ from httpx import AsyncClient
 from main import app
 from models.video_project import VideoProject, VideoStatus
 from ai.suno_client import SunoAIClient, VoiceGenerationResponse
-from ai.gemini_client import GeminiClient, ScriptGenerationResponse, ScriptScene
-from ai.stable_diffusion_client import StableDiffusionClient, ImageGenerationResponse
+from ai.gemini_client import (
+    GeminiClient,
+    ScriptGenerationResponse,
+    ScriptScene,
+)
+from ai.stable_diffusion_client import (
+    StableDiffusionClient,
+    ImageGenerationResponse,
+)
 
 client = TestClient(app)
 
@@ -109,7 +116,9 @@ class TestVideoGeneration:
 
     @patch("main.verify_token")
     @patch("main.get_db_connection")
-    def test_create_video_project_success(self, mock_db, mock_verify_token, sample_video_request):
+    def test_create_video_project_success(
+        self, mock_db, mock_verify_token, sample_video_request
+    ):
         """Test successful video project creation"""
 
         # Mock authentication
@@ -119,7 +128,11 @@ class TestVideoGeneration:
         # Mock headers
         headers = {"Authorization": "Bearer valid_token"}
 
-        response = client.post("/api/v1/video/generate", json=sample_video_request, headers=headers)
+        response = client.post(
+            "/api/v1/video/generate",
+            json=sample_video_request,
+            headers=headers,
+        )
 
         assert response.status_code == 200
         data = response.json()
@@ -132,7 +145,9 @@ class TestVideoGeneration:
     def test_create_video_project_unauthorized(self, sample_video_request):
         """Test video project creation without authentication"""
 
-        response = client.post("/api/v1/video/generate", json=sample_video_request)
+        response = client.post(
+            "/api/v1/video/generate", json=sample_video_request
+        )
         assert response.status_code == 403  # Missing Authorization header
 
     @patch("main.verify_token")
@@ -154,9 +169,14 @@ class TestVideoGeneration:
             created_at=datetime.utcnow(),
         )
 
-        with patch("models.video_project.VideoProject.get_by_id", return_value=mock_project):
+        with patch(
+            "models.video_project.VideoProject.get_by_id",
+            return_value=mock_project,
+        ):
             headers = {"Authorization": "Bearer valid_token"}
-            response = client.get("/api/v1/video/projects/project123", headers=headers)
+            response = client.get(
+                "/api/v1/video/projects/project123", headers=headers
+            )
 
             assert response.status_code == 200
             data = response.json()
@@ -189,7 +209,9 @@ class TestAIIntegration:
         client = GeminiClient("test_api_key")
 
         with patch.object(client, "_generate_content") as mock_generate:
-            mock_generate.return_value = '{"full_script": "test script", "scenes": []}'
+            mock_generate.return_value = (
+                '{"full_script": "test script", "scenes": []}'
+            )
 
             with patch.object(
                 client,
@@ -237,10 +259,15 @@ class TestAIIntegration:
             with patch.object(
                 client,
                 "_save_image",
-                return_value=("http://test.com/image.png", "http://test.com/thumb.jpg"),
+                return_value=(
+                    "http://test.com/image.png",
+                    "http://test.com/thumb.jpg",
+                ),
             ):
                 result = await client.generate_image(
-                    prompt="A modern office scene", style="modern", aspect_ratio="16:9"
+                    prompt="A modern office scene",
+                    style="modern",
+                    aspect_ratio="16:9",
                 )
 
                 assert isinstance(result, ImageGenerationResponse)
@@ -268,11 +295,14 @@ class TestVideoComposition:
                 "/tmp/img3.png",
             ]
 
-            with patch.object(composer, "_create_preview", return_value="/tmp/preview.mp4"):
+            with patch.object(
+                composer, "_create_preview", return_value="/tmp/preview.mp4"
+            ):
                 with patch.object(
-                    composer, "_upload_media", return_value="http://test.com/preview.mp4"
+                    composer,
+                    "_upload_media",
+                    return_value="http://test.com/preview.mp4",
                 ):
-
                     result = await composer.create_video(
                         script_scenes=sample_script_response.scenes,
                         voice_url="http://test.com/voice.mp3",
@@ -298,7 +328,11 @@ class TestSocialMediaIntegration:
     async def test_social_media_manager_publish(self):
         """Test social media publishing"""
 
-        from social.platforms import SocialMediaManager, PublishRequest, PublishResult
+        from social.platforms import (
+            SocialMediaManager,
+            PublishRequest,
+            PublishResult,
+        )
 
         manager = SocialMediaManager()
 
@@ -364,7 +398,9 @@ class TestDatabase:
         )
 
         with patch.object(project, "save") as mock_save:
-            await project.update_status(mock_db, VideoStatus.GENERATING_SCRIPT, 25, None)
+            await project.update_status(
+                mock_db, VideoStatus.GENERATING_SCRIPT, 25, None
+            )
 
             assert project.status == VideoStatus.GENERATING_SCRIPT
             assert project.progress == 25
diff --git a/services/video-service/video/composer.py b/services/video-service/video/composer.py
index 0b5eeb4..e7b78bf 100644
--- a/services/video-service/video/composer.py
+++ b/services/video-service/video/composer.py
@@ -118,38 +118,55 @@ class VideoComposer:
             logger.info(f"Starting video composition: {composition_id}")
 
             # Download all media assets
-            voice_path = await self._download_media(voice_url, f"{composition_id}_voice.mp3")
+            voice_path = await self._download_media(
+                voice_url, f"{composition_id}_voice.mp3"
+            )
             music_path = (
-                await self._download_media(music_url, f"{composition_id}_music.mp3")
+                await self._download_media(
+                    music_url, f"{composition_id}_music.mp3"
+                )
                 if music_url
                 else None
             )
 
             image_paths = []
             for i, image_url in enumerate(image_urls):
-                image_path = await self._download_media(image_url, f"{composition_id}_img_{i}.png")
+                image_path = await self._download_media(
+                    image_url, f"{composition_id}_img_{i}.png"
+                )
                 image_paths.append(image_path)
 
             # Create scene compositions
             scenes = []
             current_time = 0.0
 
-            for i, (script_scene, image_path) in enumerate(zip(script_scenes, image_paths)):
+            for i, (script_scene, image_path) in enumerate(
+                zip(script_scenes, image_paths)
+            ):
                 scene = SceneComposition(
                     sequence=i,
                     start_time=current_time,
                     duration=script_scene.duration,
                     image_url=image_path,
                     narration_text=script_scene.narration_text,
-                    visual_effects=self._get_scene_effects(script_scene.scene_type),
-                    transition_type=self._get_transition_type(i, len(script_scenes)),
+                    visual_effects=self._get_scene_effects(
+                        script_scene.scene_type
+                    ),
+                    transition_type=self._get_transition_type(
+                        i, len(script_scenes)
+                    ),
                 )
                 scenes.append(scene)
                 current_time += script_scene.duration
 
             # Generate preview video
             preview_path = await self._create_preview(
-                composition_id, scenes, voice_path, music_path, target_platform, include_captions
+                composition_id,
+                scenes,
+                voice_path,
+                music_path,
+                target_platform,
+                include_captions,
             )
 
             # Upload preview and get URL
@@ -169,14 +186,18 @@ class VideoComposer:
             logger.error(f"Video composition failed: {str(e)}")
             raise Exception(f"Failed to create video composition: {str(e)}")
 
-    async def render_final(self, composition_id: str, quality: str = "high") -> FinalRenderResult:
+    async def render_final(
+        self, composition_id: str, quality: str = "high"
+    ) -> FinalRenderResult:
         """Render final high-quality video"""
 
         try:
             logger.info(f"Starting final render: {composition_id}")
 
             # Load composition data (in real implementation, this would be from database)
-            composition_data = await self._load_composition_data(composition_id)
+            composition_data = await self._load_composition_data(
+                composition_id
+            )
 
             if not composition_data:
                 raise Exception(f"Composition {composition_id} not found")
@@ -187,14 +208,18 @@ class VideoComposer:
             )
 
             # Generate thumbnail
-            thumbnail_path = await self._generate_thumbnail(final_video_path, composition_id)
+            thumbnail_path = await self._generate_thumbnail(
+                final_video_path, composition_id
+            )
 
             # Get video metadata
             metadata = await self._get_video_metadata(final_video_path)
 
             # Upload final video and thumbnail
             video_url = await self._upload_media(final_video_path, "videos")
-            thumbnail_url = await self._upload_media(thumbnail_path, "thumbnails")
+            thumbnail_url = await self._upload_media(
+                thumbnail_path, "thumbnails"
+            )
 
             logger.info(f"Final render completed: {composition_id}")
 
@@ -221,7 +246,9 @@ class VideoComposer:
             async with aiohttp.ClientSession() as session:
                 async with session.get(url) as response:
                     if response.status != 200:
-                        raise Exception(f"Failed to download media: {response.status}")
+                        raise Exception(
+                            f"Failed to download media: {response.status}"
+                        )
 
                     file_path = os.path.join(self.temp_dir, filename)
                     with open(file_path, "wb") as f:
@@ -245,11 +272,18 @@ class VideoComposer:
         """Create preview video with lower quality for quick review"""
 
         settings = self.platform_settings[target_platform]
-        preview_path = os.path.join(self.temp_dir, f"{composition_id}_preview.mp4")
+        preview_path = os.path.join(
+            self.temp_dir, f"{composition_id}_preview.mp4"
+        )
 
         # Create FFmpeg filter complex for preview
         filter_complex = await self._build_filter_complex(
-            scenes, voice_path, music_path, settings, include_captions, preview=True
+            scenes,
+            voice_path,
+            music_path,
+            settings,
+            include_captions,
+            preview=True,
         )
 
         # Build FFmpeg command for preview
@@ -288,7 +322,9 @@ class VideoComposer:
 
         # Run FFmpeg
         process = await asyncio.create_subprocess_exec(
-            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
+            *cmd,
+            stdout=asyncio.subprocess.PIPE,
+            stderr=asyncio.subprocess.PIPE,
         )
 
         stdout, stderr = await process.communicate()
@@ -300,11 +336,16 @@ class VideoComposer:
         return preview_path
 
     async def _render_final_video(
-        self, composition_id: str, composition_data: Dict[str, Any], quality: str
+        self,
+        composition_id: str,
+        composition_data: Dict[str, Any],
+        quality: str,
     ) -> str:
         """Render final high-quality video"""
 
-        final_path = os.path.join(self.output_dir, f"{composition_id}_final.mp4")
+        final_path = os.path.join(
+            self.output_dir, f"{composition_id}_final.mp4"
+        )
 
         # Quality settings
         quality_settings = {
@@ -315,7 +356,9 @@ class VideoComposer:
         }
 
         settings = quality_settings.get(quality, quality_settings["high"])
-        platform_settings = self.platform_settings[composition_data["target_platform"]]
+        platform_settings = self.platform_settings[
+            composition_data["target_platform"]
+        ]
 
         # Build comprehensive FFmpeg command for final render
         cmd = ["ffmpeg", "-y", "-i", composition_data["voice_path"]]
@@ -361,7 +404,9 @@ class VideoComposer:
 
         # Run final render
         process = await asyncio.create_subprocess_exec(
-            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
+            *cmd,
+            stdout=asyncio.subprocess.PIPE,
+            stderr=asyncio.subprocess.PIPE,
         )
 
         stdout, stderr = await process.communicate()
@@ -388,7 +433,9 @@ class VideoComposer:
 
         # Scale and process images
         for i, scene in enumerate(scenes):
-            input_idx = i + (2 if music_path else 1)  # Account for audio inputs
+            input_idx = i + (
+                2 if music_path else 1
+            )  # Account for audio inputs
             filter_parts.append(
                 f"[{input_idx}:v]scale={settings['resolution']}:force_original_aspect_ratio=increase,"
                 f"crop={settings['resolution']},setsar=1[img{i}]"
@@ -396,11 +443,15 @@ class VideoComposer:
 
         # Concatenate video segments
         concat_inputs = "".join(f"[img{i}]" for i in range(len(scenes)))
-        filter_parts.append(f"{concat_inputs}concat=n={len(scenes)}:v=1:a=0[video]")
+        filter_parts.append(
+            f"{concat_inputs}concat=n={len(scenes)}:v=1:a=0[video]"
+        )
 
         # Audio mixing
         if music_path:
-            filter_parts.append("[0:a][1:a]amix=inputs=2:duration=first[audio]")
+            filter_parts.append(
+                "[0:a][1:a]amix=inputs=2:duration=first[audio]"
+            )
             audio_output = "[audio]"
         else:
             audio_output = "[0:a]"
@@ -429,10 +480,14 @@ class VideoComposer:
         else:
             return "crossfade"
 
-    async def _generate_thumbnail(self, video_path: str, composition_id: str) -> str:
+    async def _generate_thumbnail(
+        self, video_path: str, composition_id: str
+    ) -> str:
         """Generate thumbnail from video"""
 
-        thumbnail_path = os.path.join(self.temp_dir, f"{composition_id}_thumb.jpg")
+        thumbnail_path = os.path.join(
+            self.temp_dir, f"{composition_id}_thumb.jpg"
+        )
 
         cmd = [
             "ffmpeg",
@@ -449,7 +504,9 @@ class VideoComposer:
         ]
 
         process = await asyncio.create_subprocess_exec(
-            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
+            *cmd,
+            stdout=asyncio.subprocess.PIPE,
+            stderr=asyncio.subprocess.PIPE,
         )
 
         await process.communicate()
@@ -475,20 +532,32 @@ class VideoComposer:
         ]
 
         process = await asyncio.create_subprocess_exec(
-            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
+            *cmd,
+            stdout=asyncio.subprocess.PIPE,
+            stderr=asyncio.subprocess.PIPE,
         )
 
         stdout, _ = await process.communicate()
 
         if process.returncode != 0:
-            return {"duration": 0, "file_size": 0, "resolution": "unknown", "format": "mp4"}
+            return {
+                "duration": 0,
+                "file_size": 0,
+                "resolution": "unknown",
+                "format": "mp4",
+            }
 
         metadata = json.loads(stdout.decode())
 
         # Extract relevant information
         format_info = metadata.get("format", {})
         video_stream = next(
-            (s for s in metadata.get("streams", []) if s["codec_type"] == "video"), {}
+            (
+                s
+                for s in metadata.get("streams", [])
+                if s["codec_type"] == "video"
+            ),
+            {},
         )
 
         return {
@@ -515,7 +584,9 @@ class VideoComposer:
         base_url = os.getenv("MEDIA_BASE_URL", "http://localhost:8003")
         return f"{base_url}/media/{media_type}/{filename}"
 
-    async def _load_composition_data(self, composition_id: str) -> Optional[Dict[str, Any]]:
+    async def _load_composition_data(
+        self, composition_id: str
+    ) -> Optional[Dict[str, Any]]:
         """Load composition data (placeholder - would be from database)"""
 
         # This is a placeholder - in real implementation,
