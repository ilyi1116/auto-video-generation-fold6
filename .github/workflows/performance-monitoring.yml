# æ€§èƒ½ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•å·¥ä½œæµç¨‹
name: âš¡ Performance Monitoring & Benchmarks

on:
  schedule:
    # æ¯æ—¥å‡Œæ™¨ 2:00 UTC è¿è¡Œæ€§èƒ½æµ‹è¯•
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test'
        required: true
        default: 'full'
        type: choice
        options:
          - full
          - api-only
          - frontend-only
          - load-test
      duration:
        description: 'Test duration in minutes'
        required: false
        default: '10'
        type: string
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize]
    paths:
      - 'src/services/**'
      - 'src/frontend/**'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

permissions:
  contents: read
  checks: write
  pull-requests: write

jobs:
  # ===========================================
  # API æ€§èƒ½åŸºå‡†æµ‹è¯•
  # ===========================================
  api-performance-test:
    name: ğŸš€ API Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || inputs.test_type == 'full' || inputs.test_type == 'api-only'
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: test_auto_video_generation
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4

    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install locust pytest-benchmark

    - name: ğŸ—„ï¸ Setup Test Database
      env:
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_auto_video_generation
        REDIS_URL: redis://localhost:6379/0
      run: |
        python scripts/db-migration-manager.py create-db --env=test
        alembic upgrade head

    - name: ğŸš€ Start API Services
      env:
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_auto_video_generation
        REDIS_URL: redis://localhost:6379/0
        TESTING: true
      run: |
        # å¯åŠ¨æ ¸å¿ƒ API æœåŠ¡è¿›è¡Œæµ‹è¯•
        cd src/services/api-gateway && python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        cd src/services/auth-service && python -m uvicorn app.main:app --host 0.0.0.0 --port 8001 &
        
        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        sleep 15
        
        # éªŒè¯æœåŠ¡è¿è¡ŒçŠ¶æ€
        curl -f http://localhost:8000/health || exit 1
        curl -f http://localhost:8001/health || exit 1

    - name: âš¡ Run API Performance Tests
      run: |
        # åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬
        cat > performance_test.py << 'EOF'
        import time
        import requests
        import statistics
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import json

        def test_endpoint(url, method="GET", data=None, headers=None):
            """æµ‹è¯•å•ä¸ªç«¯ç‚¹çš„æ€§èƒ½"""
            try:
                start_time = time.time()
                response = requests.request(method, url, json=data, headers=headers, timeout=10)
                end_time = time.time()
                
                return {
                    'url': url,
                    'status_code': response.status_code,
                    'response_time': end_time - start_time,
                    'success': response.status_code < 400
                }
            except Exception as e:
                return {
                    'url': url,
                    'status_code': 0,
                    'response_time': 10.0,
                    'success': False,
                    'error': str(e)
                }

        def run_load_test(endpoints, concurrent_users=10, total_requests=100):
            """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
            results = []
            
            with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
                futures = []
                
                for i in range(total_requests):
                    endpoint = endpoints[i % len(endpoints)]
                    future = executor.submit(test_endpoint, endpoint['url'], endpoint.get('method', 'GET'))
                    futures.append(future)
                
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)
            
            return results

        # å®šä¹‰æµ‹è¯•ç«¯ç‚¹
        endpoints = [
            {'url': 'http://localhost:8000/health', 'method': 'GET'},
            {'url': 'http://localhost:8001/health', 'method': 'GET'},
        ]

        print("å¼€å§‹ API æ€§èƒ½æµ‹è¯•...")
        results = run_load_test(endpoints, concurrent_users=5, total_requests=50)

        # åˆ†æç»“æœ
        successful_requests = [r for r in results if r['success']]
        failed_requests = [r for r in results if not r['success']]

        if successful_requests:
            response_times = [r['response_time'] for r in successful_requests]
            
            performance_metrics = {
                'total_requests': len(results),
                'successful_requests': len(successful_requests),
                'failed_requests': len(failed_requests),
                'success_rate': len(successful_requests) / len(results) * 100,
                'avg_response_time': statistics.mean(response_times),
                'median_response_time': statistics.median(response_times),
                'p95_response_time': sorted(response_times)[int(len(response_times) * 0.95)] if len(response_times) > 0 else 0,
                'p99_response_time': sorted(response_times)[int(len(response_times) * 0.99)] if len(response_times) > 0 else 0,
                'min_response_time': min(response_times),
                'max_response_time': max(response_times)
            }
            
            print("=== æ€§èƒ½æµ‹è¯•ç»“æœ ===")
            for key, value in performance_metrics.items():
                if 'time' in key:
                    print(f"{key}: {value:.3f}s")
                elif 'rate' in key:
                    print(f"{key}: {value:.2f}%")
                else:
                    print(f"{key}: {value}")
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            with open('api_performance_results.json', 'w') as f:
                json.dump({
                    'metrics': performance_metrics,
                    'detailed_results': results
                }, f, indent=2)
            
            # è®¾ç½®æ€§èƒ½åŸºå‡†
            if performance_metrics['avg_response_time'] > 2.0:
                print("âŒ è­¦å‘Šï¼šå¹³å‡å“åº”æ—¶é—´è¶…è¿‡ 2 ç§’")
                exit(1)
            
            if performance_metrics['success_rate'] < 95:
                print("âŒ è­¦å‘Šï¼šæˆåŠŸç‡ä½äº 95%")
                exit(1)
            
            print("âœ… æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡")
        else:
            print("âŒ æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥äº†")
            exit(1)
        EOF
        
        python performance_test.py

    - name: ğŸ“Š Upload Performance Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: api-performance-results
        path: api_performance_results.json

  # ===========================================
  # å‰ç«¯æ€§èƒ½æµ‹è¯•
  # ===========================================
  frontend-performance-test:
    name: ğŸŒ Frontend Performance Test
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || inputs.test_type == 'full' || inputs.test_type == 'frontend-only'
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4

    - name: ğŸŸ¢ Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: 'src/frontend/package-lock.json'

    - name: ğŸ“¦ Install Dependencies
      working-directory: src/frontend
      run: |
        npm ci
        npm install --save-dev lighthouse @lhci/cli

    - name: ğŸ—ï¸ Build Frontend
      working-directory: src/frontend
      run: npm run build

    - name: ğŸš€ Start Frontend Server
      working-directory: src/frontend
      run: |
        npm run preview &
        sleep 10

    - name: ğŸ” Run Lighthouse Performance Audit
      working-directory: src/frontend
      run: |
        npx lighthouse http://localhost:4173 \
          --output=json \
          --output-path=lighthouse-results.json \
          --chrome-flags="--headless --no-sandbox --disable-dev-shm-usage" \
          --only-categories=performance

    - name: ğŸ“Š Analyze Lighthouse Results
      working-directory: src/frontend
      run: |
        node -e "
        const fs = require('fs');
        const results = JSON.parse(fs.readFileSync('lighthouse-results.json', 'utf8'));
        const performance = results.categories.performance.score * 100;
        
        console.log('=== Lighthouse æ€§èƒ½æµ‹è¯•ç»“æœ ===');
        console.log(\`æ€§èƒ½åˆ†æ•°: \${performance}/100\`);
        
        const metrics = results.audits;
        if (metrics['first-contentful-paint']) {
          console.log(\`é¦–æ¬¡å†…å®¹ç»˜åˆ¶: \${metrics['first-contentful-paint'].displayValue}\`);
        }
        if (metrics['largest-contentful-paint']) {
          console.log(\`æœ€å¤§å†…å®¹ç»˜åˆ¶: \${metrics['largest-contentful-paint'].displayValue}\`);
        }
        if (metrics['cumulative-layout-shift']) {
          console.log(\`ç´¯è®¡å¸ƒå±€åç§»: \${metrics['cumulative-layout-shift'].displayValue}\`);
        }
        if (metrics['total-blocking-time']) {
          console.log(\`æ€»é˜»å¡æ—¶é—´: \${metrics['total-blocking-time'].displayValue}\`);
        }
        
        // æ€§èƒ½åŸºå‡†æ£€æŸ¥
        if (performance < 80) {
          console.log('âŒ è­¦å‘Šï¼šLighthouse æ€§èƒ½åˆ†æ•°ä½äº 80');
          process.exit(1);
        } else {
          console.log('âœ… Lighthouse æ€§èƒ½æµ‹è¯•é€šè¿‡');
        }
        "

    - name: ğŸ“Š Upload Frontend Performance Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: frontend-performance-results
        path: src/frontend/lighthouse-results.json

  # ===========================================
  # è´Ÿè½½æµ‹è¯• (ä½¿ç”¨ Locust)
  # ===========================================
  load-test:
    name: ğŸ‹ï¸ Load Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || inputs.test_type == 'load-test'
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4

    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: ğŸ“¦ Install Locust
      run: |
        pip install locust requests

    - name: ğŸ³ Start Services with Docker Compose
      run: |
        cp .env.template .env
        docker-compose -f docker-compose.unified.yml up -d postgres redis
        
        # ç­‰å¾…æœåŠ¡å‡†å¤‡
        sleep 30

    - name: ğŸ‹ï¸ Create Locust Test File
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import random

        class APIUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                """ç”¨æˆ·å¼€å§‹æ—¶çš„åˆå§‹åŒ–"""
                pass
            
            @task(3)
            def health_check(self):
                """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
                self.client.get("/health")
            
            @task(1)
            def api_info(self):
                """API ä¿¡æ¯ç«¯ç‚¹"""
                self.client.get("/", catch_response=True)

        class WebsiteUser(HttpUser):
            wait_time = between(2, 5)
            host = "http://localhost:3000"
            
            @task
            def index_page(self):
                """è®¿é—®é¦–é¡µ"""
                self.client.get("/")
        EOF

    - name: ğŸš€ Run Load Test
      run: |
        # è®¾ç½®æµ‹è¯•æŒç»­æ—¶é—´
        DURATION="${{ inputs.duration || '5' }}"
        
        echo "å¼€å§‹è´Ÿè½½æµ‹è¯•ï¼ŒæŒç»­æ—¶é—´ï¼š${DURATION} åˆ†é’Ÿ"
        
        # è¿è¡Œ Locust è´Ÿè½½æµ‹è¯•
        locust -f locustfile.py \
          --host=http://localhost:8000 \
          --users=50 \
          --spawn-rate=5 \
          --run-time=${DURATION}m \
          --html=load-test-report.html \
          --csv=load-test-results \
          --headless

    - name: ğŸ“Š Upload Load Test Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: load-test-results
        path: |
          load-test-report.html
          load-test-results*.csv

  # ===========================================
  # æ€§èƒ½æŠ¥å‘Šæ±‡æ€»
  # ===========================================
  performance-summary:
    name: ğŸ“Š Performance Summary
    runs-on: ubuntu-latest
    needs: [api-performance-test, frontend-performance-test, load-test]
    if: always()
    
    steps:
    - name: ğŸ“¥ Download All Results
      uses: actions/download-artifact@v3
      with:
        path: performance-results

    - name: ğŸ“‹ Generate Performance Report
      run: |
        echo "# âš¡ æ€§èƒ½æµ‹è¯•æŠ¥å‘Š" > performance-summary.md
        echo "**æµ‹è¯•æ—¶é—´:** $(date)" >> performance-summary.md
        echo "" >> performance-summary.md
        
        echo "## API æ€§èƒ½æµ‹è¯•" >> performance-summary.md
        if [ -f "performance-results/api-performance-results/api_performance_results.json" ]; then
          echo "- âœ… API æ€§èƒ½æµ‹è¯•å®Œæˆ" >> performance-summary.md
          
          # æå–å…³é”®æŒ‡æ ‡
          python3 -c "
          import json
          try:
              with open('performance-results/api-performance-results/api_performance_results.json', 'r') as f:
                  data = json.load(f)
              metrics = data['metrics']
              print(f'- å¹³å‡å“åº”æ—¶é—´: {metrics[\"avg_response_time\"]:.3f}s')
              print(f'- æˆåŠŸç‡: {metrics[\"success_rate\"]:.2f}%')
              print(f'- 95th ç™¾åˆ†ä½å“åº”æ—¶é—´: {metrics[\"p95_response_time\"]:.3f}s')
          except:
              print('- æ— æ³•è§£æ API æµ‹è¯•ç»“æœ')
          " >> performance-summary.md
        else
          echo "- âŒ API æ€§èƒ½æµ‹è¯•æœªè¿è¡Œæˆ–å¤±è´¥" >> performance-summary.md
        fi
        
        echo "" >> performance-summary.md
        echo "## å‰ç«¯æ€§èƒ½æµ‹è¯•" >> performance-summary.md
        if [ -f "performance-results/frontend-performance-results/lighthouse-results.json" ]; then
          echo "- âœ… Lighthouse æ€§èƒ½æµ‹è¯•å®Œæˆ" >> performance-summary.md
        else
          echo "- âŒ å‰ç«¯æ€§èƒ½æµ‹è¯•æœªè¿è¡Œæˆ–å¤±è´¥" >> performance-summary.md
        fi
        
        echo "" >> performance-summary.md
        echo "## è´Ÿè½½æµ‹è¯•" >> performance-summary.md
        if [ -f "performance-results/load-test-results/load-test-report.html" ]; then
          echo "- âœ… è´Ÿè½½æµ‹è¯•å®Œæˆ" >> performance-summary.md
        else
          echo "- âŒ è´Ÿè½½æµ‹è¯•æœªè¿è¡Œæˆ–å¤±è´¥" >> performance-summary.md
        fi
        
        echo "" >> performance-summary.md
        echo "## å»ºè®®" >> performance-summary.md
        echo "1. å®šæœŸç›‘æ§æ€§èƒ½æŒ‡æ ‡å˜åŒ–" >> performance-summary.md
        echo "2. å…³æ³¨å“åº”æ—¶é—´å’ŒæˆåŠŸç‡è¶‹åŠ¿" >> performance-summary.md
        echo "3. ä¼˜åŒ–é«˜å»¶è¿Ÿçš„ API ç«¯ç‚¹" >> performance-summary.md
        echo "4. æŒç»­ä¼˜åŒ–å‰ç«¯èµ„æºåŠ è½½" >> performance-summary.md

    - name: ğŸ“Š Upload Summary Report
      uses: actions/upload-artifact@v3
      with:
        name: performance-summary-report
        path: performance-summary.md

    - name: ğŸ’¬ Comment Performance Results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          if (fs.existsSync('performance-summary.md')) {
            const summary = fs.readFileSync('performance-summary.md', 'utf8');
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: \`## âš¡ æ€§èƒ½æµ‹è¯•ç»“æœ\\n\\n\${summary}\\n\\nè¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹ [Actions é¡µé¢](\${context.payload.repository.html_url}/actions/runs/\${context.runId})\`
            });
          }